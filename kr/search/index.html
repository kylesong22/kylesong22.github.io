<!DOCTYPE html>
<html lang="en" class="no-js">
  <!-- Copyright 2019-2021 Vanessa Sochat-->
  <head>
    <meta charset="utf-8">
    <!-- Copyright 2019-2021 Vanessa Sochat-->
    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="generator" content="Hugo 0.55.6" />
    
    <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    
    <link rel="alternate" type="application/rss&#43;xml" href="/docs/index.xml">
    
    <link rel="shortcut icon" href="/kr/assets/favicons/favicon.ico" >
    <!-- <link rel="apple-touch-icon" href="/kr/assets/favicons/apple-touch-icon-180x180.png" sizes="180x180">
    <link rel="icon" type="image/png" href="/kr/assets/favicons/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/kr/assets/favicons/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/kr/assets/favicons/android-icon-36x36.png" sizes="36x36">
    <link rel="icon" type="image/png" href="/kr/assets/favicons/android-icon-48x48.png" sizes="48x48">
    <link rel="icon" type="image/png" href="/kr/assets/favicons/android-icon-72x72.png" sizes="72x72">
    <link rel="icon" type="image/png" href="/kr/assets/favicons/android-icon-96x196.png" sizes="96x196">
    <link rel="icon" type="image/png" href="/kr/assets/favicons/android-icon-144x144.png" sizes="144x144">
    <link rel="icon" type="image/png" href="/kr/assets/favicons/android-icon-192x192.png"sizes="192x192"> -->
    
    <title>Search</title>
    <meta property="og:title" content="Search" />
    <meta property="og:description" content="machbase-manual documents.
" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="" />
    <meta property="og:site_name" content="" />
    
    <meta itemprop="name" content="Search">
    <meta itemprop="description" content="machbase-manual documents.
">
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Search"/>
    <meta name="twitter:description" content="machbase-manual documents.
"/>
    
    <link rel="stylesheet" href="/kr/assets/css/main.css">
    <link rel="stylesheet" href="/kr/assets/css/palette.css">
    <script
      src="/kr/assets/js/jquery-3.3.1/jquery-3.3.1.min.js"
      integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8= sha256-T+aPohYXbm0fRYDpJLr+zJ9RmYTswGsahAoIsNiMld4="
      crossorigin="anonymous"></script>
    </head>
  

  
  <body class="td-section">
    <header>
    <nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
    <a class="navbar-brand" href="/kr/">
        <span class="navbar-logo"></span><span class="text-uppercase font-weight-bold"><img src="/kr/assets/img/logo.png" alt="Machbase manual"></span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
    <ul class="navbar-nav mt-2 mt-lg-0">
        <li class="nav-item mr-4 mb-2 mb-lg-0">
                        <!-- <a class="nav-link" href="https://github.com/machbase/dbms-manual" target="_blank"><span>GitHub</span></a> -->
        </li>
        <li class="nav-item mr-4 mb-2 mb-lg-0">
                        <!-- <a class="nav-link" href="/kr/about" ><span>About</span></a> -->
        </li>
        <li class="nav-item mr-4 mb-2 mb-lg-0">
                        <!-- <a class="nav-link" href="/kr/docs" ><span>Documentation</span></a> -->
        </li>
  <!--  -->
  <!-- <li class="nav-item mr-4 mb-2 mb-lg-0"> -->
    <!-- 
      <span class="nav-link ">Release:</span>
    
    <li class="nav-item dropdown d-none d-lg-block">
      <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Current</a>
      <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
        
        
          
            <a class="dropdown-item" href="/kr/search/">Current</a>
          
        
        
          
            
            <a class="dropdown-item" href="/kr/search/">Previous</a>
          
         -->
      </div>
    </li>
  <!-- </li> -->
  
    </ul>
</div>
<div class="navbar-nav d-none d-lg-block">
<input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="on">
    </div>

<!-- <div class="navbar-nav d-none d-lg-block">
      <a class="gh-source" data-gh-source="github" href="https://github.com/machbase/dbms-manual" title="Go to repository" data-md-state="done">
      <div class="gh-source__repository">
        <i class="fab fa fa-github fa-2x" style='padding-right:20px; float:left; margin-top:5px'></i>
        machbase/dbms-manual
      <ul class="gh-source__facts"><li class="gh-source__fact" id='stars'></li><li id="forks" class="gh-source__fact"></li></ul></div></a>
    </div> -->
  </div>


</nav>
</header>

<script>
$(document).ready(function() {
var url = "https://api.github.com/search/repositories?q=machbase/dbms-manual";
fetch(url, { 
  headers: {"Accept":"application/vnd.github.preview"}
}).then(function(e) {
return e.json()
}).then(function(r) {
 console.log(r.items[0])
 stars = r.items[0]['stargazers_count']
 forks = r.items[0]['forks_count']
 $('#stars').text(stars + " Stars")
 $('#forks').text(forks + " Forks")
});
});
</script>

    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          <div id="td-sidebar-menu" class="td-sidebar__inner">
  <form class="td-sidebar__search d-flex align-items-center">
    <input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation"></button>
  </form>  
  
  <nav class="collapse td-sidebar-nav pt-2 pl-4" id="td-section-nav">
    
      <ul class="td-sidebar-nav__section pr-md-3">
        
        
      
        <li class="td-sidebar-nav__section-title">
          <a href="" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">마크베이스 매뉴얼</a>
        </li>
        
        
          <ul>
            <li class="collapse show" id="마크베이스-매뉴얼">
              <ul class="td-sidebar-nav__section pr-md-3">
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">개요</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="개요">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-개요-마크베이스-소개" href="/kr//intro/introduction">마크베이스 소개</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-개요-마크베이스-특징" href="/kr//intro/features">마크베이스 특징</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-개요-마크베이스-제품군-소개" href="/kr//intro/products">마크베이스 제품군 소개</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">설치</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="설치">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설치-패키지-개요" href="/kr//install/package">패키지 개요</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설치-리눅스-설치" href="/kr//install/linux">리눅스 설치</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설치-windows-설치" href="/kr//install/windows">Windows 설치</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설치-라이선스-설치" href="/kr//install/license">라이선스 설치</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설치-cluster-edition-설치" href="/kr//install/cluster">Cluster Edition 설치</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설치-업그레이드-복구" href="">업그레이드 / 복구</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">주요기능 / 테이블</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="주요기능-테이블">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-주요기능-테이블-태그-테이블-tag-table" href="/kr//feature-table/tag">태그 테이블 (Tag Table)</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-주요기능-테이블-로그-테이블-log-table" href="/kr//feature-table/log">로그 테이블 (Log Table)</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-주요기능-테이블-휘발성-테이블-volatile-table" href="/kr//feature-table/volatile">휘발성 테이블 (Volatile Table)</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-주요기능-테이블-참조-테이블-lookup-table" href="/kr//feature-table/lookup">참조 테이블 (Lookup Table)</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-주요기능-테이블-스트림-stream" href="/kr//feature-table/stream">스트림 (Stream)</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-주요기능-테이블-백업-및-마운트" href="/kr//feature-table/backup-mount">백업 및 마운트</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">도구</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="도구">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-도구-유틸리티-모음" href="/kr//tools/utilities">유틸리티 모음</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-도구-mwa-machbase-web-analytics" href="/kr//tools/mwa">MWA (Machbase Web Analytics)</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-도구-태그-분석기-tag-analyzer" href="/kr//tools/tag-analyzer">태그 분석기(Tag Analyzer)</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-도구-machcoordinatoradmin" href="/kr//tools/machcoordinatoradmin">machcoordinatoradmin</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-도구-machdeployeradmin" href="/kr//tools/machdeployeradmin">machdeployeradmin</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">설정 / 모니터링</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="설정-모니터링">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설정-모니터링-meta-table" href="/kr//setting-monitoring/meta-table">Meta Table</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설정-모니터링-virtual-table" href="/kr//setting-monitoring/virtual-table">Virtual Table</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설정-모니터링-property" href="/kr//setting-monitoring/property">Property</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-설정-모니터링-property-cluster" href="/kr//setting-monitoring/property-cl">Property (Cluster)</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">SQL Reference</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="sql-reference">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sql-reference-자료형" href="/kr//sql-ref/data-types">자료형</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sql-reference-ddl" href="/kr//sql-ref/ddl">DDL</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sql-reference-dml" href="/kr//sql-ref/dml">DML</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sql-reference-select" href="/kr//sql-ref/select">SELECT</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sql-reference-select-hint" href="/kr//sql-ref/select-hint">SELECT Hint</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sql-reference-사용자-관리" href="/kr//sql-ref/user">사용자 관리</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sql-reference-지원-함수" href="/kr//sql-ref/func">지원 함수</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sql-reference-시스템-세션-관리" href="/kr//sql-ref/sys-session">시스템/세션 관리</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">SDK</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="sdk">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sdk-cli-odbc" href="/kr//sdk/cli_odbc">CLI/ODBC</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sdk-cli-odbc-예제" href="/kr//sdk/cli_odbc_example">CLI/ODBC 예제</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sdk-jdbc" href="/kr//sdk/jdbc">JDBC</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sdk-python" href="/kr//sdk/python">Python</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sdk-restful-api" href="/kr//sdk/restful_api">RESTful API</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sdk-net-connector" href="/kr//sdk/dotnet">.NET Connector</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-sdk-timezone" href="/kr//sdk/timezone">Timezone</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">Release Note</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="release-note">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-release-note-7-0-release-note" href="/kr//release-note/7_0">7.0 Release Note</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-release-note-7-5-release-note" href="/kr//release-note/7_5">7.5 Release Note</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">자주 묻는 질문(FAQ)</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="자주-묻는-질문-faq">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-마크베이스-매뉴얼-자주-묻는-질문-faq-쿼리-에러를-property-수정하여-해결하는-방법" href="/kr//faq/faq">쿼리 에러를 Property 수정하여 해결하는 방법</a>
                        
                      </li>
                    </ul>
                  
                  
                
              </ul>
            </li>
          </ul>
        
        
      </ul>
    
  </nav>
</div>

          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
              <div class="td-page-meta ml-2 pb-1 pt-2 mb-0">
              </div>
              <nav id="TableOfContents"><ul>
              <li><ul id="TOC">
                <!-- Links will be appended here-->
              </ul></li>
              </ul></nav>
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            <nav aria-label="breadcrumb" class="d-none d-md-block d-print-none">
	      <ol class="breadcrumb spb-1">
              
              
                
                  
                    <li class="breadcrumb-item active" aria-current="page">
                      <a href="/kr/search/">Search</a>
                    </li>
                  
                
              
	      </ol>
           </nav>
           <div class="td-content">        
            
              
            
            
	      <input class="form-control td-search-input" type="search" name="q" id="search-input" placeholder="&#xf002 Search this site…"  style="margin-top:5px" autofocus>
<i style="color:white; margin-right:8px; margin-left:5px" class="fa fa-search"></i>

<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>

<ul id="search-results"></ul>


<script>
	window.data = {
		
		
		
				
					
					
					"404-html": {
						"id": "404-html",
						"title": "Page Not Found",
						"version": "all",
						"categories": "",
						"url": " /404.html",
						"content": "Page Not Found\n\nUnfortunately we were unable to find the page you requested. It could be the page doesn’t exist or only exists for a specific version of these documents so please use the search feature to see if you are able to locate the information you were after."
					}
					
				
		
				
					,
					
					"release-note-7-0-html": {
						"id": "release-note-7-0-html",
						"title": "7.0 Release Note",
						"version": "all",
						"categories": "",
						"url": " /release-note/7_0.html",
						"content": "7.0 변경 및 추가 기능"
					}
					
				
		
				
					,
					
					"release-note-7-0-update-html": {
						"id": "release-note-7-0-update-html",
						"title": "7.0 변경 및 추가 기능",
						"version": "all",
						"categories": "",
						"url": " /release-note/7_0_update.html",
						"content": "목차\n\n\n  Tag 테이블 Rollup의 시간단위 설정 기능 지원\n  다중 Tag 테이블 지원\n  TAG 데이터 저장 구조 변경\n  Tag Table Index Memory 사용량 절감\n  TAG ID기반 통계값 생성 / 조회\n  JSON 타입 지원\n\n\nTag 테이블 Rollup의 시간단위 설정 기능 지원\n기존 버전에서는 Tag 테이블에 대한 기본 Rollup 작업이 1초, 1분, 1시간 단위로 수행되었다.\n\n하지만 Rollup 시간 단위를 변경할 수 없어서 데이터가 초 이상의 간격으로 들어올 경우 불필요한 자원을 사용하는 문제가 있었다.\n\nMachbase 7.0 에서는 Rollup의 시간단위 설정 기능을 통해 사용자가 원하는 Rollup 만 생성할 수 있고 실행 주기도 원하는대로 설정할 수 있도록 하였다.\n\nRollup 생성 문법을 다음과 같다.\n\nsyntax\n-- 생성\nCREATE ROLLUP rollup_name FROM tag_table_name INTERVAL timesepc;\n    \ntimespec : integer time_unit\ntime_unit : SEC, MIN, HOUR \n  \n-- 시작\nEXEC ROLLUP_START(rollup_name);\n  \n-- 중지\nEXEC ROLLUP_STOP(rollup_name);\n  \n-- 삭제\nDROP ROLLUP rollup_name;\n\n\nexample\nCREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\n  \nCREATE ROLLUP rollup_30_sec FROM tag INTERVAL 30 SEC;\nCREATE ROLLUP rollup_10_min FROM rollup_30_sec INTERVAL 10 MIN; \nCREATE ROLLUP rollup_1_hour FROM rollup_10_min INTERVAL 1 HOUR;\n  \nEXEC ROLLUP_START(rollup_30_sec);\nEXEC ROLLUP_START(rollup_10_min);\nEXEC ROLLUP_START(rollup_1_hour);\n  \n..\n..\n  \nEXEC ROLLUP_STOP(rollup_1_hour);\nEXEC ROLLUP_STOP(rollup_10_min);\nEXEC ROLLUP_STOP(rollup_1_sec);\n  \nDROP ROLLUP rollup_1_hour;\nDROP ROLLUP rollup_10_min;\nDROP ROLLUP rollup_30_sec;\n\n\n다중 Tag 테이블 지원\n기존 버전에서는 한개의 Tag 테이블만 생성할 수 있는 문제를 해결하였다.\n\n다중 Tag 테이블을 지원함으로써 다양한 스키마 형태로 들어오는 PLC 데이터를 효과적으로 저장할 수 있게 되었다.\n\n테이블 이름 또한 자유롭게 지정할 수 있고 SYS가 아닌 일반 유저로 생성 가능하다.\n\nexample\nCREATE TAG TABLE tag (tagid VARCHAR(50) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nCREATE TAG TABLE imotbl (tagid VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED, imo INTEGER, machine VARCHAR(10)) TAG_PARTITION_COUNT = 1, TAG_DATA_PART_SIZE = 20000000;\nCREATE TAG TABLE shi985 (tagid VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED, level INTEGER, itime DATETIME, strval VARCHAR(256)) TAG_PARTITION_COUNT = 2;\n  \n  \nMach&gt; show tables;\nUSER_NAME             DB_NAME                                             TABLE_NAME                                          TABLE_TYPE \n-----------------------------------------------------------------------------------------------------------------------------------------------\nSYS                   MACHBASEDB                                          IMOTBL                                              TAGDATA    \nSYS                   MACHBASEDB                                          SHI985                                              TAGDATA    \nSYS                   MACHBASEDB                                          TAG                                                 TAGDATA    \nSYS                   MACHBASEDB                                          _IMOTBL_DATA_0                                      KEYVALUE   \nSYS                   MACHBASEDB                                          _IMOTBL_META                                        LOOKUP     \nSYS                   MACHBASEDB                                          _SHI985_DATA_0                                      KEYVALUE   \nSYS                   MACHBASEDB                                          _SHI985_DATA_1                                      KEYVALUE   \nSYS                   MACHBASEDB                                          _SHI985_META                                        LOOKUP     \nSYS                   MACHBASEDB                                          _TAG_DATA_0                                         KEYVALUE   \nSYS                   MACHBASEDB                                          _TAG_DATA_1                                         KEYVALUE   \nSYS                   MACHBASEDB                                          _TAG_DATA_2                                         KEYVALUE   \nSYS                   MACHBASEDB                                          _TAG_DATA_3                                         KEYVALUE   \nSYS                   MACHBASEDB                                          _TAG_META                                           LOOKUP     \n[13] row(s) selected.\n\n\nTAG 데이터 저장 구조 변경\n디스크 사용량 감소를 위해 데이터 압축방식(delta compress)을 추가하고 칼럼단위로 저장하도록 데이터 구조가 변경되었다.\n\n기존 version 대비 데이터사용량 비교 테스트 결과 디스크 사용량이 최대 37% 감소하였다.\n\n\n  \n    \n      테스트환경\n      디스크사용량 감소\n      감소율\n    \n  \n  \n    \n      Tag 10,000건RowCount: 10억건Rollup(O)\n      11,105 MB -&gt; 6,907 MB\n      37.80% 감소\n    \n    \n      Tag 10,000건RowCount: 10억건Rollup(O)\n      11,007 MB -&gt; 8,987 MB\n      18.35% 감소\n    \n    \n      Tag 10,000건RowCount: 10억건Rollup(O)\n      14,836 MB -&gt; 12,752 MB\n      14.05% 감소\n    \n  \n\n\nTag Table Index Memory 사용량 절감\nTAG테이블 생성 시 Memory를 Index Build를 위한 Index Memory를 고정적으로 과도하게 할당하여 Memory를 효율적으로 사용할 수 없는 문제가 있어\n\nIndex Memory를 필요한 시점에 필요한 만큼의 Memory만 동적으로 할당하는 방식으로 변경하여 복수의 TAG태이블을 생성 및 동작이 가능하도록 하였다.\n\n7.0에서는 아래와 같은 테스트 환경에서 기존 version에 비해 최대 Memory 사용량이 약 40%가량 감소 했다.\n\n\n  테스트환경: Tag 테이블 1개(partition 4개), Tag 10,000건 / 데이터 10억건 성능 테스트, PROCESS_MAX_SIZE 4GB\n\n\n결과적으로 PROCESS_MAX_SIZE를 16GB로 설정 시 TAG 테이블(PARITITION 4개 기준)을 2개만 생성할 수 있었으나, 7.0에서는 동일설정 하에서 TAG테이블을\n\n10개 이상 생성하여 사용할 수 있게 되었다.\n\n운영환경에 따른 메모리 설정 가이드\n초당 최대 입력 건수: append application은 TAG 테이블 수만큼 동작하는 상황에서 모든 append application의 초당 입력하는 데이터 합\n(테이블당 append application 1개씩 동작)\n\n\n  \n    \n      운영환경\n       \n       \n      시스템 동작을 위한 최소 설정\n       \n       \n       \n       \n      시험장비사양\n       \n       \n    \n    \n      TAG 테이블 수\n      테이블당 Tag 수\n      초당 최대 입력 건수\n      Session 수(append 포함)\n      TAG_DATA_PART_SIZE\n      TAG_PARTITION_COUNT\n      PROCESS_MAX_SIZE\n      TAG_CACHE_MAX_MEMORY_SIZE\n      CPU\n      MEM\n      DISK\n    \n  \n  \n    \n      1\n      100,000\n      1,200,000/sec\n      4\n      16 M\n      1\n      2 GB\n      128 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      1\n      100,000\n      1,000,000/sec\n      4\n      4 M\n      1\n      1 GB\n      32 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      1\n      100,000\n      800,000/sec\n      4\n      1 M\n      1\n      512 MB\n      8 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      8\n      100,000\n      4,000,000/sec\n      11\n      16 M\n      1\n      8 GB\n      256 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      32\n      40,000\n      6,000,000/sec\n      35\n      16 M\n      1\n      16 GB\n      1024 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      32\n      40,000\n      4,000,000/sec\n      35\n      4 M\n      1\n      8 GB\n      256 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      64\n      20,000\n      4,000,000/sec\n      67\n      4 M\n      1\n      8 GB\n      512 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      64\n      20,000\n      1,000,000/sec\n      67\n      1 M\n      1\n      4 GB\n      128 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      128\n      10,000\n      4,000,000/sec\n      131\n      4 M\n      1\n      8 GB\n      1024 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n    \n      128\n      10,000\n      1,000,000/sec\n      131\n      1 M\n      1\n      4 GB\n      256 MB\n      16 cores\n      64 GB\n      SSD 1TB\n    \n  \n\n\nTAG 테이블의 수에 비례하여 TAG_CACHE_MAX_MEMORY_SIZE설정 값이 높아져야하며, TAG 테이블의 생성시 TAG_CACHE_MAX_MEMORY_SIZE가 부족한 경우 테이블 생성이 실패할 수 있다.\n\n\nTAG Parition이 4개이고, Partition Size가 16MB 인 경우\n\nTAG_CACHE_MAX_MEMORY_SIZE는 적어도 128MB 보다 크게 설정 해야 한다.\n\n128 MB = 4 * 16MB * 2\n\nTAG ID기반 통계값 생성 / 조회\nTag별 통계값 조회시 응답시간이 너무 긴 문제가 있으며 이를 개선하기 위해 Tag별 통계를 사전에 구성해 놓고,\n\n조회의 편의성을 위해 view와 같은 형태로 조회 기능을 제공한다.\n\n기존 version에서는 TAG통계조회시 data scan으로 인해 응답시간이 길었으나, 7.0에서는 필요한 통계정보를 사전에\n\n구성함으로써 응답시간이 크게 단축 되었다.\n\nTag 통계 정보\n\n\n  \n    \n      column\n      description\n    \n  \n  \n    \n      USER_ID\n      tag table user ID\n    \n    \n      TABLE_NAME\n      tag table name\n    \n    \n      TAG_ID\n      통계 정보의 Tag ID (사용자 관점에서는 Name이 출력된다)\n    \n    \n      MIN_TIME\n      현재까지 입력된 시간값의 최소치 (입력순서와 관계 없음)\n    \n    \n      MAX_TIME\n      현재까지 입력된 시간값의 최대치 (입력순서와 관계 없음)\n    \n    \n      MIN_VALUE\n      해당 tag의 Summarized 컬럼의 최소 값\n    \n    \n      MIN_VALUE_TIME\n      min_value와 같이 입력된 time\n    \n    \n      MAX_VALUE\n      해당 tag의 Summarized 컬럼의 최대 값\n    \n    \n      MAX_VALUE_TIME\n      max_value와 같이 입력된 time\n    \n    \n      ROW_COUNT\n      해당 Tag ID의 Row 개수\n    \n    \n      RECENT_ROW_TIME\n      마지막에 입력된 Row의 Basetime 컬럼값\n    \n  \n\n\n\n  query\n\n\nTag 테이블을 생성할때마다 통계정보를 저장하는 view 를 같이 생성한다. 통계정보를 조회하고자 할때 view 로부터 조회한다.\n\nview table name : “V$name_STAT” . (name 은 tag 테이블 이름)\n\n-- ex)\nSELECT min_time, max_time FROM v$tag_stat WHERE tagid = 'tag-01';\n  \n-- ex) 여러개의 tag\nSELECT min_time, max_time FROM v$tag_stat WHERE tagid IN ('tag-01', 'tag-02', 'tag-03');\n\n\nmin_time (또는 max_time) 에 입력된 value 를 알고싶다면 아래와 같이 조회가 가능하다.\nSELECT value FROM tag WHERE tagid = 'tag-01' AND time = (SELECT min_time FROM v$tag_stat WHERE tagid = 'tag-01')\n\n\nJSON 타입 지원\n지원 배경\n\n최근 TEXT 타입의 데이터를 전송하는 장비도 사용되고 있다.\n\nJSON 타입을 지원하면, TEXT 타입 뿐만 아니라 임의의 데이터 타입을 전송할 수 있다.\n\n따라서, 칼럼의 갯수나 구조가 비정형으로 가능하기 때문에 확장성이 매우 높아지는 효과를 기대할 수 있다.\n\n장점\n\n\n  별도의 확장 칼럼을 추가할 필요가 없기 때문에, 스키마 제약이 사라진다.\n  사용자가 임의의 데이터 타입을 마음대로 넣을 수 있다.\n  JSON 데이터 특성으로 사용성 및 편의성이 높고, 데이터를 분리할 필요가 없다.\n\n\n데이터 범위\n\n\n  JSON data 길이 : 1 ~ 32768 (32K)\n  JSON path 길이 : 1 ~ 512\n\n\nCREATE TABLE jsontbl (name VARCHAR(20), jval JSON);\n\n\n데이터 삽입\n\n\n  JSON 포맷에 맞는 TEXT를 입력하여 데이터를 삽입할 수 있다.\n  JSON 포맷에 맞지 않을 시, ERROR를 출력한다.\n\n\n-- Single\nINSERT INTO jsontbl VALUES(\"name1\", '{\"name\":\"test1\"}');\n-- Multi\nINSERT INTO jsontbl VALUES(\"name2\", '{\"name\":\"test2\", \"value\":123}');\n-- Nested\nINSERT INTO jsontbl VALUES(\"name3\", '{\"name\":{\"class1\": \"test3\"}}');\n-- Array\nINSERT INTO jsontbl VALUES(\"name4\", '{\"myarray\": [1, 2, 3, 4]}');\n\n\n데이터 추출\n\n\n  JSON 관련 함수를 사용하거나, Operator(-&gt;)를 사용하여 JSON 데이터를 부분적으로 추출할 수 있다.\n\n\n-- 함수 사용\nSELECT name, JSON_EXTRACT(jval, '$.name') FROM jsontbl;\nSELECT name, JSON_EXTRACT_INTEGER(jval, '$.myarray') FROM jsontbl;\nSELECT name, JSON_TYPEOF(jval, '$.name.class1') FROM jsontbl;\n \n-- Operator(-&gt;) 사용\nSELECT name, jval-&gt;'$.name' FROM jsontbl;\nSELECT name, jval-&gt;'$.myarray' FROM jsontbl;\nSELECT name, jval-&gt;'$.name.class1' FROM jsontbl;"
					}
					
				
		
				
					,
					
					"release-note-7-5-html": {
						"id": "release-note-7-5-html",
						"title": "7.5 Release Note",
						"version": "all",
						"categories": "",
						"url": " /release-note/7_5.html",
						"content": "7.5 변경 및 추가 기능"
					}
					
				
		
				
					,
					
					"release-note-7-5-update-html": {
						"id": "release-note-7-5-update-html",
						"title": "7.5 변경 및 추가 기능",
						"version": "all",
						"categories": "",
						"url": " /release-note/7_5_update.html",
						"content": ""
					}
					
				
		
				
					,
					
					"feature-table-log-insert-append-data-html": {
						"id": "feature-table-log-insert-append-data-html",
						"title": "데이터 입력 : Append",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/insert/append-data.html",
						"content": "마크베이스에서 제공하는 빠른 실시간 데이터 입력 API이다.\n\nC, C++, C#, Java, Python, PHP, Javascript 를 이용하여 입력할 수 있다.\n\n세부 내용은 SDK 가이드를 참고한다."
					}
					
				
		
				
					,
					
					"feature-table-backup-mount-backup-intro-html": {
						"id": "feature-table-backup-mount-backup-intro-html",
						"title": "백업 개요",
						"version": "all",
						"categories": "",
						"url": " /feature-table/backup-mount/backup-intro.html",
						"content": "BACKUP/MOUNT\n데이터베이스의 영속성을 보장하기 위해서 메모리에 저장된 데이터는 최대한 빨리 Disk에 저장된다. \n그리고 Process Failure와 같은 일반적인 장애상황이 발생할 경우, Restart Recovery를 통해서 데이터베이스를 Consistent한 상태로 만든다. \n하지만 Power Failure나 화재에 의한 Hardware의 피해가 발생할 경우, 데이터베이스의 복구는 불가하다. 이런 문제를 해결하기 위해서 별도의 디스크나 Hardware에 데이터를 주기적으로 다른 영역에 저장하여, 유사시 해당 데이터를 이용하여 데이터를 복구하는 기능이 데이터베이스 백업과 복구 기능이다.\n\n데이터베이스 백업은 언제 수행하느냐에 따라서 크게 두 가지로 나누어 진다.\n\n\n  Offline Backup\n  Online Backup\n\n\n첫번째, Offline Backup 기능은 DBMS를 Shutdown하고 데이터베이스를 복사하는 기능으로 Cold Backup이라고 부르기도 한다. \n매우 간단하지만, 사용자의 서비스가 중단되는 단점이 있으므로 운영 중에는 사용하는 경우가 거의 없으며 초기 테스트나 데이터 구축 시에만 사용하는 경향이 있다.\n\n두 번째, Online Backup은 DBMS가 동작 중일 때, 데이터베이스를 Backup하는 기능으로 Hot Backup이라고 부르기도 한다. \n이 기능은 서비스를 중단하지 않고 수행될 수 있어 사용자의 Service Availability를 증가시켜 대부분의 DBMS Backup은 Online Backup을 의미한다. \n다른 데이터베이스의 Backup과 달리 시계열 데이터베이스인 Machbase 는 Duration Backup을 제공한다. 이는 Backup시 백업될 Database의 시간을 지정하여 원하는 시간대의 데이터만 Backup할 수 있다.\n\n\n\nbackup database into disk = 'backup';\nbackup database from to_date('2015-07-14 00:00:00','YYYY-MM-DD HH24:MI:SS') to to_date('2015-07-14 23:59:59 999:999:999','YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn')\n                into disk = 'backup_20150714';\n\nBackup된 데이터베이스는 장애 복구 과정을 거쳐서 기존 데이터베이스처럼 사용될 수 있다. 이 복구 방법을 Restore라고 한다. \n이 Restore 기능은 파손된 데이터베이스를 삭제하고 백업된 데이터베이스 이미지를 Primary Database로 복구한다. 때문에 복구시 기존 데이터베이스를 삭제하고 machadmin -r 기능을 이용하여 복구한다.\nmachadmin -r 'backup'\n\nMount/unmount 기능은 Online으로 동작하는 기능으로 Backup된 데이터베이스를 현재 운영 중인 데이터베이스에 Attach하는 기능이다.\nmount database 'backup' to mountName;\numount database mountName;\n\n\nDatabase Backup\nMachbase 에서는 데이터 백업을 할 때 두 가지 옵션을 제공한다. 운영 중인 DB의 정보를 백업하는 DATABASE 백업과 필요한 Table만 선택하여 백업할 수 있는 TABLE 백업 기능을 제공한다.\n\nDB에서 제공하는 백업 명령은 다음과 같다.\n\nBACKUP [ DATABASE | TABLE table_name ]  [ time_duration ] INTO DISK = 'path/backup_name';\ntime_duration = FROM start_time TO end_time\npath = 'absolute_path' or  'relative_path'\n\n# Directory backup\n       BACKUP DATABASE INTO DISK = 'backup_dir_name';\n# Set backup duration\n      - Directory backup\n       BACKUP DATABASE FROM TO_DATE('2015-07-14 00:00:00','YYYY-MM-DD HH24:MI:SS')\n                         TO TO_DATE('2015-07-14 23:59:59','YYYY-MM-DD HH24:MI:SS')\n                         INTO DISK = '/home/machbase/backup_20150714'\n\nDB 백업을 수행할 때 옵션은 백업 타입, Time duration, 경로를 입력해야 한다. DATABASE 전체를 백업할 때는 백업 타입에 DATABASE를 입력하고, 특정 Table만 백업하려면 TABLE을 입력한 후 백업하려는 Table_Name을 입력한다. \nTIME_DURATION 구문은 필요한 기간의 데이터만 백업하도록 설정할 수 있다. FROM 항목에 백업을 원하는 날짜의 시작 시간을 입력하고 TO 항목에 백업의 마지막 날짜의 시간을 입력하면 그 기간의 데이터만 선택하여 백업할 수 있다. 예제 3번을 보면 TIME_DURATION 항목의 FROM에 ‘2015년 7월 14일 0시 0분 0초’로 설정하고 TO에 ‘2015년 7월 14일 23시 59분 59초’로 설정하여 2015년 7월 14일의 데이터만 백업되도록 설정하였다. \n만약 DURATION 항목에 대한 정보를 입력하지 않으면 FROM 항목에는 ‘1970년 1월 1일 9시 0분 0초’로 설정되고 TO 항목에는 명령을 수행하는 시간으로 자동 설정된다. DURATION 절을 이용한 시간범위 백업은 TAG테이블과 TAG table을 포함한 데이터베이스에서 사용할 수가 없으며, 증분 데이터를 백업하는 기능인 INCREMENTAL BACKUP기능을 이용해야 한다.\n\n마지막으로, 백업 수행의 결과를 저장할 저장 매체에 대한 설정이 필요하다. 디렉터리 단위로 생성하려면 DISK를 입력한다. \n주의할 점은 생성물 저장되는 PATH 정보를 지정할 수 있는데 만약 상대 경로를 입력하면 현재 운영 중인 DB의 환경설정의 DB_PATH 항목에 지정된 경로에 생성된다. 만약 DB_PATH가 아닌 다른 곳에 저장하고 싶다면 ‘/’로 시작하는 절대 경로를 입력해야 한다.\n\nIncremental Backup\n증분 백업이란 이전에 수행한 백업 이후에 입력된 데이터만을 백업하는 기능이다. 증분 백업이 수행되는 대상은 Log, Tag 테이블의 데이터만 해당하며 lookup 테이블은 항상 모든 데이터를 백업한다. \n증분 백업을 수행하기 위해서는 이전에 수행한 증분 백업 디렉토리나 전체 백업 디렉토리가 필요하다. 증분 백업은 다음과 같이 수행한다.\n\n\nMach&gt; BACKUP DATABASE INTO DISK = 'backup1'; /* full backup 수행 */\nExecuted successfully.\nMach&gt; ...\n  \nMach&gt; BACKUP DATABASE AFTER 'backup1' INTO DISK = 'backup2'; /* backup1 이후에 입력한 데이터만 증분 백업을 수행함 */\nExecuted successfully.\nMach&gt; ...\n\n\n증분 백업은 데이터베이스 전체(이때 lookup 테이블은 전체 백업이 됨), Log 테이블, Tag table에 대해서 가능하며 RESTORE기능을 이용하여 복구할 경우 증분 백업 이전에 백업한 백업 데이터도 필요하다. 현재 데이터를 삭제하고 이전 상태로 되돌리기 싫은 경우 아래에서 설명하는 MOUNT기능을 이용하면 된다.\n\nIncremental Backup 주의 사항\n위와 같이 backup1을 기준으로 증분 백업으로 backup2를 만든 경우, backup1이 유실(disk failure 등의 이유)되면, backup2를 사용하여 복구할 수 없다.\n\n같은 이유로, 증분 백업을 하였을 때 이전 백업이 유실되면 이후 백업을 사용해서 복구할 수 없다.\n\n아래와 같이 백업을 3번 진행하면 backup3의 이전 백업은 backup2가 되고 backup2의 이전 백업은 backup1이 된다.\n\n따라서, backup1이 유실되면 backup2 와 backup3 모두 사용할 수 없고, backup2가 유실되면 backup3를 사용하여 복구할 수 없다.\n\nMach&gt; BACKUP DATABASE INTO DISK = 'backup1'; /* full backup 수행 */\nExecuted successfully.\nMach&gt; ...\n  \nMach&gt; BACKUP DATABASE AFTER 'backup1' INTO DISK = 'backup2'; /* backup1 이후에 입력한 데이터만 증분 백업을 수행함 */\nExecuted successfully.\nMach&gt; ...\n \nMach&gt; BACKUP DATABASE AFTER 'backup2' INTO DISK = 'backup3'; /* backup2 이후에 입력한 데이터만 증분 백업을 수행함 */\nExecuted successfully.\nMach&gt; ...\n\n\nDatabase Restore\nDatabase Restore기능은 구문으로 제공되지 않고, Offline으로 machadmin -r 기능을 통해 복구할 수 있다. 복구전에 다음 사항을 체크해야 한다.\n\n  Machbase 가 종료되었는가?\n  이전에 생성한 DB를 삭제하였는가?\n\n\nmachadmin -r backup_database_path;\n\nbackup database into disk = '/home/machbase/backup';\n\nmachadmin -k\nmachadmin -d\nmachadmin -r /home/machbase/backup;\n\n\nDatabase Mount\n장애상황을 대비하여 대량의 데이터베이스를 주기적으로 Backup 하고 데이터를 계속 추가하는 경우, 다음과 같은 문제점이 발생한다.\n\n  데이터를 저장하기 위한 디스크 비용 증가\n  운영 중인 Machine의 물리적 Disk공간의 한계\n\n\n이 문제점을 해결하기 위해서 주기적으로 현재 서비스를 위해 필요한 데이터만을 남기고 삭제를 수행한다. 그러나 과거 데이터에 대한 참조가 필요할 경우에는 Backup 된 데이터베이스를 Restore하여 참조해야 하는데, 대단히 큰 Backup Image일 경우 복구시간이 많이 걸리고 또한 별도의 장비도 필요하다. 왜냐하면 Restore기능은 현재 운영 중인 데이터베이스를 삭제해야 수행할 수 있기 때문이다. 이런 문제를 해결하기 위해서 Machbase 는 Database Mount 기능을 제공한다.\n\nDatabase Mount기능은 Online으로 동작하는 기능으로 Backup된 데이터베이스를 현재 운영 중인 데이터베이스에 Attach하는 기능이다. 그리고 여러 개의 Backup Database을 운영 중인 Primary Database에 Attach하여 사용자는 여러 개의 Backup Database를 하나의 Database인 것처럼 참조 가능한다. 단 Mount한 Database에 대해서는 Read만 가능하다.\n\nMount DATABASE 명령은 기존에 Backup으로 생성된 데이터베이스 혹은 테이블 DATA를 현재 운영 중인 데이터베이스에서 조회 가능한 상태로 준비시켜 주는 기능이다. 그래서 Mount 된 DATABASE는 동일한 DB 명령어를 사용하여 데이터를 조회할 수 있다.\n\n현재 Database Mount 기능의 제약 사항은 다음과 같다.\n\n  Backup 정보는 Mount할 Database와 DB의 Major 번호와 Meta의 Major 번호가 호환 가능한 버전이어야 한다.\n  Backup Data를 Mount할 경우 읽기만 가능하여 Index 생성, 데이터 입력 및 삭제 등은 지원하지 않는다.\n  현재 Mount된 DATABASE의 정보는 V$STORAGE_MOUNT_DATABASES를 조회하여 확인할 수 있다.\n  증분 백업 데이터를 mount하는 경우, 그 백업데이터에 기록된 증분 데이터만 검색되며 이전에 수행한 증분데이터를 따라가서 mount해 주지는 않는다.\n\n\nMount\nMount 명령을 수행하기 위해서는 Backup_database_path 정보와 DatabaseName이 필요하다. Backup_database_path는 Backup 명령을 통하여 생성된 DB의 위치 정보를 입력해야 하고, DatabaseName에는 Database에 Mount 할 때 구분할 수 있는 이름을 지정한다. Backup_database_path는 Backup 할 때와 동일하게 상대 경로를 입력할 경우 DB의 환경변수에 설정된 DB_PATH에 지정된 디렉터리를 기준으로 검색한다.\n\nMOUNT DATABASE 'backup_database_path' TO mount_name;\n\nMOUNT DATABASE '/home/machbase/backup' TO mountdb;\n\n\nUnmount\nMount된 Database를 더 이상 사용하지 않을 경우 Umount 명령을 사용하여 제거할 수 있다.\n\nUNMOUNT DATABASE mount_name;\n\nUNMOUNT DATABASE mountdb;\n\n\nMOUNT DB에서 데이터 조회\nBackup DB의 DATA를 조회할 때 운영 중인 DB의 DATA를 조회하는 것과 동일한 SQL문을 이용하여 조회할 수 있다.\n\nMount 된 DB는 운영중인 DB의 SYS 권한의 사용자만 데이터를 조회할 수 있다. 데이터를 조회하기 위해서는 조회할 TableName 앞에 MountDBName과 UserName을 입력하고, 각각의 구분자로 ‘.’을 붙여서 사용해야 한다. MountDBName은 현재 Mount된 DB들 중 특정 DB를 지칭하기 위해 사용하고, UserName은 Mount된 DB의 Table을 소유한 User의 정보를 지칭하는 것이다.\n\nSELECT column_name FROM mount_name.user_name.table_name;\n\nSELECT * FROM mountdb.sys.backuptable;"
					}
					
				
		
				
					,
					
					"feature-table-backup-mount-html": {
						"id": "feature-table-backup-mount-html",
						"title": "백업 및 마운트",
						"version": "all",
						"categories": "",
						"url": " /feature-table/backup-mount.html",
						"content": "데이터를 정기적으로 백업하는 것은 매우 중요하다. 이 챕터에서는 마크베이스의 데이터를 어떻게 백업하며, 백업한 데이터를 복구하는지를 기술한다. 또한 마크베이스는 백업된 데이터를 복구시키지 않고 검색할 수 있는 마운트 기능을 제공한다. 마운트 기능은 백업된 데이터를 읽어야 할 때 매우 빠르게 실행할 수 있다.\n\n\n  백업 개요\n  데이터베이스 마운트\n  백업 및 복구"
					}
					
				
		
				
					,
					
					"feature-table-backup-mount-backup-restore-html": {
						"id": "feature-table-backup-mount-backup-restore-html",
						"title": "백업 및 복구",
						"version": "all",
						"categories": "",
						"url": " /feature-table/backup-mount/backup-restore.html",
						"content": "데이터베이스 백업\n마크베이스의 백업 방법은 두 가지가 있다.\n\n\n  현재 DB의 전체 백업\n  특정 테이블만 선택해서 백업\n\n\nSyntax:\nBACKUP [ DATABASE | TABLE table_name ]  [ time_duration ] INTO [ DISK ] = 'path/backup_name';\ntime_duration = FROM start_time TO end_time\npath = 'absolute_path' or  'relative_path'\nBACKUP [ DATABASE | TABLE table_name ] AFTER 'previous_backup_dir'\n\nExample:\n# Directory backup\n       BACKUP DATABASE INTO DISK = 'backup_dir_name';\n \n# Set backup duration\n      - Directory backup\n       BACKUP DATABASE FROM TO_DATE('2015-07-14 00:00:00','YYYY-MM-DD HH24:MI:SS')\n                         TO TO_DATE('2015-07-14 23:59:59','YYYY-MM-DD HH24:MI:SS')\n                         INTO DISK = '/home/machbase/backup_20150714'\n\n백업 명령을 실행할 때, 백업 타입과 duration 시간 조건절, 백업 대상 경로를 반드시 정의하여야 한다. 전체 데이터베이스를 백업하려면 “DATABASE”를, 특정 테이블을 백업하려면 “TABLE”을 백업 타입에 지정하고, 특정 테이블을 백업할 때에는 테이블 이름을 지정하여야 한다.\n\nDURATION 조건절을 이용하여 백업 대상을 지정할 수 있다. &lt;/br&gt;\n백업 대상 데이터의 시작 시간과 끝 시간을 FROM 및 TO 절에서 지정한다. \n위 예제에서  “2015-07-14 00:00:00” 가 FROM으로 정의되었고, “2015-07-14 23:59:59” 이 TO로 정의되었으므로, 사용자는 2015년 7월 14일의 전체 데이터를 백업하는 것이다. \nduration 시간 조건절을 지정하지 않으면 “1970-01-01 00:00:00” 이 FROM으로 설정되고 실행되는 현재 시점이 TO절에 설정된다.\n\nDURATION절은 Tag 테이블과 Tag 테이블을 포함하는 DATABASE에서는 이용할 수 없으며, 추가된 데이터만을 백업하려면 증분 백업 (Backup …. AFTER ‘previous_backup’) 문을 수행해야 한다.\n\n백업 경로를 지정할 때, 상대 경로를 지정하면 “$MACHBASE_HOME/dbs” 아래에 백업 파일들이 생성되므로 주의하여야 한다. 절대 경로를 지정하려면 항상 “/”로 시작하는 경로를 설정하여야 한다.\n\n데이터베이스 복구\n백업 파일에서 데이터 복원을 수행할 때는 질의 명령으로 수행할 수 없으며, “machadmin -r” 명령을 데이터베이스가 동작하지 않는 상황에서 실행해야 한다. \n백업 실행 이전에 다음의 조건들을 검토해야 한다.\n\n  마크베이스 데이터베이스가 정지 상태인가\n  현재의 데이터는 삭제되고 복구할 데이터로 대치되므로, 현재 데이터베이스의 삭제가 허용되는가\n  증분 백업에 대해서는 이전에 백업한 full backup까지의 증분 백업 파일이 필요하다.\n    Syntax:\n    machadmin -r backup_database_path\n    \n    Example:\n    ```sql\nbackup database into disk = ‘/home/machbase/backup’;\n  \n\n\nmachadmin -k\nmachadmin -d\nmachadmin -r /home/machbase/backup;\n```"
					}
					
				
		
				
					,
					
					"install-cluster-cmd-broker-warehouse-install-html": {
						"id": "install-cluster-cmd-broker-warehouse-install-html",
						"title": "Lookup / Broker / Warehouse 설치",
						"version": "all",
						"categories": "",
						"url": " /install/cluster/cmd/broker-warehouse-install.html",
						"content": "목차\n\n\n  Lookup 설치\n    \n      설치 조건\n    \n  \n  Lookup 삭제\n  Lookup 종료/중단\n  Lookup Master 변경\n  Broker 설치\n  Broker 삭제\n  Broker 종료/중단\n  Warehouse 설치\n    \n      Group1 설치\n      Group1에 노드 추가 설치\n      Group2 설치\n      Group2에 노드 추가 설치\n    \n  \n  Warehouse 삭제\n  Warehouse 종료/중단\n\n\nLookup 설치\n\nCoordinator 노드에서 lookup 노드를 추가한다.  여러 개의 lookup 노드 등록이 가능하다.\n\n해당 서버에 deployer 노드가 미리 설치되어 있어야 한다.\n\nDeployer 노드가 설치되면, 모든 작업은 coordinator 노드에서 수행하게 되며 해당 서버에 접속해서 설정하는 것은 없다.\n\n# lookup master 노드를 추가한다.                                \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5301\"  \\\n        --node-type=lookup --lookup-type=master --deployer=\"192.168.0.84:5201\"      \\\n        --home-path=\"/home/machbase/lookup1\"\n \n \n# lookup monitor 노드를 추가한다.                                \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5302\"  \\\n        --node-type=lookup --lookup-type=monitor --deployer=\"192.168.0.84:5201\"         \\\n        --home-path=\"/home/machbase/lookupm1\"\n \n \n# lookup slave 노드를 추가한다.                                \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5303\"  \\\n        --node-type=lookup --lookup-type=slave --deployer=\"192.168.0.84:5201\"       \\\n        --home-path=\"/home/machbase/lookup3\"\n \n  \n# lookup 노드를 실행한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5301\"\n \n# lookup 노드를 일괄적으로 실행할 수 있다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-lookup\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-node\n      추가할 노드명으로, “IP:PORT” 형식으로 지정한다.\n      192.168.0.84:5301\n    \n    \n      –node-type\n      노드 종류를 지정한다. coordinator, deployer, broker, lookup, warehouse 5종류가 있다.\n      lookup\n    \n    \n      –deployer\n      설치할 서버의 deployer node 정보를 등록한다.\n      192.168.0.84:5201\n    \n    \n      –lookup-type\n      lookup 노드 종류를 지정한다.  master,slave,monitor 3종류가 있다.\n      master\n    \n    \n      –home-path\n      설치할 경로를 지정한다. machbase 계정에서 /home/machabse/lookup1 로 지정한다\n      /home/machabse/lookup\n    \n  \n\n\n설치 조건\n\nLookup 노드는 Master, Slave, Monitor 3 종류가 존재하는데 아래 조건에 맞게 설치해야 한다.\n\n1. Lookup Master 노드\n    a. 반드시 1개가 존재해야 하는 Lookup 노드이다.\n    b. Monitor, Slave 노드보다 먼저 설치되어야 한다.\n2. Lookup Monitor 노드 \n    a. 반드시 존재해야 하는 Lookup 노드이다.\n    b. 안정적인 HA를 위해 각 서버에 1개씩 존재해야 한다.\n3. Lookup Slave 노드\n    a. HA를 위해 1개 이상 존재하는 것을 권장한다.(없을경우 HA를 보장할 수 없다)\n\n\nLookup 삭제\n\nCoordinator 노드에서 lookup 노드를 삭제한다.\n\n# Delete 노드를 삭제한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.84:5301\"\n\n\nLookup 종료/중단\n\nCoordinator 노드에서 lookup 노드를 종료/중단하는 방법이 있다.\n\n# lookup 노드를 종료한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --shutdown-node=\"192.168.0.84:5301\"\n \n# lookup 노드를 일괄적으로 종료할 수 있다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --shutdown-lookup\n\n\nLookup Master 변경\n\nCoordinator 노드에서 lookup master 노드를 변경하는 방법이 있다.\n\nlookup slave에 한해서 lookup master로 변경 가능하며, 기존에 lookup master는 lookup slave가 된다.\n\n# lookup master를 변경한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --set-lookup-master=\"192.168.0.84:5301\"```\n\n\nBroker 설치\n\nCoordinator 노드에서 broker 노드를 추가한다.  여러 개의 broker 노드 등록이 가능하다.\n\n해당 서버에 deployer 노드가 미리 설치되어 있어야 한다.\n\nDeployer 노드가 설치되면, 모든 작업은 coordinator 노드에서 수행하게 되며 해당 서버에 접속해서 설정하는 것은 없다.\n\n최초에 등록되는 노드가 leader broker가 되고 이후에 추가적으로 등록되는 노드는 follower broker가 된다.\n\n# broker 노드를 추가한다.                                \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5301\"  \\\n        --node-type=broker --deployer=\"192.168.0.84:5201\" --port-no=\"5656\"          \\\n        --home-path=\"/home/machbase/broker\" --package-name=machbase\n  \n# broker 노드를 실행한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5301\"\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-node\n      추가할 노드명으로, “IP:PORT” 형식으로 지정한다.  PORT 값은 CLUSTER_LINK_PORT_NO 값으로 설정된다.\n      192.168.0.84:5301\n    \n    \n      –node-type\n      노드 종류를 지정한다.  coordinator, deployer, broker, lookup,warehouse 5종류가 있다.\n      broker\n    \n    \n      –deployer\n      설치할 서버의 deployer node 정보를 등록한다.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      machbased 구동 포트를 지정한다.  Broker는 디폴트값인 5656을 지정한다. client와 machsql이 접속할 때 이 포트를 이용한다.\n      5656\n    \n    \n      –home-path\n      설치할 경로를 지정한다.  machbase 계정에서 /home/machabse/broker 로 지정한다\n      /home/machbase/broker\n    \n    \n      –package-name\n      패키지 추가할 때 지정한 package 명을 설정한다.\n      machbase\n    \n  \n\n\nBroker 삭제\n\nCoordinator 노드에서 broker 노드를 삭제한다.\n\n# broker 노드를 삭제한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.84:5301\"\n\n\nBroker 종료/중단\n\nCoordinator 노드에서 broker 노드를 종료/중단하는 방법이 있다.\n\n# broker 노드를 종료한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --shutdown-node=\"192.168.0.84:5301\"\n  \n# broker 노드를 중단한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --kill-node=\"192.168.0.84:5301\"\n\n\n또는, broker가 설치된 서버에서 직접 그 프로세스를 종료/중단하는 방법도 있다.\n\n# broker 노드를 종료한다.\n$MACHBASE_HOME/bin/machadmin -s\n  \n# broker 노드를 종료한다.\n$MACHBASE_HOME/bin/machadmin -k\n\n\nWarehouse 설치\n\nCoordinator 노드에서 active 노드와 standby 노드를 설치한다.\n\n사전에 설치된 deployer 를 통해서 설치된다.\n\nGroup1 설치\n\n첫번째 Warehouse 그룹인 Group1 노드를 설치한다.\n\n# group1 warehouse를 설치한다.                         \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.83:5401\"  \\\n        --node-type=warehouse --deployer=\"192.168.0.83:5201\" --port-no=\"5400\"       \\\n        --home-path=\"/home/machbase/warehouse_g1\" --package-name=machbase           \\\n        --replication=\"192.168.0.83:5402\"  --group=\"group1\" --no-replicate\n  \n# 설치된 노드를 구동한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5401\"\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-node\n      추가할 노드명으로 “IP:PORT” 형식으로 지정한다.  PORT값은 CLUSTER_LINK_PORT_NO 값으로 설정된다.\n      192.168.0.84:5401\n    \n    \n      –node-type\n      노드 종류를 지정한다.  coordinator, deployer, broker, warehouse, lookup 5종류가 있다.\n      warehouse\n    \n    \n      –deployer\n      설치할 서버의 deployer node 정보를 등록한다.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      machbased 구동 포트를 지정한다.  Broker에서 5656값을 설정하였으므로 동일 서버에 설치되는 경우 다른 포트를 지정해야 한다. 따라서 warehouse 사용 포트 대역인 5400 을 지정한다. client와 machsql 접속할 때 이 포트를 이용한다.\n      5400\n    \n    \n      –home-path\n      설치할 경로를 지정한다. 그룹을 구분하기 위해서 warehouse_g1, g2, g3 순으로 설정한다.\n      /home/machbase/warehouse_g1\n    \n    \n      –package-name\n      패키지 추가할 때 지정한 package 명을 설정한다.\n      machbase\n    \n    \n      –replication\n      Replication을 담당할 노드를 “IP:PORT” 형식으로 지정한다. PORT값은 해당 warehouse 사용 포트대역인 5402로 지정한다.\n      192.168.0.84:5402\n    \n    \n      –group\n      Group명을 지정한다.\n      group1\n    \n    \n      –no-replicate\n      Group내의 warehouse데이터가 있는 경우, 노드추가 시, 데이터를 복제할 것 인지 지정한다.\n       \n    \n  \n\n\nGroup1에 노드 추가 설치\n\nWarehouse Group1에 노드를 한 개 더 추가 설치한다.\n\n# group1에 warehouse node를 추가 설치한다.              \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5401\"  \\\n        --node-type=warehouse --deployer=\"192.168.0.84:5201\" --port-no=\"5400\"       \\\n        --home-path=\"/home/machbase/warehouse_g1\" --package-name=machbase           \\\n        --replication=\"192.168.0.84:5402\" --group=\"group1\" --no-replicate\n  \n# 설치된 노드를 구동한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5401\"\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-node\n      추가할 노드명으로 “IP:PORT” 형식으로 지정한다.  PORT값은 CLUSTER_LINK_PORT_NO 값으로 설정된다.\n      192.168.0.84:5401\n    \n    \n      –node-type\n      노드 종류를 지정한다.  coordinator, deployer, broker, warehouse, lookup 5종류가 있다.\n      warehouse\n    \n    \n      –deployer\n      설치할 서버의 deployer node 정보를 등록한다.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      machbased 구동 포트를 지정한다.  Broker에서 5656값을 설정하였으므로 동일 서버에 설치되는 경우 다른 포트를 지정해야 한다. 따라서 warehouse 사용 포트 대역인 5400 을 지정한다. client와 machsql 접속할 때 이 포트를 이용한다.\n      5400\n    \n    \n      –home-path\n      설치할 경로를 지정한다. 그룹을 구분하기 위해서 warehouse_g1, g2, g3 순으로 설정한다.\n      /home/machbase/warehouse_g1\n    \n    \n      –package-name\n      패키지 추가할 때 지정한 package 명을 설정한다.\n      machbase\n    \n    \n      –replication\n      Replication을 담당할 노드를 “IP:PORT” 형식으로 지정한다. PORT값은 해당 warehouse 사용 포트대역인 5402로 지정한다.\n      192.168.0.84:5402\n    \n    \n      –group\n      Group명을 지정한다.\n      group1\n    \n    \n      –no-replicate\n      Group내의 warehouse데이터가 있는 경우, 노드추가 시, 데이터를 복제할 것 인지 지정한다.\n       \n    \n  \n\n\nGroup2 설치\n\n두 번째 Warehouse 그룹인 Group2 노드를 설치한다.\n\n# group1 warehouse를 설치한다.                         \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5411\"  \\\n        --node-type=warehouse --deployer=\"192.168.0.84:5201\" --port-no=\"5410\"       \\\n        --home-path=\"/home/machbase/warehouse_g2\" --package-name=machbase           \\\n        --replication=\"192.168.0.84:5412\"  --group=\"group2\" --no-replicate\n  \n# 설치된 노드를 구동한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5411\"\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-node\n      추가할 노드명으로 “IP:PORT” 형식으로 지정한다.  PORT값은 CLUSTER_LINK_PORT_NO 값으로 설정된다.\n      192.168.0.84:5411\n    \n    \n      –node-type\n      노드 종류를 지정한다.  coordinator, deployer, broker, warehouse, lookup 5종류가 있다.\n      warehouse\n    \n    \n      –deployer\n      설치할 서버의 deployer node 정보를 등록한다.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      machbased 구동 포트를 지정한다.  Broker에서 5656값을 설정하였으므로 동일 서버에 설치되는 경우 다른 포트를 지정해야 한다. 따라서 warehouse 사용 포트 대역인 5410 을 지정한다. client와 machsql 접속할 때 이 포트를 이용한다.\n      5410\n    \n    \n      –home-path\n      설치할 경로를 지정한다. 그룹을 구분하기 위해서 warehouse_g1, g2, g3 순으로 설정한다.\n      /home/machbase/warehouse_g2\n    \n    \n      –package-name\n      패키지 추가할 때 지정한 package 명을 설정한다.\n      machbase\n    \n    \n      –replication\n      Replication을 담당할 노드를 “IP:PORT” 형식으로 지정한다. PORT값은 해당 warehouse 사용 포트대역인 5412로 지정한다.\n      192.168.0.84:5412\n    \n    \n      –group\n      Group명을 지정한다.\n      group2\n    \n    \n      –no-replicate\n      Group내의 warehouse데이터가 있는 경우, 노드추가 시, 데이터를 복제할 것 인지 지정한다.\n       \n    \n  \n\n\nGroup2에 노드 추가 설치\n\nWarehouse Group2에 노드를 한 개 더 추가 설치한다.\n\n# group1에 warehouse node를 추가 설치한다.              \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.83:5411\"  \\\n        --node-type=warehouse --deployer=\"192.168.0.83:5201\" --port-no=\"5410\"       \\\n        --home-path=\"/home/machbase/warehouse_g2\" --package-name=machbase           \\\n        --replication=\"192.168.0.83:5412\" --group=\"group2\" --no-replicate\n  \n# 설치된 노드를 구동한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.83:5411\"\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-node\n      추가할 노드명으로 “IP:PORT” 형식으로 지정한다.  PORT값은 CLUSTER_LINK_PORT_NO 값으로 설정된다.\n      192.168.0.84:5411\n    \n    \n      –node-type\n      노드 종류를 지정한다.  coordinator, deployer, broker, warehouse, lookup 5종류가 있다.\n      warehouse\n    \n    \n      –deployer\n      설치할 서버의 deployer node 정보를 등록한다.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      machbased 구동 포트를 지정한다.  Broker에서 5656값을 설정하였으므로 동일 서버에 설치되는 경우 다른 포트를 지정해야 한다. 따라서 warehouse 사용 포트 대역인 5410 을 지정한다. client와 machsql 접속할 때 이 포트를 이용한다.\n      5410\n    \n    \n      –home-path\n      설치할 경로를 지정한다. 그룹을 구분하기 위해서 warehouse_g1, g2, g3 순으로 설정한다.\n      /home/machbase/warehouse_g2\n    \n    \n      –package-name\n      패키지 추가할 때 지정한 package 명을 설정한다.\n      machbase\n    \n    \n      –replication\n      Replication을 담당할 노드를 “IP:PORT” 형식으로 지정한다. PORT값은 해당 warehouse 사용 포트대역인 5412로 지정한다.\n      192.168.0.84:5412\n    \n    \n      –group\n      Group명을 지정한다.\n      group2\n    \n    \n      –no-replicate\n      Group내의 warehouse데이터가 있는 경우, 노드추가 시, 데이터를 복제할 것 인지 지정한다.\n       \n    \n  \n\n\nWarehouse 삭제\n\nCoordinator 노드에서 warehouse 노드를 삭제한다.\n\n# warehouse 노드를 삭제한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.83:5401\"\n\n\nWarehouse 종료/중단\n\nCoordinator 노드에서 warehouse 노드를 종료/중단하는 방법이 있다.\n\n# warehouse 노드를 종료한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --shutdown-node=\"192.168.0.83:5401\"\n  \n# warehouse 노드를 중단한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --kill-node=\"192.168.0.83:5401\"\n\n\n또는 warehouse가 설치된 서버에서 직접 그 프로세스를 종료/중단하는 방법이 있다.\n\n# warehouse 노드를 종료한다.\n$MACHBASE_HOME/bin/machadmin -s\n  \n# warehouse 노드를 종료한다.\n$MACHBASE_HOME/bin/machadmin -k"
					}
					
				
		
				
					,
					
					"sdk-cli-odbc-html": {
						"id": "sdk-cli-odbc-html",
						"title": "CLI/ODBC",
						"version": "all",
						"categories": "",
						"url": " /sdk/cli_odbc.html",
						"content": "CLI란 ISO/IEC 9075-3:2003에 정의된 소프트웨어 개발 표준이다.\n\nCLI는 데이터베이스에 어떻게 SQL을 전달하고, 결과 값을 어떻게 받고 분석해야 하는지에 대한 함수 및 명세를 정의하고 있다. 이 CLI는 1990년 초창기에 개발되었고, C 와 COBOL 언어 만을 위해 개발되었고, 현재까지 그 스펙이 유지되고 있다.\n\n현재까지 가장 널리 알려진 표준 인터페이스는 ODBC(Open Database Connectivity)로서 클라이언트 프로그램이 데이터베이스의 종류와 무관하게 데이터베이스 접속할 수 있는 방법을 제시해 주고 있다. 현재 최신 ODBC API 버전은 3.52 로서 ISO와 X/Open 표준에 정의되어 있다.\n\n목차\n\n\n  목차\n  표준 CLI 함수\n    \n      접속을 위한 연결 스트링\n    \n  \n  확장 CLI 함수 (APPEND)\n    \n      Append 프로토콜의 이해\n      Append 데이터의 전송\n      Append 데이터의 에러 확인\n      서버 에러 검사를 위한 부가 옵션\n      서버 에러 발생시 Trace 로그 남기기\n      APPEND 함수 설명\n        \n          SQLAppendOpen\n          SQLAppendData (deprecated)\n          SQLAppendDataByTime(deprecated)\n          SQLAppendDataV2\n          SQLAppendDataByTimeV2\n          SQLAppendFlush\n          SQLAppendClose\n          SQLAppendSetErrorCallback\n          SQLSetConnectAppendFlush\n          SQLSetStmtAppendInterval\n        \n      \n    \n  \n  열 형식 매개변수 바인딩\n  지원되는 문자열\n\n\n표준 CLI 함수\n표준 함수의 사용법에 대해서는 다음과 같은 링크를 참조한다.\n\n\n  위키피디아\n  오픈그룹 문서\n\n\n다음의 함수를 참고하면 된다.\n\n\n  \n    \n      SQLAllocConnect\n      SQLDisconnect\n      SQLGetDescField\n      SQLPrepare\n    \n    \n      SQLAllocEnv\n      SQLDriverConnect\n      SQLGetDescRec\n      SQLPrimaryKeys\n    \n    \n      SQLAllocHandle\n      SQLExecDirect\n      SQLGetDiagRec\n      SQLStatistics\n    \n    \n      SQLAllocStmt\n      SQLExecute\n      SQLGetEnvAttr\n      SQLRowCount\n    \n    \n      SQLBindCol\n      SQLFetch\n      SQLGetFunctions\n      SQLSetConnectAttr\n    \n    \n      SQLBindParameter+\n      SQLFreeConnect\n      SQLGetInfo\n      SQLSetDescField\n    \n    \n      SQLColAttribute\n      SQLFreeEnv\n      SQLGetStmtAttr\n      SQLSetDescRec\n    \n    \n      SQLColumns\n      SQLFreeHandle\n      SQLGetTypeInfo\n      SQLSetEnvAttr\n    \n    \n      SQLConnect\n      SQLFreeStmt\n      SQLNativeSQL\n      SQLSetStmtAttr\n    \n    \n      SQLCopyDesc\n      SQLGetConnectAttr\n      SQLNumParams\n      SQLStatistics\n    \n    \n      SQLDescribeCol\n      SQLGetData\n      SQLNumResultCols\n      SQLTables\n    \n  \n\n\n접속을 위한 연결 스트링\nCLI를 통해 접속을 하기 위해서는 연결 스트링을 만들어야 하며, 각각의 내용은 다음과 같다.\n\n\n  \n    \n      연결 스트링 항목명\n      항목 설명\n    \n  \n  \n    \n      DSN\n      데이터 소스 명을 지정한다. ODBC에서는 리소스가 담긴 파일의 섹션 명을 기술하고, CLI에서는 서버명 혹은 IP 주소를 지정한다.\n    \n    \n      DBNAME\n      Machbase의 DB명을 기술한다.\n    \n    \n      SERVER\n      Machbase가 위치하는 서버의 호스트 명 혹은 IP 주소를 가리킨다.\n    \n    \n      NLS_USE\n      서로 사용할 언어 종류를 설정한다.(현재 사용되지 않으며, 차후 확장을 위해 유지한다.)\n    \n    \n      UID\n      사용자 아이디\n    \n    \n      PWD\n      사용자 패스워드\n    \n    \n      PORT_NO\n      접속할 포트 번호\n    \n    \n      PORT_DIR\n      유닉스에서 Unix domain으로 접속할 경우 사용되는 파일 경로를 지정한다. (서버에서 수정했을 경우에 지정하며, 디폴트로는 지정하지 않아도 동작한다.)\n    \n    \n      CONNTYPE\n      클라이언트와 서버의 접속 방법을 지정한다. 1: TCP/IP INET 으로 접속 2: Unix Domain 으로 접속\n    \n    \n      COMPRESS\n      Append 프로토콜을 압축할 것인지 나타낸다. 이 값이 0일 경우에는 압축하지 않고 전송한다. 이 값이 0보다 큰 임의의 값일 경우에는 그 값보다 Append 레코드가 클 경우에만 압축한다. 예) COMPRESS=512 레코드 사이즈가 512보다 클 경우에만 압축하여 동작한다. 원격 접속일 경우 압축하면 전송 성능이 향상된다.\n    \n    \n      SHOW_HIDDEN_COLS\n      숨겨진 컬럼(_arrival_time)을 select * 로 수행시 보여줄 것인지 결정한다. 0일 경우에는 보이지 않으며, 1일 경우에 해당 컬럼의 정보가 출력된다.\n    \n    \n      CONNECTION_TIMEOUT\n      최초 연결시에 얼마나 대기할 것이지 설정한다. 디폴트로는 30초가 설정되어 있다. 만일 최초 연결시 서버의 응답이 30초 보다 더 느려지는 경우를 고려하면, 이 값을 더 크게 설정해야 한다. CONNECTION_TIMEOUT에서 0 값은 Timeout에 제한이 없음을 의미하며 연결이 실패할 때에도 무한정으로 대기하므로 되도록이면 사용하지 않는 편이 좋다.\n    \n    \n      SOCKET_TIMEOUT\n      Protocol I/O에 시간이 걸리면 발생하는 timeout이다. Client에서 검사하여 대기 후 Disconnect를 수행한다. ORACLE의 Read Timeout과 같다. (MYSQL, MSSQL에서는 동일하게 SOCKET_TIMEOUT이라는 이름으로 사용한다.) Connection String에서 SOCKET_TIMEOUT=NN(초)로 설정하며 기본값은 30분(1800)으로 설정된다.\n    \n    \n      ALTERNATIVE_SERVERS\n      cluster 버전을 사용 시, 여러 대의 브로커의 정보를 추가적으로 가지게 되는 설정이다. 다중의 브로커를 등록해두었을 시, 접속되어있던 브로커가 혹시 내려가게 된 경우에도 다른 브로커에 접속한 뒤, 입력하던 데이터를 계속해서 입력하게 된다. 여러개의 브로커를 등록할 수 있으며, :의 값을 ',' 단위로 이어서 작성한다.  ex) ALTERNATIVE_SERVERS=192.168.0.10:20320,192.168.0.11:20320;\n    \n  \n\n\nCLI 접속 예제는 다음과 같다.\n\nsprintf(connStr,\"SERVER=127.0.0.1;COMPRESS=512;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n \nif (SQL_ERROR == SQLDriverConnect( gCon, NULL, (SQLCHAR *)connStr, SQL_NTS, NULL, 0, NULL, SQL_DRIVER_NOPROMPT )) {\n   ...\n}\n\n\n확장 CLI 함수 (APPEND)\nCLI 확장 함수는 Machbase 서버에 데이터를 초고속으로 입력하기 위해 제공되는 Append 프로토콜을 구현하기 위한 함수이다.\n\n이 함수는 크게 4가지의 함수로 구성되어 있는데, 채널의 오픈, 채널에 대한 데이터 입력, 채널의 플러쉬, 채널 클로징이다.\n\nAppend 프로토콜의 이해\nMachbase에서 제공하는 Append 프로토콜은 비동기 방식으로 동작한다. 비동기라 함은 클라이언트가 서버에게 요청한 특정 작업에 대한 응답이 서로 완전히 동기화되지 않고, 임의의 이벤트가 발생하는 순간에 발생하는 것을 의미한다. 즉, 클라이언트가 Append를 수행했다고 하더라도, 그 수행에 대한 결과를 바로 얻거나 확인할 수 없으며, 서버에서 준비가 되는 임의의 시점에 그것을 확인할 수 있다는 것이다. 이런 이유로 Append 프로토콜을 활용해서 응용 프로그램을 개발하는 개발자는 다음과 같은 내부 동작에 대한 이해를 가져야 한다. 이후의 설명은 클라이언트가 언제 어떻게 서버에서 발생하는 비동기 에러를 검출하고 사용자에게 되돌여주는지에 대한 것이다.\n\nAppend 데이터의 전송\nSQLExecute 혹은 SQLExecDirect()와 같은 일반적인 호출에서 Machbase는 즉시 그 결과를 클라이언트에게 되돌려주는 동기화 방식을 사용한다. 그러나, SQLAppendDataV2()는 사용자 데이터가 입력된 이후 즉시 요청을 보내지 않는다. 대신, 클라이언트 통신 버퍼가 모두 찰 때 까지 대기하고 있다가 모두 차면 그 이후에 한꺼번에 데이터를 클라이언트로 전송하게 된다. 이렇게 설계된 이유는 Append를 사용하는 클라이언트의 입력 데이터가 초당 수만에서 수십만 레코드를 가정하였기 때문에 고속의 데이터 전송을 위한 버퍼링 방식을 활용한 것이다. 이런 이유로 만일 사용자가 임의로 해당 버퍼의 내용을 전송하고자 할 경우에는 SQLAppendFlush() 함수를 호출하여, 명시적으로 데이터를 입력할 수 있다.\n\nAppend 데이터의 에러 확인\n앞에서 언급한 바와 같이 Append 프로토콜은 버퍼링되어 비동기로 동작한다. 특히, 서버에서 에러가 발생하지 않았을 경우에는 아무런 응답을 받지 않고, 에러가 발생했을 경우에만 에러를 검출하는 방식을 취하기 때문에 에러가 언제 어떻게 검출되는지 이해하는 것이 매우 중요하다. 또한, 에러를 검출하는 비용이 상대적으로 매우 크기 때문에 레코드 입력시마다 매번 검사하는 것이 매우 비효율적으로 판단되어, 현재 Machbase에서는 명시적으로 다음과 같은 경우에만 에러를 검출하도록 되어 있다. 에러가 검출될 경우에는 사용자가 설정한 에러 콜백 함수를 매번 호출하게 된다.\n\n\n  전송 버퍼가 모두 차고, 서버에게 명시적으로 데이터를 전송한 이후 검사\n  SQLAppendFlush() 내부에서 서버에게 명시적으로 데이터를 전송한 이후 검사\n  SQLAppendClose() 내부에서 종료 직전에 검사\n\n\n즉, 기본적으로 위의 3가지 경우에만 에러를 검출하도록 되어 있어, I/O의 발생을 최소화하도록 설계되었다.\n\n서버 에러 검사를 위한 부가 옵션\n성능을 최대한으로 달성하기 위해 기본적으로 설정된 에러 검출 기법은 사용자가 원하는 경우 좀 더 빈번하게 검사하고, 이를 활용할 수 있다. 즉, SQLAppendOpen() 함수의 마지막 인자인 aErrorCheckCount를 조절함으로서 가능한다. 이 값이 0일 경우에는 별도의 확인 동작을 하지 않고, 기본으로 동작한다. 그러나, 만일 이 값이 0보다 클 경우에는 SQLAppendData()의 호출 횟수마다 명시적으로 에러를 검사하도록 되어있다. 다시 말해 이 값이 10일 경우에는 10번의 Append 동작마다 에러를 검사하는 비용을 지불한다. 따라서, 이 값이 작을 경우에는 에러 검출을 위한 시스템 리소스를 많이 사용하기 때문에 적절한 숫자로 조절하여 사용해야 한다.\n\n서버 에러 발생시 Trace 로그 남기기\n만일 에러가 발생한 Append 데이터에 대해서 별도로 Trace 로그를 남기고자 할 경우에는 서버에 준비된 프로퍼티 DUMP_APPEND_ERROR 를 1로 설정한다. 이렇게 설정하면, mach.trc 파일에 해당 에러를 발생시킨 레코드에 대한 명세가 파일로 기록된다. 단, 에러의 횟수가 과도할 경우 시스템 리소스의 사용량이 급격히 늘어나, Machbase의 전체 성능을 떨어뜨릴 수 있으므로 주의하여 사용해야 한다.\n\nAPPEND 함수 설명\nSQLAppendOpen\nSQLRETURN SQLAppendOpen(SQLHSTMT   aStatementHandle,\n                        SQLCHAR   *aTableName,\n                        SQLINTEGER aErrorCheckCount );\n\n이 함수는 대상 테이블에 대한 채널을 오픈한다. 이후 이 채널을 닫아 주지 않으면 지속적으로 열린 상태가 유지된다.\n\n하나의 연결에 대해 최대 1024개의 Statement 설정이 가능하다. 각 Statement마다 SQLAppendOpen을 사용하면 된다.\n\n\n  aStatementHandle : Append를 수행할 Statement의 핸들을 나타낸다.\n  aTableName : Append를 수행할 대상 테이블의 이름을 나타낸다.\n  aErrorCheckCount : 몇 건의 데이터가 입력될 때 마다 서버의 에러를 검사할 것인지 결정한다. 이 값이 0일 경우에는 임의로 에러를 검사하지 않는다.\n\n\nSQLAppendData (deprecated)\nSQLRETURN  SQLAppendData(SQLHSTMT StatementHandle, void *aData[]);\n\n이 함수는 해당 채널에 대해 데이터를 입력하는 함수이다.\n\n\n  aData는 입력될 데이터의 포인터를 담고 있는 배열이다. 배열의 개수는 Open시에 지정한 테이블이 보유하고 있는 컬럼의 개수와 일치해야 한다.\n  리턴값은 SQL_SUCCESS, SQL_SUCCESS_WITH_INFO, SQL_ERROR가 가능하다. 특히, SQL_SUCCESS_WITH_INFO가 반환되었을 경우에는 입력된 특정 컬럼의 길이가 길어 잘리는 등의 오류가 있을 수 있으므로 결과를 다시 확인하여야 한다.\n\n\n데이터 타입에 따른 설정\n\n숫자형 및 문자형\n\n  float, double, short, int, long long, char * 과 같은 타입은 해당 값에 대한 포인터 설정 만으로 잘 동작한다.\n\n\n주소형\n\n  ipv4 의 경우에는 5 바이트 무부호 문자(unsigned char)의 배열로 넘긴다.\n  첫 번째 바이트는 4로 설정하고, 이후의 4바이트는 연속되는 주소값으로 설정한다.\n  예를 들어, 127.0.0.1의 경우에는 5바이트 배열 0x04, 0x7f, 0x00, 0x00, 0x01 의 순으로 들어가게 된다.\n\n\n// 4개의 컬럼 정보를 가지는 테이블의 경우 (short(16), int(32), long(64), varchar)\n \ntestAppendIPFunc()\n{\n   short val1 = 0;\n   int   val2 = 1;\n   long long  val3 = 2;  \n   char *val4 = \"my string\";\n   void *valueArray[4];\n \n   valueArray[0] = (void *)&amp;val1;\n   valueArray[1] = (void *)&amp;val2;\n   valueArray[2] = (void *)&amp;val3;\n   valueArray[3] = (void *)val4;\n \n   SQLAppendData(aStmt, valueArray);\n}\n\n\n데이터 타입에 따른 설정\n\ndatetime 형\n\n\n  Machbase 는 내부적으로 나노 단위 시간 해상도 값을 가지기 때문에 클라이언트에서 시간을 설정할 때는 변환과정을 거쳐야 하며, 64비트 부호없는 정수형 값으로 표현된다. 따라서 적절한 변환을 위해서는 유닉스 라이브러리인 mktime을 이용하여 초로 변환한 이후에 나노 값을 더해주어야 한다.\n  ※ Machbase의 시간 = (1970년 1월 1일 이후로부터의 총 시간 (초)) * 1,000,000,000 + mili-second * 1,000,000 + micro-second * 1000 + nano-second;\n\n\n// Date String이 \"연도-월-일 시:분:초 밀리:마이크로:나노\" 형태로 입력될 경우 코드\n \ntestAppendDateStrFunc(char *aDateString)\n{\n    int yy, int mm, int dd, int hh, int mi, int ss;\n    unsigned long t1;\n    void *valueArray[5];\n    sscanf(aDateString, \"%d-%d-%d %d:%d:%d %d:%d:%d\",\n        &amp;yy, &amp;mm, &amp;dd, &amp;hh, &amp;mi, &amp;ss, &amp;mmm, &amp;uuu, &amp;nnn);\n    sTm.tm_year = yy - 1900;\n    sTm.tm_mon = mm - 1;\n    sTm.tm_mday = dd;\n    sTm.tm_hour = hh;\n    sTm.tm_min = mi;\n    sTm.tm_sec = ss;\n    t1 = mktime(&amp;sTm);\n    t1 = t1 * 1000000000L;\n    t1 = t1 + (mmm*1000000L) + (uuu*1000) + nnn;\n \n    valueArray[4] = &amp;t1;\n    SQLAppendData(aStmt, valueArray);\n}\n\n\nSQLAppendDataByTime(deprecated)\n\nSQLRETURN  SQLAppendDataByTime(SQLHSTMT StatementHandle, SQLBIGINT aTime, void *aData[]);\n\n이 함수는 해당 채널에 대해 데이터를 입력하는 함수이며, DB에 저장되는 _arrival_time 값을 현재 시간이 아닌 특정 시간의 값으로 설정할 수 있다.\n\n예를 들면, 1개월전 로그 파일에 있는 날짜를 그 당시의 날짜로 입력하고자 할때 사용된다.\n\n\n  aTime은 _arrival_time으로 설정된 time 값이다.\n  aData는 입력될 데이터의 포인터를 담고 있는 배열이다.\n  배열의 개수는 Open시에 지정한 테이블이 보유하고 있는 컬럼의 개수와 일치해야 한다.\n\n\n나머지 사항은 SQLAppendData()함수를 참고하여 작성하면 된다.\n\n// 4개의 컬럼 정보를 가지는 테이블의 경우  (short(16), int(32), long(64), varchar)\n \ntestAppendFuncWithTime()\n{\n   long long sTime = 1;\n   short val1 = 0;\n   int   val2 = 1;\n   long long  val3 = 2;  \n   char *val4 = \"my string\";\n   void *valueArray[4];\n \n   valueArray[0] = (void *)&amp;val1;\n   valueArray[1] = (void *)&amp;val2;\n   valueArray[2] = (void *)&amp;val3;\n   valueArray[3] = (void *)val4;\n \n   SQLAppendDataByTime(aStmt, sTime, valueArray);\n}\n\n\nSQLAppendDataV2\n\nSQLRETURN  SQLAppendDataV2(SQLHSTMT StatementHandle, SQL_APPEND_PARAM *aData);\n\n\n이 함수는 Machbase 2.0 부터 새로 도입된 Append 함수로서, 기존의 함수에서 불편했던 입력 방식을 편리하게 대폭 개선한 함수이다.\n\n특히, 2.0에서 도입된 TEXT와 BINARY 타입의 경우는 SQLAppendDataV2() 함수에서만 입력이 가능하다.\n\n\n  각 타입에 맞는 NULL 입력 가능\n  VARCHAR 입력시 스트링 길이 입력 가능\n  IPv4, IPv6 입력시 바이너리 및 스트링 형태의 데이터 입력 가능\n  TEXT, BINARY 타입에 대한 데이터 길이 지정 가능\n\n\n함수 인자는 다음과 같이 구성된다.\n\n\n  aData는 SQL_APPEND_PARAM 이라는 인자배열을 가리키는 포인터이다. 이 배열의 개수는 Open시에 지정한 테이블이 보유하고 있는 컬럼의 개수와 일치해야 한다.\n  리턴값은 SQL_SUCCESS, SQL_SUCCESS_WITH_INFO, SQL_ERROR 가 가능하다. 특히, SQL_SUCCESS_WITH_INFO가 반환되었을 경우에는 입력된 특정 컬럼의 길이가 길어 잘리는 등의 오류가 있을 수 있으므로 결과를 다시 확인하여야 한다.\n\n\n아래는 실제로 V2에서 사용될 SQL_APPEND_PARAM 의 정의이며, 이 내용은 machbase_sqlcli.h 에 포함되어 있다.\n\ntypedef struct machAppendVarStruct\n{\n    unsigned int mLength;\n    void *mData;\n} machAppendVarStruct;\n \n/* for IPv4, IPv6 as bin or string representation */\ntypedef struct machbaseAppendIPStruct\n{\n    unsigned char   mLength; /* 0:null, 4:ipv4, 6:ipv6, 255:string representation */\n    unsigned char   mAddr[16];\n    char           *mAddrString;\n} machbaseAppendIPStruct;\n \n/* Date time*/\ntypedef struct machbaseAppendDateTimeStruct\n{\n    long long       mTime;\n#if defined(SUPPORT_STRUCT_TM)\n    struct tm       mTM;\n#endif\n    char           *mDateStr;\n    char           *mFormatStr;\n} machbaseAppendDateTimeStruct;\n \ntypedef union machbaseAppendParam\n{\n    short                        mShort;\n    unsigned short               mUShort;\n    int                          mInteger;\n    unsigned int                 mUInteger;\n    long long                    mLong;\n    unsigned long long           mULong;\n    float                        mFloat;\n    double                       mDouble;\n    machbaseAppendIPStruct       mIP;\n    machbaseAppendVarStruct      mVar;     /* for all varying type */\n    machbaseAppendVarStruct      mVarchar; /* alias */\n    machbaseAppendVarStruct      mText;    /* alias */\n    machbaseAppendVarStruct      mBinary;  /* binary */\n    machbaseAppendVarStruct      mBlob;    /* reserved alias */\n    machbaseAppendVarStruct      mClob;    /* reserved alias */\n    machbaseAppendDateTimeStruct mDateTime;\n} machbaseAppendParam;\n \n#define SQL_APPEND_PARAM machbaseAppendParam\n\n위에서 볼 수 있듯이 내부적으로 machbaseAppendParam 이라는 공용 구조체가 하나의 인자를 담고 있는 구조이다. 각 데이터 타입에 대해 데이터 및 스트링에 대한 길이 및 값을 명시적으로 입력할 수 있도록 되어 있다. 실제 사용 예는 다음과 같다.\n\n고정 길이 숫자형 타입의 입력\n\n고정 길이 숫자형 타입이라 함은 short, ushort, integer, uinteger, long, ulong, float, double 을 말한다. 이 타입의 경우 SQL_APPEND_PARAM의 구조체 멤버에 직접 값을 대입함으로써 입력 가능하다.\n\n\n  \n    \n      데이터베이스 타입\n      NULL 매크로\n      SQL_APPEND_PARAM 멤버\n    \n  \n  \n    \n      SHORT\n      SQL_APPEND_SHORT_NULL\n      mShort\n    \n    \n      USHORT\n      SQL_APPEND_USHORT_NULL\n      mUShort\n    \n    \n      INTEGER\n      SQL_APPEND_INTEGER_NULL\n      mInteger\n    \n    \n      UINTEGER\n      SQL_APPEND_UINTEGER_NULL\n      mUInteger\n    \n    \n      LONG\n      SQL_APPEND_LONG_NULL\n      mLong\n    \n    \n      ULONG\n      SQL_APPEND_ULONG_NULL\n      mULong\n    \n    \n      FLOAT\n      SQL_APPEND_FLOAT_NULL\n      mFloat\n    \n    \n      DOUBLE\n      SQL_APPEND_DOUBLE_NULL\n      mDouble\n    \n  \n\n\n다음은 실제 값을 입력하는 예제이다.\n\n// Table Schema가 8개의 컬럼이고, 각각 SHORT, USHORT, INTEGER, UINTEGER, LONG, ULONG, FLOAT, DOUBLE로 이루어진 것으로 가정한다.\n \nvoid testAppendExampleFunc()\n{\n    SQL_APPEND_PARAM sParam[8];\n \n    /* fixed column */\n    sParam[0].mShort = SQL_APPEND_SHORT_NULL;\n    sParam[1].mUShort = SQL_APPEND_USHORT_NULL;\n    sParam[2].mInteger = SQL_APPEND_INTEGER_NULL;\n    sParam[3].mUInteger = SQL_APPEND_UINTEGER_NULL;\n    sParam[4].mLong = SQL_APPEND_LONG_NULL;\n    sParam[5].mULong = SQL_APPEND_ULONG_NULL;\n    sParam[6].mFloat = SQL_APPEND_FLOAT_NULL;\n    sParam[7].mDouble = SQL_APPEND_DOUBLE_NULL;\n \n    SQLAppendDataV2(Stmt, sParam);\n \n    /* FIXED COLUMN Value */\n    sParam[0].mShort = 2;\n    sParam[1].mUShort = 3;\n    sParam[2].mInteger = 4;\n    sParam[3].mUInteger = 5;\n    sParam[4].mLong = 6;\n    sParam[5].mULong = 7;\n    sParam[6].mFloat = 8.4;\n    sParam[7].mDouble = 10.9;\n \n    SQLAppendDataV2(Stmt, sParam);\n}\n\n\n날짜형 타입의 입력\n\n아래는 DATETIME형의 데이터를 입력하는 예이다. 편의를 위해 몇가지의 매크로가 준비되어 있다.\n\nSQL_APPEND_PARAM에서 mDateTime 멤버에 대한 조작을 수행한다. 아래의 매크로는 mDateTime 구조체에서 mTime이라는 64비트 정수값에 대해 설정함으로써 날짜를 지정할 수 있다.\n\ntypedef struct machbaseAppendDateTimeStruct\n{\n    long long       mTime;\n#if defined(SUPPORT_STRUCT_TM)\n    struct tm       mTM;\n#endif\n    char           *mDateStr;\n    char           *mFormatStr;\n} machbaseAppendDateTimeStruct;\n\n\n\n  \n    \n      매크로\n      설명\n    \n  \n  \n    \n      SQL_APPEND_DATETIME_NOW\n      현재의 클라이언트 시간을 입력한다.\n    \n    \n      SQL_APPEND_DATETIME_STRUCT_TM\n      mDateTime의 struct tm 구조체인 mTM에 값을 설정하고, 그 값을 데이터베이스로 입력한다.\n    \n    \n      SQL_APPEND_DATETIME_STRING\n      mDateTime의 스트링형에 대한 값을 설정하고, 이를 데이터베이스로 입력한다. mDateStr : 실제 날짜 스트링 값이 할당 mFormatStr : 날짜 스트링에 대한 포맷 스트링 할당\n    \n    \n      SQL_APPEND_DATETIME_NULL\n      날짜 컬럼의 값을 NULL로 입력한다.\n    \n    \n      임의의 64비트 값\n      이 값이 실제 datetime으로 입력된다. 이 값을 1970년 1월 1일 이후로부터 나노세컨드 단위의 시간이 흐른 정수값을 나타낸다. 예를 들어, 만일 이 값이 10억 (1,000,000,000) 이라면, 1970년 1월 1일 0시 0분 1초를 나타낸다.(GMT)\n    \n  \n\n\n// 다음은 각각의 경우에 대해 실제 값을 입력하는 예제이다. 하나의 DATETIME 컬럼이 존재한다고 가정한다.\nvoid testAppendDateTimeFunc()\n{\n    SQL_mach_PARAM sParam[1];\n    /* NULL 입력 */\n    sParam[0].mDateTime.mTime   = SQL_APPEND_DATETIME_NULL;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* 현재 시간 입력 */\n    sParam[0].mDateTime.mTime      = SQL_APPEND_DATETIME_NOW;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* 임의의 값 입력 :1970.1.1일 이후로부터의 현재까지 나노세컨드의 값 */\n    sParam[0].mDateTime.mTime      = 1234;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /*  스트링 포맷 기준 입력 */\n    sParam[0].mDateTime.mTime      = SQL_APPEND_DATETIME_STRING;\n    sParam[0].mDateTime.mDateStr   = \"23/May/2014:17:41:28\";\n    sParam[0].mDateTime.mFormatStr = \"DD/MON/YYYY:HH24:MI:SS\";\n    SQLAppendDataV2(Stmt, sParam);\n \n    /*  struct tm의 값을 변경하여 입력 */\n    sParam[0].mDateTime.mTime      = SQL_APPEND_DATETIME_STRUCT_TM;\n    sParam[0].mDateTime.mTM.tm_year = 2000 - 1900;\n    sParam[0].mDateTime.mTM.tm_mon  =  11;\n    sParam[0].mDateTime.mTM.tm_mday  = 31;\n    SQLAppendDataV2(Stmt, sParam);\n}\n\n\n인터넷 주소형 타입의 입력\n\n아래는 IPv4와 IPv6 형의 데이터를 입력하는 예이다. 이 역시 편의를 위해 몇가지의 매크로가 준비되어 있다. SQL_APPEND_PARAM에서 mLength 멤버에 대한 조작을 수행한다.\n\n/* for IPv4, IPv6 as bin or string representation */\ntypedef struct machbaseAppendIPStruct\n{\n    unsigned char   mLength; /* 0:null, 4:ipv4, 6:ipv6, 255:string representation */\n    unsigned char   mAddr[16];\n    char           *mAddrString;\n} machbaseAppendIPStruct;\n\n\n\n  \n    \n      매크로 (mLength 에 설정)\n      설명\n    \n  \n  \n    \n      SQL_APPEND_IP_NULL\n      해당 컬럼에 NULL 값을 입력\n    \n    \n      SQL_APPEND_IP_IPV4\n      mAddr이 IPv4를 가지고 있음\n    \n    \n      SQL_APPEND_IP_IPV6\n      mAddr이 IPv6를 가지고 있음\n    \n    \n      SQL_APPEND_IP_STRING\n      mAddrString이 주소 문자열을 가지고 있음.\n    \n  \n\n\n다음은 각각의 경우에 대해 실제 값을 입력하는 예제이다.\n\nvoid testAppendIPFunc()\n{\n    SQL_APPEND_PARAM sParam[1];\n    /* NULL */\n    sParam[0].mIP.mLength  = SQL_APPEND_IP_NULL;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* 배열을 직접 수정 */\n    sParam[0].mIP.mLength  = SQL_APPEND_IP_IPV4;\n    sParam[0].mIP.mAddr[0] = 127;\n    sParam[0].mIP.mAddr[1] = 0;\n    sParam[0].mIP.mAddr[2] = 0;\n    sParam[0].mIP.mAddr[3] = 1;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* IPv4 from binary */\n    sParam[0].mIP.mLength  = SQL_APPEND_IP_IPV4;\n    *(in_addr_t *)(sParam[0].mIP.mAddr) = inet_addr(\"192.168.0.1\");\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* IPv4 : ipv4 from string */\n    sParam[0].mIP.mLength     = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = \"203.212.222.111\";\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* IPv4 : ipv4 from invalid string */\n    sParam[0].mIP.mLength     = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = \"ip address is not valid\";\n    SQLAppendDataV2(Stmt, sParam);                           // invalid IP value\n \n    /* IPv6 : ipv6 from binary bytes */\n    sParam[0].mIP.mLength  = SQL_APPEND_IP_IPV6;\n    sParam[0].mIP.mAddr[0]  = 127;\n    sParam[0].mIP.mAddr[1]  = 127;\n    sParam[0].mIP.mAddr[2]  = 127;\n    sParam[0].mIP.mAddr[3]  = 127;\n    sParam[0].mIP.mAddr[4]  = 127;\n    sParam[0].mIP.mAddr[5]  = 127;\n    sParam[0].mIP.mAddr[6]  = 127;\n    sParam[0].mIP.mAddr[7]  = 127;\n    sParam[0].mIP.mAddr[8]  = 127;\n    sParam[0].mIP.mAddr[9]  = 127;\n    sParam[0].mIP.mAddr[10] = 127;\n    sParam[0].mIP.mAddr[11] = 127;\n    sParam[0].mIP.mAddr[12] = 127;\n    sParam[0].mIP.mAddr[13] = 127;\n    sParam[0].mIP.mAddr[14] = 127;\n    sParam[0].mIP.mAddr[15] = 127;\n    SQLAppendDataV2(Stmt, sParam);\n \n    sParam[0].mIP.mLength     = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = \"::127.0.0.1\";\n    SQLAppendDataV2(Stmt, sParam);\n \n    sParam[0].mIP.mLength     = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = \"FFFF:FFFF:1111:2222:3333:4444:7733:2123\";\n    SQLAppendDataV2(Stmt, sParam);\n}\n\n\nIP 타입을 문자열 (STRING) 로 입력할경우 SQLAppendDataV2 이후에 각각 자료형에 맞게 mLength가 4 또는 6으로 바뀌게 된다.\n따라서 반복문에서 코딩할 경우 매번 SQLAppendDataV2() 전에, mLength 를 SQL_APPEND_IP_STRING 으로 지정해주어야한다.\n\n가변 데이터형(문자 및 이진 데이터) 입력\n\n가변 데이터 형에는 VARCHAR 및 TEXT 그리고, BLOB과 CLOB이 포함된다. 기존함수에서는 VARCHAR 만이 지원되었고, 또한 스트링의 길이를 사용자가 입력할 수 있는 방법이 없었다. 그런 이유로 매번 strlen() 함수를 통해 길이를 얻어야 했지만, 함수 V2 부터는 사용자가 직접 가변 데이터형에 대한 길이를 지정할 수 있게 되었다. 따라서, 만일 사용자가 그 길이를 미리 알고 있다면, 더 빠르게 데이터를 입력할 수 있다. 내부적으로는 가변 데이터형이 하나의 구조체로 되어 있지만, 개발 편의를 위해 각 데이터타입에 따라 멤버를 별도로 만들어 놓았다.\n\ntypedef struct machAppendVarStruct\n{\n    unsigned int mLength;\n    void *mData;\n} machAppendVarStruct;\n\n\n가변 데이터형의 입력시에는 데이터의 길이를 mLength에 설정하고, 원시 데이터 포인터를 mData로 설정하면 된다. 만일 mLength의 길이가 정의된 스키마보다 클 경우에는 자동으로 잘려서 입력된다. 이때 SQLAppendDataV2() 함수는 SQL_SUCCESS_WITH_INFO을 리턴하게 되고, 더불어 관련 경고 메시지를 내부 구조체에 채운다. 이 경고 메시지를 확인하기 위해서는 SQLError() 함수를 이용하면 된다.\n\n\n  \n    \n      데이터베이스 타입\n      NULL 매크로\n      SQL_APPEND_PARAM 멤버 (mVar를 사용해도 무방함)\n    \n  \n  \n    \n      VARCHAR\n      SQL_APPEND_VARCHAR_NULL\n      mVarchar\n    \n    \n      TEXT\n      SQL_APPEND_TEXT_NULL\n      mText\n    \n    \n      BINARY\n      SQL_APPEND_BINARY_NULL\n      mBinary\n    \n    \n      BLOB\n      SQL_APPEND_BLOB_NULL\n      mBlob\n    \n    \n      CLOB\n      SQL_APPEND_CLOB_NULL\n      mClob\n    \n  \n\n\n다음은 각각의 환경에 대해 실제 값을 입력하는 예제이다. 하나의 VARCHAR 컬럼이 존재한다고 가정한다.\n\nCREATE TABLE ttt (name VARCHAR(10));\n\n\n\nvoid testAppendVarcharFunc()\n{\n    SQL_mach_PARAM sParam[1];\n \n    /*  VARCHAR : NULL */\n    sParam[0].mVarchar.mLength = SQL_APPEND_VARCHAR_NULL\n    SQLAppendDataV2(Stmt, sParam); /* OK */\n \n    /*  VARCHAR : string */\n    strcpy(sVarchar, \"MY VARCHAR\");\n    sParam[0].mVarchar.mLength = strlen(sVarchar);\n    sParam[0].mVarchar.mData   = sVarchar;\n    SQLAppendDataV2(Stmt, sParam); /* OK */\n \n    /*  VARCHAR : Truncation! */\n    strcpy(sVarchar, \"MY VARCHAR9\"); /* Truncation! */\n    sParam[0].mVarchar.mLength = strlen(sVarchar);\n    sParam[0].mVarchar.mData   = sVarchar;\n    SQLAppendDataV2(Stmt, sParam);  /* SQL_SUCCESS_WITH_INFO */\n}\n\n\n다음은 Text 타입에 대한 입력 예제이다.\n\nCREATE TABLE ttt (doc TEXT);\n\n\nvoid testAppendFunc()\n{\n    SQL_mach_PARAM sParam[1];\n \n    /*  VARCHAR : NULL */\n    sParam[0].mText.mLength = SQL_APPEND_TEXT_NULL\n    SQLAppendDataV2(Stmt, sParam); /* OK */\n \n    /*  VARCHAR : string */\n    strcpy(sText, \"This is the sample document for tutorial.\");\n    sParam[0].mVar.mLength = strlen(sText);\n    sParam[0].mVar.mData   = sText;\n    SQLAppendDataV2(Stmt, sParam); /* OK */\n}\n\n\nSQLAppendDataByTimeV2\n\nSQLRETURN  SQLAppendDataByTimeV2(SQLHSTMT StatementHandle, SQLBIGINT aTime, SQL_APPEND_PARAM  *aData);\n\n\n이 함수는 해당 채널에 대해 데이터를 입력하는 함수이며, DB에 저장되는 _arrival_time 값을 현재 시간이 아닌 특정 시간의 값으로 설정할 수 있다. 예를 들면, 1개월전 로그 파일에 있는 날짜를 그 당시의 날짜로 입력하고자 할때 사용된다.\n\n\n  aTime은 _arrival_time으로 설정될 time값이다. 1970년 1월 1일 이후로부터의 현재까지 nano second 값을 입력해야 한다. 또한 입력되는 값이 과거부터 현재순으로 순차적으로 정렬되어 있어야 한다.\n  aData는 입력될 데이터의 포인터를 담고 있는 배열이다. 배열의 개수는 Open시에 지정한 테이블이 보유하고 있는 컬럼의 개수와 일치해야 한다.\n\n\n나머지 사항은 SQLAppendDataV2()함수를 참고하여 작성하면 된다.\n\n### SQLAppendFlush\n\n SQLRETURN SQLAppendFlush(SQLHSTMT StatementHandle);\n\n\n이 함수는 현재 채널 버퍼에 쌓여있는 데이터를 Machbase 서버로 즉시 전송한다.\n\n### SQLAppendClose\n\n SQLRETURN SQLAppendClose(SQLHSTMT   aStmtHandle,\n                         SQLBIGINT* aSuccessCount,\n                         SQLBIGINT* aFailureCount);\n\n\n이 함수는 현재 열린 채널을 닫는다. 만일 열려지지 않은 채널이 존재할 경우 에러가 발생한다.\n\n\n  aSuccessCount : Append를 성공한 레코드 개수 값을 가진다.\n  aFailureCount : Append를 실패한 레코드 개수 값을 가진다.\n\n\nSQLAppendSetErrorCallback\n\nSQLRETURN SQLAppendSetErrorCallback(SQLHSTMT aStmtHandle, SQLAppendErrorCallback aFunc);\n\n\n이 함수는 SQLAppendOpen()이 성공한 다음 Append시 에러가 발생했을 때 호출되는 콜백 함수를 설정한다. 만일 이 함수를 설정하지 않을 경우에는 서버에 에러가 발생하더라도, 클라이언트에서는 무시하게 된다.\n\n\n  aStmtHandle : 에러를 확인할 Statement를 지정한다.\n  aFunc : Append 실패시 호출할 함수 포인터를 지정한다.\n\n\nSQLAppendErrorCallback의 프로토타입은 다음과 같다.\n\ntypedef void (*SQLAppendErrorCallback)(SQLHSTMT aStmtHandle,\n                                     SQLINTEGER aErrorCode,\n                                     SQLPOINTER aErrorMessage,\n                                         SQLLEN aErrorBufLen,\n                                     SQLPOINTER aRowBuf,\n                                         SQLLEN aRowBufLen);\n\n\n\n  aStatementHandle : 에러를 발생한 Statement 핸들\n  aErrorCode : 에러의 원인이 된 32비트 에러 코드\n  aErrorMessage : 해당 에러코드에 대한 문자열\n  aErrorBufLen : aErrorMessage의 길이\n  aRowBuf : 에러를 발생시킨 레코드의 상세 명세가 담긴 문자열\n  aRowBufLen : aRowBuf의 길이\n\n\n에러 콜백(dumpError)의 사용 예\n\nvoid dumpError(SQLHSTMT    aStmtHandle,\n               SQLINTEGER  aErrorCode,\n               SQLPOINTER  aErrorMessage,\n               SQLLEN      aErrorBufLen,\n               SQLPOINTER  aRowBuf,\n               SQLLEN      aRowBufLen)\n{\n    char       sErrMsg[1024] = {0, };\n    char       sRowMsg[32 * 1024] = {0, };\n \n    if (aErrorMessage != NULL)\n    {\n        strncpy(sErrMsg, (char *)aErrorMessage, aErrorBufLen);\n    }\n \n    if (aRowBuf != NULL)\n    {\n        strncpy(sRowMsg, (char *)aRowBuf, aRowBufLen);\n    }\n \n    fprintf(stdout, \"Append Error : [%d][%s]\\n[%s]\\n\\n\", aErrorCode, sErrMsg, sRowMsg);\n}\n \n \n......\n \n    if( SQLAppendOpen(m_IStmt, TableName, aErrorCheckCount) != SQL_SUCCESS )\n    {\n        fprintf(stdout, \"SQLAppendOpen error\\n\");\n        exit(-1);\n    }\n    // 콜백을 설정한다.\n    assert(SQLAppendSetErrorCallback(m_IStmt, dumpError) == SQL_SUCCESS);\n \n    doAppend(sMaxAppend);\n \n    if( SQLAppendClose(m_IStmt, &amp;sSuccessCount, &amp;sFailureCount) != SQL_SUCCESS )\n    {\n        fprintf(stdout, \"SQLAppendClose error\\n\");\n        exit(-1);\n    }\n}\n\n\nSQLSetConnectAppendFlush\n\nSQLRETURN SQL_API SQLSetConnectAppendFlush(SQLHDBC hdbc, SQLINTEGER option)\n\n\nAppend에 의해서 입력된 데이터는 통신 버퍼에 기록되어 전송대기 상태에서 사용자가 SQLAppendFlush 함수를 호출하거나 통신 버퍼가 가득 차게 되면 서버로 전송된다. 사용자가 버퍼가 가득 차 있지 않아도 일정 주기로 서버에게 Append에 의한 데이터를 전송하게 하려면 이 함수를 이용하면 된다. 이 함수는 매 100ms 주기로 마지막으로 전송한 시간과 현재 시간의 차이를 계산하여 지정된 시간(설정하지 않은 경우에는 1초)가 지난 경우 통신 버퍼의 내용을 서버에 전달한다.\n\n매개변수는 다음과 같다.\n\n\n  hdbc : DB의 connection handle이다.\n  option : 0이면 auto flush를 off, 0이 아닌 값이면 auto flush를 on으로 한다.\n\n\n연결되지 않은 hdbc에 대해서 실행하면 오류로 처리된다.\n\nSQLSetStmtAppendInterval\n\nSQLRETURN SQL_API SQLSetStmtAppendInterval(SQLHSTMT hstmt, SQLINTEGER fValue)\n\n\nSQLSetConnectAppendFlush를 이용해서 시간 단위 flush기능을 켰을 경우, 특정 statement에 대해서는 자동 flush를 끄거나 flush 주기를 조정하고 싶을 경우 이 함수를 사용한다.\n\n매개변수는 다음과 같다.\n\n\n  hstmt : flush주기를 조정하고자 하는 statement handle이다.\n  fValue : flush주기를 조정하고자 하는 값이다. 0이면 flush를 하지 않으며 단위는 ms이다. 100ms마다 flush할지를 결정하는 스레드가 실행되므로 100의 배수로 설정한다. 정확히 원하는 시점에 자동 flush가 실행되지는 않는다. 1000이 기본 값이다.\n\n\n시간 기반 flush가 실행중이지 않은 경우라도 이 함수의 실행은 성공한다.\n\nError 확인 및 설명\n\nAppend 관련 함수를 사용할때 에러를 확인하는 방법과 코드에 대한 설명이다. CLI 함수에서 return 값이 SQL_SUCCESS가 아닌 경우 아래 코드를 이용하여 에러 메시지를 확인할 수 있다.\n\nSQLINTEGER errNo;\nint msgLength;\nchar sqlState[6];\nchar errMsg[1024];\n \nif (SQL_SUCCESS == SQLError ( env, con, stmt, (SQLCHAR *)sqlState, &amp;errNo,\n                              (SQLCHAR *)errMsg, 1024, &amp;msgLength ))\n{\n    //error code값을 5자리 숫자로 지정한다.\n    printf(\"ERROR-%05d: %s\\n\", errNo, errMsg);\n}\n\n\nAppend관련 함수에서 리턴되는 에러 메시지는 아래와 같다.\n\n\n  \n    \n      함수\n      message\n      설명\n    \n  \n  \n    \n      SQLAppendOpen\n      statement is already opened.\n      중복으로 SQLAppendOpen을 하는 경우 발생함.\n    \n    \n       \n      Failed to close stream protocol.\n      스트림 프로토콜 종료에 실패함.\n    \n    \n       \n      Failed to read protocol.\n      네트워크 읽기 오류가 발생함.\n    \n    \n       \n      cannot read column meta.\n      column meta 정보 구조가 잘못됨\n    \n    \n       \n      cannot allocate memory.\n      내부 버퍼 메모리 할당 오류가 발생함.\n    \n    \n       \n      cannot allocate compress memory.\n      압축 버퍼 메모리 할당 오류가 발생함.\n    \n    \n       \n      invalid return after reading column meta.\n      return값에 오류가 있음.\n    \n    \n      SQLAppendData\n      statement is not opened.\n      AppendOpen을 하지 않고 AppendData를 call함.\n    \n    \n       \n      column() truncated :\n      varchar 타입 컬럼에 지정된 사이즈 보다 큰 데이터를 입력하는 경우 발생함.\n    \n    \n       \n      Failed to add binary.\n      통신버퍼에 쓰기 오류가 발생함.\n    \n    \n      SQLAppendClose\n      statement is not opened.\n      AppendOpen상태가 아님.\n    \n    \n       \n      Failed to close stream protocol.\n      스트림 프로토콜 종료에 실패함.\n    \n    \n       \n      Failed to close buffer protocol.\n      버퍼 프로토콜 종료에 실패함.\n    \n    \n       \n      cannot read column meta.\n      column meta정보 구조가 잘못됨.\n    \n    \n       \n      invalid return after reading column meta.\n      return값에 오류가 있음.\n    \n    \n      SQLAppendFlush\n      statement is not opened.\n      AppendOpen상태가 아님\n    \n  \n\n\n열 형식 매개변수 바인딩\n\n이를 위해서 Machbase 5.5 이후 버전에서는 열 형색 매개변수 바인딩을 지원한다. (행 형식 매개변수 바인딩은 아직 지원되지 않는다.)\n\n함수 SQLSetStmtAttr()의 인자 Attribute에 SQL_ATTR_PARAM_BIND_TYPE을 설정하고 인자 param에 SQL_PARAM_BIND_BY_COLUMN을 설정한다. 바인드할 각  칼럼에 대해서 매개변수를 배열로 설정하고, 지시자 변수 또한 배열로 설정한다. 이후 SQLBindParameter()를 이 매개변수를 전달하여 호출한다.\n\n아래 그림은 각 매개변수 배열에 대해 열 형식 바인딩이 동작하는 방식을 보여준다.\n\n\n  \n    \n      Column A(parameter A)\n      Column B(parameter B)\n      Column C(parameter C)\n    \n  \n  \n    \n      Value_ArrayIndicator/length array\n      Value_ArrayIndicator/length array\n      Value_ArrayIndicator/length array\n    \n  \n\n\n아래 예제는 열 형식 매개변수 바인딩을 이용하여 대량의 데이터를 삽입하는 예제이다.\n\n#define DESC_LEN 51\n#define ARRAY_SIZE 10\nSQLCHAR * Statement = \"INSERT INTO Parts (PartID, Description, Price) VALUES (?, ?, ?)\";\n \n/* 바인드할 매개변수 배열 */\nSQLUINTEGER PartIDArray[ARRAY_SIZE];\nSQLCHAR DescArray[ARRAY_SIZE][DESC_LEN];\nSQLREAL PriceArray[ARRAY_SIZE];\n/* 바인드할 지사자 변수 배열 */\nSQLINTEGER PartIDIndArray[ARRAY_SIZE], DescLenOrIndArray[ARRAY_SIZE], PriceIndArray[ARRAY_SIZE];\nSQLUSMALLINT i, ParamStatusArray[ARRAY_SIZE];\nSQLUINTEGER ParamsProcessed;\n \n// Set the SQL_ATTR_PARAM_BIND_TYPE statement attribute to use\n// column-wise binding.\nSQLSetStmtAttr(hstmt, SQL_ATTR_PARAM_BIND_TYPE, SQL_PARAM_BIND_BY_COLUMN, 0);\n// Specify the number of elements in each parameter array.\nSQLSetStmtAttr(hstmt, SQL_ATTR_PARAMSET_SIZE, ARRAY_SIZE, 0);\n// Specify an array in which to return the status of each set of\n// parameters.\nSQLSetStmtAttr(hstmt, SQL_ATTR_PARAM_STATUS_PTR, ParamStatusArray, 0);\n// Specify an SQLUINTEGER value in which to return the number of sets of\n// parameters processed.\nSQLSetStmtAttr(hstmt, SQL_ATTR_PARAMS_PROCESSED_PTR, &amp;ParamsProcessed, 0);\n// Bind the parameters in column-wise fashion.\nSQLBindParameter(hstmt, 1, SQL_PARAM_INPUT, SQL_C_ULONG, SQL_INTEGER, 5, 0,\n    PartIDArray, 0, PartIDIndArray);\nSQLBindParameter(hstmt, 2, SQL_PARAM_INPUT, SQL_C_CHAR, SQL_CHAR, DESC_LEN - 1, 0,\n    DescArray, DESC_LEN, DescLenOrIndArray);\nSQLBindParameter(hstmt, 3, SQL_PARAM_INPUT, SQL_C_FLOAT, SQL_REAL, 7, 0,\n    PriceArray, 0, PriceIndArray);\n\n\n지원되는 문자열\n\n마크베이스는 기본적으로 UTF-8 방식을 사용하여 문자열 데이터를 저장한다.\n\nUTF-8 이외의 방식으로 문자열을 입/출력하는 Windows의 경우 ODBC에서 아래와 같이 변환한다.\n\n\n  \n    \n      OS\n      Unicode/Non-Unicode\n      문자열 변환\n      Note\n    \n  \n  \n    \n      Windows\n      Unicode (UTF-16)\n      UTF-16 ⟷ UTF-8\n      N/A\n    \n    \n      Windows\n      Non-Unicode (MBCS)\n      MBCS ⟷ UTF-8\n      Windows 설정의 Non-Unicode 어플리케이션의 기본 문자열을 사용함\n    \n    \n      Linux\n      UTF-8\n      N/A\n      UTF-8 만 지원됨"
					}
					
				
		
				
					,
					
					"sdk-cli-odbc-example-html": {
						"id": "sdk-cli-odbc-example-html",
						"title": "CLI/ODBC 예제",
						"version": "all",
						"categories": "",
						"url": " /sdk/cli_odbc_example.html",
						"content": "목차\n\n\n  목차\n  응용 프로그램 개발\n    \n      CLI 설치 확인\n      Makefile 작성 가이드\n      컴파일 및 링크\n    \n  \n  샘플 프로그램\n    \n      접속 예제\n      데이터 입력 및 출력 예제\n      Prepare Execute 예제\n      확장 함수 Append 예제\n      테이블 열 정보 획득 예제\n        \n          SQLDescribeCol\n          SQLColumns\n        \n      \n    \n  \n  멀티 쓰레드 append 예제\n\n\n응용 프로그램 개발\n\nCLI 설치 확인\n\n마크베이스가 설치된 디렉터리의 include 및 lib에 다음과 같은 파일이 있으면 응용 프로그램을 개발할 수 있는 환경이 완비된 것이다.\n\nMach@localhost:~/machbase_home$ ls -l include lib install/\ninclude:\ntotal 176\n-rwxrwxr-x 1 mach mach 31449 Jun 18 19:26 machbase_sqlcli.h\n \ninstall/:\ntotal 12\n-rw-rw-r-- 1 mach mach 1667 Jun 18 19:26 machbase_env.mk\n \nlib:\ntotal 16196\n-rw-rw-r-- 1 mach mach  78603 Jun 18 19:26 machbase.jar\n-rw-rw-r-- 1 mach mach 964290 Jun 18 19:26 libmachbasecli.a\n\n\nMakefile 작성 가이드\n\nmach@localhost:~/machbase_home$ cd sample/\nmach@localhost:~/machbase_home/sample$ cd cli/\nmach@localhost:~/machbase_home/sample/cli$ ls\nMakefile sample1_connect.c\n\n마크베이스 패키지를 설치했다면, 다음 경로에 샘플 프로그램이 설치되어 있을 것이다.\n\ninclude $(MACHBASE_HOME)/install/machbase_env.mk\nINCLUDES += $(LIBDIR_OPT)/$(MACHBASE_HOME)/include\n \nall : sample1_connect\n \nsample1_connect : sample1_connect.o\n$(LD_CC) $(LD_FLAGS) $(LD_OUT_OPT)$@ $&lt; $(LIB_OPT)machbasecli$(LIB_AFT) $(LIBDIR_OPT)$(MACHBASE_HOME)/lib $(LD_LIBS)\n \nsample1_connect.o : sample1_connect.c\n$(COMPILE.cc) $(CC_FLAGS) $(INCLUDES) $(CC_OUT_OPT)$@ $&lt;\n \nclean :\nrm -f sample1_connect\n\n\n컴파일 및 링크\n\n주어진 샘플에 대해 다음과 같이 수행하면 실행 파일이 만들어진다.\n\nmach@localhost:~/machbase_home/sample/cli$ make\ngcc -c -g -W -Wall -rdynamic -O3 -finline-functions -fno-omit-frame-pointer -fno-strict-aliasing -m64 -mtune=k8 -g -W -Wall -rdynamic -O3 -finline-functions -fno-omit-frame-pointer -fno-strict-aliasing -m64 -mtune=k8 -I/home/machbase/machbase_home/include -I. -L//home/machbase/machbase_home/include -osample1_connect.o sample1_connect.c\ngcc -m64 -mtune=k8 -L/home/machbase/machbase_home/lib -osample1_connect sample1_connect.o -lmachbasecli -L/home/machbase/machbase_home/lib -lm -lpthread -ldl -lrt -rdynamic\nmach@localhost:~/machbase_home/sample/cli$ ls -al\ntotal 1196\ndrwxrwxr-x 2 mach mach 4096 Jun 18 20:15 .\ndrwxrwxr-x 4 mach mach 4096 Jun 18 19:26 ..\n-rw-rw-r-- 1 mach mach 483 Jun 18 19:26 Makefile\n-rwxrwxr-x 1 mach mach 1196943 Jun 18 20:15 sample1_connect\n-rw-rw-r-- 1 mach mach 549 Jun 18 19:26 sample1_connect.c\n-rw-rw-r-- 1 mach mach 8168 Jun 18 20:15 sample1_connect.o\n\n필요에 따라 얼마든지 위의 샘플 Makefile을 수정하여 응용 프로그램을 작성할 수 있을 것이다.\n\n샘플 프로그램\n\n접속 예제\n\nCLI를 이용하여 접속하는 예제 프로그램을 작성해 보기로 한다.\n\n샘플 파일명은 sample1_connect.c 로 한다.\n\nMACHBASE_PORT_NO는 $MACHBASE_HOME/conf/machbase.conf 파일에 있는 PORT_NO 값과 같아야 한다.\n\n\nsample1_connect.c\n\n\n    #include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;machbase_sqlcli.h&gt;\n \n#define MACHBASE_PORT_NO 5656\n \nSQLHENV gEnv;\nSQLHDBC gCon;\nSQLHSTMT gStmt;\n \nvoid connectDB()\n{\n    char connStr[1024];\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n \n    if (SQL_ERROR == SQLAllocEnv(&amp;gEnv)) {\n        printf(\"SQLAllocEnv error!!\\n\");\n        exit(1);\n    }\n    if (SQL_ERROR == SQLAllocConnect(gEnv, &amp;gCon)) {\n        printf(\"SQLAllocConnect error!!\\n\");\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n    sprintf(connStr,\"SERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n    if (SQL_ERROR == SQLDriverConnect( gCon, NULL,\n                                       (SQLCHAR *)connStr,\n                                       SQL_NTS,\n                                       NULL, 0, NULL,\n                                       SQL_DRIVER_NOPROMPT ))\n    {\n        printf(\"connection error\\n\");\n        if (SQL_SUCCESS == SQLError ( gEnv, gCon, NULL, NULL, &amp;errNo,\n                                      errMsg, 1024, &amp;msgLength ))\n        {\n            printf(\"mach-%d : %s\\n\", errNo, errMsg);\n        }\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n    printf(\"connected ... \\n\");\n}\n \nvoid disconnectDB()\n{\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n \n    if (SQL_ERROR == SQLDisconnect(gCon))\n    {\n        printf(\"disconnect error\\n\");\n \n        if( SQL_SUCCESS == SQLError( gEnv, gCon, NULL, NULL, &amp;errNo,\n                                     errMsg, 1024, &amp;msgLength ))\n        {\n            printf(\"mach-%d : %s\\n\", errNo, errMsg);\n        }\n    }\n \n    SQLFreeConnect(gCon);\n    SQLFreeEnv(gEnv);\n}\n \nint main()\n{\n    connectDB();\n    disconnectDB();\n    return 0;\n}\n    \n\n  \n\n\nMakefile에 sample1_connect.c를 등록하고 컴파일하여 실행하면 다음과 같이 나온다.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample1_connect\nconnected ...\n\n\n데이터 입력 및 출력 예제\n\n아래의 예제 소스에서는 CREATE TABLE 구문을 이용하여 테이블을 생성하고, 간단한 데이터 값들을 임의로 생성해서 INSERT 구문을 사용해서 데이터를 입력하고, SELECT 구문을 이용하여 데이터를 출력한다. 이를 활용하여 직접 값을 입력하고 확인할 때 각 타입별로 어떻게 설정을 해야 하는지 알수 있을 것이다.\n\n샘플 파일명은 sample2_insert.c 라고 한다.\n\n\nsampe2_insert.c\n\n\n    #include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;machbase_sqlcli.h&gt;\n \n#define MACHBASE_PORT_NO 5656\n \nSQLHENV gEnv;\nSQLHDBC gCon;\nSQLHSTMT gStmt;\nSQLCHAR gErrorState[6];\n \nvoid connectDB()\n{\n    char connStr[1024];\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n    if (SQL_ERROR == SQLAllocEnv(&amp;gEnv)) {\n        printf(\"SQLAllocEnv error!!\\n\");\n        exit(1);\n    }\n    if (SQL_ERROR == SQLAllocConnect(gEnv, &amp;gCon)) {\n        printf(\"SQLAllocConnect error!!\\n\");\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n    sprintf(connStr,\"SERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n    if (SQL_ERROR == SQLDriverConnect( gCon, NULL,\n                                       (SQLCHAR *)connStr,\n                                       SQL_NTS,\n                                       NULL, 0, NULL,\n                                       SQL_DRIVER_NOPROMPT ))\n    {\n        printf(\"connection error\\n\");\n        if (SQL_SUCCESS == SQLError ( gEnv, gCon, NULL, NULL, &amp;errNo,\n                                      errMsg, 1024, &amp;msgLength ))\n        {\n            printf(\" mach-%d : %s\\n\", errNo, errMsg);\n        }\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n    printf(\"connected ... \\n\");\n}\n \nvoid disconnectDB()\n{\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n    if (SQL_ERROR == SQLDisconnect(gCon)) {\n        printf(\"disconnect error\\n\");\n        if( SQL_SUCCESS == SQLError( gEnv, gCon, NULL, NULL, &amp;errNo,\n                                     errMsg, 1024, &amp;msgLength ))\n        {\n            printf(\" mach-%d : %s\\n\", errNo, errMsg);\n        }\n    }\n    SQLFreeConnect(gCon);\n    SQLFreeEnv(gEnv);\n}\n \nvoid outError(const char *aMsg, SQLHSTMT stmt)\n{\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n    printf(\"ERROR : (%s)\\n\", aMsg);\n    if (SQL_SUCCESS == SQLError( gEnv, gCon, stmt, NULL, &amp;errNo,\n                                 errMsg, 1024, &amp;msgLength ))\n    {\n        printf(\" mach-%d : %s\\n\", errNo, errMsg);\n    }\n    exit(-1);\n}\n \nvoid executeDirectSQL(const char *aSQL, int aErrIgnore)\n{\n    SQLHSTMT stmt;\n    if (SQLAllocStmt(gCon, &amp;stmt) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"AllocStmt error\", stmt);\n    }\n    if (SQLExecDirect(stmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        printf(\"sql_exec_direct error[%s] \\n\", aSQL);\n        outError(\"sql_exec_direct error\", stmt);\n    }\n    if (SQL_ERROR == SQLFreeStmt(stmt, SQL_DROP))\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"FreeStmt Error\", stmt);\n    }\n}\n \nvoid prepareExecuteSQL(const char *aSQL)\n{\n    SQLHSTMT stmt;\n    if (SQLAllocStmt(gCon, &amp;stmt) == SQL_ERROR)\n    {\n        outError(\"AllocStmt error\", stmt);\n    }\n    if (SQLPrepare(stmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR)\n    {\n        printf(\"Prepare error[%s]\\n\", aSQL);\n        outError(\"Prepare error\", stmt);\n    }\n    if (SQLExecute(stmt) == SQL_ERROR)\n    {\n        outError(\"prepared execute error\", stmt);\n    }\n    if (SQL_ERROR == SQLFreeStmt(stmt, SQL_DROP))\n    {\n        outError(\"FreeStmt Error\", stmt);\n    }\n}\n \nvoid createTable()\n{\n    executeDirectSQL(\"DROP TABLE CLI_SAMPLE1\", 1);\n    executeDirectSQL(\"CREATE TABLE CLI_SAMPLE1(seq short, score integer, total long, percentage float, ratio double, id varchar(10), srcip ipv4, dstip ipv6, reg_date datetime, textlog text, image binary)\", 0);\n}\n \nvoid selectTable()\n{\n    SQLHSTMT stmt;\n    const char *aSQL = \"SELECT seq, score, total, percentage, ratio, id, srcip, dstip, reg_date, textlog, image FROM CLI_SAMPLE1\";\n    int i=0;\n    SQLLEN Len = 0;\n    short seq;\n    int score;\n    long total;\n    float percentage;\n    double ratio;\n    char id [11];\n    char srcip[16];\n    char dstip[40];\n    SQL_TIMESTAMP_STRUCT regdate;\n    char log [1024];\n    char image[1024];\n    if (SQLAllocStmt(gCon, &amp;stmt) == SQL_ERROR) {\n        outError(\"AllocStmt Error\", stmt);\n    }\n    if (SQLPrepare(stmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR) {\n        printf(\"Prepare error[%s] \\n\", aSQL);\n        outError(\"Prepare error\", stmt);\n    }\n    if (SQLExecute(stmt) == SQL_ERROR) {\n        outError(\"prepared execute error\", stmt);\n    }\n    SQLBindCol(stmt, 1, SQL_C_SHORT, &amp;seq, 0, &amp;Len);\n    SQLBindCol(stmt, 2, SQL_C_LONG, &amp;score, 0, &amp;Len);\n    SQLBindCol(stmt, 3, SQL_C_BIGINT, &amp;total, 0, &amp;Len);\n    SQLBindCol(stmt, 4, SQL_C_FLOAT, &amp;percentage, 0, &amp;Len);\n    SQLBindCol(stmt, 5, SQL_C_DOUBLE, &amp;ratio, 0, &amp;Len);\n    SQLBindCol(stmt, 6, SQL_C_CHAR, id, sizeof(id), &amp;Len);\n    SQLBindCol(stmt, 7, SQL_C_CHAR, srcip, sizeof(srcip), &amp;Len);\n    SQLBindCol(stmt, 8, SQL_C_CHAR, dstip, sizeof(dstip), &amp;Len);\n    SQLBindCol(stmt, 9, SQL_C_TYPE_TIMESTAMP, &amp;regdate, 0, &amp;Len);\n    SQLBindCol(stmt, 10, SQL_C_CHAR, log, sizeof(log), &amp;Len);\n    SQLBindCol(stmt, 11, SQL_C_CHAR, image, sizeof(image), &amp;Len);\n    while (SQLFetch(stmt) == SQL_SUCCESS)\n    {\n        printf(\"===== %d ========\\n\", i++);\n        printf(\"seq = %d\", seq);\n        printf(\", score = %d\", score);\n        printf(\", total = %ld\", total);\n        printf(\", percentage = %.2f\", percentage);\n        printf(\", ratio = %g\", ratio);\n        printf(\", id = %s\", id);\n        printf(\", srcip = %s\", srcip);\n        printf(\", dstip = %s\", dstip);\n        printf(\", regdate = %d-%02d-%02d %02d:%02d:%02d\",\n               regdate.year, regdate.month, regdate.day,\n               regdate.hour, regdate.minute, regdate.second);\n        printf(\", log = %s\", log);\n        printf(\", image = %s\\n\", image);\n    }\n    if (SQL_ERROR == SQLFreeStmt(stmt, SQL_DROP))\n    {\n        outError(\"FreeStmt eror\", stmt);\n    }\n}\n \nvoid directInsert()\n{\n    int i;\n    char query[2 * 1024];\n    short seq;\n    int score;\n    long total;\n    float percentage;\n    double ratio;\n    char id [11];\n    char srcip [16];\n    char dstip [40];\n    char reg_date [40];\n    char log [1024];\n    char image [1024];\n    for(i=1; i&lt;10; i++)\n    {\n        seq = i;\n        score = i+i;\n        total = (seq + score) * 10000;\n        percentage = (float)score/total;\n        ratio = (double)seq/total;\n        sprintf(id, \"id-%d\", i);\n        sprintf(srcip, \"192.168.0.%d\", i);\n        sprintf(dstip, \"2001:0DB8:0000:0000:0000:0000:1428:%04d\", i);\n        sprintf(reg_date, \"2015-03-31 15:26:%02d\", i);\n        sprintf(log, \"text log-%d\", i);\n        sprintf(image, \"binary image-%d\", i);\n        memset(query, 0x00, sizeof(query));\n        sprintf(query, \"INSERT INTO CLI_SAMPLE1 VALUES(%d, %d, %ld, %f, %f, '%s', '%s', '%s',TO_DATE('%s','YYYY-MM-DD HH24:MI:SS'),'%s','%s')\",\n                seq, score, total, percentage, ratio, id, srcip, dstip, reg_date, log, image);\n        prepareExecuteSQL(query);\n        printf(\"%d record inserted\\n\", i);\n    }\n}\n \nint main()\n{\n    connectDB();\n    createTable();\n    directInsert();\n    selectTable();\n    disconnectDB();\n    return 0;\n}\n    \n  \n\n\nMakefile에 sample2_insert.c를 등록하고 컴파일하여 실행하면 다음과 같이 나온다.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample2_insert\n \nconnected ...\n1 record inserted\n2 record inserted\n3 record inserted\n4 record inserted\n5 record inserted\n6 record inserted\n7 record inserted\n8 record inserted\n9 record inserted\n===== 0 ========\nseq = 9, score = 18, total = 270000, percentage = 0.00, ratio = 3.3e-05, id = id-9, srcip = 192.168.0.9, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0009, regdate = 2015-03-31 15:26:09, log = text log-9, image = 62696E61727920696D6167652D39\n===== 1 ========\nseq = 8, score = 16, total = 240000, percentage = 0.00, ratio = 3.3e-05, id = id-8, srcip = 192.168.0.8, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0008, regdate = 2015-03-31 15:26:08, log = text log-8, image = 62696E61727920696D6167652D38\n===== 2 ========\nseq = 7, score = 14, total = 210000, percentage = 0.00, ratio = 3.3e-05, id = id-7, srcip = 192.168.0.7, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0007, regdate = 2015-03-31 15:26:07, log = text log-7, image = 62696E61727920696D6167652D37\n===== 3 ========\nseq = 6, score = 12, total = 180000, percentage = 0.00, ratio = 3.3e-05, id = id-6, srcip = 192.168.0.6, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0006, regdate = 2015-03-31 15:26:06, log = text log-6, image = 62696E61727920696D6167652D36\n===== 4 ========\nseq = 5, score = 10, total = 150000, percentage = 0.00, ratio = 3.3e-05, id = id-5, srcip = 192.168.0.5, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0005, regdate = 2015-03-31 15:26:05, log = text log-5, image = 62696E61727920696D6167652D35\n===== 5 ========\nseq = 4, score = 8, total = 120000, percentage = 0.00, ratio = 3.3e-05, id = id-4, srcip = 192.168.0.4, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0004, regdate = 2015-03-31 15:26:04, log = text log-4, image = 62696E61727920696D6167652D34\n===== 6 ========\nseq = 3, score = 6, total = 90000, percentage = 0.00, ratio = 3.3e-05, id = id-3, srcip = 192.168.0.3, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0003, regdate = 2015-03-31 15:26:03, log = text log-3, image = 62696E61727920696D6167652D33\n===== 7 ========\nseq = 2, score = 4, total = 60000, percentage = 0.00, ratio = 3.3e-05, id = id-2, srcip = 192.168.0.2, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0002, regdate = 2015-03-31 15:26:02, log = text log-2, image = 62696E61727920696D6167652D32\n===== 8 ========\nseq = 1, score = 2, total = 30000, percentage = 0.00, ratio = 3.3e-05, id = id-1, srcip = 192.168.0.1, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0001, regdate = 2015-03-31 15:26:01, log = text log-1, image = 62696E61727920696D6167652D31\n\n\nPrepare Execute 예제\n\n데이터를 binding하여 INSERT하는 예제 프로그램을 작성해 보자.\n\n마크베이스에서 데이터를 binding 하는 방식으로 값을 입력할수 있는데 이를 이용할시에는 데이터의 값들의 타입들을 명확히 지정해주고, 긴 문자열 타입들의 경우에는 길이 값을 반드시 지정해줘야 한다.\n\n아래의 예제를 통해서 각 타입별로 데이터를 binding하는 방법을 알수 있다.\n\n파일명은 sample3_prepare.c 라고 한다.\n\n\nsample3_prepare.c\n\n\n    #include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;machbase_sqlcli.h&gt;\n#include &lt;time.h&gt;\n \n#define MACHBASE_PORT_NO 5656\n \nSQLHENV gEnv;\nSQLHDBC gCon;\nSQLHSTMT gStmt;\nSQLCHAR gErrorState[6];\n \nvoid connectDB()\n{\n    char sConnStr[1024];\n \n    SQLINTEGER sErrorNo;\n    SQLSMALLINT sMsgLength;\n    SQLCHAR sErrorMsg[1024];\n \n    if (SQL_ERROR == SQLAllocEnv(&amp;gEnv)) {\n        printf(\"SQLAllocEnv error!!\\n\");\n        exit(1);\n    }\n \n    if (SQL_ERROR == SQLAllocConnect(gEnv, &amp;gCon)) {\n        printf(\"SQLAllocConnect error!!\\n\");\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n \n    sprintf(sConnStr,\"SERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n \n    if (SQL_ERROR == SQLDriverConnect( gCon, NULL,\n                                       (SQLCHAR *)sConnStr,\n                                       SQL_NTS,\n                                       NULL, 0, NULL,\n                                       SQL_DRIVER_NOPROMPT ))\n    {\n        printf(\"connection error\\n\");\n \n        if (SQL_SUCCESS == SQLError ( gEnv, gCon, NULL, NULL, &amp;sErrorNo,\n                                      sErrorMsg, 1024, &amp;sMsgLength ))\n        {\n            printf(\" mach-%d : %s\\n\", sErrorNo, sErrorMsg);\n        }\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n \n    printf(\"connected ... \\n\");\n \n}\n \nvoid disconnectDB()\n{\n    SQLINTEGER sErrorNo;\n    SQLSMALLINT sMsgLength;\n    SQLCHAR sErrorMsg[1024];\n \n    if (SQL_ERROR == SQLDisconnect(gCon)) {\n        printf(\"disconnect error\\n\");\n \n        if( SQL_SUCCESS == SQLError( gEnv, gCon, NULL, NULL, &amp;sErrorNo,\n                                     sErrorMsg, 1024, &amp;sMsgLength ))\n        {\n            printf(\" mach-%d : %s\\n\", sErrorNo, sErrorMsg);\n        }\n    }\n \n    SQLFreeConnect(gCon);\n    SQLFreeEnv(gEnv);\n}\n \nvoid outError(const char *aMsg, SQLHSTMT aStmt)\n{\n    SQLINTEGER sErrorNo;\n    SQLSMALLINT sMsgLength;\n    SQLCHAR sErrorMsg[1024];\n \n    printf(\"ERROR : (%s)\\n\", aMsg);\n \n    if (SQL_SUCCESS == SQLError( gEnv, gCon, aStmt, NULL, &amp;sErrorNo,\n                                 sErrorMsg, 1024, &amp;sMsgLength ))\n    {\n        printf(\" mach-%d : %s\\n\", sErrorNo, sErrorMsg);\n    }\n    exit(-1);\n}\n \nvoid executeDirectSQL(const char *aSQL, int aErrIgnore)\n{\n    SQLHSTMT sStmt;\n \n    if (SQLAllocStmt(gCon, &amp;sStmt) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"AllocStmt error\", sStmt);\n    }\n \n    if (SQLExecDirect(sStmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        printf(\"sql_exec_direct error[%s] \\n\", aSQL);\n        outError(\"sql_exec_direct error\", sStmt);\n    }\n \n    if (SQL_ERROR == SQLFreeStmt(sStmt, SQL_DROP))\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"FreeStmt Error\", sStmt);\n    }\n}\n \nvoid createTable()\n{\n    executeDirectSQL(\"DROP TABLE CLI_SAMPLE\", 1);\n    executeDirectSQL(\"CREATE TABLE CLI_SAMPLE(seq short, score integer, total long, percentage float, ratio double, id varchar(10), srcip ipv4, dstip ipv6, reg_date datetime, tlog text, image binary)\", 0);\n}\n \nvoid selectTable()\n{\n    SQLHSTMT sStmt;\n    const char *aSQL = \"SELECT seq, score, total, percentage, ratio, id, srcip, dstip, reg_date, tlog, image FROM CLI_SAMPLE\";\n \n    int i=0;\n    short sSeq;\n    int sScore;\n    long sTotal;\n    float sPercentage;\n    double sRatio;\n    char sId [20];\n    char sSrcIp[20];\n    char sDstIp[50];\n    SQL_TIMESTAMP_STRUCT sRegDate;\n    char sLog [1024];\n    char sImage[1024];\n    SQL_LEN sLen;\n \n    if (SQLAllocStmt(gCon, &amp;sStmt) == SQL_ERROR) {\n        outError(\"AllocStmt Error\", sStmt);\n    }\n \n    if (SQLPrepare(sStmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR) {\n        printf(\"Prepare error[%s] \\n\", aSQL);\n        outError(\"Prepare error\", sStmt);\n    }\n \n    if (SQLExecute(sStmt) == SQL_ERROR) {\n        outError(\"prepared execute error\", sStmt);\n    }\n \n    SQLBindCol(sStmt, 1, SQL_C_SSHORT, &amp;sSeq, 0, &amp;sLen);\n    SQLBindCol(sStmt, 2, SQL_C_SLONG, &amp;sScore, 0, &amp;sLen);\n    SQLBindCol(sStmt, 3, SQL_C_SBIGINT, &amp;sTotal, 0, &amp;sLen);\n    SQLBindCol(sStmt, 4, SQL_C_FLOAT, &amp;sPercentage, 0, &amp;sLen);\n    SQLBindCol(sStmt, 5, SQL_C_DOUBLE, &amp;sRatio, 0, &amp;sLen);\n    SQLBindCol(sStmt, 6, SQL_C_CHAR, sId, sizeof(sId), &amp;sLen);\n    SQLBindCol(sStmt, 7, SQL_C_CHAR, sSrcIp, sizeof(sSrcIp), &amp;sLen);\n    SQLBindCol(sStmt, 8, SQL_C_CHAR, sDstIp, sizeof(sDstIp), &amp;sLen);\n    SQLBindCol(sStmt, 9, SQL_C_TYPE_TIMESTAMP, &amp;sRegDate, 0, &amp;sLen);\n    SQLBindCol(sStmt, 10, SQL_C_CHAR, sLog, sizeof(sLog), &amp;sLen);\n    SQLBindCol(sStmt, 11, SQL_C_CHAR, sImage, sizeof(sImage), &amp;sLen);\n \n    while (SQLFetch(sStmt) == SQL_SUCCESS)\n    {\n        printf(\"===== %d ========\\n\", i++);\n        printf(\"seq = %d\", sSeq);\n        printf(\", score = %d\", sScore);\n        printf(\", total = %ld\", sTotal);\n        printf(\", percentage = %.2f\", sPercentage);\n        printf(\", ratio = %g\", sRatio);\n        printf(\", id = %s\", sId);\n        printf(\", srcip = %s\", sSrcIp);\n        printf(\", dstip = %s\", sDstIp);\n        printf(\", regdate = %d-%02d-%02d %02d:%02d:%02d\",\n               sRegDate.year, sRegDate.month, sRegDate.day,\n               sRegDate.hour, sRegDate.minute, sRegDate.second);\n        printf(\", log = %s\", sLog);\n        printf(\", image = %s\\n\", sImage);\n    }\n \n    if (SQL_ERROR == SQLFreeStmt(sStmt, SQL_DROP))\n    {\n        outError(\"FreeStmt eror\", sStmt);\n    }\n}\n \nvoid prepareInsert()\n{\n    SQLHSTMT sStmt;\n    int i;\n    short sSeq;\n    int sScore;\n    long sTotal;\n    float sPercentage;\n    double sRatio;\n    char sId [20];\n    char sSrcIp [20];\n    char sDstIp [50];\n    long reg_date;\n    char sLog [100];\n    char sImage [100];\n    int sLength[5];\n \n    const char *sSQL = \"INSERT INTO CLI_SAMPLE VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )\";\n \n    if (SQLAllocStmt(gCon, &amp;sStmt) == SQL_ERROR)\n    {\n        outError(\"AllocStmt error\", sStmt);\n    }\n \n    if (SQLPrepare(sStmt, (SQLCHAR *)sSQL, SQL_NTS) == SQL_ERROR)\n    {\n        printf(\"Prepare error[%s]\\n\", sSQL);\n        outError(\"Prepare error\", sStmt);\n    }\n \n    for(i=1; i&lt;10; i++)\n    {\n        sSeq = i;\n        sScore = i+i;\n        sTotal = (sSeq + sScore) * 10000;\n        sPercentage = (float)(sScore+2)/sScore;\n        sRatio = (double)(sSeq+1)/sTotal;\n        sprintf(sId, \"id-%d\", i);\n        sprintf(sSrcIp, \"192.168.0.%d\", i);\n        sprintf(sDstIp, \"2001:0DB8:0000:0000:0000:0000:1428:%04x\", i);\n        reg_date = i*10000;\n        sprintf(sLog, \"log-%d\", i);\n        sprintf(sImage, \"image-%d\", i);\n \n        if (SQLBindParameter(sStmt,\n                             1,\n                             SQL_PARAM_INPUT,\n                             SQL_C_SSHORT,\n                             SQL_SMALLINT,\n                             0,\n                             0,\n                             &amp;sSeq,\n                             0,\n                             NULL) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 1\", sStmt);\n        }\n \n        if (SQLBindParameter(sStmt,\n                             2,\n                             SQL_PARAM_INPUT,\n                             SQL_C_SLONG,\n                             SQL_INTEGER,\n                             0,\n                             0,\n                             &amp;sScore,\n                             0,\n                             NULL) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 2\", sStmt);\n        }\n \n        if (SQLBindParameter(sStmt,\n                             3,\n                             SQL_PARAM_INPUT,\n                             SQL_C_SBIGINT,\n                             SQL_BIGINT,\n                             0,\n                             0,\n                             &amp;sTotal,\n                             0,\n                             NULL) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 3\", sStmt);\n        }\n \n        if (SQLBindParameter(sStmt,\n                             4,\n                             SQL_PARAM_INPUT,\n                             SQL_C_FLOAT,\n                             SQL_FLOAT,\n                             0,\n                             0,\n                             &amp;sPercentage,\n                             0,\n                             NULL) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 4\", sStmt);\n        }\n \n        if (SQLBindParameter(sStmt,\n                             5,\n                             SQL_PARAM_INPUT,\n                             SQL_C_DOUBLE,\n                             SQL_DOUBLE,\n                             0,\n                             0,\n                             &amp;sRatio,\n                             0,\n                             NULL) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 5\", sStmt);\n        }\n \n        sLength[0] = strlen(sId);\n        if (SQLBindParameter(sStmt,\n                             6,\n                             SQL_PARAM_INPUT,\n                             SQL_C_CHAR,\n                             SQL_VARCHAR,\n                             0,\n                             0,\n                             sId,\n                             0,\n                             (SQLLEN *)&amp;sLength[0]) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 6\", sStmt);\n        }\n \n        sLength[1] = strlen(sSrcIp);\n        if (SQLBindParameter(sStmt,\n                             7,\n                             SQL_PARAM_INPUT,\n                             SQL_C_CHAR,\n                             SQL_IPV4,\n                             0,\n                             0,\n                             sSrcIp,\n                             0,\n                             (SQLLEN *)&amp;sLength[1]) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 7\", sStmt);\n        }\n \n        sLength[2] = strlen(sDstIp);\n        if (SQLBindParameter(sStmt,\n                             8,\n                             SQL_PARAM_INPUT,\n                             SQL_C_CHAR,\n                             SQL_IPV6,\n                             0,\n                             0,\n                             sDstIp,\n                             0,\n                             (SQLLEN *)&amp;sLength[2]) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 8\", sStmt);\n        }\n \n        if (SQLBindParameter(sStmt,\n                             9,\n                             SQL_PARAM_INPUT,\n                             SQL_C_SBIGINT,\n                             SQL_DATE,\n                             0,\n                             0,\n                             &amp;reg_date,\n                             0,\n                             NULL) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 9\", sStmt);\n        }\n \n        sLength[3] = strlen(sLog);\n        if (SQLBindParameter(sStmt,\n                             10,\n                             SQL_PARAM_INPUT,\n                             SQL_C_CHAR,\n                             SQL_VARCHAR,\n                             0,\n                             0,\n                             sLog,\n                             0,\n                             (SQLLEN *)&amp;sLength[3]) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 10\", sStmt);\n        }\n \n        sLength[4] = strlen(sImage);\n        if (SQLBindParameter(sStmt,\n                             11,\n                             SQL_PARAM_INPUT,\n                             SQL_C_CHAR,\n                             SQL_BINARY,\n                             0,\n                             0,\n                             sImage,\n                             0,\n                             (SQLLEN *)&amp;sLength[4]) == SQL_ERROR)\n        {\n            outError(\"BindParameter error 11\", sStmt);\n        }\n \n        if( SQLExecute(sStmt) == SQL_ERROR) {\n            outError(\"prepare execute error\", sStmt);\n        }\n \n        printf(\"%d prepared record inserted\\n\", i);\n \n    }\n \n    if (SQL_ERROR == SQLFreeStmt(sStmt, SQL_DROP)) {\n        outError(\"FreeStmt\", sStmt);\n    }\n}\n \nint main()\n{\n    connectDB();\n    createTable();\n    prepareInsert();\n    selectTable();\n    disconnectDB();\n \n    return 0;\n}\n    \n\n  \n\n\nMakefile에 sample3_prepare.c를 등록하고 컴파일하여 실행하면 다음과 같이 나온다.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample3_prepare\n \nconnected ...\n1 prepared record inserted\n2 prepared record inserted\n3 prepared record inserted\n4 prepared record inserted\n5 prepared record inserted\n6 prepared record inserted\n7 prepared record inserted\n8 prepared record inserted\n9 prepared record inserted\n===== 0 ========\nseq = 9, score = 18, total = 270000, percentage = 1.11, ratio = 3.7037e-05, id = id-9, srcip = 192.168.0.9, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0009, regdate = 1970-01-01 09:00:00, log = log-9, image = 696D6167652D39\n===== 1 ========\nseq = 8, score = 16, total = 240000, percentage = 1.12, ratio = 3.75e-05, id = id-8, srcip = 192.168.0.8, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0008, regdate = 1970-01-01 09:00:00, log = log-8, image = 696D6167652D38\n===== 2 ========\nseq = 7, score = 14, total = 210000, percentage = 1.14, ratio = 3.80952e-05, id = id-7, srcip = 192.168.0.7, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0007, regdate = 1970-01-01 09:00:00, log = log-7, image = 696D6167652D37\n===== 3 ========\nseq = 6, score = 12, total = 180000, percentage = 1.17, ratio = 3.88889e-05, id = id-6, srcip = 192.168.0.6, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0006, regdate = 1970-01-01 09:00:00, log = log-6, image = 696D6167652D36\n===== 4 ========\nseq = 5, score = 10, total = 150000, percentage = 1.20, ratio = 4e-05, id = id-5, srcip = 192.168.0.5, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0005, regdate = 1970-01-01 09:00:00, log = log-5, image = 696D6167652D35\n===== 5 ========\nseq = 4, score = 8, total = 120000, percentage = 1.25, ratio = 4.16667e-05, id = id-4, srcip = 192.168.0.4, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0004, regdate = 1970-01-01 09:00:00, log = log-4, image = 696D6167652D34\n===== 6 ========\nseq = 3, score = 6, total = 90000, percentage = 1.33, ratio = 4.44444e-05, id = id-3, srcip = 192.168.0.3, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0003, regdate = 1970-01-01 09:00:00, log = log-3, image = 696D6167652D33\n===== 7 ========\nseq = 2, score = 4, total = 60000, percentage = 1.50, ratio = 5e-05, id = id-2, srcip = 192.168.0.2, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0002, regdate = 1970-01-01 09:00:00, log = log-2, image = 696D6167652D32\n===== 8 ========\nseq = 1, score = 2, total = 30000, percentage = 2.00, ratio = 6.66667e-05, id = id-1, srcip = 192.168.0.1, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0001, regdate = 1970-01-01 09:00:00, log = log-1, image = 696D6167652D31\n\n\n확장 함수 Append 예제\n\n마크베이스에서는 대량의 데이터를 파일로부터 읽어서 고속으로 입력하는 방법으로 Append 프로토콜을 제공하고 있다. 이 Append 프로토콜을 이용하는 예제 프로그램을 작성해 보자.\n\n먼저 마크베이스에서 제공하는 다양한 타입별로 append 하는 방식의 예제를 살펴보자. Append 방식은 각 타입별로 편리하게 입력해 줄 수 있도록 각각의 설정값들이 정해져있다. 그러므로 모든 방법별로 사용하는 입력하는 방식에 대한 숙지를 한다면 더욱더 효율적으로 프로그램을 작성할 수 있을 것이다. 아래쪽에 있는 예제 코드에 그 방법들이 모두 나와있다.\n\n파일명은 sample4_append1.c 라고 한다.\n\n\nsample4_append1.c\n\n\n    #include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;machbase_sqlcli.h&gt;\n#include &lt;arpa/inet.h&gt;\n \n#if __linux__\n#include &lt;sys/time.h&gt;\n#endif\n \n#if defined(SUPPORT_STRUCT_TM)\n# include &lt;time.h&gt;\n#endif\n \n#define MACHBASE_PORT_NO 5656\n#define MAX_APPEND_COUNT 0xFFFFFFFF\n#define ERROR_CHECK_COUNT 100\n \n#define ERROR -1\n#define SUCCESS 0\n \nSQLHENV gEnv;\nSQLHDBC gCon;\nSQLHSTMT gStmt;\nSQLCHAR gErrorState[6];\n \nvoid connectDB();\nvoid disconnectDB();\nvoid outError(const char *aMsg);\nvoid executeDirectSQL(const char *aSQL, int aErrIgnore);\nvoid createTable();\nvoid appendOpen();\nvoid appendData();\nint appendClose();\ntime_t getTimeStamp();\n \nint main()\n{\n    unsigned int sCount=0;\n    time_t sStartTime, sEndTime;\n \n    connectDB();\n    createTable();\n \n    appendOpen();\n    sStartTime = getTimeStamp();\n    appendData();\n    sEndTime = getTimeStamp();\n    appendClose();\n \n    printf(\"timegap = %ld microseconds for %d records\\n\", sEndTime - sStartTime, sCount);\n    printf(\"%.2f records/second\\n\", ((double)sCount/(double)(sEndTime - sStartTime))*1000000);\n \n    disconnectDB();\n    return SUCCESS;\n}\n \nvoid connectDB()\n{\n    char sConnStr[1024];\n \n    if (SQL_ERROR == SQLAllocEnv(&amp;gEnv)) {\n        outError(\"SQLAllocEnv error!!\");\n    }\n \n    if (SQL_ERROR == SQLAllocConnect(gEnv, &amp;gCon)) {\n        outError(\"SQLAllocConnect error!!\");\n    }\n \n    sprintf(sConnStr,\"SERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n \n    if (SQL_ERROR == SQLDriverConnect( gCon, NULL,\n                                       (SQLCHAR *)sConnStr, SQL_NTS,\n                                       NULL, 0, NULL,\n                                       SQL_DRIVER_NOPROMPT ))\n    {\n        outError(\"connection error\\n\");\n    }\n \n    if (SQL_ERROR == SQLAllocStmt(gCon, &amp;gStmt) )\n    {\n        outError(\"AllocStmt error\");\n    }\n \n    printf(\"connected ... \\n\");\n}\n \nvoid disconnectDB()\n{\n    if( SQL_ERROR == SQLFreeStmt(gStmt, SQL_DROP) )\n    {\n        outError(\"SQLFreeStmt error\");\n    }\n \n    if (SQL_ERROR == SQLDisconnect(gCon)) {\n        outError(\"disconnect error\");\n    }\n    SQLFreeConnect(gCon);\n    SQLFreeEnv(gEnv);\n}\n \nvoid outError(const char *aMsg)\n{\n    SQLINTEGER sErrorNo;\n    SQLSMALLINT sMsgLength;\n    SQLCHAR sErrorMsg[1024];\n \n    printf(\"ERROR : (%s)\\n\", aMsg);\n    if (SQL_SUCCESS == SQLError( gEnv, gCon, gStmt, NULL, &amp;sErrorNo,\n                                 sErrorMsg, 1024, &amp;sMsgLength ))\n    {\n        printf(\" mach-%d : %s\\n\", sErrorNo, sErrorMsg);\n    }\n \n    if( gStmt )\n    {\n        SQLFreeStmt(gStmt, SQL_DROP);\n    }\n \n    if( gCon )\n    {\n        SQLFreeConnect( gCon );\n    }\n \n    if( gEnv )\n    {\n        SQLFreeEnv( gEnv );\n    }\n    exit(ERROR);\n}\n \nvoid executeDirectSQL(const char *aSQL, int aErrIgnore)\n{\n    SQLHSTMT sStmt;\n \n    if (SQLAllocStmt(gCon, &amp;sStmt) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"AllocStmt error\");\n    }\n \n    if (SQLExecDirect(sStmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        printf(\"sql_exec_direct error[%s] \\n\", aSQL);\n        outError(\"sql_exec_direct error\");\n    }\n \n    if (SQL_ERROR == SQLFreeStmt(sStmt, SQL_DROP))\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"FreeStmt Error\");\n    }\n}\n \nvoid createTable()\n{\n    executeDirectSQL(\"DROP TABLE CLI_SAMPLE\", 1);\n    executeDirectSQL(\"CREATE TABLE CLI_SAMPLE(short1 short, integer1 integer, long1 long, float1 float, double1 double, datetime1 datetime, varchar1 varchar(10), ip ipv4, ip2 ipv6, text1 text, bin1 binary)\", 0);\n}\n \nvoid appendOpen()\n{\n    const char *sTableName = \"CLI_SAMPLE\";\n \n    if( SQLAppendOpen(gStmt, (SQLCHAR *)sTableName, ERROR_CHECK_COUNT) != SQL_SUCCESS )\n    {\n        outError(\"SQLAppendOpen error\");\n    }\n \n    printf(\"append open ok\\n\");\n}\n \nvoid appendData()\n{\n    SQL_APPEND_PARAM sParam[11];\n    char sVarchar[10] = {0, };\n    char sText[100] = {0, };\n    char sBinary[100] = {0, };\n \n    memset(sParam, 0, sizeof(sParam));\n \n    /* NULL FOR ALL*/\n    /* fixed column */\n    sParam[0].mShort = SQL_APPEND_SHORT_NULL;\n    sParam[1].mInteger = SQL_APPEND_INTEGER_NULL;\n    sParam[2].mLong = SQL_APPEND_LONG_NULL;\n    sParam[3].mFloat = SQL_APPEND_FLOAT_NULL;\n    sParam[4].mDouble = SQL_APPEND_DOUBLE_NULL;\n    /* datetime */\n    sParam[5].mDateTime.mTime = SQL_APPEND_DATETIME_NULL;\n    /* varchar */\n    sParam[6].mVarchar.mLength = SQL_APPEND_VARCHAR_NULL;\n    /* ipv4 */\n    sParam[7].mIP.mLength = SQL_APPEND_IP_NULL;\n    /* ipv6 */\n    sParam[8].mIP.mLength = SQL_APPEND_IP_NULL;\n    /* text */\n    sParam[9].mText.mLength = SQL_APPEND_TEXT_NULL;\n    /* binary */\n    sParam[10].mBinary.mLength = SQL_APPEND_BINARY_NULL;\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* FIXED COLUMN Value */\n    sParam[0].mShort = 2;\n    sParam[1].mInteger = 4;\n    sParam[2].mLong = 6;\n    sParam[3].mFloat = 8.4;\n    sParam[4].mDouble = 10.9;\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* DATETIME : absolute value */\n    sParam[5].mDateTime.mTime = MACH_UINT64_LITERAL(1000000000);\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* DATETIME : current */\n    sParam[5].mDateTime.mTime = SQL_APPEND_DATETIME_NOW;\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* DATETIME : string format*/\n    sParam[5].mDateTime.mTime = SQL_APPEND_DATETIME_STRING;\n    sParam[5].mDateTime.mDateStr = \"23/May/2014:17:41:28\";\n    sParam[5].mDateTime.mFormatStr = \"DD/MON/YYYY:HH24:MI:SS\";\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* DATETIME : struct tm format*/\n    sParam[5].mDateTime.mTime = SQL_APPEND_DATETIME_STRUCT_TM;\n    sParam[5].mDateTime.mTM.tm_year = 2000 - 1900;\n    sParam[5].mDateTime.mTM.tm_mon = 11;\n    sParam[5].mDateTime.mTM.tm_mday = 31;\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* VARCHAR : string */\n    strcpy(sVarchar, \"MY VARCHAR\");\n    sParam[6].mVar.mLength = strlen(sVarchar);\n    sParam[6].mVar.mData = sVarchar;\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* IPv4 : ipv4 from binary bytes */\n    sParam[7].mIP.mLength = SQL_APPEND_IP_IPV4;\n    sParam[7].mIP.mAddr[0] = 127;\n    sParam[7].mIP.mAddr[1] = 0;\n    sParam[7].mIP.mAddr[2] = 0;\n    sParam[7].mIP.mAddr[3] = 1;\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* IPv4 : ipv4 from binary */\n    sParam[7].mIP.mLength = SQL_APPEND_IP_IPV4;\n    *(in_addr_t *)(sParam[7].mIP.mAddr) = inet_addr(\"192.168.0.1\");\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* IPv4 : ipv4 from string */\n    sParam[7].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[7].mIP.mAddrString = \"203.212.222.111\";\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* IPv6 : ipv6 from binary bytes */\n    sParam[8].mIP.mLength = SQL_APPEND_IP_IPV6;\n    sParam[8].mIP.mAddr[0] = 127;\n    sParam[8].mIP.mAddr[1] = 127;\n    sParam[8].mIP.mAddr[2] = 127;\n    sParam[8].mIP.mAddr[3] = 127;\n    sParam[8].mIP.mAddr[4] = 127;\n    sParam[8].mIP.mAddr[5] = 127;\n    sParam[8].mIP.mAddr[6] = 127;\n    sParam[8].mIP.mAddr[7] = 127;\n    sParam[8].mIP.mAddr[8] = 127;\n    sParam[8].mIP.mAddr[9] = 127;\n    sParam[8].mIP.mAddr[10] = 127;\n    sParam[8].mIP.mAddr[11] = 127;\n    sParam[8].mIP.mAddr[12] = 127;\n    sParam[8].mIP.mAddr[13] = 127;\n    sParam[8].mIP.mAddr[14] = 127;\n    sParam[8].mIP.mAddr[15] = 127;\n    SQLAppendDataV2(gStmt, sParam);\n    sParam[8].mIP.mLength = SQL_APPEND_IP_NULL; /* recover */\n \n    /* TEXT : string */\n    memset(sText, 'X', sizeof(sText));\n    sParam[9].mVar.mLength = 100;\n    sParam[9].mVar.mData = sText;\n    SQLAppendDataV2(gStmt, sParam);\n \n    /* BINARY : datas */\n    memset(sBinary, 0xFA, sizeof(sBinary));\n    sParam[10].mVar.mLength = 100;\n    sParam[10].mVar.mData = sBinary;\n    SQLAppendDataV2(gStmt, sParam);\n}\n \nint appendClose()\n{\n    int sSuccessCount = 0;\n    int sFailureCount = 0;\n \n    if( SQLAppendClose(gStmt, &amp;sSuccessCount, &amp;sFailureCount) != SQL_SUCCESS )\n    {\n        outError(\"SQLAppendClose error\");\n    }\n \n    printf(\"append close ok\\n\");\n    printf(\"success : %d, failure : %d\\n\", sSuccessCount, sFailureCount);\n    return sSuccessCount;\n}\n \ntime_t getTimeStamp()\n{\n#if _WIN32 || _WIN64\n \n#if defined(_MSC_VER) || defined(_MSC_EXTENSIONS)\n#define DELTA_EPOCH_IN_MICROSECS 11644473600000000Ui64\n#else\n#define DELTA_EPOCH_IN_MICROSECS 11644473600000000ULL\n#endif\n    FILETIME sFT;\n    unsigned __int64 sTempResult = 0;\n \n    GetSystemTimeAsFileTime(&amp;sFT);\n \n    sTempResult |= sFT.dwHighDateTime;\n    sTempResult &lt;&lt;= 32;\n    sTempResult |= sFT.dwLowDateTime;\n \n    sTempResult -= DELTA_EPOCH_IN_MICROSECS;\n    sTempResult /= 10;\n \n    return sTempResult;\n#else\n    struct timeval sTimeVal;\n    int sRet;\n \n    sRet = gettimeofday(&amp;sTimeVal, NULL);\n \n    if (sRet == 0)\n    {\n        return (time_t)(sTimeVal.tv_sec * 1000000 + sTimeVal.tv_usec);\n    }\n    else\n    {\n        return 0;\n    }\n#endif\n}\n    \n\n  \n\n\nMakefile에 sample4_append1.c를 등록하고 컴파일하여 실행하면 다음과 같이 나온다.\n\n[mach@localhost cli]$ make sample4_append1\ngcc -c -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -I/home/mach/machbase_home/include -I. -L//home/mach/machbase_home/include -osample4_append1.o sample4_append1.c\ngcc -m64 -mtune=k8 -L/home/mach/machbase_home/lib -osample4_append1 sample4_append1.o -lmachcli -L/home/mach/machbase_home/lib -lm -lpthread -ldl -lrt -rdynamic\n[mach@localhost cli]$ ./sample4_append1\nconnected ...\nappend open ok\nappend close ok\nsuccess : 13, failure : 0\ntimegap = 48 microseconds for 13 records\n270833.33 records/second\n[mach@localhost cli]$\n \nYou can check what is inserted after MACH_SQL.\n \nMach&gt; select * from CLI_SAMPLE;\nSHORT1 INTEGER1 LONG1 FLOAT1 DOUBLE1\n-----------------------------------------------------------------------------------------------------------\nDATETIME1 VARCHAR1 IP IP2\n------------------------------------------------------------------------------------------------------------------------------\nTEXT1\n------------------------------------------------------------------------------------\nBIN1\n------------------------------------------------------------------------------------\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 203.212.222.111 NULL\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXX\nFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFA\nFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFA\nFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFA\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 203.212.222.111 NULL\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXX\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 203.212.222.111 7F7F:7F7F:7F7F:7F7F:7F7F:7F7F:7F7F:7F7F\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 203.212.222.111 NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 192.168.0.1 NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 127.0.0.1 NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 NULL NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2014-05-23 17:41:28 000:000:000 NULL NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2015-04-09 16:44:11 134:256:000 NULL NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n1970-01-01 09:00:01 000:000:000 NULL NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n1970-01-01 09:00:00 000:000:000 NULL NULL NULL\nNULL\nNULL\n[12] row(s) selected.\n\n\n이제 파일을 이용해서 고속으로 append하는 방식을 사용해 보자. 실제로 업무에서 사용되는 많은 양의 로그, 패킷등의 값들을 고속으로 입력하는 데 유용한 예제이다. 파일명은 sample4_append2.c 라고 한다.\n\n미리 입력할 데이터를 data.txt에 저장해 두어야 한다.\n\n./make_data\n\n\n미리 주어진 make_data.c 를 수정하면 상황에 맞게 data.txt 파일을 생성할 수 있다.\n\n\nmake_data.c\n\n\n    #include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/time.h&gt;\n#include &lt;machbase_sqlcli.h&gt;\n \n#define MACHBASE_PORT_NO 5656\n#define MAX_APPEND_COUNT 0xFFFFFFFF\n#define ERROR_CHECK_COUNT 100\n \nSQLHENV gEnv;\nSQLHDBC gCon;\nSQLHSTMT gStmt;\nSQLCHAR gErrorState[6];\n \nvoid connectDB();\nvoid disconnectDB();\nvoid outError(const char *aMsg);\nvoid executeDirectSQL(const char *aSQL, int aErrIgnore);\nvoid createTable();\nvoid appendOpen();\nint appendData();\nvoid appendClose();\ntime_t getTimeStamp();\n \nint main()\n{\n    unsigned int sCount=0;\n    time_t sStartTime, sEndTime;\n \n    connectDB();\n    createTable();\n \n    appendOpen();\n    sStartTime = getTimeStamp();\n    sCount = appendData();\n    sEndTime = getTimeStamp();\n \n    appendClose();\n \n    printf(\"timegap = %ld microseconds for %d records\\n\", sEndTime - sStartTime, sCount);\n    printf(\"%.2f records/second\\n\", ((double)sCount/(double)(sEndTime - sStartTime))*1000000);\n \n    disconnectDB();\n \n    return 0;\n}\n \nvoid connectDB()\n{\n    char sConnStr[1024];\n \n    if (SQL_ERROR == SQLAllocEnv(&amp;gEnv)) {\n        outError(\"SQLAllocEnv error!!\");\n    }\n \n    if (SQL_ERROR == SQLAllocConnect(gEnv, &amp;gCon)) {\n        outError(\"SQLAllocConnect error!!\");\n    }\n \n    sprintf(sConnStr,\"SERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n \n    if (SQL_ERROR == SQLDriverConnect( gCon, NULL,\n                                       (SQLCHAR *)sConnStr, SQL_NTS,\n                                       NULL, 0, NULL,\n                                       SQL_DRIVER_NOPROMPT ))\n    {\n        outError(\"connection error!!\");\n    }\n \n    if( SQL_ERROR == SQLAllocStmt(gCon, &amp;gStmt) )\n    {\n        outError(\"SQLAllocStmt error!!\");\n    }\n \n    printf(\"connected ... \\n\");\n}\n \nvoid disconnectDB()\n{\n    if( SQL_ERROR == SQLFreeStmt(gStmt, SQL_DROP) )\n    {\n        outError(\"SQLFreeStmt error\");\n    }\n \n    if (SQL_ERROR == SQLDisconnect(gCon)) {\n        outError(\"disconnect error\");\n    }\n \n    SQLFreeConnect(gCon);\n    SQLFreeEnv(gEnv);\n}\n \nvoid outError(const char *aMsg)\n{\n    SQLINTEGER sErrorNo;\n    SQLSMALLINT sMsgLength;\n    SQLCHAR sErrorMsg[1024];\n \n    printf(\"ERROR : (%s)\\n\", aMsg);\n \n    if (SQL_SUCCESS == SQLError( gEnv, gCon, gStmt, NULL, &amp;sErrorNo,\n                                 sErrorMsg, 1024, &amp;sMsgLength ))\n    {\n        printf(\" mach-%d : %s\\n\", sErrorNo, sErrorMsg);\n    }\n \n    if( gStmt )\n    {\n        SQLFreeStmt( gStmt, SQL_DROP );\n    }\n    if( gCon )\n    {\n        SQLFreeConnect( gCon );\n    }\n    if( gEnv )\n    {\n        SQLFreeEnv( gEnv );\n    }\n    exit(-1);\n}\n \nvoid executeDirectSQL(const char *aSQL, int aErrIgnore)\n{\n    SQLHSTMT sStmt;\n \n    if (SQLAllocStmt(gCon, &amp;sStmt) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"AllocStmt error\");\n    }\n \n    if (SQLExecDirect(sStmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"sql_exec_direct error\");\n    }\n \n    if (SQL_ERROR == SQLFreeStmt(sStmt, SQL_DROP))\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"FreeStmt Error\");\n    }\n}\n \nvoid createTable()\n{\n    executeDirectSQL(\"DROP TABLE CLI_SAMPLE\", 1);\n    executeDirectSQL(\"CREATE TABLE CLI_SAMPLE(seq short, score integer, total long, percentage float, ratio double, id varchar(10), srcip ipv4, dstip ipv6, reg_date datetime, tlog text, image binary)\", 0);\n \n    printf(\"table created\\n\");\n}\n \nvoid appendOpen()\n{\n    const char *sTableName = \"CLI_SAMPLE\";\n \n    if( SQLAppendOpen(gStmt, (SQLCHAR *)sTableName, ERROR_CHECK_COUNT) != SQL_SUCCESS )\n    {\n        outError(\"SQLAppendOpen error!!\");\n    }\n \n    printf(\"append open ok\\n\");\n}\n \nint appendData()\n{\n    FILE *sFp;\n    char sBuf[1024];\n    int j;\n    char *sToken;\n    unsigned int sCount=0;\n    SQL_APPEND_PARAM sParam[11];\n \n    sFp = fopen(\"data.txt\", \"r\");\n    if( !sFp )\n    {\n        printf(\"file open error\\n\");\n        exit(-1);\n    }\n \n    printf(\"append data start\\n\");\n \n    memset(sBuf, 0, sizeof(sBuf));\n \n    while( fgets(sBuf, 1024, sFp ) != NULL )\n    {\n        if( strlen(sBuf) &lt; 1)\n        {\n            break;\n        }\n \n        j=0;\n        sToken = strtok(sBuf,\",\");\n \n        while( sToken != NULL )\n        {\n            memset(sParam+j, 0, sizeof(sParam));\n            switch(j){\n                case 0 : sParam[j].mShort = atoi(sToken); break; //short\n                case 1 : sParam[j].mInteger = atoi(sToken); break; //int\n                case 2 : sParam[j].mLong = atol(sToken); break; //long\n                case 3 : sParam[j].mFloat = atof(sToken); break; //float\n                case 4 : sParam[j].mDouble = atof(sToken); break; //double\n                case 5 : //string\n                case 9 : //text\n                case 10 : //binary\n                         sParam[j].mVar.mLength = strlen(sToken);\n                         strcpy(sParam[j].mVar.mData, sToken);\n                         break;\n                case 6 : //ipv4\n                case 7 : //ipv6\n                         sParam[j].mIP.mLength = SQL_APPEND_IP_STRING;\n                         strcpy(sParam[j].mIP.mAddrString, sToken);\n                         break;\n                case 8 : //datetime\n                         sParam[j].mDateTime.mTime = SQL_APPEND_DATETIME_STRING;\n                         strcpy(sParam[j].mDateTime.mDateStr, sToken);\n                         sParam[j].mDateTime.mFormatStr = \"DD/MON/YYYY:HH24:MI:SS\";\n                         break;\n            }\n \n            sToken = strtok(NULL, \",\");\n \n            j++;\n        }\n        if( SQLAppendDataV2(gStmt, sParam) != SQL_SUCCESS )\n        {\n            printf(\"SQLAppendData error\\n\");\n            return 0;\n        }\n        if ( ((sCount++) % 10000) == 0)\n        {\n            printf(\".\");\n        }\n \n        if( ((sCount) % 100) == 0 )\n        {\n            if( SQLAppendFlush( gStmt ) != SQL_SUCCESS )\n            {\n                outError(\"SQLAppendFlush error\");\n            }\n        }\n        if (sCount == MAX_APPEND_COUNT)\n        {\n            break;\n        }\n    }\n \n    printf(\"\\nappend data end\\n\");\n \n    fclose(sFp);\n \n    return sCount;\n}\n \nvoid appendClose()\n{\n    int sSuccessCount = 0;\n    int sFailureCount = 0;\n \n    if( SQLAppendClose(gStmt, &amp;sSuccessCount, &amp;sFailureCount) != SQL_SUCCESS )\n    {\n        outError(\"SQLAppendClose error\");\n    }\n \n    printf(\"append close ok\\n\");\n    printf(\"success : %d, failure : %d\\n\", sSuccessCount, sFailureCount);\n}\n \ntime_t getTimeStamp()\n{\n    struct timeval tv;\n    gettimeofday(&amp;tv, NULL);\n    return tv.tv_sec*1000000+tv.tv_usec;\n}\n    \n\n  \n\n\nMakefile에 sample4_append2.c를 등록하고 컴파일하여 실행하면 다음과 같이 나온다.\n\n[mach@localhost cli]$ make\ngcc -c -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -I/home/mach/machbase_home/include -I. -L//home/mach/machbase_home/include -osingle_append2.o single_append2.c\ngcc -m64 -mtune=k8 -L/home/mach/machbase_home/lib -osingle_append2 single_append2.o -lmachcli -L/home/mach/machbase_home/lib -lm -lpthread -ldl -lrt -rdynamic\n[mach@localhost cli]$ ./single_append2\nconnected ...\ntable created\nappend open ok\nappend data start\n....................................................................................................\nappend data end\nappend close ok\nsuccess : 1000000, failure : 0\ntimegap = 1641503 microseconds for 1000000 records\n609197.79 records/second\n\n\n테이블 열 정보 획득 예제\n\n테이블 열 정보를 획득하는 방법은 다양하지만 그중에 SQLDescribeCol과 SQLColumns를 이용한 방법을 살펴본다.\n\nSQLDescribeCol\n\nSQLDescribeCol은 테이블 열의 번호, 이름, 버퍼 크기, 길이, 타입 등을 가져오는 함수로 이를 이용해서 데이터베이스 내부에서 원하는 내용을 손쉽게 가져올수 있다.\n\n예제 파일명은 sample5_describe.c 라고 한다.\n\n\nsample5_describe.c\n\n\n    #include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;machbase_sqlcli.h&gt;\n#include &lt;time.h&gt;\n \n#define MACHBASE_PORT_NO 5656\n \nSQLHENV gEnv;\nSQLHDBC gCon;\nSQLHSTMT gStmt;\nSQLCHAR gErrorState[6];\n \nvoid connectDB()\n{\n    char connStr[1024];\n \n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n \n    if (SQL_ERROR == SQLAllocEnv(&amp;gEnv)) {\n        printf(\"SQLAllocEnv error!!\\n\");\n        exit(1);\n    }\n \n    if (SQL_ERROR == SQLAllocConnect(gEnv, &amp;gCon)) {\n        printf(\"SQLAllocConnect error!!\\n\");\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n \n    sprintf(connStr,\"SERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n \n    if (SQL_ERROR == SQLDriverConnect( gCon, NULL,\n                                       (SQLCHAR *)connStr,\n                                       SQL_NTS,\n                                       NULL, 0, NULL,\n                                       SQL_DRIVER_NOPROMPT ))\n    {\n        printf(\"connection error\\n\");\n \n        if (SQL_SUCCESS == SQLError ( gEnv, gCon, NULL, NULL, &amp;errNo,\n                                      errMsg, 1024, &amp;msgLength ))\n        {\n            printf(\" mach-%d : %s\\n\", errNo, errMsg);\n        }\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n \n    if (SQLAllocStmt(gCon, &amp;gStmt) == SQL_ERROR)\n    {\n        outError(\"AllocStmt error\", gStmt);\n    }\n \n    printf(\"connected ... \\n\");\n \n}\n \nvoid disconnectDB()\n{\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n \n    if (SQL_ERROR == SQLDisconnect(gCon)) {\n        printf(\"disconnect error\\n\");\n \n        if( SQL_SUCCESS == SQLError( gEnv, gCon, NULL, NULL, &amp;errNo,\n                                     errMsg, 1024, &amp;msgLength ))\n        {\n            printf(\" mach-%d : %s\\n\", errNo, errMsg);\n        }\n    }\n \n    SQLFreeConnect(gCon);\n    SQLFreeEnv(gEnv);\n}\n \nvoid outError(const char *aMsg, SQLHSTMT stmt)\n{\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n \n    printf(\"ERROR : (%s)\\n\", aMsg);\n \n    if (SQL_SUCCESS == SQLError( gEnv, gCon, stmt, NULL, &amp;errNo,\n                                 errMsg, 1024, &amp;msgLength ))\n    {\n        printf(\" mach-%d : %s\\n\", errNo, errMsg);\n    }\n    exit(-1);\n}\n \nvoid executeDirectSQL(const char *aSQL, int aErrIgnore)\n{\n    SQLHSTMT stmt;\n \n    if (SQLAllocStmt(gCon, &amp;stmt) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"AllocStmt error\", stmt);\n    }\n \n    if (SQLExecDirect(stmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        printf(\"sql_exec_direct error[%s] \\n\", aSQL);\n        outError(\"sql_exec_direct error\", stmt);\n    }\n \n    if (SQL_ERROR == SQLFreeStmt(stmt, SQL_DROP))\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"FreeStmt Error\", stmt);\n    }\n}\n \nvoid createTable()\n{\n    executeDirectSQL(\"DROP TABLE CLI_SAMPLE\", 1);\n    executeDirectSQL(\"CREATE TABLE CLI_SAMPLE(seq short, score integer, total long, percentage float, ratio double, id varchar(10), srcip ipv4, dstip ipv6, reg_date datetime, tlog text, image binary)\", 0);\n \n}\n \nint main()\n{\n    char sSqlStr[] = \"select * from cli_sample\";\n    SQLCHAR sColName[32];\n    SQLSMALLINT sColType;\n    SQLSMALLINT sColNameLen;\n    SQLSMALLINT sNullable;\n    SQLULEN sColLen;\n    SQLSMALLINT sDecimalDigits;\n    SQLLEN sOutlen;\n    SQLCHAR* sData;\n    SQLLEN sDisplaySize;\n    int i;\n \n    SQLSMALLINT sColumns;\n \n    connectDB();\n \n    createTable();\n \n    if(SQLPrepare(gStmt, (SQLCHAR*)sSqlStr, SQL_NTS))\n    {\n        outError(\"sql prepare fail\", gStmt);\n        return -1;\n    }\n \n    if(SQLNumResultCols(gStmt, &amp;sColumns) != SQL_SUCCESS )\n    {\n        printf(\"get col length error \\n\");\n        return -1;\n    }\n \n    printf(\"----------------------------------------------------------------\\n\");\n    printf(\"%32s%16s%10s\\n\",\"Name\",\"Type\",\"Length\");\n    printf(\"----------------------------------------------------------------\\n\");\n \n    for(i = 0; i &lt; sColumns; i++)\n    {\n        SQLDescribeCol(gStmt,\n                       (SQLUSMALLINT)(i + 1),\n                       sColName,\n                       sizeof(sColName),\n                       &amp;sColNameLen,\n                       &amp;sColType,\n                       (SQLULEN *)&amp;sColLen,\n                       &amp;sDecimalDigits,\n                       (SQLSMALLINT *)&amp;sNullable);\n \n        printf(\"%32s%16d%10d\\n\",sColName, sColType, sColLen);\n    }\n \n    printf(\"----------------------------------------------------------------\\n\");\n \n    disconnectDB();\n \n    return 0;\n}\n    \n\n  \n\n\n위의 파일을 추가하고 make를 실행하면 아래와 같이 원하는 열의 내용들이 나타나는 것을 볼 수 있다.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample5_describe\nconnected ...\n----------------------------------------------------------------\nName Type Length\n----------------------------------------------------------------\nSEQ 5 5\nSCORE 4 10\nTOTAL -5 19\nPERCENTAGE 6 27\nRATIO 8 27\nID 12 10\nSRCIP 2104 15\nDSTIP 2106 60\nREG_DATE 9 31\nTLOG 2100 67108864\nIMAGE -2 67108864\n----------------------------------------------------------------\n[mach@localhost cli]$\n\n\nSQLColumns\n\nSQLColumns은 현재 테이블 내에 존재하는 컬럼들의 정보를 알아낼 수 있는 함수이다. 마크베이스에서도 위와 같은 함수를 지원하고 있으며 이를 이용하여 컬럼 각각의 정보들을 알아낼 수 있다.\n\n파일이름은 sample6_columns.c라고 한다.\n\n\nsample6_columns.c\n\n\n    #include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;machbase_sqlcli.h&gt;\n \n#include &lt;time.h&gt;\n \n#define MACHBASE_PORT_NO 5656\n \nSQLHENV gEnv;\nSQLHDBC gCon;\nSQLHSTMT gStmt;\nSQLCHAR gErrorState[6];\n \nvoid connectDB()\n{\n    char connStr[1024];\n \n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n \n    if (SQL_ERROR == SQLAllocEnv(&amp;gEnv)) {\n        printf(\"SQLAllocEnv error!!\\n\");\n        exit(1);\n    }\n \n    if (SQL_ERROR == SQLAllocConnect(gEnv, &amp;gCon)) {\n        printf(\"SQLAllocConnect error!!\\n\");\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n \n    sprintf(connStr,\"SERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n \n    if (SQL_ERROR == SQLDriverConnect( gCon, NULL,\n                                       (SQLCHAR *)connStr,\n                                       SQL_NTS,\n                                       NULL, 0, NULL,\n                                       SQL_DRIVER_NOPROMPT ))\n    {\n        printf(\"connection error\\n\");\n \n        if (SQL_SUCCESS == SQLError ( gEnv, gCon, NULL, NULL, &amp;errNo,\n                                      errMsg, 1024, &amp;msgLength ))\n        {\n            printf(\" mach-%d : %s\\n\", errNo, errMsg);\n        }\n        SQLFreeEnv(gEnv);\n        exit(1);\n    }\n \n    if (SQLAllocStmt(gCon, &amp;gStmt) == SQL_ERROR)\n    {\n        outError(\"AllocStmt error\", gStmt);\n    }\n \n    printf(\"connected ... \\n\");\n \n}\n \nvoid disconnectDB()\n{\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n \n    if (SQL_ERROR == SQLDisconnect(gCon)) {\n        printf(\"disconnect error\\n\");\n \n        if( SQL_SUCCESS == SQLError( gEnv, gCon, NULL, NULL, &amp;errNo,\n                                     errMsg, 1024, &amp;msgLength ))\n        {\n            printf(\" mach-%d : %s\\n\", errNo, errMsg);\n        }\n    }\n \n    SQLFreeConnect(gCon);\n    SQLFreeEnv(gEnv);\n}\n \nvoid outError(const char *aMsg, SQLHSTMT stmt)\n{\n    SQLINTEGER errNo;\n    SQLSMALLINT msgLength;\n    SQLCHAR errMsg[1024];\n \n    printf(\"ERROR : (%s)\\n\", aMsg);\n \n    if (SQL_SUCCESS == SQLError( gEnv, gCon, stmt, NULL, &amp;errNo,\n                                 errMsg, 1024, &amp;msgLength ))\n    {\n        printf(\" mach-%d : %s\\n\", errNo, errMsg);\n    }\n    exit(-1);\n}\n \nvoid executeDirectSQL(const char *aSQL, int aErrIgnore)\n{\n    SQLHSTMT stmt;\n \n    if (SQLAllocStmt(gCon, &amp;stmt) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"AllocStmt error\", stmt);\n    }\n \n    if (SQLExecDirect(stmt, (SQLCHAR *)aSQL, SQL_NTS) == SQL_ERROR)\n    {\n        if (aErrIgnore != 0) return;\n        printf(\"sql_exec_direct error[%s] \\n\", aSQL);\n        outError(\"sql_exec_direct error\", stmt);\n    }\n \n    if (SQL_ERROR == SQLFreeStmt(stmt, SQL_DROP))\n    {\n        if (aErrIgnore != 0) return;\n        outError(\"FreeStmt Error\", stmt);\n    }\n}\n \nvoid createTable()\n{\n    executeDirectSQL(\"DROP TABLE CLI_SAMPLE\", 1);\n    executeDirectSQL(\"CREATE TABLE CLI_SAMPLE(seq short, score integer, total long, percentage float, ratio double, id varchar(10), srcip ipv4, dstip ipv6, reg_date datetime, tlog text, image binary)\", 0);\n}\n \nint main()\n{\n    SQLCHAR sColName[32];\n    SQLSMALLINT sColType;\n    SQLCHAR sColTypeName[16];\n    SQLSMALLINT sColNameLen;\n    SQLSMALLINT sColTypeLen;\n    SQLSMALLINT sNullable;\n    SQLULEN sColLen;\n    SQLSMALLINT sDecimalDigits;\n    SQLLEN sOutlen;\n    SQLCHAR* sData;\n    SQLLEN sDisplaySize;\n    int i;\n \n    SQLSMALLINT sColumns;\n \n    connectDB();\n \n    createTable();\n \n    if(SQLColumns(gStmt, NULL, 0, NULL, 0, \"cli_sample\", SQL_NTS, NULL, 0) != SQL_SUCCESS)\n    {\n        printf(\"sql columns error!\\n\");\n        return -1;\n    }\n \n    SQLBindCol(gStmt, 4, SQL_C_CHAR, sColName, sizeof(sColName), &amp;sColNameLen);\n    SQLBindCol(gStmt, 5, SQL_C_SSHORT, &amp;sColType, 0, &amp;sColTypeLen);\n    SQLBindCol(gStmt, 6, SQL_C_CHAR, sColTypeName, sizeof(sColTypeName), NULL);\n    SQLBindCol(gStmt, 7, SQL_C_SLONG, &amp;sColLen, 0, NULL);\n \n    printf(\"--------------------------------------------------------------------------------\\n\");\n    printf(\"%32s%16s%16s%10s\\n\",\"Name\",\"Type\",\"TypeName\",\"Length\");\n    printf(\"--------------------------------------------------------------------------------\\n\");\n \n    while( SQLFetch(gStmt) != SQL_NO_DATA )\n    {\n        printf(\"%32s%16d%16s%10d\\n\",sColName, sColType, sColTypeName, sColLen);\n    }\n    printf(\"--------------------------------------------------------------------------------\\n\");\n \n    disconnectDB();\n \n    return 0;\n}\n    \n\n  \n\n\n위의 파일을 추가하고 make를 실행한다. 결과는 다음과 같다.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample6_columns\nconnected ...\n--------------------------------------------------------------------------------\nName Type TypeName Length\n--------------------------------------------------------------------------------\n_ARRIVAL_TIME 93 DATE 31\nSEQ 5 SMALLINT 5\nSCORE 4 INTEGER 10\nTOTAL -5 BIGINT 19\nPERCENTAGE 6 FLOAT 27\nRATIO 8 DOUBLE 27\nID 12 VARCHAR 10\nSRCIP 2104 IPV4 15\nDSTIP 2106 IPV6 60\nREG_DATE 93 DATE 31\nTLOG 2100 TEXT 67108864\nIMAGE -2 BINARY 67108864\n--------------------------------------------------------------------------------\n\n\n멀티 쓰레드 append 예제\n\n하나의 프로그램에서 여러 스레드를 이용해 여러 테이블에 append하는 예제이다.\n\n파일 이름은 sample8_multi_session_multi_table.c로 한다.\n\n\nsample8_multi_session_multi_table.c\n\n\n    #include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;pthread.h&gt;\n#include &lt;machbase_sqlcli.h&gt;\n \n#define MACHBASE_PORT_NO       5656\n#define ERROR_CHECK_COUNT   100\n \n#define LOG_FILE_CNT        3\n#define MAX_THREAD_NUM      LOG_FILE_CNT\n \n#define RC_FAILURE          -1\n#define RC_SUCCESS          0\n \n#define UNUSED(aVar) do { (void)(aVar); } while(0)\n \nchar *gTableName[LOG_FILE_CNT] = {\"table_f1\", \"table_f2\", \"table_event\"};\nchar *gFileName[LOG_FILE_CNT] =  {\"suffle_data1.txt\",\"suffle_data2.txt\",\"suffle_data3.txt\"};\n \nvoid printError(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt, char *aMsg);\nint connectDB(SQLHENV *aEnv, SQLHDBC *aCon);\nvoid disconnectDB(SQLHENV aEnv, SQLHDBC aCon);\nint executeDirectSQL(SQLHENV aEnv, SQLHDBC aCon, const char *aSQL, int aErrIgnore);\nint appendOpen(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt, char* aTableName);\nint appendClose(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt);\nint createTables(SQLHENV aEnv, SQLHDBC aCon);\n \n/*\n * error code returned from CLI lib\n */\nvoid printError(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt, char *aMsg)\n{\n    SQLINTEGER      sNativeError;\n    SQLCHAR         sErrorMsg[SQL_MAX_MESSAGE_LENGTH + 1];\n    SQLCHAR         sSqlState[SQL_SQLSTATE_SIZE + 1];\n    SQLSMALLINT     sMsgLength;\n \n    if( aMsg != NULL )\n    {\n        printf(\"%s\\n\", aMsg);\n    }\n \n    if( SQLError(aEnv, aCon, aStmt, sSqlState, &amp;sNativeError,\n                 sErrorMsg, SQL_MAX_MESSAGE_LENGTH, &amp;sMsgLength) == SQL_SUCCESS )\n    {\n        printf(\"SQLSTATE-[%s], Machbase-[%d][%s]\\n\", sSqlState, sNativeError, sErrorMsg);\n    }\n}\n \n/*\n * error code returned from Machbase server\n */\n \nvoid appendDumpError(SQLHSTMT    aStmt,\n                     SQLINTEGER  aErrorCode,\n                     SQLPOINTER  aErrorMessage,\n                     SQLLEN      aErrorBufLen,\n                     SQLPOINTER  aRowBuf,\n                     SQLLEN      aRowBufLen)\n{\n    char       sErrMsg[1024] = {0, };\n    char       sRowMsg[32 * 1024] = {0, };\n \n    UNUSED(aStmt);\n \n    if (aErrorMessage != NULL)\n    {\n        strncpy(sErrMsg, (char *)aErrorMessage, aErrorBufLen);\n    }\n \n    if (aRowBuf != NULL)\n    {\n        strncpy(sRowMsg, (char *)aRowBuf, aRowBufLen);\n    }\n \n    fprintf(stdout, \"Append Error : [%d][%s]\\n[%s]\\n\\n\", aErrorCode, sErrMsg, sRowMsg);\n}\n \n \nint connectDB(SQLHENV *aEnv, SQLHDBC *aCon)\n{\n    char sConnStr[1024];\n \n    if( SQLAllocEnv(aEnv) != SQL_SUCCESS )\n    {\n        printf(\"SQLAllocEnv error\\n\");\n        return RC_FAILURE;\n    }\n \n    if( SQLAllocConnect(*aEnv, aCon) != SQL_SUCCESS )\n    {\n        printf(\"SQLAllocConnect error\\n\");\n \n        SQLFreeEnv(*aEnv);\n        *aEnv = SQL_NULL_HENV;\n \n        return RC_FAILURE;\n    }\n \n    sprintf(sConnStr,\"SERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n \n    if( SQLDriverConnect( *aCon, NULL,\n                          (SQLCHAR *)sConnStr,\n                          SQL_NTS,\n                          NULL, 0, NULL,\n                          SQL_DRIVER_NOPROMPT ) != SQL_SUCCESS\n      )\n    {\n \n        printError(*aEnv, *aCon, NULL, \"SQLDriverConnect error\");\n \n        SQLFreeConnect(*aCon);\n        *aCon = SQL_NULL_HDBC;\n \n        SQLFreeEnv(*aEnv);\n        *aEnv = SQL_NULL_HENV;\n \n        return RC_FAILURE;\n    }\n \n    return RC_SUCCESS;\n}\n \n \nvoid disconnectDB(SQLHENV aEnv, SQLHDBC aCon)\n{\n    if( SQLDisconnect(aCon) != SQL_SUCCESS )\n    {\n        printError(aEnv, aCon, NULL, \"SQLDisconnect error\");\n    }\n \n    SQLFreeConnect(aCon);\n    aCon = SQL_NULL_HDBC;\n \n    SQLFreeEnv(aEnv);\n    aEnv = SQL_NULL_HENV;\n}\n \n \nint executeDirectSQL(SQLHENV aEnv, SQLHDBC aCon, const char *aSQL, int aErrIgnore)\n{\n    SQLHSTMT sStmt = SQL_NULL_HSTMT;\n \n    if( SQLAllocStmt(aCon, &amp;sStmt) != SQL_SUCCESS )\n    {\n        if( aErrIgnore == 0 )\n        {\n            printError(aEnv, aCon, sStmt, \"SQLAllocStmt Error\");\n            return RC_FAILURE;\n        }\n    }\n \n    if( SQLExecDirect(sStmt, (SQLCHAR *)aSQL, SQL_NTS) != SQL_SUCCESS )\n    {\n \n        if( aErrIgnore == 0 )\n        {\n            printError(aEnv, aCon, sStmt, \"SQLExecDirect Error\");\n \n            SQLFreeStmt(sStmt,SQL_DROP);\n            sStmt = SQL_NULL_HSTMT;\n            return RC_FAILURE;\n        }\n    }\n \n    if( SQLFreeStmt(sStmt, SQL_DROP) != SQL_SUCCESS )\n    {\n        if (aErrIgnore == 0)\n        {\n            printError(aEnv, aCon, sStmt, \"SQLFreeStmt Error\");\n            sStmt = SQL_NULL_HSTMT;\n            return RC_FAILURE;\n        }\n    }\n    sStmt = SQL_NULL_HSTMT;\n \n    return RC_SUCCESS;\n}\n \n \nint appendOpen(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt, char* aTableName)\n{\n    if( aTableName == NULL )\n    {\n        printf(\"append open wrong table name\");\n        return RC_FAILURE;\n    }\n \n    if( SQLAppendOpen(aStmt, (SQLCHAR *)aTableName, ERROR_CHECK_COUNT) != SQL_SUCCESS )\n    {\n        printError(aEnv, aCon, aStmt, \"SQLAppendOpen error\");\n        return RC_FAILURE;\n    }\n    return RC_SUCCESS;\n}\n \n \nint appendClose(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt)\n{\n    int sSuccessCount = 0;\n    int sFailureCount = 0;\n \n    if( SQLAppendClose(aStmt, &amp;sSuccessCount, &amp;sFailureCount) != SQL_SUCCESS )\n    {\n        printError(aEnv, aCon, aStmt, \"SQLAppendClose error\");\n        return RC_FAILURE;\n    }\n \n    printf(\"append result success : %d, failure : %d\\n\", sSuccessCount, sFailureCount);\n \n    return RC_SUCCESS;\n}\n \n \nint createTables(SQLHENV aEnv, SQLHDBC aCon)\n{\n    int      i;\n    char    *sSchema[] = { \"srcip1 ipv4, srcip2 ipv6, srcport short, dstip1 ipv4, dstip2 ipv6, dstport short, data1 long, data2 long\",\n        \"srcip1 ipv4, srcip2 ipv6, srcport short, dstip1 ipv4, dstip2 ipv6, dstport short, data1 long, data2 long\",\n        \"machine ipv4, err integer, msg varchar(30)\"\n    };\n \n    char sDropQuery[256];\n    char sCreateQuery[256];\n \n    for(i = 0; i &lt; LOG_FILE_CNT; i++)\n    {\n        snprintf(sDropQuery, 256, \"DROP TABLE %s\", gTableName[i]);\n        snprintf(sCreateQuery, 256, \"CREATE TABLE %s ( %s )\", gTableName[i], sSchema[i]);\n \n        executeDirectSQL(aEnv, aCon, sDropQuery, 1);\n        executeDirectSQL(aEnv, aCon, sCreateQuery, 0);\n    }\n \n    return RC_SUCCESS;\n}\n \n \nint appendF1(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt, FILE *aFp)\n{\n    SQL_APPEND_PARAM sParam[8];\n    SQLRETURN        sRC;\n \n    SQLINTEGER      sNativeError;\n    SQLCHAR         sErrorMsg[SQL_MAX_MESSAGE_LENGTH + 1];\n    SQLCHAR         sSqlState[SQL_SQLSTATE_SIZE + 1];\n    SQLSMALLINT     sMsgLength;\n \n    char             sData[4][64];\n \n    memset(sParam, 0, sizeof(sParam));\n \n    fscanf(aFp, \"%s %s %hd %s %s %hd %lld %lld\\n\",\n           sData[0], sData[1], &amp;sParam[2].mShort,\n           sData[2], sData[3], &amp;sParam[5].mShort,\n           &amp;sParam[6].mLong, &amp;sParam[7].mLong);\n \n    sParam[0].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = sData[0];\n \n    sParam[1].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[1].mIP.mAddrString = sData[1];\n \n    sParam[3].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[3].mIP.mAddrString = sData[2];\n \n    sParam[4].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[4].mIP.mAddrString = sData[3];\n \n    sRC = SQLAppendDataV2(aStmt, sParam);\n    if( !SQL_SUCCEEDED(sRC) )\n    {\n        if( SQLError(aEnv, aCon, aStmt, sSqlState, &amp;sNativeError,\n                     sErrorMsg, SQL_MAX_MESSAGE_LENGTH, &amp;sMsgLength) != SQL_SUCCESS )\n        {\n            return RC_FAILURE;\n        }\n \n        printf(\"SQLSTATE-[%s], Machbase-[%d][%s]\\n\", sSqlState, sNativeError, sErrorMsg);\n \n        if( sNativeError != 9604 &amp;&amp;\n            sNativeError != 9605 &amp;&amp;\n            sNativeError != 9606 )\n        {\n            return RC_FAILURE;\n        }\n        else\n        {\n            //data value error in one record, so return success to keep attending\n        }\n    }\n    return RC_SUCCESS;\n}\n \n \nint appendF2(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt, FILE* aFp)\n{\n    SQL_APPEND_PARAM sParam[8];\n    SQLRETURN        sRC;\n \n    SQLINTEGER      sNativeError;\n    SQLCHAR         sErrorMsg[SQL_MAX_MESSAGE_LENGTH + 1];\n    SQLCHAR         sSqlState[SQL_SQLSTATE_SIZE + 1];\n    SQLSMALLINT     sMsgLength;\n \n    char             sData[4][64];\n \n    memset(sParam, 0, sizeof(sParam));\n \n    fscanf(aFp, \"%s %s %hd %s %s %hd %lld %lld\\n\",\n           sData[0], sData[1], &amp;sParam[2].mShort,\n           sData[2], sData[3], &amp;sParam[5].mShort,\n           &amp;sParam[6].mLong, &amp;sParam[7].mLong);\n \n    sParam[0].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = sData[0];\n \n    sParam[1].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[1].mIP.mAddrString = sData[1];\n \n    sParam[3].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[3].mIP.mAddrString = sData[2];\n \n    sParam[4].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[4].mIP.mAddrString = sData[3];\n \n    sRC = SQLAppendDataV2(aStmt, sParam);\n    if( !SQL_SUCCEEDED(sRC) )\n    {\n        if( SQLError(aEnv, aCon, aStmt, sSqlState, &amp;sNativeError,\n                     sErrorMsg, SQL_MAX_MESSAGE_LENGTH, &amp;sMsgLength) != SQL_SUCCESS )\n        {\n            return RC_FAILURE;\n        }\n \n        printf(\"SQLSTATE-[%s], Machbase-[%d][%s]\\n\", sSqlState, sNativeError, sErrorMsg);\n \n        if( sNativeError != 9604 &amp;&amp;\n            sNativeError != 9605 &amp;&amp;\n            sNativeError != 9606 )\n        {\n            return RC_FAILURE;\n        }\n        else\n        {\n            //data value error in one record, so return success to keep attending\n        }\n    }\n    return RC_SUCCESS;\n}\n \n \nint appendEvent(SQLHENV aEnv, SQLHDBC aCon, SQLHSTMT aStmt, FILE* aFp)\n{\n    SQL_APPEND_PARAM sParam[3];\n    SQLRETURN        sRC;\n \n    SQLINTEGER      sNativeError;\n    SQLCHAR         sErrorMsg[SQL_MAX_MESSAGE_LENGTH + 1];\n    SQLCHAR         sSqlState[SQL_SQLSTATE_SIZE + 1];\n    SQLSMALLINT     sMsgLength;\n \n    char             sData[2][20];\n \n    memset(sParam, 0, sizeof(sParam));\n \n    fscanf(aFp, \"%s %d %s\\n\",sData[0], &amp;sParam[1].mInteger, sData[1]);\n \n    sParam[0].mIP.mLength = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = sData[0];\n \n    sParam[2].mVarchar.mLength = strlen(sData[1]);\n    sParam[2].mVarchar.mData = sData[1];\n \n    sRC = SQLAppendDataV2(aStmt, sParam);\n    if( !SQL_SUCCEEDED(sRC) )\n    {\n        if( SQLError(aEnv, aCon, aStmt, sSqlState, &amp;sNativeError,\n                     sErrorMsg, SQL_MAX_MESSAGE_LENGTH, &amp;sMsgLength) != SQL_SUCCESS )\n        {\n            return RC_FAILURE;\n        }\n \n        printf(\"SQLSTATE-[%s], Machbase-[%d][%s]\\n\", sSqlState, sNativeError, sErrorMsg);\n \n        if( sNativeError != 9604 &amp;&amp;\n            sNativeError != 9605 &amp;&amp;\n            sNativeError != 9606 )\n        {\n            return RC_FAILURE;\n        }\n        else\n        {\n            //data value error in one record, so return success to keep attending\n        }\n    }\n    return RC_SUCCESS;\n}\n \n \nvoid *eachThread(void *aIdx)\n{\n    SQLHENV    sEnv = SQL_NULL_HENV;\n    SQLHDBC    sCon = SQL_NULL_HDBC;\n    SQLHSTMT   sStmt[LOG_FILE_CNT] = {SQL_NULL_HSTMT,};\n \n    FILE*      sFp;\n    int        i;\n    int        sLogType;\n \n    int        sThrNo = *(int *)aIdx;\n \n    // Alloc ENV and DBC\n    if( connectDB(&amp;sEnv, &amp;sCon) == RC_SUCCESS )\n    {\n        printf(\"[%d]connectDB success.\\n\", sThrNo);\n    }\n    else\n    {\n        printf(\"[%d]connectDB failure.\\n\", sThrNo);\n        goto error;\n    }\n \n    // set timed flush true\n    if( SQLSetConnectAppendFlush(sCon, 1) != SQL_SUCCESS )\n    {\n        printError(sEnv, sCon, NULL, \"SQLSetConnectAppendFlush Error\");\n        goto error;\n    }\n \n    for( i = 0; i &lt; LOG_FILE_CNT; i++ )\n    {\n        // Alloc stmt\n        if( SQLAllocStmt(sCon,&amp;sStmt[i]) != SQL_SUCCESS )\n        {\n            printError(sEnv, sCon, sStmt[i], \"SQLAllocStmt Error\");\n            goto error;\n        }\n \n        if( appendOpen(sEnv, sCon, sStmt[i], gTableName[i]) == RC_FAILURE )\n        {\n            printError(sEnv, sCon, sStmt[i], \"SQLAppendOpen Error\");\n            goto error;\n        }\n        else\n        {\n            printf(\"[%d-%d]appendOpen success.\\n\", sThrNo, i);\n        }\n \n        if( SQLAppendSetErrorCallback(sStmt[i], appendDumpError) != SQL_SUCCESS )\n        {\n            printError(sEnv, sCon, sStmt[i], \"SQLAppendSetErrorCallback Error\");\n            goto error;\n        }\n \n        // set timed flush interval as 2 seconds\n        if( SQLSetStmtAppendInterval(sStmt[i], 2000) != SQL_SUCCESS )\n        {\n            printError(sEnv, sCon, sStmt[i], \"SQLSetStmtAppendInterval Error\");\n            goto error;\n        }\n    }\n \n    sFp = fopen((char*)gFileName[sThrNo], \"rt\");\n    if( sFp == NULL )\n    {\n        printf(\"file open error - [%d][%s]\\n\", sThrNo, gFileName[sThrNo]);\n    }\n    else\n    {\n        printf(\"file open success - [%d][%s]\\n\", sThrNo, gFileName[sThrNo]);\n \n        for( i = 0; !feof(sFp); i++ )\n        {\n            fscanf(sFp, \"%d \", &amp;sLogType);\n            switch(sLogType)\n            {\n                case 1://f1\n                    if( appendF1(sEnv, sCon, sStmt[0], sFp) == RC_FAILURE )\n                    {\n                        goto error;\n                    }\n                    break;\n                case 2://f2\n                    if( appendF2(sEnv, sCon, sStmt[1],sFp) == RC_FAILURE )\n                    {\n                        goto error;\n                    }\n                    break;\n                case 3://event\n                    if(appendEvent(sEnv, sCon, sStmt[2], sFp) == RC_FAILURE )\n                    {\n                        goto error;\n                    }\n                    break;\n                default:\n                    printf(\"unknown type error\\n\");\n                    break;\n            }\n \n            if( (i%10000) == 0 )\n            {\n                fprintf(stdout, \".\");\n                fflush(stdout);\n            }\n        }\n        printf(\"\\n\");\n \n        fclose(sFp);\n    }\n \n    for( i = 0; i &lt; LOG_FILE_CNT; i++)\n    {\n        printf(\"[%d-%d]appendClose start...\\n\", sThrNo, i);\n        if( appendClose(sEnv, sCon, sStmt[i]) == RC_FAILURE )\n        {\n            printf(\"[%d-%d]appendClose failure\\n\", sThrNo, i);\n        }\n        else\n        {\n            printf(\"[%d-%d]appendClose success\\n\", sThrNo, i);\n        }\n \n        if( SQLFreeStmt(sStmt[i], SQL_DROP) != SQL_SUCCESS )\n        {\n            printError(sEnv, sCon, sStmt[i], \"SQLFreeStmt Error\");\n        }\n        sStmt[i] = SQL_NULL_HSTMT;\n    }\n \n    disconnectDB(sEnv, sCon);\n \n    printf(\"[%d]disconnected.\\n\", sThrNo);\n \n    pthread_exit(NULL);\n \nerror:\n    for( i = 0; i &lt; LOG_FILE_CNT; i++)\n    {\n        if( sStmt[i] != SQL_NULL_HSTMT )\n        {\n            appendClose(sEnv, sCon, sStmt[i]);\n \n            if( SQLFreeStmt(sStmt[i], SQL_DROP) != SQL_SUCCESS )\n            {\n                printError(sEnv, sCon, sStmt[i], \"SQLFreeStmt Error\");\n            }\n            sStmt[i] = SQL_NULL_HSTMT;\n        }\n    }\n \n    if( sCon != SQL_NULL_HDBC )\n    {\n        disconnectDB(sEnv, sCon);\n    }\n \n    pthread_exit(NULL);\n}\n \n \nint initTables()\n{\n    SQLHENV     sEnv  = SQL_NULL_HENV;\n    SQLHDBC     sCon  = SQL_NULL_HDBC;\n \n    if( connectDB(&amp;sEnv, &amp;sCon) == RC_SUCCESS )\n    {\n        printf(\"connectDB success.\\n\");\n    }\n    else\n    {\n        printf(\"connectDB failure.\\n\");\n        goto error;\n    }\n \n    if( createTables(sEnv, sCon) == RC_SUCCESS )\n    {\n        printf(\"createTables success.\\n\");\n    }\n    else\n    {\n        printf(\"createTables failure.\\n\");\n        goto error;\n    }\n \n    disconnectDB(sEnv, sCon);\n \n    return RC_SUCCESS;\n \nerror:\n \n    if( sCon != SQL_NULL_HDBC )\n    {\n        disconnectDB(sEnv, sCon);\n    }\n \n    return RC_FAILURE;\n}\n \n \nint main()\n{\n    pthread_t sThread[MAX_THREAD_NUM];\n    int       sNum[MAX_THREAD_NUM];\n    int       sRC;\n    int       i;\n \n    initTables();\n \n    //\n    //eachThread has own ENV,DBC and STMT\n    //\n    for(i = 0; i &lt; MAX_THREAD_NUM; i++)\n    {\n        sNum[i] = i;\n \n        sRC = pthread_create(&amp;sThread[i], NULL, (void *)eachThread, (void*)&amp;sNum[i]);\n        if ( sRC != RC_SUCCESS )\n        {\n            printf(\"Error in Thread create[%d] : %d\\n\", i, sRC);\n            return RC_FAILURE;\n        }\n    }\n \n    for(i = 0; i &lt; MAX_THREAD_NUM; i++)\n    {\n        sRC = pthread_join(sThread[i], NULL);\n        if( sRC != RC_SUCCESS )\n        {\n            printf(\"Error in Thread[%d] : %d\\n\", i, sRC);\n            return RC_FAILURE;\n        }\n        printf(\"%d thread join\\n\", i+1);\n    }\n \n    return RC_SUCCESS;\n}\n    \n\n  \n\n\nmake 코드를 추가하고 실행 파일을 실행해본다. 쓰레드를 이용하므로 출력 순서가 다를 수 있다. 실행 결과는 다음과 같다.\n\n[mach@localhost cli]$ make sample8_multi_session_multi_table\ngcc -c -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -I/home/mach/machbase_home/include -I. -L//home/mach/machbase_home/include -osample8_multi_session_multi_table.o sample8_multi_session_multi_table.c\ngcc -m64 -mtune=k8 -L/home/mach/machbase_home/lib -osample8_multi_session_multi_table sample8_multi_session_multi_table.o -lmachcli  -L/home/mach/machbase_home/lib -lm -lpthread -ldl -lrt -rdynamic\n[mach@localhost cli]$ ./sample8_multi_session_multi_table\nconnectDB success.\ncreateTables success.\n[0]connectDB success.\n[1]connectDB success.\n[2]connectDB success.\n[1-0]appendOpen success.\n[0-0]appendOpen success.\n[2-0]appendOpen success.\n[1-1]appendOpen success.\n[2-1]appendOpen success.\n[0-1]appendOpen success.\n[1-2]appendOpen success.\n[2-2]appendOpen success.\nfile open success - [1][suffle_data2.txt]\nfile open success - [2][suffle_data3.txt]\n[0-2]appendOpen success.\nfile open success - [0][suffle_data1.txt]\n.......................................................................................\n \n[1-0]appendClose start...\n..\n[0-0]appendClose start...\nappend result success : 100000, failure : 0\n[1-0]appendClose success\n[1-1]appendClose start...\nappend result success : 100000, failure : 0\n[1-1]appendClose success\n[1-2]appendClose start...\nappend result success : 100000, failure : 0\n[1-2]appendClose success\nappend result success : 100000, failure : 0\n[0-0]appendClose success\n[0-1]appendClose start...\n.append result success : 100000, failure : 0\n[0-1]appendClose success\n[0-2]appendClose start...\nappend result success : 100000, failure : 0\n[0-2]appendClose success\n \n[2-0]appendClose start...\nappend result success : 100000, failure : 0\n[2-0]appendClose success\n[2-1]appendClose start...\nappend result success : 100000, failure : 0\n[2-1]appendClose success\n[2-2]appendClose start...\nappend result success : 100000, failure : 0\n[2-2]appendClose success\n[1]disconnected.\n[2]disconnected.\n[0]disconnected.\n1 thread join\n2 thread join\n3 thread join\n\n\nmachsql을 통해 아래와 같이 결과를 확인할 수 있다.\n\n[mach@localhost cli]$ machsql\n \n \n=================================================================\n     Machbase Client Query Utility\n     Release Version 3.5.0\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase Server Addr (Default:127.0.0.1) :\nMachbase User ID  (Default:SYS)\nMachbase User Password : manager\nMACH_CONNECT_MODE=INET, PORT=5656\nMach&gt; select count(*) from table_f1;\ncount(*)\n-----------------------\n300000\n[1] Row Selected.\nMach&gt; select count(*) from table_f2;\ncount(*)\n-----------------------\n300000\n[1] row(s) selected.\nMach&gt; select count(*) from table_event;\ncount(*)\n-----------------------\n300000\n[1] row(s) selected."
					}
					
				
		
				
					,
					
					"install-cluster-cluster-cmd-html": {
						"id": "install-cluster-cluster-cmd-html",
						"title": "Cluster Edition 설치 (Command-line)",
						"version": "all",
						"categories": "",
						"url": " /install/cluster/cluster-cmd.html",
						"content": "Coordinator / Deployer 설치, Package 추가\n  Lookup / Broker / Warehouse 설치"
					}
					
				
		
				
					,
					
					"install-cluster-cluster-env-html": {
						"id": "install-cluster-cluster-env-html",
						"title": "Cluster Edition 설치 준비",
						"version": "all",
						"categories": "",
						"url": " /install/cluster/cluster-env.html",
						"content": "파일 LIMIT 확인 및 변경\n\n열 수 있는 최대 파일 개수를 늘려야 하므로, 아래와 같이 수행한다.\n\n\n  \n    /etc/security/limits.conf 파일을 아래와 같이 수정한다.\n\n     sudo vi /etc/security/limits.conf\n *       hard   nofile      65535\n *       soft   nofile      65535\n    \n  \n  \n    재부팅 한다.\n\n     sudo reboot\n # 또는\n sudo shutdown -r now\n    \n  \n  \n    아래 명령어를 실행하여 결과를 확인한다. 65535 가 출력되면 성공적으로 변경된 것이다.\n\n     ulimit -Sn\n    \n    서버 시간 동기화\n  \n\n\n각 Host 간 서버 시간을 동기화해야 한다. 이미 동기화를 하고 있다면 확인 차원에서 점검하도록 하자.\n\n\n  \n    모든 서버의 시간을 time 서버와 동기화한다.\n\n     # 다음 명령어로 동기화한다.                               \n /usr/bin/rdate -s time.bora.net &amp;&amp; /sbin/clock -w\n    \n  \n  \n    Time 서버를 활용할 수 없는 경우 직접 명령어로 수정한다.\n\n     # 다음 명령어로 수정한다.                                \n date -s \"2017-10-31 11:15:30\"\n    \n  \n  \n    변경된 시간을 확인한다\n\n     # 다음 명령어로 확인한다.                                 \n date\n    \n  \n\n\n네트워크 커널 파라미터 변경\n\n\n  \n    현재 설정된 값을 확인한다.\n\n     # 다음 명령어로 확인한다.                                 \n sysctl -a | egrep 'mem_(max|default)|tcp_.*mem'\n    \n  \n  \n    아래 명령어로 설정값을 변경한다.(64GB Memory 기준)\n\n     sysctl -w net.core.rmem_default = \"33554432\"     # 32MB\n sysctl -w net.core.wmem_default = \"33554432\"\n sysctl -w net.core.rmem_max     = \"268435456\"    # 256MB\n sysctl -w net.core.wmem_max     = \"268435456\"    # 최소 256KB 기본 32MB 최대 256MB\n sysctl -w net.ipv4.tcp_rmem     = \"262144 33554432 268435456\"\n sysctl -w net.ipv4.tcp_wmem     = \"262144 33554432 268435456\"\n    \n # 8388608 Page * 4KB = 32GB (가용 메모리에 따라 변경 필요)\n sysctl -w net.ipv4.tcp_mem      = \"8388608 8388608 8388608\"\n    \n  \n  \n    변경값을 유지하려면 /etc/sysctl.conf 파일에 추가하고 호스트 OS 를 재시작한다.\n\n     # /etc/sysctl.conf 파일 수정한다.\n net.core.rmem_default = \"33554432\"\n net.core.wmem_default = \"33554432\"\n net.core.rmem_max     = \"268435456\"\n net.core.wmem_max     = \"268435456\"\n net.ipv4.tcp_rmem     = \"262144 33554432 268435456\"\n net.ipv4.tcp_wmem     = \"262144 33554432 268435456\"\n net.ipv4.tcp_mem      = \"8388608 8388608 8388608\"\n    \n  \n\n\n사용자 생성\n\n\n  \n    Machbase 설치를 위한 리눅스 OS 사용자 machbase를 생성한다.\n사용자 계정 디렉토리는 /home/machbase 로 생성되도록 한다.\n\n     # 다음 명령어로 사용자 \"machbase\"를 추가한다.\n $ sudo useradd machbase --home-dir \"/home/machbase\"\n    \n  \n  \n    사용자 “machbase”의 비밀번호를 설정한다.\n\n     sudo passwd machbase"
					}
					
				
		
				
					,
					
					"install-cluster-html": {
						"id": "install-cluster-html",
						"title": "Cluster Edition 설치",
						"version": "all",
						"categories": "",
						"url": " /install/cluster.html",
						"content": "Cluster Edition 설치 준비\n  Cluster Edition 설치 (Command-line)"
					}
					
				
		
				
					,
					
					"intro-edition-cluster-html": {
						"id": "intro-edition-cluster-html",
						"title": "Cluster Edition",
						"version": "all",
						"categories": "",
						"url": " /intro/edition/cluster.html",
						"content": "목차\n\n\n  왜 Cluster Edition을 써야 하는가?\n  용어\n  구조\n  데이터 저장/조회\n  이중화\n  복구 방법\n  지원되지 않는 기능\n  지원 하드웨어 및 운영체제\n\n\n왜 Cluster Edition을 써야 하는가?\n\nCluster Edition은, 빠른 입력 속도와 표준 SQL로 조회가 가능한 Machbase Fog Edition 으로도 처리할 수 없는 초대용량의 데이터 입력/조회를 분산 환경에서 처리할 수 있는 제품이다.\n\nMachbase 초고속으로 시계열 데이터를 입력받을 수 있는 Fog Edition 을 서비스하고 있다. 그러나, 다음의 단점이 존재한다.\n\n\n  하나의 프로세스로 구성되어 있기 때문에, 가용성이 떨어진다.\n  데이터를 분석할 때 하나의 프로세스가 전담하므로, 대용량 데이터 분석 성능을 늘리는 데 한계가 있다.\n\n\n위의 단점을 극복하기 위해, 즉 가용성을 확보하고 대용량 데이터를 저장하고 분석하는데 확장성을 확보할 수 있는 새로운 제품군이 필요하다. 이런 요구사항을 만족할 수 있는 것이 Machbase Cluster Edition 이다.\n\n용어\n\nHost\n\n물리적인 서버 1대, 또는 클라우드/VM 에서의 OS 인스턴스 1대를 나타낸다.\n\nNode\n\n서버에 상주하는 Machbase Process 를 나타낸다.\n\nProcess 타입은 아래의 Node Type 과 동일하다.\n\n\n  Coordinator\n  Deployer\n  Lookup\n  Broker\n  Warehouse\n\n\n구조\n\nMachbase Cluster Edition 은, Host 에 상주하는 여러 Node 가 하나의 클러스터를 구성한다.\n\n\n\n고가용성\n\n내부의 모든 Node 중 하나가 중단되어도 서비스가 지속될 수 있도록 한다\n\n고확장성\n\n데이터 저장을 분산할 수 있고, 분산된 데이터에서 병렬 분석이 가능하므로 클러스터를 확장할수록 성능이 증가한다.\n\nNode의 분류\n\n각 Node는 다음과 같이 분류할 수 있다.\n\n\n  \n    \n      분류\n      설명\n      프로세스 이름\n    \n  \n  \n    \n      Coordinator\n      모든 범용 서버와 Node를 관리하는 프로세스\n      machcoordinatord\n    \n    \n      Deployer\n      Host 마다 하나씩 상주하는 프로세스 Broker/Warehouse 의 설치와 업그레이드, 상태 관찰을 담당한다.\n      machdeployerd\n    \n    \n      Lookup\n      Lookup 테이블 데이터를 가지고 있는 프로세스\n      machlookupd\n    \n    \n      Broker\n      실제 클라이언트 프로그램을 맞이하는 프로세스 클라이언트의 데이터 입력/데이터 조회 쿼리를 Warehouse 에 분산 수행시키는 역할을 한다.\n      machbased\n    \n    \n      Warehouse\n      실제 데이터를 저장하고 있는 프로세스 전체 클러스터 데이터 중 일부를 저장하고 있으며, Broker 로부터 전달받은 명령을 수행한다.\n      machbased\n    \n  \n\n\nCoordinator\n\nCoordinator는 모든 Node의 상태를 관리하는 Process로, 최대 2개를 가질 수 있다.\n\n먼저 생성된 Coordinator를 Primary Coordinator, 그 다음을 Secondary Coordinator라 하고 Primary Coordinator만이 모든 Node의 상태를 관리한다.\n\nPrimary Coordinator가 종료하면 Secondary Coordinator가 Primary Coordinator로 격상된다.\n\nDeployer\n\nCoordinator에 의해 관리되지만, 단순히 Broker/Warehouse/Lookup Node의 설치/제거를 담당하는 Process 이다.\n\n보통은 Node 를 설치할 Host 에 1대씩 추가하지만, 설치 성능을 위해 여러 Deployer를 추가할 수도 있다.\n\n\n\n\n\n서버에서 설치 예제\n\n아래 그림(a)는, 범용 서버 4대에 2개의 Coordinator, 2개의 Broker, 4개의 Warehouse Active, 4개의 Warehouse Standby Node를 설치한 것을 나타낸 것이다.\n그림 처럼, 범용 서버의 호스트 이름과 할당된 포트 번호가 이어진 ‘hostname:port’ 로 각 Node를 구분할 수 있다.\n\n\n\nLookup\n\nLookup은 Lokkup Table 데이터 관리를 위해 존재한다.\n\nBroker\n\nBroker는 말 그대로 Client의 명령을 Warehouse에게 전달하고, Warehouse의 결과를 Client에 모아서 전달하는 역할을 한다.\n\n\n  데이터 입력 시, Broker는 Warehouse에게 데이터 입력을 골고루 가도록 한다.\n  이터 조회 시, Broker는 Warehouse에게 데이터를 가져오도록 한 뒤에 모든 결과를 모아서 전달한다.\n\n\nBroker는 Log Table의 데이터를 가지고 있지 않지만, Volatile Table의 데이터는 가지고 있는다.\n\nWarehouse\n\nWarehouse 는 Log Table 데이터를 직접 저장하게 되고, Broker가 전달한 명령을 실제로 수행하는 역할을 한다.\n\nBroker 처럼 Warehouse 에도 직접 클라이언트 접속이 가능하지만, 데이터 입력/갱신/삭제는 할 수 없고 오로지 해당 Warehouse 데이터 조회만 가능하다.\n\nWarehouse Group\n\nWarehouse 는, 자신이 속할 Group 을 지정할 수 있다.\n\n\n  Broker 가 데이터를 입력할 때, 같은 Group 에 있는 모든 Warehouse는 동일한 레코드를 입력받는다.\n  Group 의 특정 Warehouse 가 탈락하더라도, 데이터 조회는 이상 없다.\n  Group 에 새로운 Warehouse 가 추가되면, 이중화 (Replication) 를 통해 동일한 레코드를 유지한다.\n\n\nWarehouse Group 의 상태\n\n\n  \n    \n      상태\n      INSERT/APPEND\n      SELECT\n    \n  \n  \n    \n      Normal\n      O\n      O\n    \n    \n      Readonly\n      X\n      O\n    \n  \n\n\nReadonly 상태로 변하는 조건은 다음과 같다.\n\n\n  INSERT/APPEND 도중, Group 의 일부 Warehouse 가 입력에 실패하는 경우\n    \n      실패한 Warehouse 와 성공한 Warehouse 간의 데이터 불일치가 발생했기 때문에,\n실패한 Warehouse 는 Scrapped 상태로 만들고 해당 Group 은 더 이상의 입력을 받지 않기 위해 Readonly 상태로 전환된다. (warning)\n    \n  \n  새로운 Warehouse 가 추가된 경우\n    \n      이중화 과정이 진행되는 동안에도 입력을 받게 되면, 이중화 끝을 알 수 없기 때문에 Readonly 상태로 전환된다. (warning)\n    \n  \n\n\nNode의 Port 관리\n\n\n\n\n  \n    \n      Port 구분\n      설명\n      필요한 Node\n    \n  \n  \n    \n      Cluster Port\n      Node 간 통신을 위한 Port\n      모든 Node\n    \n    \n      Service Port\n      Client 가 직접 접속하는 Port\n      Broker/Warehouse\n    \n    \n      Admin Port\n      관리 목적으로 통신하는 Port\n      Coordinator/Deployer\n    \n    \n      Replication Port\n      Warehouse 간에 Replication 용 통신을 위한 Port\n      Warehouse\n    \n  \n\n\n직접 접속 후, 수행 가능한 명령\n\n다음은 각 Node에 직접 접속해서, 명령 수행이 가능한 것과 불가능한 것을 표로 나타낸 것이다.\n모든 Node는 Client 를 통한 접속이 가능하지만, Node 종류에 따라 불가능한 쿼리가 존재한다\n\n\n  \n    \n       \n      Broker (Leader)\n      Broker (non-leader)\n      Warehouse Standby\n    \n  \n  \n    \n      Client 접속\n      O\n      O\n      O\n    \n    \n      DDL\n      O\n      X\n      X\n    \n    \n      DELETE\n      O\n      O\n      X\n    \n    \n      INSERT\n      O\n      O\n      X\n    \n    \n      APPEND\n      O\n      O\n      X\n    \n    \n      SELECT\n      O\n      O\n      O\n    \n  \n\n\n데이터 저장/조회\n\nMachbase Cluster Edition 은 데이터를 분산 저장하고, 분산 쿼리 수행으로 계산되는 결과를 수집할 수 있다. 여기서는 테이블 종류에 따라 어떻게 저장되고 조회되는지 설명한다.\n\n데이터 저장\n\nLog Table\n\nBroker를 통해 Log Table에 데이터를 입력하는 경우, 모든 Warehouse에 분산 저장된다. (입력을 수행하는 Broker에는 데이터가 저장되지 않는다.) Coordinator가 각 Warehouse의 데이터베이스 크기를 판단, 그 기준으로 Broker가 분산 저장한다.\n\nWarehouse를 통해 직접 Log Table에 데이터를 입력하는 경우, 해당 Warehouse에만 저장된다. 분산 알고리즘, 네트워크 병목으로 인한 성능 저하를 피하고자 하는 경우에 선택할 수 있다.\n\nVolatile Table\n\nBroker를 통해 Volatile Table에 데이터를 입력하는 경우, 해당 Broker에 저장된다. 즉, 다른 Broker에는 해당 데이터가 입력되지 않고 동기화되지도 않는다.\n\nVolatile Table에 대한 이중화를 지원하지 않는 이유는, DELETE가 가능한 Volatile Table의 특성에 맞추면 이중화 성능에 영향을 미치기 때문이다.\n\nVolatile Table은 Broker에서만 생성되므로, Warehouse에서 입력할 수 없다.\n\nLookup Table\n\nBroker를 통해 Lookup 테이블에 데이터를 입력하는 경우, 입력한 데이터는 Lookup 노드에에 저장되며, Replication을 통해 다른 Broker들에게 복제된다.\n\nTag Table\n\nLog 테이블의 저장 방식과 동일하다. 단, 신규 TagID를 포함하는 데이터를 입력할 경우 Leader Broker를 통해서만 입력할 수 있다.\n\n데이터 조회\n\nLog Table\n\nBroker를 통해 Log Table에 데이터를 조회하는 경우, 모든 Warehouse에 쿼리가 분산된다. 각 Warehouse는 쿼리를 실제로 수행하는데, 필요한 경우엔 Warehouse 간 중간 결과를 교환한다. 이렇게 생성된 부분 결과를 Broker가 수집, 최종 결과를 반환한다.\n\nWarehouse를 통해 Log Table에 데이터를 조회하는 경우, 해당 Warehouse에서만 쿼리가 수행된다. 이 과정은 Fog Edition 에서의 쿼리 수행과 동일하다.\n\nLookup / Volatile Table\n\nBroker를 통해 Volatile Table에 데이터를 조회하는 경우, Broker에서만 쿼리가 수행된다. 이 과정은 Fog Edition 에서의 쿼리 수행과 동일하다.\n\nWarehouse를 통해서는 JOIN을 할 수 없는데, Volatile Table이 생성되지 않기 때문이다.\n\n두 테이블 간 JOIN\n\nBroker를 통해서 Log Table과 Volatile Table 간 JOIN을 하는 경우, 접속한 Broker와 나머지 Warehouse 가 동시에 쿼리를 수행한다. Broker는 Volatile Table 결과를 Warehouse에게 나눠주며, Warehouse는 Broker가 전달한 데이터를 JOIN 해서 결과를 반환한다. 이렇게 생성된 부분 결과를 Broker가 수집, 최종 결과를 반환한다.\n\nWarehouse를 통해서는 JOIN을 할 수 없는데, Volatile Table이 생성되지 않기 때문이다.\n\n이중화\n\n이중화란, 기존에 설치된 Node 의 실패를 대비해 똑같은 Node 를 복제하는 과정 또는 그 상태를 의미한다.\n\nCoordinator Replication\n\nCluster Edition 에서 Coordinator는 최대 2개까지 생성이 가능하다.\n\n두 Coordinator는 지속적으로 Cluster Node 정보를 계속 유지한다.\n어느 한 쪽이 비정상적으로 종료되더라도, 나머지 Coordinator가 Cluster Node 관리를 계속 할 수 있다.\n\nPrimary Coordinator가 재시작되면 기존의 secondary coordinator가 primary로 격상되고, 재시작하는 coordinator는 secondary가 된다.\n\nLookup Replication\n\n기본적으로 Lookup Master가 Lookup Table 데이터를 관리하지만, Lookup Slave를 추가하여 데이터를 복제하도록 할 수 있다.\n\nBroker Replication\n\nBroker는 Replication 대상이 아니다.\n\n따라서 Broker A에 들어있는 Volatile Table의 데이터 레코드는 Broker B에 똑같이 유지되지 않는다. (not synchronized)\n다만, Cluster 전체의 테이블/인덱스 스키마는 모두 동일하므로, Broker A에 Volatile Table VOL_TBL1 이 존재하면 Broker B에도 Volatile Table VOL_TBL1 이 존재한다.\n\nWarehouse Replication\n\nGroup 에 새로운 Warehouse 가 추가되는 경우, 다음 과정을 통해서 Warehouse 가 이중화된다.\n\n\n  Coordinator 가, 새로운 Warehouse 에게 DDL 이중화를 시작한다.\n  Group 이 Readonly 상태로 전환된다.\n  Group 중 1개의 Warehouse 가, 새로운 Warehouse 에게 데이터 이중화를 시작한다.\n  데이터 이중화가 끝나면 Group 은 Normal 상태로 전환된다.\n  데이터 입력의 경우, Broker 가 같은 Group 에는 같은 데이터를 전송함으로써 이중화를 보장한다.\n\n\n복구 방법\n\nNode가 비정상적으로 종료되어도, 아래와 같은 방법으로 서비스를 계속할 수 있다.\n\n자세한 내용은 운영 가이드를 참고한다.\n\n\n  \n    \n      종류\n      Fail-over 방법\n    \n  \n  \n    \n      Coordinator\n      Primary Coordinator가 비정상 종료되어도, Secondary Coordinator 가 Primary Coordinator가 되면서 클러스터 관리를 계속 할 수 있다. 최악의 상황으로 Coordinator가 모두 종료되어도, 클러스터 관리를 할 수 없을 뿐 전체 서비스 (데이터 입력/조회) 는 계속 할 수 있다.  (물론, 이 때 Broker나 Warehouse가 종료되면 클러스터 관리를 할 수 없다.)\n    \n    \n      Deployer\n      해당 Host 로 Node Operation (ADD, REMOVE..) 을 할 수 없고, 해당 Host 의 통계 정보를 수집할 수 없다.\n    \n    \n      Lookup\n      Lookup Master에세 장애가 발생하면 Lookup Monitor가 자동으로 감지하여 Lookup Slave 중 하나를 Lookup Master로 변경하여 계속적인 서비스 이용이 가능하게 한다. 기존에 Lookup Slave가 존재하지 않았다면 데이터 복제가 되지 않는 상태이기에 안정적인 HA를 위해 Lookup Slave는 하나 이상 존재하는 것을 권장한다.\n    \n    \n      Broker\n      Broker가 종료되어도, 다른 Broker가 존재한다면 계속 서비스를 지속할 수 있다.단, 종료된 Broker와 연결된 클라이언트의 접속은 끊어지기 때문에 다른 Broker로 재접속해야 한다.\n    \n    \n      Warehouse\n      Group 에 다른 Warehouse(s) 가 존재하면, 해당 Warehouse(s) 가 SELECT 와 APPEND 를 그대로 참여한다.\n    \n  \n\n\n지원되지 않는 기능\n\nQuery Statement\n\nTABLESPACE\n\n현재 Cluster Edition 에서는 테이블 스페이스 구분을 하지 않는다.\n\nBACKUP / MOUNT\n\n현재 Cluster Edition 에서는 데이터베이스 구분을 하지 않는다.\n\nLOAD IN FILE\n\nCSV 파일을 읽어 분산하는 기능은 현재 구현되어 있지 않다.\n\nALTER TABLE FORGERY CHECK\n\n고객의 데이터가 변경되었는지 검증하기 위한 구문인데, Result File을 한 곳에 모을 수 없다.\n\nClause / Function\n\nUNION ALL\n\n실행 단위가 복잡하게 생성되므로, 현재 지원되지 않는다.\n\nGROUP_CONCAT() function\n\n각 Warehouse 에서 수집한 부분 그룹에 대한 CONCAT 내용 전체를, 단순 누적으로 처리할 수 없다.\n(GROUP CONCAT에서 ORDER BY를 하는 경우)\n\nTS_CHANGE_COUNT() function\n\n각 Warehouse 에서 수집한 부분 그룹에 대한 TS_CHANGE_COUNT 결과를 단순 누적으로 처리할 수 없다.\n\n게다가 TS_CHANGE_COUNT() 는 전체 결과가 정렬되어 있어야 의미가 있는데, Warehouse에 분산된 결과를 대상으로 하면 의미가 없다.\n\n지원 하드웨어 및 운영체제\n\n\n  \n    \n      CPU\n      Intel Core i Series (Nehalem~) 이상 권장\n    \n  \n  \n    \n      Memory\n      설치될 Node 1개 당 2GB 이상 권장\n    \n    \n      운영체제\n      Linux (Any distribution)"
					}
					
				
		
				
					,
					
					"install-cluster-cmd-coordinator-deployer-install-html": {
						"id": "install-cluster-cmd-coordinator-deployer-install-html",
						"title": "Coordinator / Deployer 설치, Package 추가",
						"version": "all",
						"categories": "",
						"url": " /install/cluster/cmd/coordinator-deployer-install.html",
						"content": "Coordinator 설치\n\n환경 설정\n\nmachbase 계정으로 로그인한 후에,  machbase 권한으로 다음과 같이 파일을 수정하여 설치 디렉터리와 경로 정보에 대한 환경을 설정한다.\n\n# .bashrc 편집한다.                                  \nexport MACHBASE_COORDINATOR_HOME=~/coordinator\nexport MACHBASE_DEPLOYER_HOME=~/deployer\nexport MACHBASE_HOME=~/coordinator\nexport PATH=$MACHBASE_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$MACHBASE_HOME/lib:$LD_LIBRARY_PATH\n \n# 변경된 내용을 반영한다.\nsource .bashrc\n\n\n디렉터리 생성 및 압축 해제\n\n전용 디렉터리를 생성하고 패키지 압축 파일을 해당 디렉터리에 압축 해제한다.\n\n# 디렉터리 생성한다.                                     \nmkdir $MACHBASE_COORDINATOR_HOME\n  \n# 압축 해제한다.\ntar zxvf machbase-ent-x.y.z.official-LINUX-X86-64-release.tgz -C $MACHBASE_COORDINATOR_HOME\n\n\n포트 설정 및 서비스 구동\n\nmachbase.conf 파일을 수정하여 포트를 설정하고 서비스를 구동한다.\n\n# machbase.conf 파일에서 포트번호를 설정한다.\ncd $MACHBASE_COORDINATOR_HOME/conf\ncp machbase.conf.sample machbase.conf\nvi machbase.conf                                      \nCLUSTER_LINK_HOST       = 192.168.0.83 (추가할 노드 ip)\nCLUSTER_LINK_PORT_NO    = 5101\nHTTP_ADMIN_PORT         = 5102\n  \n# 메타 정보를 생성하고 서비스 구동한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -c\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -u\n\n\n노드 등록 및 확인\n\nCoordinator 노드를 추가하고 확인한다.\n\n# 노드 등록.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.83:5101\" --node-type=coordinator                                      \n  \n# 노드 확인.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-node\n      추가할 노드명으로 “IP:PORT” 형식으로 지정한다. PORT값은 CLUSTER_LINK_PORT_NO 값이다.\n      192.168.0.83:5101\n    \n    \n      –node-type\n      노드 종류를 지정한다. coordinator / deployer / broker / warehouse 4종류가 있다.\n      coordinator\n    \n  \n\n\nCoordinator 삭제\n\nCoordinator가 설치된 서버로 접속하여 Coordinator 프로세스를 정상 종료시킨 후 해당 Coordinator 디렉토리를 삭제한다.\n\n# coordinator를 종료하고 디렉토리를 삭제한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -s\nrm -rf $MACHBASE_COORDINATOR_HOME\n\n\nSecondary Coordinator 설치\n\nPrimary Coordinator 외에 추가 Coordinator 를 설치하는 경우, 다음을 주의한다.\n\n\n  Secondary Coordinator 의 Startup 이전에, Primary Coordinator 에 가서 Secondary Coordinator 를 Add-Node 해야 한다.\n  Secondary Coordinator 의 Startup 을 할 때, –primary 옵션으로 Primary Coordinator 를 지정해야 한다.\n  Secondary Coordinator 에 Primary Coordinator 를 Add-Node 해서는 안 된다.\n\n\n이 경우를 지키지 않는다면, Secondary Coordinator 역시 Primary Coordinator 처럼 행동한다.\n\n디렉터리 생성 및 압축 해제\n\n전용 디렉터리를 생성하고 패키지 압축 파일을 해당 디렉터리에 해제한다.\n\n# 디렉터리 생성한다.                                     \nmkdir $MACHBASE_COORDINATOR_HOME\n  \n# 압축 해제한다.\ntar zxvf machbase-ent-x.y.z.official-LINUX-X86-64-release.tgz -C $MACHBASE_COORDINATOR_HOME\n\n\n포트 설정\n\nmachbase.conf 파일을 수정하여 포트 설정만 한다. 서비스 구동하면 Primary Coordinator 처럼 작동한다.\n\n# machbase.conf 파일에서 포트 설정한다.\ncd $MACHBASE_COORDINATOR_HOME/conf\nvi machbase.conf                                      \nCLUSTER_LINK_HOST       = 192.168.0.83 (추가할 노드 ip)\nCLUSTER_LINK_PORT_NO    = 5111\nHTTP_ADMIN_PORT         = 5112\n\n\n노드 등록 및 확인\n\nPrimary Coordinator 에서, Secondary Coordinator 노드를 추가하고 확인한다.\n\n# 노드 등록.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.83:5111\" --node-type=coordinator                                      \n  \n# 노드 확인.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\n서비스 구동\n\n이제 Secondary Coordinator를 구동한다. Startup을 할 때, –primary 옵션으로 Primary Coordinator를 지정해야 한다.\n\n# 메타 정보를 생성하고 서비스를 구동한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -c\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -u --primary=\"192.168.0.83:5101\"\n\n\nSecondary Coordinator 삭제\n\nPrimary Coordinator에 등록된 Secondary Coordinator를 삭제한 후 Secondary Coordinator의 프로세스를 정상 종료시켜야 한다.\n\n# 노드 삭제.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.83:5101\"\n  \n# secondary coordinator를 종료하고 디렉토리를 삭제한다.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -s\nrm -rf $MACHBASE_COORDINATOR_HOME\n  \n# 노드 확인.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –remove-node\n      삭제할 노드명으로, “IP:PORT” 형식으로 지정한다. PORT 값은 CLUSTER_LINK_PORT_NO 값이다.\n      192.168.0.84:5201\n    \n  \n\n\nDeployer 설치\n\nDeployer는 broker와 warehouse가 설치되는 모든 Host, 즉 서버에 미리 설치해야 한다.\n\n환경 설정\n\n다음과 같이, 설치 디렉터리와 경로에 대한 환경을 설정한다.\n\n# .bashrc 편집한다.                                     \nexport MACHBASE_DEPLOYER_HOME=~/deployer\nexport MACHBASE_HOME=~/deployer\nexport PATH=$MACHBASE_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$MACHBASE_HOME/lib:$LD_LIBRARY_PATH\n  \n# 변경된 내용을 반영한다.\nsource .bashrc\n\n\n디렉터리 생성 및 압축 해제\n\n전용 디렉터리를 생성하고 패키지 압축 파일을 해당 디렉터리에 압축 해제한다.\n\n# 디렉터리를 생성한다.                                     \nmkdir $MACHBASE_DEPLOYER_HOME\n \n# 압축을 해제한다.\ntar zxvf machbase-ent-x.y.z.official-LINUX-X86-64-release.tgz -C $MACHBASE_DEPLOYER_HOME\n\n\n포트 설정 및 서비스 구동\n\nmachbase.conf 파일을 수정하여 포트를 설정하고 서비스를 구동한다.\n\n# machbase.conf 파일에서 포트를 설정한다.\ncd $MACHBASE_DEPLOYER_HOME/conf\nvi machbase.conf\nCLUSTER_LINK_HOST       = 192.168.0.84\nCLUSTER_LINK_PORT_NO    = 5201\nHTTP_ADMIN_PORT         = 5202\n  \n# 메타 정보를 생성하고 서비스를 구동한다.\n$MACHBASE_DEPLOYER_HOME/bin/machdeployeradmin -c\n$MACHBASE_DEPLOYER_HOME/bin/machdeployeradmin -u\n\n\n노드 등록 및 확인\n\n이 작업은 coordinator 노드에서 수행해야 한다.\n\nDeployer 노드를 추가하고 확인한다.\n\n# 노드 등록.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5201\" --node-type=deployer                                         \n \n# 노드 확인.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-node\n      추가할 노드명으로, “IP:PORT” 형식으로 지정한다. PORT 값은 CLUSTER_LINK_PORT_NO 값이다.\n      192.168.0.84:5201\n    \n    \n      –node-type\n      노드 종류를 지정한다. coordinator / deployer / broker / warehouse 4종류가 있다.\n      deployer\n    \n  \n\n\nDeployer 삭제\n\nCoordinator 노드에서 Deployer 노드를 삭제하고, Deployer가 있는 서버에서 Deployer 프로세스를 정상 종료시켜야 한다.\n\n# 노드 삭제.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.84:5201\"\n \n# deployer를 종료하고 디렉토리를 삭제한다.\n$MACHBASE_DEPLOYER_HOME/bin/machdeployeradmin -d\nrm -rf $MACHBASE_DEPLOYER_HOME\n  \n# 노드 확인.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\n패키지 추가\n\nCoordinator에 broker와 warehouse로 설치될 패키지를 추가 등록한다. 이때 등록되는 패키지로 MWA가 제외된 lightweight 버전을 사용한다.\n\n# 설치 패키지를 추가 등록한다.                             \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-package=machbase \\\n    --file-name=\"/home/machbase/machbase-ent-x.y.z.official-LINUX-X86-64-release-lightweight.tgz\"\n\n\n\n  \n    \n      옵션 항목\n      설명\n      예시\n    \n  \n  \n    \n      –add-package\n      추가할 패키지명을 지정한다.\n      machbase\n    \n    \n      –file-name\n      패키지 파일의 전체 경로와 파일명을 지정한다. Broker와 warehouse 설치만을 위한 패키지이므로, MWA 파일이 제외된 lightweight 패키지를 지정한다.\n      /home/machbase/machbase-ent-5.0.0.official-LINUX-X86-64-release-lightweight.tgz\n    \n  \n\n\n패키지 삭제\n\nCoordinator에 등록한 패키지를 삭제한다.\n\n# 등록한 패키지를 삭제한다.                             \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-package=machbase"
					}
					
				
		
				
					,
					
					"feature-table-volatile-create-drop-html": {
						"id": "feature-table-volatile-create-drop-html",
						"title": "휘발성 테이블 생성 및 관리",
						"version": "all",
						"categories": "",
						"url": " /feature-table/volatile/create-drop.html",
						"content": "휘발성 테이블의 생성과 삭제 방식은 다음과 같다.\n\n생성\n\ncreate volatile table vtable (id1 integer, name varchar(20));\n\n\n삭제\n\ndrop table vtable;"
					}
					
				
		
				
					,
					
					"feature-table-lookup-create-drop-html": {
						"id": "feature-table-lookup-create-drop-html",
						"title": "참조 테이블 생성 및 관리",
						"version": "all",
						"categories": "",
						"url": " /feature-table/lookup/create-drop.html",
						"content": "참조 테이블의 생성 방법은 다음과 같다.\n\n생성\n\n참조 테이블은 Primary Key를 반드시 지정해야한다.\n\nCREATE LOOKUP TABLE lktable (id INTEGER PRIMARY KEY, name VARCHAR(20));\n\n\n삭제\n\nDROP TABLE lktable;"
					}
					
				
		
				
					,
					
					"feature-table-log-create-drop-html": {
						"id": "feature-table-log-create-drop-html",
						"title": "로그 테이블 생성 및 삭제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/create-drop.html",
						"content": "로그 테이블은 다음과 같이 간단하게 생성할 수 있다.\n\nsensor_data 라는 테이블을 생성하고 삭제해 보도록 하자.\n\n마크베이스에서 사용가능한 데이터 타입은 SQL 레퍼런스의 자료형 에서 확인하면 된다.\n\n목차\n\n\n  로그 테이블 생성\n  로그 테이블 삭제\n\n\n로그 테이블 생성\n\n‘CREATE TABLE’ 구문으로 로그 테이블을 생성한다.\nMach&gt; CREATE TABLE sensor_data\n      (\n          id VARCHAR(32),\n          val DOUBLE\n       );\nCreated successfully.\n \nMach&gt; DROP TABLE sensor_data;\nDropped successfully.\n\n\n로그 테이블 삭제\n\n‘DROP TABLE’ 구문으로 로그 테이블을 삭제한다\n\n-- DROP은 데이터와 테이블 모두 삭제한다.\nMach&gt; DROP TABLE sensor_data;\nDropped successfully.\n \n \n-- TRUNCATE는 데이터만 삭제하고 테이블은 유지한다.\nMach&gt; TRUNCATE TABLE sensor_data;\nTruncated successfully."
					}
					
				
		
				
					,
					
					"feature-table-tag-create-drop-html": {
						"id": "feature-table-tag-create-drop-html",
						"title": "태그 테이블 생성 및 삭제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/create-drop.html",
						"content": "사용자는 테이블 타입으로 TAG라고 명시적으로 지정하여야 하며, 이후에 이 테이블을 조작함으로써 센서 데이터를 다양한 형태로 활용할 수 있다.\n\n과거 버전과 달리 테이블 이름을 TAG로 하지 않아도 되며, 자유롭게 지정 가능하다.\n\n데이터베이스가 처음 설치되었을 때는 TAG 테이블이 없다는 점에 유의하라.\n\nTAG 테이블은 기본적으로 사용자의 센서 데이터를 저장하기 위한 목적이므로, 아래의 세가지 필수 항목은 반드시 포함되어야 한다.\n\n\n  이름\n  입력 시간\n  값\n\n\n그러나, 마크베이스의 TAG 테이블은 위의 세가지 뿐만 아니라 부가 컬럼의 입력도 허용하기 때문에 위의 필수 컬럼을 위한 키워드를 동반한다.\n\n7.5 버전부터는 태그 값에 SUMMARIZED 키워드는 선택 사항이다.\n\n\n  태그 이름 : PRIMARY KEY\n  태그 입력 시간 : BASETIME\n\n\n그리고, 이 태그 이름은 다음 장에 설명될 태그 메타 정보로서 활용된다.\n\n목차\n\n\n  태그 테이블 생성\n  추가 센서 컬럼\n  추가 메타데이터 컬럼\n  테이블 프로퍼티 지정\n  태그 테이블 삭제\n\n\n태그 테이블 생성\n\n가장 간단한 태그 테이블은 아래와 같이 생성된다.\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME, value DOUBLE);\n[ERR-02253: Mandatory column definition (PRIMARY KEY / BASETIME) is missing.]\n==&gt; 위와 같이 키워드를 넣지 않으면, 위와 같은 에러가 발생한다.\n \nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nExecuted successfully.\n==&gt; 통계 정보 활용을 위해서는 태그 값에 SUMMARIZED 키워드를 추가해야 한다.\n \nMach&gt; desc tag;\n[ COLUMN ]              \n----------------------------------------------------------------\nNAME      TYPE        LENGTH\n----------------------------------------------------------------\nNAME      varchar        20\nTIME      datetime       31\nVALUE     double         17\n \nMach&gt; CREATE TAG TABLE other_tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE);\nExecuted successfully.\n \nMach&gt; desc other_tag;\n[ COLUMN ]              \n----------------------------------------------------------------\nNAME      TYPE        LENGTH\n----------------------------------------------------------------\nNAME      varchar        20\nTIME      datetime       31\nVALUE     double         17\n\n\n즉, TAG 라는 이름을 가진 테이블이 생성되었다. 성능 향상을 위해 4개의 파티션으로 나뉘어진 내부 테이블이 생성된다.\n\n추가 센서 컬럼\n\n실제로 TAG 테이블을 활용할 때 단지 3개의 컬럼만으로는 주어진 문제를 해결하기 힘든 경우가 있다.\n\n특히, 입력되는 센서 데이터의 정보가 이름과 시간, 값 뿐만 아니라 특정 그룹이나 인터넷 주소의 경우도 있기 때문에 아래와 같이 추가할 수 있다.\n\nMach&gt; create tag table TAG (name varchar(20) primary key, time datetime basetime, value double, grpid short, myip ipv4) ;\nExecuted successfully.\n \nMach&gt; desc tag;\n[ COLUMN ]              \n----------------------------------------------------------------\nNAME             TYPE        LENGTH\n----------------------------------------------------------------\nNAME             varchar         20\nTIME             datetime        31\nVALUE            double          17\nGRPID            short            6       &lt;=== 추가됨\nMYIP             ipv4            15       &lt;=== 추가됨\n\n\n그러나, 5.5를 포함한 구버전에서는 VARCHAR 타입의 값은 부가 컬럼에 들어갈 수 없다는 점에 유의하자.\n\nMach&gt; create tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized, myname varchar(100)) ;\n[ERR-01851: Variable length columns are not allowed in tag table.]\n\n\n문자열 타입의 경우에는 위와 같이 에러가 발생한다. 5.6 이후의 버전에서는TAG 테이블의 추가 컬럼에서도 VARCHAR를 지원한다.\n\n추가 메타데이터 컬럼\n\nTAG 테이블에는 센서 컬럼 추가만 가능한 것이 아니라, 각 태그 이름에 종속된 정보를 함께 입력할 수 있다.\n\n이 정보는 센서 데이터에 중복 저장할 필요가 없는 정보이기 때문에, 효율적으로 관리하기 위한 별도의 컬럼 정의 구문인 METADATA (…) 을 추가해야 한다.\n\nMach&gt; create tag table TAG (name varchar(20) primary key, time datetime basetime, value double)\n   2  metadata (room_no integer, tag_description varchar(100));\n\n\n여기서 room_no, tag_description 은 name 에 종속된 정보이다. 예를 들면, 이런 정보를 입력해 둘 수 있다.\n\n\n  \n    \n      name\n      room_no\n      tag_description\n    \n  \n  \n    \n      temp_001\n      1\n      It reads current temperature as Celsius\n    \n    \n      humid_001\n      1\n      It reads current humidity as percentage\n    \n  \n\n\n입력한 이후에는, TAG 테이블에서 SELECT 를 통해 같이 조회할 수 있다.\n\n\nMach&gt; SELECT name, time, value, tag_description FROM tag LIMIT 1;\nname                  time                            value\n--------------------------------------------------------------------------------------\ntag_description\n------------------------------------------------------------------------------------\ntemp_001              2019-03-01 09:52:17 000:000:000 25.3\nIt reads current temperature as Celsius\n\n\n테이블 프로퍼티 지정\n\n태그 테이블 생성 시, 아래 3가지 프로퍼티를 지정할 수 있다.\n\n\n  \n    \n      이름\n      설명\n      값\n    \n  \n  \n    \n      TAG_STAT_ENABLE\n      TAG ID별 통계 정보를 저장하는 기능의 활성화 여부를 지정할 수 있다.\n      Default: 1Min: 0 (disable)Max: 1 (enable)\n    \n    \n      TAG_PARTITION_COUNT\n      메모리 및 CPU 사용량을 조절하기 위해 파티션 개수를 지정할 수 있다.\n      Default: 4Min: 1Max: 1024\n    \n    \n      TAG_DATA_PART_SIZE\n      파티션 별 메모리 및 CPU 사용량을 조절하기 위해 데이터 크기를 지정할 수 있다.  BYTE 단위로 지정하며, MB 단위로 ALIGN 된다.\n      Default: 16777216 (16MB)Min: 1048576 (1MB)Max: 1073741824 (1GB)\n    \n  \n\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE) TAG_PARTITION_COUNT=1;\nExecuted successfully.\n \nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE) TAG_DATA_PART_SIZE=1048576;\nExecuted successfully.\n \nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE) TAG_STAT_ENABLE=0;\nExecuted successfully.\n \nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE) TAG_PARTITION_COUNT=2, TAG_STAT_ENABLE=1;\nExecuted successfully.\n\n\n태그 테이블 삭제\n\n만일 생성된 태그 테이블을 다시 만들거나, 필요가 없어져 디스크 공간을 확보해야 하는 경우에는 다음과 같은 DROP 명령어를 통해 삭제할 수 있다.\n\nTAG 테이블에 관련된 모든 자료, 즉 태그 데이터, 메타데이터 테이블이 모두 삭제되므로 유의해야 한다.\n\nMach&gt; DROP TABLE tag;\nDropped successfully.\n \nMach&gt; DESC tag;\ntag does not exist."
					}
					
				
		
				
					,
					
					"tools-utilities-csv-html": {
						"id": "tools-utilities-csv-html",
						"title": "CSVIMPORT/CSVEXPORT",
						"version": "all",
						"categories": "",
						"url": " /tools/utilities/csv.html",
						"content": "‘csvimport’와 ‘csvexport’ 는 CSV 파일을 마크베이스 서버에 import/export 하기 위해 사용되는 도구이다.\n\nCSV 파일에 대해서 machloader 를 이용하여 보다 간단하게 사용할 수 있도록 옵션을 단순화하였다.\n\n아래 기술된 Option이외에도 machloader에서 사용할 수 있는 옵션을 모두 사용 가능하다.\n\ncsvimport\n\ncsvimport를 이용하면 쉽게 CSV 파일을 서버에 입력할 수 있다.\n\n기본 사용법\n\n테이블 명과 데이터 파일명을 다음의 옵션에 따라 입력하여 수행한다.\n\nOption:\n-t: 테이블명 지정 옵션\n-d: 데이터 파일명 지정 옵션\n* 옵션을 지정하지 않고 테이블명과 데이터 파일명만으로도 수행할 수 있다.\n\n\nExample:\ncsvimport -t table_name -d table_name.csv\ncsvimport table_name file_path\ncsvimport file_path table_name\n\n\nCSV 헤더 제외\n\n입력시에 CSV 파일의 헤더를 제외하고 입력하려면 다음의 옵션을 사용한다.\n\nOption:\n-H: csv 파일의 첫번째 라인을 헤더로 인식하고 입력하지 않는다.\n\n\nExample:\ncsvimport -t table_name -d table_name.csv -H\n\n\n테이블 자동 생성\n\n입력시 입력할 테이블을 생성하지 않은 경우, 다음 옵션을 통해서 테이블 생성도 동시에 수행할 수 있다.\n\nOption:\n-C: import할 때 테이블을 자동 생성한다. 칼럼명은 c0, c1, ... 자동으로 생성된다. 생성되는 칼럼은 varchar(32767) 타입이다.\n-H: import할 때 csv 헤더명으로 칼럼명을 생성한다.\n\n\nExample:\ncsvimport -t table_name -d table_name.csv -C\ncsvimport -t table_name -d table_name.csv -C -H\n\n\ncsvexport\n\n‘csvexport’로 데이터베이스 테이블 데이터를 CSV 파일로 쉽게 export할 수 있다.\n\n기본 사용법\n\nOption:\n-t: 테이블명 지정 옵션\n-d: 데이터 파일명 지정 옵션\n* 옵션을 지정하지 않고 테이블명과 데이터 파일명만으로도 수행할 수 있다.\n\n\nExample:\ncsvexport -t table_name -d table_name.csv\ncsvexport table_name file_path\ncsvexport file_path table_name\n\n\nCSV 헤더 사용\n\n다음의 옵션을 이용하면, export할 CSV 파일에 칼럼명으로 헤더를 추가할 수 있다.\n\nOption:\n-H: 테이블 칼럼명으로 csv 파일의 헤더를 생성한다.\n\n\nExample:\ncsvexport -t table_name -d table_name.csv -H"
					}
					
				
		
				
					,
					
					"sql-ref-data-types-html": {
						"id": "sql-ref-data-types-html",
						"title": "자료형",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/data-types.html",
						"content": "목차\n\n  데이터 타입 테이블\n  SQL 자료형 표\n\n\n데이터 타입 테이블\n\n\n  \n    \n      타입명\n      설명\n      값 범위\n      NULL 값\n       \n    \n  \n  \n    \n      short\n      16비트 부호 있는 정수형 데이터 타입\n      -32767 ~ 32767\n      -32768\n       \n    \n    \n      ushort\n      16비트 무부호 정수형 데이터 타입\n      0 ~ 65534\n      65535\n       \n    \n    \n      integer\n      32비트 부호 있는 정수형 데이터 타입\n      -2147483647 ~ 2147483647\n      -2147483648\n       \n    \n    \n      uinteger\n      32비트 무부호 정수형 데이터 타입\n      0 ~ 4294967294\n      4294967295\n       \n    \n    \n      long\n      64비트 부호 있는 정수형 데이터 타입\n      -9223372036854775807 ~ 9223372036854775807\n      -9223372036854775808\n       \n    \n    \n      ulong\n      64비트 무부호 정수형 데이터 타입\n      0~18446744073709551614\n      18446744073709551615\n       \n    \n    \n      float\n      32비트 부동 소수점 테이타 타입\n      -\n      -\n       \n    \n    \n      double\n      64비트 부동 소수점 테이타 타입\n      -\n      -\n       \n    \n    \n      datetime\n      시간 및 날짜\n      1970-01-01 00:00:00 000:000:000 ~ 2262-04-11 23:47:​16.854:775:807\n      -\n       \n    \n    \n      varchar\n      가변길이 문자열 (UTF-8)\n      길이 : 1 ~ 32768 (32K)\n      -\n       \n    \n    \n      ipv4\n      Version 4의 인터넷 주소 타입 (4 바이트)\n      “0.0.0.0” ~ “255.255.255.255”\n      -\n       \n    \n    \n      ipv6\n      Version 6의 인터넷 주소 타입 (16 바이트)\n       \n      “0000:0000:0000:0000:0000:0000:0000:0000” ~ “FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF”\n      -\n    \n    \n      text\n      텍스트 데이터형 (키워드 인덱스 생성가능)\n      길이 : 0 ~ 64M\n      -\n       \n    \n    \n      binary\n      바이너리 데이터형\n      (인덱스 생성 불가능)\n      길이 : 0 ~ 64M\n      -\n    \n    \n      json\n      json 데이터 타입\n      json data 길이 : 1 ~ 32768 (32K)json path 길이 : 1 ~ 512\n      -\n       \n    \n  \n\n\nshort\n\nC 언어의 16비트 부호있는 정수형 데이터와 동일하다. 최소 음수값에 대해서는 NULL로 인식한다. “int16” 이라고 표시해도 된다.\n\ninteger\n\nC 언어의 32비트 부호있는 정수형 데이터와 동일하다. 최소 음수값에 대해서는 NULL로 인식한다. “int32” 또는 “int” 라고 표시해도 된다.\n\nlong\n\nC 언어의 64비트 부호있는 정수형 데이터와 동일하다. 최소 음수값에 대해서는 NULL로 인식한다. “int64” 라고 표시해도 된다.\n\nfloat\n\nC 언어의 32비트 부동 소수점 데이터타입 float와 동일하다. 양수 최대값에 대해 NULL로 인식한다.\n\ndouble\n\nC 언어의 64비트 부동 소수점 데이터타입 double과 동일하다. 양수 최대값에 대해 NULL로 인식한다.\n\ndatetime\n\n마크베이스에서는 이 타입은 1970년 1월 1일 자정 이후에 흘러간 시간의 나노값을 유지한다.\n\n따라서, 마크베이스는 datetime 타입 관련 모든 함수에 대해서 nano 단위까지 값을 처리할 수 있도록 제공한다.\n\nvarchar\n\n가변 문자열 데이터 타입이며, 길이는 최대 32K byte까지 생성이 가능하다.\n\n이 길이의 기준은 영문 1자를 기준으로 한 것이기 때문에 UTF-8에서 표현하는 실제 출력되는 문자 개수와는 서로 다르며, 적절한 길이로 설정해야 한다.\n\nIPv4\n\n이 타입은 인터넷 프로토콜 버전 4에서 사용되는 주소를 저장할 수 있는 타입이다.\n\n내부적으로 4바이트를 사용하여 표현하고 있으며, “0.0.0.0” 부터 “255.255.255.255”까지 모두 표현 가능하다.\n\nIPv6\n\n이 타입은 인터넷 프로토콜 버전 6에서 사용되는 주소를 저장할 수 있는 타입이다.\n\n내부적으로 16바이트를 사용하여 표현하고 있으며, “0000:0000:0000:0000:0000:0000:0000:0000” 부터 “FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF” 까지 표현 가능하다.\n\n데이터 입력시에는 축약형도 지원하기 때문에 : 기호를 활용하여 다음과 같이 표현할 수 있다.\n\n  ”::FFFF:1232” : 앞자리가 모두 0일 경우\n  ”::FFFF:192.168.0.3” : IPv4 형 호환형 지원\n  ”::192.168.3.1” : deprecated 된 IPv4 형 호환형 지원\n\n\ntext\n\n이 타입은 VARCHAR 의 크기를 넘어선 문자열 혹은 문서를 저장하기 위한 데이터 타입이다.\n\n이 데이터 타입은 키워드 인덱스를 통해 검색이 가능하며, 최대 64메가 바이트의 텍스트를 저장할 수 있다.\n이 타입은 주로 큰 텍스트 파일을 별도의 컬럼으로 저장하고, 검색하기 위한 용도로 사용된다.\n\nbinary\n\n이 타입은 비정형 데이터를 컬럼형태로 저장하기 위해 지원되는 타입이다.\n\n이미지나 동영상 혹은 음성과 같은 바이너리 데이터를 저장하는데 사용되는데 이 타입에 대해 인덱스를 생성하여 검색할 수 없다.\n저장하기 위한 최대 데이터 크기는 TEXT 타입과 동일하게 64 메가 바이트까지 가능하다.\n\njson\n\n이 타입은 json 데이터를 저장하기 위해 지원되는 타입이다.\n\njson이란, “Key-Value”의 쌍으로 이루어진 데이터 오브젝트를 텍스트형으로 저장한 포맷이다.\n저장하기 위한 최대 데이터 크기는 varchar 타입과 동일하게 32K byte까지 생성이 가능하다.\n\nSQL 자료형 표\n\n아래는, 마크베이스 자료형과 대응되는 SQL 자료형, C 자료형을 표로 나타냈다.\n\n\n  \n    \n      Machbase Datatype\n      Machbase CLI Datatype\n      SQL Datatype\n      C Datatype\n      Basic types for C\n      Description\n    \n  \n  \n    \n      short\n      SQL_SMALLINT\n      SQL_SMALLINT\n      SQL_C_SSHORT\n      int16_t (short)\n      16비트 부호 있는 정수형 데이터 타입\n    \n    \n      ushort\n      SQL_USMALLINT\n      SQL_SMALLINT\n      SQL_C_USHORT\n      uint16_t (unsigned short)\n      16비트 무부호 정수형 데이터 타입\n    \n    \n      integer\n      SQL_INTEGER\n      SQL_INTEGER\n      SQL_C_SLONG\n      int32_t (int)\n      32비트 부호 있는 정수형 데이터 타입\n    \n    \n      uinteger\n      SQL_UINTEGER\n      SQL_INTEGER\n      SQL_C_ULONG\n      uint32_t (unsigned int)\n      32비트 무부호 정수형 데이터 타입\n    \n    \n      long\n      SQL_BIGINT\n      SQL_BIGINT\n      SQL_C_SBIGINT\n      int64_t (long long)\n      64비트 부호 있는 정수형 데이터 타입\n    \n    \n      ulong\n      SQL_UBIGINT\n      SQL_BIGINT\n      SQL_C_UBIGINT\n      uint64_t (unsigned long long)\n      64비트 무부호 정수형 데이터 타입\n    \n    \n      float\n      SQL_FLOAT\n      SQL_REAL\n      SQL_C_FLOAT\n      float\n      32비트 부동 소수점 데이터 타입\n    \n    \n      double\n      SQL_DOUBLE\n      SQL_FLOAT, SQL_DOUBLE\n      SQL_C_DOUBLE\n      double\n      64비트 부동 소수점 데이터 타입\n    \n    \n      datetime\n      SQL_TIMESTAMPSQL_TIME\n      SQL_TYPE_TIMESTAMPSQL_BIGINTSQL_TYPE_TIME\n      SQL_C_TYPE_TIMESTAMPSQL_C_UBIGINTSQL_C_TIME\n      char * (YYYY-MM-DD HH24:MI:SS 출력 포맷)int64_t (timestamp: nano seconds)struct tm\n      시간 및 날짜\n    \n    \n      varchar\n      SQL_VARCHAR\n      SQL_VARCHAR\n      SQL_C_CHAR\n      char *\n      문자열\n    \n    \n      ipv4\n      SQL_IPV4\n      SQL_VARCHAR\n      SQL_C_CHAR\n      char * (ip 문자열 입력)unsigned char[4]\n      Version 4 인터넷 주소 타입\n    \n    \n      ipv6\n      SQL_IPV6\n      SQL_VARCHAR\n      SQL_C_CHAR\n      char * (ip 문자열 입력)unsigned char[16]\n      Version 6 인터넷 주소 타입\n    \n    \n      text\n      SQL_TEXT\n      SQL_LONGVARCHAR\n      SQL_C_CHAR\n      char *\n      텍스트\n    \n    \n      binary\n      SQL_BINARY\n      SQL_BINARY\n      SQL_C_BINARY\n      char *\n      바이너리 데이터\n    \n    \n      json\n      SQL_JSON\n      SQL_JSON\n      SQL_C_CHAR\n      json_t\n      json 데이터 타입"
					}
					
				
		
				
					,
					
					"feature-table-backup-mount-database-mount-html": {
						"id": "feature-table-backup-mount-database-mount-html",
						"title": "데이터베이스 마운트",
						"version": "all",
						"categories": "",
						"url": " /feature-table/backup-mount/database-mount.html",
						"content": "MOUNT\n  UNMOUNT\n  마운트된 데이터베이스에서 데이터 읽기\n\n\n데이터를 분석하기 위해서 대량의 데이터를 지속적으로 저장하면 그 양이 매우 증가하므로 다음의 문제가 발생한다.\n\n\n  대량의 데이터 저장에 의한 디스크 비용 증가\n  데이터 분석용 장비의 디스크 한계\n\n\n문제 해결을 위해서는 오래된 데이터를 백업하고, 주기적으로 삭제할 필요가 있다. 이후에 오래된 데이터를 읽을 필요가 있을 때, 백업된 데이터베이스를 읽기 위해서 데이터 복구를 실행하면 복구 과정에서 실행 시간이 오래 걸리는 것 뿐만 아니라, 데이터베이스를 오프라인 상태로 변환하고 현재 데이터를 모두 삭제한 상태에서 복구를 실행해야 하므로 서비스를 계속 진행하기 위해서는 별도의 장비가 필요한 문제점이 있다. \n마크베이스는 이 문제를 해결하기 위해서 MOUNT 명령을 지원한다.\n\nMOUNT명령은 데이터베이스가 서비스를 진행하면서도 백업된 데이터를 읽어들여서 현재 실행중인 데이터베이스와 별개로 새로운 데이터베이스를 생성한다. \n하나의 서버에서 여러 개의 백업된 데이터베이스를 추가하여 동시에 데이터를 검색할 수 있으나, 마운트된 데이터베이스는 읽기 전용으로 데이터의 추가와 삭제는 불가능하다.\n\n데이터베이스 MOUNT명령은 백업 데이터와 주 데이터베이스 내용을 동시에 읽을 수 있도록 한다. 따라서 마운트된 데이터베이스는 기존의 데이터 검색 방법과 동일하게 데이터를 검색할 수 있다.\n\nMOUNT 명령을 실행하기 위해서는 다음의 제약조건을 만족시켜야 한다.\n\n\n  백업 데이터베이스의 버전과 메타데이터 버전이 호환 가능해야 한다.\n  마운트된 백업 데이터베이스에는 테이블 생성, 인덱스 생성 및 삭제, 데이터 추가 및 삭제를 실행할 수 없다.\n\n\n마운트된 데이터베이스들에 대한 정보는 V$STORAGE_MOUNT_DATABASES 메타 테이블에서 얻을 수 있다.\n\nMOUNT\n마운트 명령을 실행하기 위해서는 백업 데이터베이스 경로명과 마운트할 데이터베이스 이름을 입력해야 한다.\n\n백업 데이터베이스 경로는 백업 명령으로 실행한 디렉토리의 위치를 설정한다. 마운트할 데이터베이스 이름은 운영중인 데이터베이스와 구별하기 위해서 별도의 이름을 부여해야 한다.\n\n백업 데이터베이스 경로명은 절대 경로명 (“/”문자로 시작되는 경로명)을 입력하거나, 백업 명령과 동일한 규칙으로 $MACHBASE_HOME/dbs를 기준으로 한 상대 경로명을 이용할 수 있다.\n\nSyntax:\nMOUNT DATABASE 'backup_database_path' TO mount_name;\n\nExample:\nMOUNT DATABASE '/home/machbase/backup' TO mountdb;\n\n\nUNMOUNT\n마운드된 데이터베이스 데이터가 더 이상 읽을 필요가 없다면, 마운트 상태를 해제하기 위해 UNMOUNT 명령을 사용한다.\nSyntax:\nUNMOUNT DATABASE mount_name;\n\nExample:\nUNMOUNT DATABASE mountdb;\n\n\n마운트된 데이터베이스에서 데이터 읽기\n마운트된 데이터베이스에서 데이터를 검색할 때는 기존과 동일한 SQL문을 이용한다.\n\nSYS유저만 마운트된 데이터를 읽을 수 있다. \nSQL문에서 마운트된 데이터베이스의 테이블을 지정하기 위해서는 mount_name과 user_name을 “.” 문자로 연결하여 지정해야 한다.\n\nSyntax:\nSELECT column_name FROM mount_name.user_name.table_name;\n\nExample:\nSELECT * FROM mountdb.sys.backuptable;"
					}
					
				
		
				
					,
					
					"sql-ref-ddl-html": {
						"id": "sql-ref-ddl-html",
						"title": "DDL",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/ddl.html",
						"content": "목차\n\n  목차\n    \n      CREATE TABLESPACE\n        \n          DATA DISK\n        \n      \n      CREATE TABLE\n        \n          테이블 종류\n          테이블 컬럼\n          테이블 프로퍼티(Table Property)\n            \n              컬럼 프로퍼티(Column Property)\n              MINMAX Cache 개념\n              MINMAX Cache 컬럼\n              기본 키(Primary Key)\n              AUTO_DEL 절\n            \n          \n          SEQUENCE for Lookup Table\n            \n              Sequence 지원 Machbase Edition\n              Sequence 지원 Table 종류\n              Lookup 테이블 생성 시 Sequence 설정\n              Sequence 컬럼의 사용\n            \n          \n        \n      \n      CREATE TAG TABLE\n      CREATE INDEX\n        \n          Index Type\n          KEYWORD Index\n          LSM Index\n          LSM Index Property\n          BITMAP 인덱스\n          RED-BLACK 인덱스\n          Index Property\n            \n              PART_VALUE_COUNT\n            \n          \n        \n      \n      DROP TABLESPACE\n      DROP TABLE\n      DROP INDEX\n      ALTER TABLESPACE\n        \n          ALTER TABLESPACE MODIFY DATADISK\n        \n      \n      ALTER TABLE\n        \n          ALTER TABLE SET\n          ALTER TABLE ADD COLUMN\n          ALTER TABLE DROP COLUMN\n          ALTER TABLE RENAME COLUMN\n          ALTER TABLE MODIFY COLUMN\n          ALTER TABLE RECLAIM STORAGE\n          ALTER TABLE RENAME TO\n          ALTER TABLE AUTO DELETE\n          ALTER TABLE ADD RETENTION\n          ALTER TABLE DROP RETENTION\n        \n      \n      TRUNCATE TABLE\n      CREATE ROLLUP\n      DROP ROLLUP\n      CREATE RETENTION\n      DROP RETENTION\n    \n  \n\n\nCREATE TABLESPACE\n\ncreate_tablespace_stmt:\n\n\n\ndatadisk_list:\n\n\n\ndata_disk:\n\n\n\ndata_disk_property:\n\n\n\ncreate_tablespace_stmt ::= 'CREATE TABLESPACE' tablespace_name 'DATADISK' datadisk_list\ndatadisk_list ::= data_disk ( ',' data_disk )*\ndata_disk ::= disk_name data_disk_property\ndata_disk_property ::= '(' 'DISK_PATH' '=' '\"' path '\"' ( ',' 'PARALLEL_IO' '=' number )? ')'\n\n-- 예제\ncreate tablespace tbs1 datadisk disk1 (disk_path=\"\"); -- $MACHBASE_HOME/dbs/ 에 생성\ncreate tablespace tbs1 datadisk disk1 (disk_path=\"tbs1_disk1\"); -- $MACHBASE_HOME/dbs/tbs1_disk1 에 생성되며 이때 tbs1_disk1폴더가 존재해야 한다.\ncreate tablespace tbs2 datadisk disk1 (disk_path=\"tbs2_disk1\", parallel_io = 5);\ncreate tablespace tbs1 datadisk disk1 (disk_path=\"tbs1_disk1\", parallel_io = 10), disk2 (disk_path=\"tbs1_disk2\"), disk3 (disk_path=\"tbs1_disk3\");\n\n\nCREATE TABLESPACE 구문은 로그 Table 또는 로그 Table의 Index가 저장될 Tablespace를 $MACHBASE_HOME/dbs/ 에 생성한다.\n\nTablespace는 여러 개의 Disk를 가질 수 있다. Table과 Index의 데이터를 저장하는 각각의 Partition File들이 저장될 때, Tablespace에 속한 Data Disk들에 분산되어 저장된다.\n\n만약 2개 이상의 Disk를 사용 시 Index와 Table의 여러 File이 각 Disk에 분산 저장되고, 각각의 Device에서 Parallel 하게 IO가 수행되어 Disk 개수가 늘어날수록 Disk I/O Throughput 이 높아져 다량의 Data를 빠르게 Disk에 저장할 수 있는 이점이 있다.\n\n또한, Table과 Index의 Tablespace를 별도로 생성하고 각각 다른 Disk를 정의할 경우, Physical Disk의 재구성 없이, Logical 하게 Table과 Index의 I/O를 분리할 수 있다.\n\nDATA DISK\n\nTablespace에 속한 Disk를 정의한다. 각 Disk는 다음과 같은 속성을 가진다.\n\n\n  \n    \n      속성\n      설명\n    \n  \n  \n    \n      data_disk_property\n      Disk의 속성을 지정한다.\n    \n    \n      disk_name\n      Disk 객체의 이름을 지정한다. 추후에 Alter Tablespace구문을 통해서 Disk객체의 속성을 변경할 때 사용한다.\n    \n    \n      disk_path\n      Disk의 Directory Path를 지정한다. 이 Directory는 Create 되어 있어야 한다. 상대 Path로 Path를 지정시 $MACHBASE_HOME/dbs 기준으로 PATH를 찾는다. 예를 들어 PATH=’disk1’일 경우 Disk Path를 $MACHBASE_HOME/dbs/disk1으로 인식한다.\n    \n    \n      parallel_io\n      Disk의 IO Request를 Parallel하게 몇 개까지 허용할 지를 결정한다. (DEF: 3, MIN: 1, MAX: 128)\n    \n  \n\n\nCREATE TABLE\n\ncreate_table_stmt:\n\n\n\ncolumn_list:\n\n\n\ncolumn_property_list:\n\n\n\ntable_property_list:\n\n\n\ncolumn_type:\n\n\n\nauto_del:\n\n\n\ncreate_table_stmt ::= 'CREATE' ( 'LOG' | 'VOLATILE' | 'LOOKUP' )? 'TABLE' table_name '(' column_list ')' autodel? table_property_list?\ncolumn_list ::= column_name column_type column_property_list? 'PRIMARY KEY'? 'NOT NULL'? ( ',' column_name column_type column_property_list? 'PRIMARY KEY'? 'NOT NULL'? )*\ncolumn_property_list ::= 'PROPERTY' '(' ( 'PART_PAGE_COUNT' | 'PAGE_VALUE_COUNT' | 'MINMAX_CACHE_SIZE' | 'MAX_CACHE_PART_COUNT' ) '=' value ( ',' ( 'PART_PAGE_COUNT' | 'PAGE_VALUE_COUNT' | 'MINMAX_CACHE_SIZE' | 'MAX_CACHE_PART_COUNT' ) '=' value )* ')'\ntable_property_list ::=  ('TAG_PARTITION_COUNT' ) '=' value\ncolumn_type ::= 'SHORT' | 'USHORT' | 'INTEGER' | 'UINTEGER' | 'LONG' | 'ULONG' | 'FLOAT' | 'DOUBLE' | 'DATETIME' | 'VARCHAR' '(' size ')' | 'IPV4' | 'IPV6' | 'TEXT' | 'BINARY'\n\n-- 5개의 column을 가진 ctest table을 만든다.\nMach&gt; CREATE TABLE ctest (id INTEGER, name VARCHAR(20), sipv4 IPV4, dipv6 IPV6, comment TEXT);\nCreated successfully.\n\n\n테이블 종류\n\n\n  \n    \n      테이블 종류\n      설명\n    \n  \n  \n    \n      LOG_TABLE\n      CREATE TABLE 사이에 아무런 키워드를 넣지 않았다면 Log Table이 생성된다.\n    \n    \n      VOLATILE_TABLE\n      VOLATILE_TABLE은 모든 데이터가 임시 메모리에 상주하는 임시 테이블이며 로그 테이블을 조인하여 결과를 향상시킵니다만, Machbase 서버가 종료 되자마자 사라집니다.\n    \n    \n      LOOKUP_TABLE\n      VOLATILE_TABLE과 마찬가지로 LOOKUP_TABLE은 메모리의 모든 데이터를 저장함으로써 빠른 쿼리 처리를 수행 할 수 있습니다.\n    \n  \n\n\n테이블 컬럼\n\n테이블 컬럼은 문자열을 그대로 넣는 것이 보통이지만, 특수 문자를 넣기 위해 홑따옴표 (‘) 또는 쌍따옴표 (“) 로 감싸서 넣을 수도 있다.\n\nMach&gt; CREATE TABLE special_tbl ( with.dot INTEGER );\n[ERR-02010: Syntax error: near token (.dot INTEGER )).]\nCREATE TABLE special_tbl ( \"with.dot\" INTEGER ); -- 가능\nCREATE TABLE special_tbl ( 'with.dot' INTEGER ); -- 가능\n\n\n해당 컬럼을 SELECT 쿼리를 통해 조회할 때도 홑따옴표 또는 쌍따옴표로 감싸야 하는데, 두 방법엔 차이가 존재한다.\n\n  홑따옴표로 감싸게 되면, 컬럼이 아닌 String Literal 로 취급된다.\n  쌍따옴표로 감싸게 되면, SELECT 쿼리 결과 컬럼의 이름 그대로 출력된다.\n\n\nMach&gt; SELECT 'with.dot' FROM special_tbl;\n'with.dot'\n--------------\n[0] row(s) selected.\n \nMach&gt; SELECT \"with.dot\" FROM special_tbl;\nwith.dot\n--------------\n[0] row(s) selected.\n\n\n테이블 프로퍼티(Table Property)\n\nTable에 대한 속성을 지정한다.\n\n\n  \n    \n      프로퍼티 이름\n      사용 가능한 테이블 종류\n    \n  \n  \n    \n      TAG_PARTITION_COUNT\n      TAG TABLE\n    \n    \n      TAG_DATA_PART_SIZE\n      TAG TABLE\n    \n    \n      TAG_STAT_ENABLE\n      TAG TABLE\n    \n  \n\n\nTAG_PARTITION_COUNT(Default:4)\n\nTAG Table에 대해 지원되는 속성으로 TAG 테이블을 내부적으로 몇 개의 파티션 테이블에 저장할 것인지 결정한다. TAG의 수나 서버의 성능에 따라 설정하여야 한다.\n\nTAG_DATA_PART_SIZE(Default:16MB)\n\nTAG Table에 대해 지원되는 속성으로 파티션 테이블 별 데이터 사이즈를 결정한다.\n\nTAG_STAT_ENABLE(Default:1)\n\nTAG Table에 대해 지원되는 속성으로 TAG ID 별 통계 정보 저장 여부를 결정한다.\n\n컬럼 프로퍼티(Column Property)\n\nColumn에 대한 속성을 지정한다.\n\n\n  \n    \n      프로퍼티 이름\n      사용 가능한 테이블 종류\n    \n  \n  \n    \n      PART_PAGE_COUNT\n      LOG TABLE\n    \n    \n      PAGE_VALUE_COUNT\n      LOG TABLE\n    \n    \n      MAX_CACHE_PART_COUNT\n      LOG TABLE\n    \n    \n      MINMAX_CACHE_SIZE\n      LOG TABLE\n    \n  \n\n\nPART_PAGE_COUNT\n\n이 프로퍼티는 하나의 파티션이 가지는 Page의 개수를 나타낸다. 하나의 파티션이 가지는 Value의 개수는 PART_PAGE_COUNT * PAGE_VALUE_COUNT가 된다.\n\nPAGE_VALUE_COUNT\n\n이 프로퍼티는 하나의 Page가 가지는 Value의 개수를 나타낸다.\n\nMAX_CACHE_PART_COUNT (Default : 0)\n\n이 프로퍼티는 성능 향상을 위한 캐시 영역을 설정하는 것이다.\n\n마크베이스 가 파티션에 접근할 때 해당 파티션의 메타 정보를 메모리에 담고 있는 구조체를 먼저 찾게 되는데, 몇 개의 파티션 정보를 메모리에 담고 있을지 결정한다. 크면 클수록 성능에 도움이 될 것이나, 메모리 사용량이 늘어난다. 최소값은 1 최대값은 65535이다.\n\nMINMAX_CACHE_SIZE (Default : 10240)\n\n이 프로퍼티는 해당 Column의 MINMAX를 위한 캐시 메모리를 얼마나 사용할 것인지 지정하는 것이다. 0번째 Hidden Column인 _ARRIVAL_TIME의 경우 기본적으로 100MB으로 지정이 된다. 하지만 다른 Column들은 기본적으로 10KB로 지정되어 있다. 이 크기는 Table의 생성 이후에도 “ALTER TABLE MODIFY” 구문을 통해서 이 값은 변경이 가능하다.\n\nNOT NULL Constraint\n\n컬럼 값에 NULL을 허용하지 않을 경우 NOT NULL을 지정하고, 허용할 경우(Default)에는 생략한다.\n\n테이블 생성 이후에 정의된 이 제약조건을 삭제하거나 추가하기 위해서는 ALTER TABLE MODIFY COLUMN 명령으로 제약조건을 변경할 수 있다.\n\n# 컬럼 c1은 not null로, c2는 not null 제약 조건 없이 생성한다.\nCREATE TABLE t1(c1 INTEGER NOT NULL, c2 VARCHAR(200));\n\n\nPre-defined System Columns\n\nCreate Table 문을 이용하여 테이블을 생성하면 시스템은 두 개의 사전 정의된 시스템 컬럼을 추가로 생성한다. _ARRIVAL_TIME 및 _RID컬럼이다.\n\n_ARRIVAL_TIME 컬럼은 DATETIME 타입의 컬럼으로 INSERT 문이나 AppendData로 데이터를 삽입하는 시점의 시스템 time을 기준으로 삽입되며, 해당 값은 생성된 레코드의 unique key로 사용될 수 있다. 이 컬럼의 값은 순서가 보장되는 경우(과거-현재 순으로) machloader나 INSERT 문에서 값을 지정하여 삽입할 수 있다. DURATION 조건절을 이용하여 데이터를 검색할 경우, 이 컬럼의 값을 기준으로 데이터를 검색한다.\n\n_RID 컬럼은 특정 레코드가 갖는 유일한 값으로 시스템이 생성한다. 이 컬럼의 데이터 타입은 64bit 정수이며, 이 컬럼에 대해서는 사용자가 값을 지정할 수 없고 인덱스도 생성할 수 없다. 데이터 INSERT시에 자동으로 생성된다. _RID 컬럼의 값으로 레코드를 검색할 수 있다.\n\ncreate volatile table t1111 (i1 integer);\nCreated successfully.\nMach&gt; desc t1111;\n \n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\n_ARRIVAL_TIME                 datetime            8              \nI1                            integer             4              \n \nMach&gt;insert into t1111 values (1);\n1 row(s) inserted.\nMach&gt;select _rid from t1111;\n_rid                \n-----------------------\n0                   \n[1] row(s) inserted.\n \nMach&gt;select i1 from t1111 where _rid = 0;\ni1         \n--------------\n1          \n[1] row(s) selected.\n\n\nMINMAX Cache 개념\n\n일반적으로 Disk DBMS에서는 특정 값을 인덱스를 활용하여 검색할 경우 해당 인덱스가 포함된 디스크 영역에 대해 접근하고, 해당 값이 포함된 최종 디스크 페이지를 찾아가도록 구현되어 있다.\n\n반면, 마크베이스는 시계열 정보를 유지하기 위해 시간순으로 파티션 되어있는 구조이며, 이것은 특정한 하나의 인덱스 정보가 시간순으로 조각조각의 파일로 나누어져 있다는 의미이다. 따라서, 마크베이스의 인덱스를 이용할 때는 이러한 파티션으로 조각나 있는 인덱스 파일을 순차적으로 검색한다.\n\n만일 검색해야 할 대상 데이터의 범위가 1000개의 파티션으로 나뉘어져 있다면 1000번의 파일을 매번 열어서 검색해야 한다는 의미이다. 비록 효율적인 컬럼형 데이터베이스 구조로 설계되어 있긴 하나, 이러한 I/O 비용이 인덱스 파티션 개수의 크기에 비례하기 때문에 그 성능을 향상하기 위한 방법이 MINMAX_CACHE 구조이다.\n\n이 MINMAX_CACHE는 해당 파티션의 인덱스 파일 정보를 메모리에 담고 있는 구조체로서 해당 컬럼의 최소 및 최대 값을 메모리에 유지하는 연속된 메모리 공간이다. 이런 구조를 유지함으로써 특정 값이 포함된 파티션을 검색할 경우 그 값이 해당 인덱스의 최소값 보다 작거나 최대 값보다 클 경우에는 아예 해당 파티션을 건너뛸 수 있기 때문에 고성능의 데이터 분석이 가능해진다.\n\n\n\n위의 그림에서 볼 수 있듯이 85라는 값을 찾기 위해서 5개의 파티션 중에서 MIN/MAX에 포함된 1번과 5번 파티션만을 실제로 검색하게 되며, 2, 3, 4 번 파티션은 아예 건너뛰는 모습을 볼 수 있다.\n\nMINMAX Cache 컬럼\n\n테이블 생성 시에 특정 컬럼에 대해 MINMAX Cache를 사용할 것인지 결정할 수 있다.\n\n만일 이 컬럼이 minmax_cache_size가 0이 아닌 값으로 설정되었으면, 해당 컬럼에 인덱스 검색 시 MINMAX Cache가 동작하게 되며, MINMAX_CACHE_SIZE = 0일 경우에는 동작하지 않는다.\n\n이런 MINMAX Cache를 사용할 때 다음과 같은 사항에 주의한다.\n\n  MINMAX Cache 는 해당 컬럼에 인덱스를 명시적으로 생성하지 않아도 적용된다.\n  모든 컬럼의 default는  MINMAX_CACHE_SIZE가 10KB로 설정되있고 Alter Table 구문을 활용하여 적정한 크기의 메모리 크기를 재설정할 수 있다.\n  숨어있는 컬럼인 _arrival_time은 디폴트로 100MB이며, 자동으로 MINMAX Cache 메모리를 사용한다.\n  VARCHAR 타입의 경우에는 MINMAX Cache 의 대상이 되지 않는다. 따라서 VARCHAR 타입을 명시적으로 캐시 사용 여부를 지정하면, 에러가 발생한다.\n  해당 테이블이 하나 생성될 때 프로퍼티에 설정된  MINMAX_CACHE_SIZE 만큼 최대 메모리가 더 사용될 수 있다. 파티션 개수가 늘수록 메모리가 점진적으로 늘어나 위의 최대 메모리만큼 늘어난다.\n  만일 해당 테이블에 레코드가 하나도 들어있지 않으면, MINMAX Cache  메모리는 전혀 할당되지 않는다.\n\n\n아래는 실제 MINMAX를 활용한 테이블 생성 예를 나타낸다.\n\n-- VARCHAR에 대한 MINMAX_CACHE_SIZE = 0은 의미상으로 허용된다.\nCREATE TABLE ctest (id INTEGER, name VARCHAR(100) PROPERTY(MINMAX_CACHE_SIZE = 0));\nCreated successfully.\nMach&gt;\n \n-- id 컬럼에 캐시가 적용되었다.\nCREATE TABLE ctest2 (id INTEGER PROPERTY(MINMAX_CACHE_SIZE = 10240), name VARCHAR(100) PROPERTY(MINMAX_CACHE_SIZE = 0));\nCreated successfully.\nMach&gt;\n \n-- id1, id2, id3 컬럼에 적용되었다.\nCREATE TABLE ctest3 (id1 INTEGER PROPERTY(MINMAX_CACHE_SIZE = 10240), name VARCHAR(100) PROPERTY(MINMAX_CACHE_SIZE = 0), id2 LONG PROPERTY(MINMAX_CACHE_SIZE = 1024), id3 IPV4 PROPERTY(MINMAX_CACHE_SIZE = 1024), id4 SHORT);\nCreated successfully.\nMach&gt;\n \n-- Column단위로 MINMAX_CACHE_SIZE가 지정되거나, 0으로 설정되었다.\nCREATE TABLE ctest4 (id1 INTEGER PROPERTY(MINMAX_CACHE_SIZE=10240), name VARCHAR(100) PROPERTY(MINMAX_CACHE_SIZE=0), id2 LONG PROPERTY(MINMAX_CACHE_SIZE=10240), id3 IPV4 PROPERTY(MINMAX_CACHE_SIZE=0), id4 SHORT);\nCreated successfully.\nMach&gt;\n\n\n기본 키(Primary Key)\n\nVolatile/Lookup 테이블의 컬럼에 부여할 수 있는 제약 사항으로, 해당 컬럼의 값이 중복되는 것을 방지한다. Volatile/Lookup 테이블이 항상 기본 키를 가지고 있을 필요는 없으나, 기본 키가 없으면 INSERT ON DUPLICATE KEY UPDATE 구문을 사용할 수 없다.\n\n기본 키를 부여하면, 기본 키에 대응되는 레드-블랙 트리 인덱스가 생성된다.\n\nAUTO_DEL 절\n\n데이터의 양을 제한하여 디스크 저장 공간을 유지할 수 있다. Log 테이블에 대해서만 지원하며, CREATE TABLE문에서 Table property를 지정하기 전에 AUTO_DEL절을 이용하여 설정한다. AUTO_DEL절은 저장 시간 기준 혹은 레코드 수 기준으로 설정할 수 있다.\n\nCREATE TABLE t1 (c1 INT) KEEP 30 DAY AFTER APPEND INTERVAL 5 SECOND;\n\n\n위 예제는 자동 삭제가 수행된 이후 5초가 지나고 추가 입력이 있는 경우, 입력된지 30일이 지난 데이터를 삭제한다. interval로 지정된 기간이 길면 자동 delete가 수행되는 시간이 길어져서 입력에 영향을 끼칠 가능성이 있으며, 너무 짧으면 시스템 성능에 전체적으로 영향을 줄 수 있다.\n\n아래의 예제는 보존할 데이터의 건 수로 자동 삭제 기능을 지정하는 예이다.\n\nCREATE TABLE t1 (c1 INT) KEEP 3 RECORD AFTER APPEND INTERVAL 5 RECORD;\n\n\n입력 데이터 5건 마다 해당 테이블의 레코드의 수를 검사하여 3건 이상이면 3건만 남기도록 자동 삭제를 수행한다.\n\nSEQUENCE for Lookup Table\n\nLookup 테이블의 Unique한 Record를 생성하고 Data의 입력 순서를 결정하기 위해 위해 Sequence가 추가되었다.\n\n이 기능은 Lookup 테이블에서 datetime column을 사용하여 Record의 순서를 구분하는 방식을 사용했을 때\n\ndatetime 값이 중복될 경우 Record의 순서를 구분하기 어렵고 데이터 중복으로 인한 Application의 오류가 발생하는 등의 문제점을 해결하기 위해 추가되었다.\n\nSequence 지원 Machbase Edition\n\nStandard / Cluster\n\nSequence 지원 Table 종류\n\nLookup Table\n\nLookup 테이블 생성 시 Sequence 설정\n\nCreate Table SQL 문으로 Lookup 테이블을 생성할 때 Sequence로 사용할 컬럼에 PROPERTY 절을 추가하여 Sequence를 설정하겠다고 명시하면 된다.\n\nSequence로 설정할 컬럼은 LONG datatype(64bit, unsigned)만 지원하며 이외의 datatype은 지원하지 않는다.\n\n추가로, Sequence의 시작값을 설정할 수 있는데 1로 설정한 경우 Sequence가 1부터 시작이 된다. (0이나 음수는 지원 안함)\n\nCREATE LOOKUP TABLE table_name (v1 LONG PROPERTY(SEQUENCE=1), v2 VARCHAR(10));\n\n\nSequence 컬럼의 사용\n\nLookup 테이블의 Sequence 컬럼은 기본적으로 일반 Long 컬럼과 동일하게 사용이 가능하며 이렇게 사용할 경우 Sequence 값은 자동으로 증가하지 않는다.\n\nSequence 컬럼에 직접 값을 입력하는 것이 허용되며 심지어 중복 값을 입력하는 것도 가능하다.\n\n대신, Sequence 기능을 사용하려면 nextval 이라는 새로 추가된 Sequence 전용 Function을 사용하여 Sequence값을 증가시키는 방식으로 사용해야 한다.\n\n내부적으로는 Sequence로 설정된 컬럼의 값 중 가장 큰 값에 대해 저장하고 있기 때문에 이후에 nextval Function을 사용하여 입력할 때 Sequence 컬럼 값 중 가장 큰 값 + 1 의 값이 저장된다.\n\nSequence 컬럼 사용 예:\n-- Sequence 컬럼에 nextval Function을 사용하여 다음 Sequence 값 입력\nINSERT INTO table_name (v1, v2) values (nextval(v1), 'aaaa');\n   \n-- Sequence 컬럼에 직접 값을 입력\nINSERT INTO table_name (v1, v2) values (100, 'aaaa');\n   \n-- Sequence 컬럼에 연산을 통한 값을 입력\nINSERT INTO table_name (v1, v2) values (1 + 99, 'aaaa');\n   \n-- Sequence 컬럼을 포함한 Lookup 테이블에 대한 정상 Select\nSELECT v1, v2 FROM table_name;\n  \n-- Sequence 컬럼에 대한 잘못된 Select (nextval 컬럼은 insert 시에만 사용 가능)\nSELECT nextval(v1), v2 FROM table_name;\n\n\nCREATE TAG TABLE\n\ncreate_tag_table_stmt:\n\n\n\ntag_column_list:\n\n\n\ncreate_tag_table_stmt ::= 'CREATE' 'TAGDATA' 'TABLE' table_name '(' tag_column_list ( 'metadata' '(' column_list ')' )? ')'\ntag_column_list ::= column_name column_type column_property_list? 'PRIMARY KEY'? 'BASETIME'? 'SUMMARIZED'? 'NOT NULL'? ( ',' column_name column_typecolumn_property_list? 'PRIMARY KEY'? 'BASETIME'? 'SUMMARIZED'? 'NOT NULL'? )*\n\n\n태그 테이블 생성시 반드시 primary key, basetime, summarized 가 들어가야 한다.\n\n--예제\nCREATE TAGDATA TABLE tag (name varchar(20) primary key, time datetime basetime, value double summarized);\nCREATE TAGDATA TABLE tag (name varchar(20) primary key, int_column int, time datetime basetime, value double summarized, value2 float);\nCREATE TAGDATA TABLE tag (name varchar(20) primary key, time datetime basetime, value double summarized, value2 float) METADATA (i1 int);\n\n\nCREATE INDEX\n\ncreate_index_stmt:\n\n\n\nindex_type:\n\n\n\ntable_space:\n\n\n\nindex_property_list:\n\n\n\ncreate_index_stmt ::= 'CREATE' 'INDEX' index_name 'ON' table_name '(' column_name ')' index_type? table_space? index_property_list?\nindex_type ::= 'INDEX_TYPE' ( 'KEYWORD' | 'BITMAP' | 'REDBLACK' )\ntable_space ::= 'TABLESPACE' table_space_name\nindex_property_list ::= ( 'MAX_LEVEL' | 'PAGE_SIZE' | 'BITMAP_ENCODE' | 'PART_VALUE_COUNT' ) '=' value\n\n\nIndex Type\n\n생성할 Index Type을 지정한다. Keyword Index가 아닌 경우 Index Type을 지정하지 않으면 Table Type에 따라서 Default Index Type으로 Index가 생성된다.\n\n\n  \n    \n      Table Type\n      Default Index Type\n    \n  \n  \n    \n      Volatile Table\n      REDBLACK\n    \n    \n      Lookup Table\n      REDBLACK\n    \n    \n      Log Table\n      LSM\n    \n  \n\n\nKEYWORD Index\n\n텍스트 검색을 위한 인덱스로써 로그 테이블의 varchar와 text 컬럼에만 생성 가능하며, 단일 컬럼에 대해서만 생성할 수 있다.\n\nLSM Index\n\nLSM(Log Structure Merge) Index로 Big Data에 저장 및 검색에 최적화된 Index이다. LSM Index들의 Partition들은 Level 별로 유지되고 하위 Level의 Partition들이 Merge되어 상위 Level로 이동한다. 그리고 상위 Level의 Partition 생성에 사용된 하위 Partition들은 삭제된다.\n\n이러한 Index Level Partition Building은 Background Thread 에 의해서 수행된다. 상위 Level Partition은 하위 Level의 Partition 들이 Merge되어 하나의 Partition으로 생성되기 때문에 Index를 통한 검색 시 다음과 같은 장점이 존재한다.\n\n\n  Key가 중복된 경우, 한 번만 저장되기 때문에 Key 저장을 위한 Disk Space가 절약된다.\n  여러 개의 Partition에 대한 Searching보다 하나의 Index Partition에 대한 검색 시 File Open 및 Close 비용이 줄어들고, 접근하는 Index Page의 개수 또한 줄어든다.\n\n\nLSM Index Property\n\n\n  \n    \n      항목\n      설명\n    \n  \n  \n    \n      MAX_LEVEL(DEFAULT = 3, MIN = 0, MAX = 3 )\n      LSM Index의 최대 Level로서 현재 3이 최대값이다. 그리고 하나의 Partition의 최대 Record 건수는 2억건을 초과할 수 없다. 각 Level의 Partition 크기는 이전 Partition의 Value 개수 * 10이다. 예를 들면 MAX_LEVEL = 3, PART_VALUE_COUNT가 100,000 이면 Level 0 = 100,000, Level 1 = 1,000,0000, Level 2 = 10,000,000, Level 3 = 100,000,000 이다. 만약 마지막 Level의 Partition Size가 2억건을 초과하면 Index 생성이 실패한다.\n    \n    \n      PAGE_SIZE(DEFAULT = 512 * 1024, MIN = 32 * 1024,MAX = 1 * 1024 * 1024)\n      Index의 Key Value와 Bitmap 값이 저장되는 Page의 크기를 지정한다. Default는 512K이다.\n    \n    \n      BITMAP_ENCODE(DEFAULT = EQUAL, RANGE)\n      인덱스의 Bitmap 타입을 설정한다. BITMAP_ENCODE=EQUAL(기본값)의 경우 키값과 같은 값에 대한 bitmap을 생성하고 BITMAP=RANGE인 경우 키값의 range에 따른 bitmap을 생성한다. 질의 조건으로 = 을 주로 사용하는 경우 BITMAP_ENCODE=EQUAL로, 특정 범위값을 질의 조건으로 주로 사용하는 경우 BITMAP_ENCODE=RANGE로 설정하는 편이 좋다. BITMAP=RANGE인 경우 생성 비용은 EQUAL에 비해서 약간 증가한다.\n    \n  \n\n\nBITMAP 인덱스\n\n데이터 분석을 위한 인덱스로서, 로그 테이블에만 생성 가능하다. 그리고 varchar, text, binary를 제외한 모든 컬럼에 생성 가능하며, 단일 컬럼에 대해서만 생성할 수 있다.\n\nRED-BLACK 인덱스\n\n실시간 데이터 검색을 위한 메모리 인덱스로서, Volatile/Lookup 테이블에만 생성 가능하다. 그리고 이 테이블의 모든 컬럼에 생성 가능하며 단일 컬럼에 대해서만 생성할 수 있다.\n\nIndex Property\n\nLSM Index 에서 적용할 수 있는 Property 는 다음과 같다.\n\nPART_VALUE_COUNT\n\nIndex의 Partition에 저장되는 Row 개수를 나타낸다.\n\n--예제\n-- c1 컬럼에 index가 적용되었다.\nCREATE INDEX index1 on table1 ( c1 )\n-- varchar type의 var_column에 keyword index가 적용되고 page_size의 단위는 100000가 되었다.\nCREATE INDEX index2 on table1 (var_column) INDEX_TYPE KEYWORD PAGE_SIZE=100000;\n\n\nDROP TABLESPACE\n\ndrop_tablespace_stmt:\n\n\n\ndrop_table_stmt ::= 'DROP TABLESPACE' tablespace_name\n\n\n지정된 Tablespace를 삭제한다. 하지만 Tablespace에 생성된 객체가 존재하는 경우, 삭제가 실패한다.\n\n--예제\nDROP TABLESPACE TablespaceName;\n\n\nDROP TABLE\n\ndrop_table_stmt:\n\n\n\ndrop_table_stmt ::= 'DROP TABLE' table_name\n\n\n지정된 테이블을 삭제한다. 단, 해당 테이블을 검색 중인 다른 세션이 존재할 경우에는 에러를 내면서 실패한다.\n\n--예제\nDROP TABLE TableName;\n\n\nDROP INDEX\n\ndrop_index_stmt:\n\n\n\ndrop_index_stmt ::= 'DROP INDEX' index_name\n\n\n지정된 인덱스를 삭제한다. 단, 해당 테이블을 검색 중인 다른 세션이 존재할 경우에는 에러를 내면서 실패한다.\n\n-- 예제\nDROP INDEX IndexName;\n\n\nALTER TABLESPACE\n\nALTER TABLESPACE 구문은 지정된 Tablespace에 관련된 정보를 변경하는데 사용된다.\n\nALTER TABLESPACE MODIFY DATADISK\n\n이 구문은 Tablespace의 DATADISK의 속성을 변경하는데 사용된다.\n\nalter_tablespace_stmt:\n\n\n\nalter_tablespace_stmt ::= 'ALTER TABLESPACE' table_name 'MODIFY DATADISK' disk_name 'SET' 'PARALLEL_IO' '=' value\n\n\n-- 예제\nALTER TABLESPACE tbs1 MODIFY DATADISK disk1 SET PARALLEL_IO = 10;\n\n\nALTER TABLE\n\nALTER TABLE 구문은 지정된 테이블의 스키마 정보를 변경시키기 위한 용도로 사용되며 Log Table 만 사용 가능하다.\n\nALTER TABLE SET\n\n이 구문은 Table의 Property를 변경하는 구문이다. 현재 동적으로 변경 가능한 Property는 없다.\n\nALTER TABLE ADD COLUMN\n\nalter_table_add_stmt:\n\n\n\nalter_table_add_stmt ::= 'ALTER TABLE' table_name 'ADD COLUMN' '(' column_name column_type ( 'DEFAULT' value )? ')'\n\n\n이 구문은 테이블에 특정 컬럼을 실시간으로 추가하는 기능이다. 컬럼의 이름과 타입을 추가하고, DEFAULT 구문을 통해 기본 데이터 값을 설정할 수 있다.\n\n-- 예제-1\nalter table atest2 add column (id4 float);\n \n-- 예제-2\nalter table atest2 add column (id6 double  default 5);\nalter table atest2 add column (id7 ipv4  default '192.168.0.1');\nalter table atest2 add column (id8 varchar(4) default 'hello');\n\n\nALTER TABLE DROP COLUMN\n\nalter_table_drop_stmt:\n\n\n\nalter_table_drop_stmt ::= 'ALTER TABLE' table_name 'DROP COLUMN' '(' column_name ')'\n\n\n이 구문은 테이블에 특정 컬럼을 실시간으로 삭제하는 기능이다.\n\n-- 예제\nalter table atest2 drop column (id4);\nalter table atest2 drop column (id8);\n\n\nALTER TABLE RENAME COLUMN\n\nalter_table_column_rename_stmt:\n\n\n\nalter_table_column_rename_stmt ::= 'ALTER TABLE' table_name 'RENAME COLUMN' old_column_name 'TO' new_column_name\n\n\n이 구문은 테이블의 특정 컬럼명을 변경하는 기능이다.\n\n-- 예제\nalter table atest2 rename column id7 to id7_rename;\n\n\nALTER TABLE MODIFY COLUMN\n\nalter_table_modify_stmt:\n\n\n\nalter_table_modify_stmt ::= 'ALTER TABLE' table_name 'MODIFY COLUMN' ( '(' column_name 'VARCHAR' '(' new_size ')' ')' | column_name ( 'NOT'? 'NULL' | 'SET' 'MINMAX_CACHE_SIZE' '=' value ) )\n\n\n이 구문은 테이블의 특정 컬럼의 속성을 변경하는 것이다. 현재는 VARCHAR 타입의 컬럼 길이와 그외 타입에 대한 MINMAX CACHE 속성과 NOT NULL 제약조건을 수정하는 것이 가능하다.\n\nVARCHAR SIZE\n\n이 구문은 VARCHAR 타입의 컬럼 길이만 변경하는 것을 지원한다. 이 동작은 기존의 데이터를 보존하기 위해 그 길이가 줄어들 수는 없으며, 언제나 증가해야 한다.\n\nALTER TABLE table_name MODIFY COLUMN (column_name VARCHAR(new_size));\n\n\n-- 예제 : TABLE 이 이렇게 만들어졌다고 가정하자.\n-- create table atest5 (id integer, name varchar(5), id3 double, id4 float);\n \n-- 에러 발생: 다른 타입으로 변경할 수 없음.\nalter table atest5 modify column (id varchar(10));\n \n-- 에러 발생: VARCHAR 길이를 더 작게 할 수 없음.\nalter table atest5 modify column (name varchar(3));\n \n-- 에러 발생: VARCHAR의 최대 크기 32767 이상 넘을 수 없음.\nalter table atest5 modify column (name varchar(32768));\n \n-- 성공\nalter table atest5 modify column (name varchar(128));\n\n\nMINMAX_CACHE_SIZE\n\n이 구문은 특정 컬럼에 대해 MINMAX_CACHE_SIZE를 변경한다.\n\nALTER TABLE table_name MODIFY COLUMN column_name SET MINMAX_CACHE_SIZE=value;\n\n\n-- 예제 : TABLE 이 이렇게 만들어졌다고 가정하자.\ncreate table atest9 (id integer, name varchar(100));\n \n-- 에러: VARCHAR에는 적용 안됨.\nalter table atest9 modify column name set minmax_cache_size=0;\n[ERR-02139 : MINMAX CACHE is not allowed for VARCHAR column(NAME).]\n \n-- 변경 성공\nalter table atest9 modify column id set minmax_cache_size=10240;\n\n\nNOT NULL\n\n컬럼에 NOT NULL 제약 조건을 추가한다. NOT NULL 제약 조건을 추가할 경우 NULL값이 있는 컬럼에 대해서는 DDL연산이 실패한다.\n\n만약 컬럼에 NULL값을 허용하고 싶은 경우에는 다음 절의 MODIFY COLUMN NULL 명령어를 이용한다.\n\nALTER TABLE table_name MODIFY COLUMN column_name NOT NULL;\n\n\n-- t1.c1에 NOT NULL 제약조건을 추가한다.\nalter table t1 modify column c1 not null;\n\n\nNULL\n\nNOT NULL 제약조건을 해제한다. LSM 인덱스의 min_max 캐시로 인한 성능 향상을 얻을 수 없다. \nNULL 값의 입력이 가능해 진다.\n\nALTER TABLE table_name MODIFY COLUMN column_name NULL;\n\n\n-- t1.c1에 NOT NULL 제약조건을 해제한다.\nalter table t1 modify column c1 null;\n\n\nALTER TABLE RECLAIM STORAGE\n\nalter_table_reclaim_storage_stmt:\n\n\n\nalter_table_reclaim_storage_stmt ::= 'ALTER TABLE' table_name 'RECLAIM STORAGE'\n\n\nTag 테이블에서 사용되지 않는 데이터를 삭제하여 가용공간을 확보한다.\n\n시스템 프로퍼티 DISK_TAG_AUTO_RECLAIM 가 1 인 경우(기본값) 특별히 수행하지 않아도 자동으로 수행된다.\n\n이 값을 0으로 설정한 경우에는 원하는 시점에 이 질의를 수행하여 저장 공간을 확보할 수 있다.\n\nTag 테이블에 대해서만 용가능하다.\n\n-- tag table의 저장공간을 확보한다.\nalter table tag reclaim storage;\n\n\nALTER TABLE RENAME TO\n\nalter_table_rename_stmt:\n\n\n\nalter_table_rename_stmt ::= 'ALTER TABLE' table_name 'RENAME TO' new_name\n\n\n테이블의 이름을 변경한다.\n\n메타 테이블들은 이름을 변경할 수 없고, 변경될 이름에 $문자는 사용할 수 없다. 테이블 이름 변경은 log 테이블에 대해서만 가능하다.\n\n-- user 테이블의 이름을 employee로 변경한다.\nALTER TABLE user RENAME TO employee\n\n\nALTER TABLE AUTO DELETE\n\nalter_table_auto_del_stmt:\n\nalter_table_auto_del_stmt ::=  'ALTER TABLE' table_name ('KEEP' 'DISABLE' || 'KEEP' value ('DAY' | 'HOUR' | 'MINUTE' | 'SECOND' | 'RECORD') 'AFTER APPEND' 'INTERVAL' value ('DAY' | 'HOUR' | 'MINUTE' | 'SECOND' | 'RECORD') )\n\n\n\n\n테이블의 자동 삭제 주기를 설정하거나 사용 여부를 결정할 수 있다.\n\nlog 테이블에서만 가능하다.\n\n-- logtbl 테이블의 자동 삭제를 30일간의 데이터만 유지하고 이를 append 이후 5초 간격으로 검사하도록 변경한다.\nALTER TABLE logtbl keep 30 day after append interval 5 second;\n\n\nALTER TABLE ADD RETENTION\n\nalter_table_add_retention_stmt:\n\nalter_table_add_retention_stmt ::=  'ALTER TABLE' table_name 'ADD RETENTION' policy_name\n\n\n\n\nALTER TABLE tag ADD RETENTION policy_1d_1h;\n\n\nALTER TABLE DROP RETENTION\n\nalter_table_drop_retention_stmt:\n\nalter_table_drop_retention_stmt ::=  'ALTER TABLE' table_name 'DROP RETENTION'\n\n\n\n\nALTER TABLE tag DROP RETENTION;\n\n\nTRUNCATE TABLE\n\ntruncate_table_stmt:\n\n\n\ntruncate_table_stmt ::= 'TRUNCATE TABLE' table_name\n\n\n-- ctest 테이블의 모든 데이터를 삭제한다.\nMach&gt; truncate table ctest;\nTruncated successfully.\n\n\n지정된 테이블에 존재하는 모든 데이터를 삭제한다. 단, 해당 테이블을 검색 중인 다른 세션이 존재할 경우에는 에러를 내면서 실패한다.\n\nCREATE ROLLUP\n\ncreate_rollup_stmt:\n\n\n\ncreate_rollup_stmt ::= 'CREATE ROLLUP' rollup_name 'ON' src_table_name '('src_table_column')' 'INTERVAL' number ('SEC' | 'MIN' | 'HOUR')\n\n\n-- tag table의 value 칼럼을 대상으로 rollup을 생성한다.\nMach&gt; CREATE ROLLUP _rollup_tag_value_sec ON tag(value) INTERVAL 1 SEC;\nExecuted successfully\n\n\nDROP ROLLUP\n\ndrop_rollup_stmt:\n\n\n\ndrop_rollup_stmt ::= 'DROP ROLLUP' rollup_name\n\n\n-- rollup을 삭제한다.\nMach&gt; DROP ROLLUP _rollup_tag_value_sec;\nExecuted successfully\n\n\nCREATE RETENTION\n\ncreate_retention_stmt:\n\n\n\ncreate_retention_stmt ::= 'CREATE RETENTION' policy_name 'DURATION' duration ( 'MONTH' | 'DAY' ) 'INTERVAL' interval ( 'DAY' | 'HOUR' )\n\n\n-- retention policy를 생성한다.\nMach&gt; CREATE RETENTION policy_1d_1h DURATION 1 DAY INTERVAL 1 HOUR;\nExecuted successfully\n\n\nDROP RETENTION\n\ndrop_retention_stmt:\n\n\n\ndrop_retention_stmt ::= 'DROP RETENTION' policy_name\n\n\n-- retention policy를 삭제한다.\nMach&gt; DROP RETENTION policy_1d_1h;\nExecuted successfully"
					}
					
				
		
				
					,
					
					"feature-table-volatile-delete-html": {
						"id": "feature-table-volatile-delete-html",
						"title": "휘발성 테이블의 삭제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/volatile/delete.html",
						"content": "데이터 삭제\n\n휘발성 테이블은 조건 절(WHERE 절)에서 기본 키 값 조건을 이용해 데이터를 삭제할 수 있다.\n\n  휘발성 테이블에 기본 키 컬럼이 지정되어 있어야 한다.\n  (기본 키 컬럼) = (값)의 조건만 허용하며, 다른 조건과 함께 사용할 수 없다.\n  기본 키 컬럼이 아닌 다른 컬럼을 사용할 수 없다.\n\n\nMach&gt; create volatile table vtable (id integer primary key, name varchar(20));\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(3, 'north device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(4, 'south device');\n1 row(s) inserted.\nMach&gt; select * from vtable;\nID          NAME                 \n-------------------------------------\n1           west device          \n2           east device          \n3           north device         \n4           south device         \n[4] row(s) inserted.\nMach&gt; delete from vtable where id = 2;\n[1] row(s) deleted.\nMach&gt; select * from vtable;\nID          NAME                 \n-------------------------------------\n1           west device          \n3           north device         \n4           south device         \n[3] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-table-lookup-delete-html": {
						"id": "feature-table-lookup-delete-html",
						"title": "참조 데이터의 삭제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/lookup/delete.html",
						"content": "휘발성 테이블과 동일하다."
					}
					
				
		
				
					,
					
					"feature-table-log-delete-html": {
						"id": "feature-table-log-delete-html",
						"title": "로그 데이터의 삭제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/delete.html",
						"content": "마크베이스에서의 DELETE 구문은 로그 테이블에 대해서 수행 가능하다.\n\n또한, 중간의 임의 위치에 있는 데이터를 삭제할 수 없으며, 임의의 위치부터 연속적으로 마지막(가장 오래된 로그) 레코드까지 지울 수 있도록 구현되었다.\n\n이는 로그 데이터의 특성을 살린 정책으로서 한번 입력되면 수정이 없고, 공간 확보를 위해 파일을 삭제하는 행위를 DB 형식으로 표현한 것이다.\n\n아래는 사용할 수 있는 표현의 종류이다.\n\n목차\n\n  문법\n  예제\n\n\n문법\n\nDELETE FROM table_name;\nDELETE FROM table_name OLDEST number ROWS;\nDELETE FROM table_name EXCEPT number ROWS;\nDELETE FROM table_name EXCEPT number [YEAR | MONTH | WEEK | DAY | HOUR | MINUTE | SECOND];\nDELETE FROM table_name BEFORE datetime_expr;\n\n\n예제\n\n-- 모든 데이터를 삭제한다.\nmach&gt;DELETE FROM devices;\n10 row(s) deleted.\n \n-- 가장 오래된 5건을 삭제한다.\nmach&gt;DELETE FROM devices OLDEST 5 ROWS;\n10 row(s) deleted.\n \n-- 최근 5건을 제외하고 모두 삭제한다.\nmach&gt;DELETE FROM devices EXCEPT 5 ROWS;\n15 row(s) deleted.\n \n-- 2018년 6월 1일 이전의 데이터를 모두 삭제한다.\nmach&gt;DELETE FROM devices BEFORE TO_DATE('2018-06-01', 'YYYY-MM-DD');\n50 row(s) deleted."
					}
					
				
		
				
					,
					
					"feature-table-tag-manipulate-delete-html": {
						"id": "feature-table-tag-manipulate-delete-html",
						"title": "태그 데이터의 삭제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/manipulate/delete.html",
						"content": "목차\n\n\n  태그 데이터 삭제 제약 사항\n  DELETE 구문 수행\n  ROLLUP 데이터의 삭제\n\n\n태그 데이터 삭제 제약 사항\n\n마크베이스는 태그 테이블 전체에 대해서 특정 시간 이전의 전체 데이터에 대한 삭제만을 지원한다.\n\n불가능한 태그 데이터 삭제 조건\n\n\n  특정 태그 아이디 삭제\n  특정 시간 범위의 데이터를 삭제\n  특정 태그의 특정 시간 범위 데이터 삭제\n\n\n가능한 태그 데이터 삭제 조건\n\n\n  특정 시간 이전의 전체 태그에 대한 삭제\n  전체 삭제\n\n\nDELETE 구문 수행\n\n특정 시간 이전의 전체 태그에 대한 삭제\n\nBEFORE 구문의 시간을 지정하면 그 시간 이전의 태그는 모두 삭제한다.\n\nDELETE FROM TAG BEFORE TO_DATE('Time-string');\n\n\n# 현재 데이터\nMach&gt; select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\nTAG_0001 2018-01-01 01:00:00 000:000:000 1\nTAG_0001 2018-01-02 02:00:00 000:000:000 2\nTAG_0001 2018-01-03 03:00:00 000:000:000 3\nTAG_0001 2018-01-04 04:00:00 000:000:000 4\nTAG_0001 2018-01-05 05:00:00 000:000:000 5\nTAG_0001 2018-01-06 06:00:00 000:000:000 6\nTAG_0001 2018-01-07 07:00:00 000:000:000 7\nTAG_0001 2018-01-08 08:00:00 000:000:000 8\nTAG_0001 2018-01-09 09:00:00 000:000:000 9\nTAG_0001 2018-01-10 10:00:00 000:000:000 10\nTAG_0002 2018-02-01 01:00:00 000:000:000 11\nTAG_0002 2018-02-02 02:00:00 000:000:000 12\nTAG_0002 2018-02-03 03:00:00 000:000:000 13\nTAG_0002 2018-02-04 04:00:00 000:000:000 14\nTAG_0002 2018-02-05 05:00:00 000:000:000 15\nTAG_0002 2018-02-06 06:00:00 000:000:000 16\nTAG_0002 2018-02-07 07:00:00 000:000:000 17\nTAG_0002 2018-02-08 08:00:00 000:000:000 18\nTAG_0002 2018-02-09 09:00:00 000:000:000 19\nTAG_0002 2018-02-10 10:00:00 000:000:000 20\n[20] row(s) selected.\n \nMach&gt; delete from tag before to_date('2018-02-01');\n10 row(s) deleted.\n \nMach&gt; select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\nTAG_0002 2018-02-01 01:00:00 000:000:000 11\nTAG_0002 2018-02-02 02:00:00 000:000:000 12\nTAG_0002 2018-02-03 03:00:00 000:000:000 13\nTAG_0002 2018-02-04 04:00:00 000:000:000 14\nTAG_0002 2018-02-05 05:00:00 000:000:000 15\nTAG_0002 2018-02-06 06:00:00 000:000:000 16\nTAG_0002 2018-02-07 07:00:00 000:000:000 17\nTAG_0002 2018-02-08 08:00:00 000:000:000 18\nTAG_0002 2018-02-09 09:00:00 000:000:000 19\nTAG_0002 2018-02-10 10:00:00 000:000:000 20\n[10] row(s) selected.\n\n\n전체 삭제\n\n아무 조건이 없는 경우 전체 데이터가 삭제된다.\n\nMach&gt; delete from tag;\n10 row(s) deleted.\n \nMach&gt; select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\n[0] row(s) selected.\n\n\nROLLUP 데이터의 삭제\n\nDELETE FROM TAG ROLLUP BEFORE TO_DATE('Time-string');\n\n\n위에서 BEFORE 구문의 시간을 지정하면 그 시간 이전의 시, 분, 초 ROLLUP 데이터가 삭제된다.\n\n만일 BEFORE 구문을 지정하지 않는 경우에는 ROLLUP의 모든 데이터를 삭제한다."
					}
					
				
		
				
					,
					
					"sql-ref-dml-html": {
						"id": "sql-ref-dml-html",
						"title": "DML",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/dml.html",
						"content": "목차\n\n\n  INSERT\n    \n      INSERT ON DUPLICATE KEY UPDATE\n    \n  \n  INSERT SELECT\n  UPDATE\n    \n      UPDATE METADATA\n    \n  \n  DELETE\n  DELETE WHERE\n  LOAD DATA INFILE\n\n\nINSERT\n\ninsert_stmt:\n\n\n\ninsert_column_list:\n\n\n\nvalue_list:\n\n\n\nset_list:\n\n\n\ninsert_stmt ::= 'INSERT INTO' table_name ( '(' insert_column_list ')' )? 'METADATA'? 'VALUES' '(' value_list ')' ( 'ON DUPLICATE KEY UPDATE' ( 'SET' set_list )? )?\ninsert_column_list ::= column_name ( ',' column_name )*\nvalue_list ::= value ( ',' value )*\nset_list ::= column_name '=' value ( ',' column_name '=' value )*\n\n\ncreate table test (number int,name varchar(20));\nCreated successfully.\ninsert into test values (1,\"test\");\n1 row(s) inserted.\ninsert into test(name,number) values (\"test\",2);\n1 row(s) inserted.\n\n\n특정 테이블에 값을 입력하는 구문이다. 한 가지 특이한 점은 Column_List에서 지정되지 않은 컬럼에는 모두 NULL 값으로 채워진다는 것이다. 이는 입력의 편의성과 저장 공간의 효율화를 위해 채택된 로그 파일의 특성을 고려한 정책이다.\n\nMETADATA는 tag table에만 사용이 가능하다.\n\nINSERT ON DUPLICATE KEY UPDATE\n\n마크베이스는, 흔히 알려진 UPSERT 기능과 유사한 구문을 지원한다.\n\n기본 키가 지정된 Lookup/Volatile 테이블에 값을 입력할 때 사용할 수 있는 특수 구문으로, 기본 키 값이 중복되는 데이터가 이미 테이블에 존재하는 경우에는 기존 데이터의 값이 변경된다.\n물론, 키 값이 중복되는 데이터가 존재하지 않는 경우에는 새로운 데이터로 삽입된다.\n\n이 구문을 사용하기 위해서, 휘발성 테이블에 기본 키가 지정되어 있어야 한다.\n\n삽입되는 데이터의 컬럼 값과 갱신되는 데이터의 컬럼 값을 다르게 하고자 하는 경우, 또는 삽입되는 데이터의 컬럼 값이 아닌 다른 컬럼 값을 갱신하고자 하는 경우에는 SET 절을 추가로 입력할 수 있다.\n\n\n  SET 절에는 ‘컬럼=값’으로 구성되며, 각각을 콤마로 구분해야 한다.\n  SET 절에서 기본 키 값을 변경해서는 안 된다.\n\n\nINSERT SELECT\n\ninsert_select_stmt:\n\n\n\ninsert_select_stmt ::= 'INSERT INTO' table_name ( '(' insert_column_list ')' )? select_stmt\n\n\n특정 table에 대해서 SELECT 문의 수행 결과를 삽입하는 문장이다. 기본적으로는 다른 DBMS와 유사하지만 다음의 차이점이 있다.\n\n\n  _ARRIVAL_TIME 컬럼 값은 select 및 INSERT 컬럼 리스트에서 지정되지 않으면 INSERT SELECT 문이 수행되는 시점의 시간 값으로 입력된다.\n  VARCHAR 타입의 컬럼에 대해서 삽입되는 입력값이 컬럼의 최대 길이보다 큰 경우, 오류를 발생시키지 않고 해당 컬럼의 최대 길이만큼 잘라서 입력된다.\n  형 변환이 가능한 경우(숫자형-&gt;숫자형)에는 입력되는 컬럼 값에 맞게 삽입된다.\n  수행 도중 오류가 발생한 경우 ROLLBACK되지 않는다.\n  _ARRIVAL_TIME 컬럼의 값을 지정하여 삽입하는 경우, 새로 입력되는 값이 기존의 값보다 이전 시간을 갖고 있으면 입력되지 않는다.\n\n\ncreate table t1 (i1 integer, i2 varchar(60), i3 varchar(5));\nCreated successfully.\n \ninsert into t1 values (1, 'a', 'ddd' );\n1 row(s) inserted.\ninsert into t1 values (2, 'kkkkkkkkkkkkkkkkkkkkk', 'c');\n1 row(s) inserted.\n \ninsert into t1 select * from t1;\n2 row(s) inserted.\ncreate table t2 (i1 integer, i2 varchar(60), i3 varchar(5));\n \ninsert into t2 (_arrival_time, i1, i2, i3) select _arrival_time, * from t1;\n4 row(s) inserted.\n\n\nUPDATE\n\n\n  5.5 부터 제공되는 기능입니다.\n\n\nupdate_stmt:\n\n\n\nupdate_expr_list:\n\n\n\nupdate_expr:\n\n\n\nupdate_stmt ::= 'UPDATE' table_name ( 'METADATA' )? 'SET' update_expr_list 'WHERE' primary_key_column '=' value\nupdate_expr_list ::= update_expr ( ',' update_expr)*\nupdate_expr ::= column '=' value\n\n\nINSERT ON DUPLICATE KEY UPDATE 를 통한 UPSERT 가 아닌, UPDATE 구문도 제공된다.\n\n역시, 기본 키 (Primary Key) 가 지정된 Lookup/Volatile 테이블에 값을 입력할 때 사용할 수 있다. WHERE 절에는 기본 키의 일치 조건식을 작성해야 한다.\n\nUPDATE METADATA\n\nTAGDATA 테이블에 한해서, 메타데이터를 업데이트 하고자 할 때 사용한다.\n\nUPDATE TAG METADATA SET ...\n\n\n\n  TAGDATA 테이블의 메타데이터는 INSERT ON DUPLICATE KEY UPDATE 를 통해 입력/수정할 수 없다.\n\n\nDELETE\n\ndelete_stmt:\n\n\n\ntime_unit:\n\n\n\ndelete_stmt ::= 'DELETE FROM' table_name ( 'OLDEST' number 'ROWS' | 'EXCEPT' number ( 'ROWS' | time_unit ) | 'BEFORE' datetime_expression )? 'NO WAIT'?\ntime_unit ::= 'DURATION' number time_unit ( ( 'BEFORE' | 'AFTER' ) number time_unit )?\n\n\n마크베이스에서의 DELETE BEFORE 구문은 로그 테이블, Tag 테이블, Rollup table에 대해서 수행 가능하다. 중간의 임의 위치에 있는 데이터를 삭제할 수 없으며, 임의의 위치부터 연속적으로 마지막(가장 오래된 로그) 레코드까지 지울 수 있도록 구현되었다.\n\n이는 로그 데이터의 특성을 살린 정책으로서 한번 입력되면 수정이 없고, 공간 확보를 위해 파일을 삭제하는 행위를 DB 형식으로 표현한 것이다.\n\nDURATION, OLDEST, EXCEPT 구문은 TAG 및 Rollup 테이블에 대해서는 사용할 수 없다.\n\n-- 모두 삭제하라.\nDELETE FROM devices;\n \n-- 가장 오래된 마지막 N건을 삭제하라.\nDELETE FROM devices OLDEST N ROWS;\n \n-- 최근 N건을 제외하고 모두 삭제하라.\nDELETE FROM devices EXCEPT N ROWS;\n \n-- 지금부터 N일치를 남기고 모두 삭제하라.\nDELETE FROM devices EXCEPT N DAY;\n \n-- 2014년 6월 1일 이전의 데이터를 모두 삭제하라.\nDELETE FROM devices BEFORE TO_DATE('2014-06-01', 'YYYY-MM-DD');\n \n-- tag 데이터의 시간 기준 삭제\nDELETE FROM tag BEFORE TO_DATE('2014-06-01', 'YYYY-MM-DD');\n \n-- tag rollup 데이터의 시간 기준 삭제\nDELETE FROM tag ROLLUP BEFORE TO_DATE('2014-06-01', 'YYYY-MM-DD');\n\n\nDELETE WHERE\n\ndelete_where_stmt:\n\n\n\ndelete_where_stmt ::= 'DELETE FROM' table_name 'WHERE' column_name '=' value\n\n\ncreate volatile table t1 (i1 int primary key, i2 int);\nCreated successfully.\ninsert into t1 values (2,2);\n1 row(s) inserted.\ndelete from t1 where i1 = 2;\n1 row(s) deleted.\n\n\n휘발성 테이블에 대해서만 수행 가능한 구문으로, WHERE 절에 작성된 조건에 일치하는 레코드만 삭제할 수 있다.\n\n\n  기본 키가 지정된 휘발성 테이블에 대해서만 수행 가능하다.\n  WHERE 절에는 (기본 키 컬럼) = (값) 의 조건만 허용되며, 다른 조건과 함께 작성할 수 없다.\n  기본 키 컬럼이 아닌 다른 컬럼을 조건에 사용할 수 없다.\n\n\ndelete_from_tag_where_stmt:\n\n\n\ndelete_from_tag_where_stmt ::= 'DELETE FROM' table_name 'WHERE' tag_name '=' value ( and tag_time '&lt;' datetime_expression  )?\n\n\nTag 테이블은 아래와 같이 2가지 방식의 삭제쿼리가, 추가적으로 지원된다.\n\n\n  Tag name 기준 삭제\n  Tag name과 Tag time 기준 삭제\n\n\n-- tag name 기준 삭제\nDELETE FROM tag where tag_name = 'my_tag_2021'\n \n-- tag name 와 tag time 기준 삭제\nDELETE FROM tag where tag_name = 'my_tag_2021' and tag_time &lt; TO_DATE('2021-07-01', 'YYYY-MM-DD');\n\n\n\n  삭제 쿼리가 실행된 후에, 삭제된 row가 저장공간에서 물리적으로 삭제되기 까지 걸리는 시간은, DBMS의 동작상황에 따라서 다를 수 있다.\n\n\nLOAD DATA INFILE\n\nload_data_infile_stmt:\n\n\n\nload_data_infile_stmt: 'LOAD DATA INFILE' file_name 'INTO TABLE' table_name ( 'TABLESPACE' tbs_name )? ( 'AUTO' ( 'BULKLOAD' | 'HEADUSE' | 'HEADUSE_ESCAPE' ) )? ( ( 'FIELDS' | 'COLUMNS' ) ( 'TERMINATED BY' char )? ( 'ENCLOSED BY' char )? )? ( 'TRIM' ( 'ON' | 'OFF' ) )? ( 'IGNORE' number ( 'LINES' | 'ROWS' ) )? ( 'MAX_LINE_LENGTH' number )? ( 'ENCODED BY' coding_name )? ( 'ON ERROR' ( 'STOP' | 'IGNORE' ) )?\n\n\nCSV 포맷의 데이터 파일을 서버에서 직접 읽어서, 옵션에 따라 서버에서 직접 테이블 및 컬럼들을 생성하여 이를 입력하는 기능이다.\n\n각 옵션에 대해서 설명하면 다음과 같다.\n\n\n  \n    \n      옵션\n      설명\n       \n    \n  \n  \n    \n      AUTO mode_stringmode_string =(BULKLOAD | HEADUSE | HEADUSE_ESCAPE)\n      해당 테이블을 생성하고 컬럼 타입(자동 생성시 varchar type) 및 컬럼명을 자동으로 생성한다.BULKLOAD: 데이터 한 개의 row를 하나의 컬럼으로 입력한다. 컬럼으로 구분할 수 없는 데이터에 대해서 사용한다.HEADUSE: 데이터 파일의 첫 번째 라인에 기술되어 있는 컬럼 명을 테이블의 컬럼명으로 사용하고, 그 라인에 기술된 수 만큼의 컬럼을 생성한다.HEADUSE_ESCAPE: HEADUSE 옵션과 유사하지만, 컬럼명이 DB의 예약어와 같을 경우 발생할 수 있는 오류를 회피하기 위해 컬럼명의 앞뒤로 ‘’ 문자를 덧붙이고, 컬럼명에 특수문자가 존재하면 그 문자를 ‘’ 문자로 변경한다.\n       \n    \n    \n      (FIELDS|COLUMNS) TERMINATED BY ‘term_char’ESCAPED BY ‘escape_char’\n      데이터 라인을 파싱하기 위한 구분 문자(term_char)와 이스케이프 문자(escape_char)를 지정한다. 일반적인 CSV 파일의 경우 구분 문자는 , 이며 이스케이프 문자는 ‘이다.\n       \n    \n    \n      ENCODED BY coding_namecoding_name ={ UTF8(default) | MS949 | KSC5601 | EUCJP | SHIFTJIS | BIG5 | GB231280 }\n      데이터 파일의 인코딩 옵션을 지정한다. 기본 값은 UTF-8이다.\n       \n    \n    \n      TRIM (ON | OFF)\n      컬럼의 빈 공간을 제거하거나 유지한다. 기본값은 ON이다.\n       \n    \n    \n      IGNORE number (LINES | ROWS)\n      숫자로 지정된 라인 또는 행 만큼의 데이터를 무시한다. CSV 포맷 파일의 헤더 등을 무시하거나 VCF 헤더를 무시하기 위해서 사용한다.\n      한 라인의 최대 길이를 지정한다. 기본값은 512K이며, 데이터가 더 큰 경우에는 더 큰 값을 지정할 수 있다.\n    \n    \n      ON ERROR (STOP | IGNORE)\n      입력 도중 에러가 발생할 경우 수행할 동작을 지정한다. STOP인 경우 입력을 중단하고 IGNORE인 경우 에러가 발생한 라인을 건너뛰고 계속 입력한다.기본값은 IGNORE이다.\n       \n    \n  \n\n\n-- default field delimiter(,)  field encloser (\") 를 사용하여 데이터를 입력한다.\nLOAD DATA INFILE '/tmp/aaa.csv' INTO TABLE Sample_data ;\n \n-- 하나의 컬럼을 갖는 NEWTABLE을 생성해서 한 라인을 한 컬럼으로 입력한다.\nLOAD DATA INFILE '/tmp/bbb.csv' INTO TABLE NEWTABLE AUTO BULKLOAD;\n \n-- csv의 첫번째 라인을 컬럼 정보로 이용하여 NEWTABLE을 생성하고, 이를 그 테이블에 입력한다.\nLOAD DATA INFILE '/tmp/bbb.csv' INTO TABLE NEWTABLE AUTO HEADUSE;\n  \n-- 첫번째 라인은 무시하고 필드 구분자는 ; enclosing 문자는 ' 로 지정해서 입력한다.\nLOAD DATA INFILE '/tmp/ccc.csv' INTO TABLE Sample_data FIELDS TERMINATED BY ';' ENCLOSED BY '\\''  IGNORE 1 LINES ON ERROR IGNORE;\n\n\n\n  AUTO 옵션을 사용하지 않는 경우 테이블의 모든 컬럼은 VARCHAR 또는 TEXT 타입으로 생성해야 한다."
					}
					
				
		
				
					,
					
					"install-linux-docker-install-html": {
						"id": "install-linux-docker-install-html",
						"title": "Docker 설치",
						"version": "all",
						"categories": "",
						"url": " /install/linux/docker-install.html",
						"content": "마크베이스는 Docker 이미지를 제공한다.  Docker가 이미 설치되어 있다고 가정하고 마크베이스를 Docker로 설치하는 과정을 설명한다.\n\nDocker 설치는 Docker 설치 페이지를 참조하여 진행한다. 마크베이스의 Docker Hub는 이 페이지를 참고한다.\n\n$ docker pull machbase/machbase\nUsing default tag: latest\nlatest: Pulling from machbase/machbase\n3a291d7fe8d1: Pull complete\nf1e7bd0ef2d1: Pull complete\n78632f9cbb53: Pull complete\nf4f6c5358244: Pull complete\na3e04b27f9cd: Pull complete\na3ed95caeb02: Pull complete\ne03e135c0eda: Pull complete\n26612cd7ebc1: Pull complete\nb61e71cf4bc2: Pull complete\n09c9c411b936: Pull complete\n2b1cdec8c664: Pull complete\nfd9a9a288691: Pull complete\nd8852dedc8a1: Pull complete\ncba7e30dbb6f: Pull complete\nc7ead0fa7c49: Pull complete\n6af02fe4c01f: Pull complete\nd18db958464f: Pull complete\n1fb93627ec0f: Pull complete\n265b8b73294a: Pull complete\nf122e6396b46: Pull complete\n3b2f248fb414: Pull complete\n07ed5a8f0935: Pull complete\n44ec57c5ed31: Pull complete\n59383e5f4c61: Pull complete\n542101ec7002: Pull complete\n\n\n# 설치된 마크베이스 이미지를 확인한다.\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nmachbase/machbase   latest              dfb90844e7da        2 months ago        1.09 GB\nDigest: sha256:aa6a982d35946b3fb33930de91cad61bfe7d3e9a559080526ed8e9a511c82c2b\nStatus: Downloaded newer image for machbase/machbase:latest\n\n\n# 마크베이스 이미지를 실행한다.\n$ docker run -it machbase/machbase\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDatabase created successfully.\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for Machbase server start.\nMachbase server started successfully.\nSERVER HAS BEEN RESET\nSERVER STARTED, PID : 56\n     Connection URL : http://172.17.0.2:5001\nmachbase@5ba45a22d140:~$"
					}
					
				
		
				
					,
					
					"sdk-dotnet-html": {
						"id": "sdk-dotnet-html",
						"title": ".NET CONNECTOR",
						"version": "all",
						"categories": "",
						"url": " /sdk/dotnet.html",
						"content": "ADO.NET 드라이버 일부 기능을 지원하는 .NET (C#) Connector 라이브러리를 제공하고 있다.\n\n라이브러리 위치는 $MACHBASE_HOME/lib/ 에서 DLL 형태로 제공하고 있으며, .NET 버전에 따라 다른 DLL 을 제공한다.\n\n\n  \n    \n      .NET Framework 4.0 : machNetConnector.dll\n    \n  \n  \n    \n      .NET Core 2.0 : machNetConnectorCore.dll\n    \n  \n\n\n클래스\n아래 소개된 기능 외의 것은 아직 구현되어 있지 않거나, 올바르게 작동되지 않을 수 있다.\n미구현으로 명시된 메서드나 필드를 부르는 경우, NotImplementedException 또는 NotSupportedException 을 발생시킨다.\n\n마크베이스와의 연결을 담당하는 클래스이다.\n\nDbConnection 과 같이 IDisposable 을 상속받기 때문에, Dispose() 를 통한 객체 해제나 using() 문을 이용한 객체의 자동 Dispose를 지원한다.MachConnection : DbConnection\n\n\n  \n    \n      생성자\n      설명\n    \n  \n  \n    \n      MachConnection(string aConnectionString)\n      Connection String 을 입력으로, MachConnection 을 생성한다.\n    \n  \n\n\n\n  \n    \n      메서드\n      설명\n    \n  \n  \n    \n      Open()\n      입력받은 Connection String 으로 실제 연결을 시도한다.\n    \n    \n      Close()\n      연결 중이라면, 해당 연결을 종료한다.\n    \n    \n      BeginDbTransaction(IsolationLevel isolationLevel)\n      (미구현) MACHBASE 는 특별한 Transaction 이 없으므로 해당 객체 역시 지원하지 않는다.\n    \n    \n      CreateDbCommand()\n      (미구현) 아직은, 명시적으로 MachCommand 를 만들도록 유도한다.\n    \n    \n      ChangeDatabase(string databaseName)\n      (미구현) MACHBASE 는 DATABASE 구분이 없다.\n    \n  \n\n\n\n  \n    \n      필드\n      설명\n    \n  \n  \n    \n      State\n      System.Data.ConnectionState 값을 나타낸다.\n    \n    \n      StatusString\n      연결된 MachCommand 로 수행하는 상태를 나타낸다.Error Message 를 꾸미는 용도로 내부에서 사용되며, 작업이 시작된 상태를 나타내기 때문에 이 값으로 쿼리 상태를 체크하는 것은 적절하지 않다.\n    \n    \n      Database\n      (미구현)\n    \n    \n      DataSource\n      (미구현)\n    \n    \n      ServerVersion\n      (미구현)\n    \n  \n\n\nConnection String\n각 항목은 semicolon (;) 으로 구분되며, 다음을 사용할 수 있다.\n동일 항목에 있는 여러 Keyword 는, 모두 같은 의미이다.\n\n\n  \n    \n      Keyword\n      설명\n      예제\n      기본값\n    \n  \n  \n    \n      SERVER\n      Hostname\n      SERVER=192.168.0.1\n       \n    \n    \n      PORTPORT_NO\n      Port No.\n      PORT=5656\n      5656\n    \n    \n      USERIDUSERNAMEUSERUID\n      사용자 ID\n      USER=SYS\n      SYS\n    \n    \n      PASSWORDPWD\n      사용자 패스워드\n      PWD=manager\n       \n    \n    \n      CONNECT_TIMEOUTConnectionTimeoutconnectTimeout\n      연결 최대 시간\n      CONNECT_TIMEOUT\n      60초\n    \n    \n      COMMAND_TIMEOUTcommandTimeout\n      각 명령 수행 최대 시간\n      COMMAND_TIMEOUT\n      60초\n    \n  \n\n\n예제로, 아래와 같은 문자열을 준비해 둘 수 있다.\nString sConnString = String.Format(\"SERVER={0};PORT_NO={1};UID=;PWD=MANAGER;CONNECT_TIMEOUT=10000;COMMAND_TIMEOUT=50000\", SERVER_HOST, SERVER_PORT);\n\n\nMachCommand : DbCommand\nMachConnection 을 이용해 SQL 명령 또는 APPEND 를 수행하는 클래스이다.\nDbCommand 와 같이 IDisposable 을 상속받기 때문에, Dispose() 를 통한 객체 해제나 using() 문을 이용한 객체의 자동 Dispose를 지원한다.\n\n\n  \n    \n      생성자\n      설명\n    \n  \n  \n    \n      MachCommand(string aQueryString, MachConnection)\n      연결할 MachConnection 객체와 함께, 수행할 쿼리를 입력해서 생성한다.\n    \n    \n      MachCommand(MachConnection)\n      연결할 MachConnection 객체를 입력해서 생성한다. 수행할 쿼리가 없는 경우 (e.g. APPEND) 에만 사용한다.\n    \n  \n\n\n\n  \n    \n      메서드\n      설명\n    \n  \n  \n    \n      void CreateParameter() /void CreateDbParameter()\n      새로운 MachParameter 를 생성한다.\n    \n    \n      void Cancel()\n      (미구현)\n    \n    \n      void Prepare()\n      (미구현)\n    \n    \n      MachAppendWriterAppendOpen(aTableName, aErrorCheckCount = 0, MachAppendOption = None)\n      APPEND 를 시작한다. MachAppendWriter 객체를 반환한다.aTableName : 대상 테이블 이름aErrorCheckCount : APPEND-DATA 로 입력한 레코드 누적 개수가 일치할 때 마다, 서버로 보내 실패 여부를 확인한다.말하자면, 자동 APPEND-FLUSH 지점을 정하는 셈이다.MachAppendOption : 현재는 하나의 옵션만 제공하고 있다.MachAppendOption.None : 아무런 옵션도 붙지 않는다.MachAppendOption.MicroSecTruncated : DateTime 객체의 값 입력 시, microsecond 까지만 표현된 값을 입력한다.(DateTime 객체의 Ticks 값은 100 nanosecond 까지 표현된다.)\n    \n    \n      voidAppendData(MachAppendWriter aWriter, List aDataList)\n      MachAppendWriter 객체를 통해, 데이터가 들어있는 리스트를 받아 데이터베이스에 입력한다.List 에 들어간 데이터 순서대로, 각각의 자료형은 테이블에 표현된 컬럼의 자료형과 일치해야 한다.List 에 들어있는 데이터가 모자라거나 넘치면, 에러를 발생시킨다.\n    \n    \n      voidAppendDataWithTime(MachAppendWriter aWriter, List aDataList, DateTime aArrivalTime)\n      AppendData() 에서, _arrival_time 값을 DateTime 객체로 명시적으로 넣을 수 있는 메서드이다.\n    \n    \n      voidAppendDataWithTime(MachAppendWriter aWriter, List aDataList, ulong aArrivalTimeLong)\n      AppendData() 에서, _arrival_time 값을 ulong 객체로 명시적으로 넣을 수 있는 메서드이다.ulong 값을 _arrival_time 값으로 입력할 때 발생할 수 있는 문제는 위의 AppendData() 를 참고한다.\n    \n    \n      void AppendFlush(MachAppendWriter aWriter)\n      AppendData() 로 입력한 데이터들을 즉시 서버로 보내, 데이터 입력을 강제한다.호출 빈도가 잦을 수록, 성능은 떨어지지만 시스템 오류로 인한 데이터 유실율을 낮출 수 있고 에러 검사를 빠르게 할 수 있다.호출 빈도가 뜸할 수록, 데이터 유실이 발생할 가능성이 크고 에러 검사가 지연되지만 성능은 크게 올라간다.\n    \n    \n      void AppendClose(MachAppendWriter aWriter)\n      APPEND 를 마친다. 내부적으로 AppendFlush() 를 호출한 뒤에 실제 프로토콜을 마친다.\n    \n    \n      int ExecuteNonQuery()\n      입력받았던 쿼리를 수행한다. 쿼리가 영향을 미친 레코드 개수를 반환한다.보통 SELECT 를 제외한 쿼리를 수행할 때 사용한다.\n    \n    \n      object ExecuteScalar()\n      입력받았던 쿼리를 수행한다. 쿼리 Targetlist 의 첫 번째 값을 객체로 반환한다.보통 SELECT 쿼리, 그 중에서도 결과가 1건만 나오는 SELECT (Scalar Query) 를 수행해 DbDataReader 없이 결과를 받고자 하는 경우 사용한다.\n    \n    \n      DbDataReader ExecuteDbDataReader(CommandBehavior aBehavior)\n      입력받았던 쿼리를 수행해, 해당 쿼리의 결과를 읽어 올 수 있는 DbDataReader 를 생성해 반환한다.\n    \n  \n\n\n\n  \n    \n      필드\n      설명\n    \n  \n  \n    \n      Connection /DbConnection\n      연결된 MachConnection.\n    \n    \n      ParameterCollection /DbParameterCollection\n      Binding 목적으로 사용할 MachParameterCollection.\n    \n    \n      CommandText\n      쿼리 문자열.\n    \n    \n      CommandTimeout\n      특정 작업 수행 중, 서버로부터 응답을 기다리기까지의 시간.MachConnection 에 설정된 값을 따르며, 여기서는 값 참조만 할 수 있다.\n    \n    \n      FetchSize\n      한번에 서버로부터 Fetch 할 레코드 개수. 기본값은 3000 이다.\n    \n    \n      IsAppendOpened\n      APPEND 작업 중인 경우, Append 가 이미 열려있는지 아닌지를 판단한다.\n    \n    \n      CommandType\n      (미구현)\n    \n    \n      DesignTimeVisible\n      (미구현)\n    \n    \n      UpdatedRowSource\n      (미구현)\n    \n  \n\n\nMachDataReader : DbDataReader\nFetch 한 결과를 읽어들이는 클래스이다. 명시적으로 생성이 불가능하고 MachCommand.ExecuteDbDataReader() 로 생성된 객체만 사용이 가능하다.\n\n\n  \n    \n      메서드\n      설명\n    \n  \n  \n    \n      string GetName(int ordinal)\n      ordinal 번째 컬럼 이름을 반환한다.\n    \n    \n      string GetDataTypeName(int ordinal)\n      ordinal 번째 컬럼의 자료형 이름을 반환한다.\n    \n    \n      Type GetFieldType(int ordinal)\n      ordinal 번째 컬럼의 자료형을 반환한다.\n    \n    \n      int GetOrdinal(string name)\n      컬럼 이름이 위치한 인덱스를 반환한다.\n    \n    \n      object GetValue(int ordinal)\n      현재 위치한 레코드의 ordinal 번째 값을 반환한다.\n    \n    \n      bool IsDBNull(int ordinal)\n      현재 위치한 레코드의 ordinal 번째 값이 NULL 인지 여부를 반환한다.\n    \n    \n      int GetValues(object[] values)\n      현재 위치한 레코드의 모든 값들을 전부 설정하고, 그 개수를 반환한다.\n    \n    \n      xxxx GetXXXX(int ordinal)\n      ordinal 번째 컬럼 값을, 자료형 (XXXX) 에 맞춰 반환한다.BooleanByteCharInt16/32/64DateTimeStringDecimalDoubleFloat\n    \n    \n      bool Read()\n      다음 레코드를 읽는다. 결과가 존재하지 않으면 False 를 반환한다.\n    \n    \n      DataTable GetSchemaTable()\n      (미지원)\n    \n    \n      bool NextResult()\n      (미지원)\n    \n  \n\n\n\n  \n    \n      필드\n      설명\n    \n  \n  \n    \n      FetchSize\n      한번에 서버로부터 Fetch 할 레코드 개수. 기본값은 3000 이며 여기서는 수정할 수 없다.\n    \n    \n      FieldCount\n      결과 컬럼 개수.\n    \n    \n      this[int ordinal]\n      object GetValue(int ordinal) 와 동일하다.\n    \n    \n      this[string name]\n      object GetValue(GetOrdinal(name)) 와 동일하다.\n    \n    \n      HasRows\n      결과가 존재하는지 여부를 나타낸다.\n    \n    \n      RecordsAffected\n      MachCommand 의 것과 달리, 여기서는 Fetch Count 를 나타낸다.\n    \n  \n\n\nMachParameterCollection : DbParameterCollection\nMachCommand 에 필요한 파라메터를 바인딩하는 클래스이다.\n\n바인딩한 이후에 수행하게 되면, 해당 값이 같이 수행된다.\nPrepared Statement 개념이 구현되어 있지 않아, Binding 이후 Execute 를 해도 수행 성능은 최초 수행한 것과 같다.\n\n\n\n  \n    \n      메서드\n      설명\n    \n  \n  \n    \n      MachParameterAdd(string parameterName, DbType dbType)\n      파라메터 이름과 타입을 지정해, MachParameter 를 추가한다.추가된 MachParameter 객체를 반환한다.\n    \n    \n      int Add(object value)\n      값을 추가한다. 추가된 인덱스를 반환한다.\n    \n    \n      void AddRange(Array values)\n      단순 값의 배열을 모두 추가한다.\n    \n    \n      MachParameterAddWithValue(string parameterName, object value)\n      파라메터 이름과 그 값을 추가한다.추가된 MachParameter 객체를 반환한다.\n    \n    \n      bool Contains(object value)\n      해당 값이 추가되었는지 여부를 판단한다.\n    \n    \n      bool Contains(string value)\n      해당 파라메터 이름이 추가되었는지 여부를 판단한다.\n    \n    \n      void Clear()\n      파라메터들을 모두 삭제한다.\n    \n    \n      int IndexOf(object value)\n      해당 값의 인덱스를 반환한다.\n    \n    \n      int IndexOf(string parameterName)\n      해당 파라메터 이름의 인덱스를 반환한다.\n    \n    \n      void Insert(int index, object value)\n      특정 인덱스에, 해당 값을 추가한다.\n    \n    \n      void Remove(object value)\n      해당 값을 포함한 파라메터를 삭제한다.\n    \n    \n      void RemoveAt(int index)\n      인덱스에 위치한 파라메터를 삭제한다.\n    \n    \n      void RemoveAt(string parameterName)\n      해당 이름을 가진 파라메터를 삭제한다.\n    \n  \n\n\n\n  \n    \n      필드\n      설명\n    \n  \n  \n    \n      Count\n      파라메터 개수\n    \n    \n      this[int index]\n      index 번째의 MachParameter 를 나타낸다.\n    \n    \n      this[string name]\n      파라메터 이름과 일치하는 순서의 MachParameter 를 나타낸다.\n    \n  \n\n\nMachParameter : DbParameter\nMachCommand 에 필요한 파라메터를 각각 바인딩한 정보를 담는 클래스이다.\n\n특별히 메서드는 지원하지 않는다.\n\n\n  \n    \n      필드\n      설명\n    \n  \n  \n    \n      ParameterName\n      파라메터 이름\n    \n    \n      Value\n      값\n    \n    \n      Size\n      값의 크기\n    \n    \n      Direction\n      ParameterDirection (Input / Output / InputOutput / ReturnValue)기본값은 Input 이다.\n    \n    \n      DbType\n      DB Type\n    \n    \n      MachDbType\n      MACHBASE DB TypeDB Type 과 다를 수 있다.\n    \n    \n      IsNullable\n      NULL 가능 여부\n    \n    \n      HasSetDbType\n      DB Type 이 지정되었는지 여부\n    \n  \n\n\nMachException : DbException\n마크베이스에서 나타나는 에러를 표시하는 클래스이다.\n\n에러 메시지가 설정되어 있는데, 모든 에러 메시지는 MachErrorMsg 에서 확인할 수 있다.\n\n\n  \n    \n      필드\n      설명\n    \n  \n  \n    \n      int MachErrorCode\n      MACHBASE 에서 제공하는 에러 코드\n    \n  \n\n\nMachAppendWriter\nMachCommand 를 사용하는 별도의 클래스로 APPEND 를 지원한다.\nADO.NET 표준이 아닌, MACHBASE 의 Append Protocol 을 지원하기 위한 클래스이다.\n\n별도의 생성자 없이 MachCommand 의 AppendOpen() 으로 생성된다.\n\n\n  \n    \n      메서드\n      설명\n    \n  \n  \n    \n      void SetErrorDelegator(ErrorDelegateFuncType aFunc)\n      에러가 발생했을 때 호출할 ErrorDelegateFunc 을 지정한다.\n    \n  \n\n\n\n  \n    \n      필드\n      설명\n    \n  \n  \n    \n      SuccessCount\n      입력 성공한 레코드 개수. AppendClose() 이후 설정된다.\n    \n    \n      FailureCount\n      입력 실패한 레코드 개수. AppendClose() 이후 설정된다.\n    \n    \n      Option\n      AppendOpen() 때 입력받은 MachAppendOption\n    \n  \n\n\nErrorDelegateFuncType\npublic delegate void ErrorDelegateFuncType(MachAppendException e);\n\nMachAppendWriter 에서, APPEND 도중 MACHBASE 서버 측에서 발생하는 Error 를 감지하기 위한 함수를 지정할 수 있다.\n\n.NET 에서는 이 함수형을 Delegator Function 으로 지정한다.\n\nMachAppendException : MachException\nMachException 과 동일하지만, 다음 점이 다르다.\n\n\n  에러 메시지가 서버 측으로부터 수신된다.\n  에러가 발생한 데이터 버퍼를 획득할 수 있다. (comma-separated) 이 데이터를 가공해 다시 APPEND 하거나 기록하는 용도로 사용할 수 있다.\n\n\n해당 예외는 ErrorDelegateFunc 내부에서만 획득이 가능하다.\n\n\n  \n    \n      메서드\n      설명\n    \n  \n  \n    \n      GetRowBuffer()\n      에러가 발생한 데이터 버퍼를 획득할 수 있다.\n    \n  \n\n\nMachTransaction\n지원하지 않는다.\n\n샘플 코드\n연결\nMachConnection 을 만들어 Open() - Close() 하면 된다.\nString sConnString = String.Format(\"SERVER={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nMachConnection sConn = new MachConnection(sConnString);\nsConn.Open();\n//... do something\nsConn.Close();\n\nusing 구문을 사용하면, Connection 종료 작업인 Close() 를 호출하지 않아도 된다.\nString sConnString = String.Format(\"SERVER={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    sConn.Open();\n    //... do something\n} // you don't need to call sConn.Close();\n\n쿼리 수행\nMachCommand 를 만들어 쿼리를 수행하면 된다.\nString sConnString = String.Format(\"SERVER={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    String sQueryString = \"CREATE TABLE tab1 ( col1 INTEGER, col2 VARCHAR(20) )\";\n    MachCommand sCommand = new MachCommand(sQueryString , sConn)\n    try\n    {\n        sCommand.ExecuteNonQuery();\n    }\n    catch (MachException me)\n    {\n        throw me;\n    }\n}\n\n이 역시 using 구문을 사용하면, MachCommand 해제 작업을 곧바로 진행할 수 있다.\nString sConnString = String.Format(\"SERVER={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    String sQueryString = \"CREATE TABLE tab1 ( col1 INTEGER, col2 VARCHAR(20) )\";\n    using(MachCommand sCommand = new MachCommand(sQueryString , sConn))\n    {\n        try\n        {\n            sCommand.ExecuteNonQuery();\n        }\n        catch (MachException me)\n        {\n            throw me;\n        }\n    }\n}\n\nSELECT 수행\nSELECT 쿼리를 가진 MachCommand 를 실행해 MachDataReader 를 얻을 수 있다.\n\nMachDataReader 를 통해 레코드를 하나씩 Fetch 할 수 있다.\nString sConnString = String.Format(\"SERVER={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    String sQueryString = \"SELECT * FROM tab1;\";\n    using(MachCommand sCommand = new MachCommand(sQueryString , sConn))\n    {\n        try\n        {\n            MachDataReader sDataReader = sCommand.ExecuteReader();\n            while (sDataReader.Read())\n            {\n                for (int i = 0; i &lt; sDataReader.FieldCount; i++)\n                {\n                    Console.WriteLine(String.Format(\"{0} : {1}\",\n                                                    sDataReader.GetName(i),\n                                                    sDataReader.GetValue(i)));\n                }\n            }\n        }\n        catch (MachException me)\n        {\n            throw me;\n        }\n    }\n}\n\n파라메터 바인딩\nMachParameterCollection 을 생성한 다음, MachCommand 에 연결해서 수행할 수 있다.\nString sConnString = String.Format(\"SERVER={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    string sSelectQuery = @\"SELECT *\n        FROM tab2\n        WHERE CreatedDateTime &lt; @CurrentTime\n        AND CreatedDateTime &gt;= @PastTime\";\n \n    using (MachCommand sCommand = new MachCommand(sSelectQuery, sConn))\n    {\n        DateTime sCurrtime = DateTime.Now;\n        DateTime sPastTime = sCurrtime.AddMinutes(-1);\n \n        try\n        {\n            sCommand.ParameterCollection.Add(new MachParameter { ParameterName = \"@CurrentTime\", Value = sCurrtime });\n            sCommand.ParameterCollection.Add(new MachParameter { ParameterName = \"@PastTime\", Value = sPastTime });\n \n            MachDataReader sDataReader = sCommand.ExecuteReader();\n \n            while (sDataReader.Read())\n            {\n                for (int i = 0; i &lt; sDataReader.FieldCount; i++)\n                {\n                    Console.WriteLine(String.Format(\"{0} : {1}\",\n                                                    sDataReader.GetName(i),\n                                                    sDataReader.GetValue(i)));\n                }\n            }\n        }\n        catch (MachException me)\n        {\n            throw me;\n        }\n    }\n}\n\nAPPEND\nMachCommand 에서 AppendOpen() 을 수행하면, MachAppendWriter 객체를 얻을 수 있다.\n\n이 객체와 MachCommand 를 이용해, 입력 레코드 1건을 리스트로 준비해 AppendData() 를 수행하면 입력이 이뤄진다.\n\nAppendFlush() 를 하면 모든 레코드의 입력이 반영되며, AppendClose() 를 통해 Append 전체 과정을 종료할 수 있다.\nString sConnString = String.Format(\"SERVER={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    using (MachCommand sAppendCommand = new MachCommand(sConn))\n    {\n        MachAppendWriter sWriter = sAppendCommand.AppendOpen(\"tab2\");\n        sWriter.SetErrorDelegator(AppendErrorDelegator);\n \n        var sList = new List&lt;object&gt;();\n        for (int i = 1; i &lt;= 100000; i++)\n        {\n            sList.Add(i);\n            sList.Add(String.Format(\"NAME_{0}\", i % 100));\n \n            sAppendCommand.AppendData(sWriter, sList);\n \n            sList.Clear();\n \n            if (i % 1000 == 0)\n            {\n                sAppendCommand.AppendFlush();\n            }\n        }\n \n        sAppendCommand.AppendClose(sWriter);\n        Console.WriteLine(String.Format(\"Success Count : {0}\", sWriter.SuccessCount));\n        Console.WriteLine(String.Format(\"Failure Count : {0}\", sWriter.FailureCount));\n    }\n}\n\nprivate static void AppendErrorDelegator(MachAppendException e)\n{\n    Console.WriteLine(\"{0}\", e.Message);\n    Console.WriteLine(\"{0}\", e.GetRowBuffer());\n}"
					}
					
				
		
				
					,
					
					"feature-table-tag-duplication-removal-html": {
						"id": "feature-table-tag-duplication-removal-html",
						"title": "태그 테이블 중복제거 설정",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/duplication-removal.html",
						"content": "중복제거설정\nTAG table 생성 시 중복제거 기간을 table property로 전달한다. 중복제거 설정이 가능한 최대 기간은 30일이다.\n\n-- 기존 입력된 데이터 중 1일 이내의 데이터와 중복된 데이터가 입력될 경우 새로 입력된 데이터를 삭제\n  \nCREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED) TAG_DUPLICATE_CHECK_DURATION=1;\n\n\nm$sys_table_property에서 중복제거 설정정보를 확인할  수 있다.\nSELECT * FROM m$sys_table_property WHERE id={table_id} AND name = 'TAG_DUPLICATE_CHECK_DURATION';\n\n\n데이터 입.출력 예시(중복제거기간 1일 설정)\n-- 입력된 총 6건의 데이터중 4건의 데이터가 중복 이고 그중 1건은 system시간(1970-01-03 09:00:00 000:000:003)으로 부터\n-- 1일전 데이터이며 조회시 새로 입력된 데이터중 설정기간(1일)내의 데이터와 중북인 데이터가 출력이 안됨\n\nINSERT INTO tag VALUES('tag1', '1970-01-01 09:00:00 000:000:001', 0);\nINSERT INTO tag VALUES('tag1', '1970-01-02 09:00:00 000:000:001', 0);    \nINSERT INTO tag VALUES('tag1', '1970-01-02 09:00:00 000:000:002', 0);\nINSERT INTO tag VALUES('tag1', '1970-01-02 09:00:00 000:000:002', 1);\nINSERT INTO tag VALUES('tag1', '1970-01-03 09:00:00 000:000:003', 0);\nINSERT INTO tag VALUES('tag1', '1970-01-01 09:00:00 000:000:001', 0);\n    \nSELECT * FROM tag WHERE name = 'tag1';\nNAME                  TIME                            VALUE                       \n--------------------------------------------------------------------------------------\ntag1                  1970-01-01 09:00:00 000:000:001 0\ntag1                  1970-01-02 09:00:00 000:000:001 0                           \ntag1                  1970-01-02 09:00:00 000:000:002 0\ntag1                  1970-01-03 09:00:00 000:000:003 0      \ntag1                  1970-01-01 09:00:00 000:000:001 0\n  \n\n\n중복제거 제약 사항\n\n  테이블 생성 시 중복제거 정책이 설정되면 이후에 변경할 수 없음\n  중복제거 설정은 일 단위로 설정할 수 있으며 최대 30일까지 설정이 가능함\n  기존 입력된 데이터가 이미 삭제된 경우 동일한 데이터가 다시 들어와도 중복제거 대상이 아님"
					}
					
				
		
				
					,
					
					"intro-editions-html": {
						"id": "intro-editions-html",
						"title": "마크베이스 제품군 소개",
						"version": "all",
						"categories": "",
						"url": " /intro/editions.html",
						"content": "마크베이스는 다음과 같은 2가지의 제품군을 가지고 각각에 맞는 형태의 비지니스 환경에 적용할 수 있다.\n\n\n  Standard Edition\n  Cluster Edition"
					}
					
				
		
				
					,
					
					"feature-table-log-ex-html": {
						"id": "feature-table-log-ex-html",
						"title": "로그 테이블 활용 샘플 예제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/ex.html",
						"content": "마크베이스 패키지를 설치하면 로그 테이블을 생성하고 로그 데이터를 생성된 테이블에 입력하고 조회하는 튜토리얼을 제공한다.\n\n아래 경로에서 확인할 수 있다.\n\n[machbase@localhost tutorials]$ cd $MACHBASE_HOME/tutorials\n[machbase@localhost tutorials]$ ls -l\ntotal 0\ndrwxrwxr-x 2 machbase machbase 103 Oct 30 16:10 backup_mount\ndrwxrwxr-x 2 machbase machbase  44 Oct 30 16:10 connect_r\ndrwxrwxr-x 2 machbase machbase 177 Oct 30 16:10 csvload\ndrwxrwxr-x 2 machbase machbase  49 Oct 30 16:10 export_data\ndrwxrwxr-x 2 machbase machbase  32 Oct 30 16:10 install_docker_image\ndrwxrwxr-x 2 machbase machbase  49 Oct 30 16:10 ip_address\ndrwxrwxr-x 2 machbase machbase  75 Oct 30 16:10 searchtext\ndrwxrwxr-x 2 machbase machbase  93 Oct 30 16:10 time_series\n[machbase@localhost tutorials]$\n\n\n목차\n\n  로그 테이블 생성\n  로그 데이터 입력\n  로그 데이터 조회\n  인덱스 생성 및 조회\n  시계열 데이터 조회\n  인터넷 주소형 데이터 조회\n\n\n로그 테이블 생성\n\n입력할 로그 데이터는 다음은 csv 포맷의 파일이다.\n\n[machbase@localhost csvload]$ cd $MACHBASE_HOME/tutorials/csvload\n[machbase@localhost csvload]$ more sample_data.csv\n2015-05-20 06:00:00,63.214.191.124,2296,122.195.164.32,5416,12,GET /twiki/bin/view/Main/TWikiGroups?rev=1.2 HTTP/1.1,200,5162\n2015-05-20 06:00:07,212.237.153.79,6203,71.129.68.118,8859,67,GET /twiki/bin/view/Main/WebChanges HTTP/1.1,200,40520\n2015-05-20 06:00:07,243.9.49.80,344,122.195.164.32,6203,46,GET /twiki/bin/view/Main/TWikiGroups?rev=1.2 HTTP/1.1,200,5162\n2015-05-20 06:00:07,232.191.241.129,5377,174.47.129.59,1247,17,GET /mailman/listinfo/hsdivision HTTP/1.1,200,6291\n2015-05-20 06:00:07,121.67.24.216,2296,212.237.153.79,6889,68,GET /twiki/bin/view/TWiki/WebTopicEditTemplate HTTP/1.1,200,3732\n2015-05-20 06:00:07,31.224.72.52,450,100.46.183.122,10541,20,GET /twiki/bin/view/Main/WebChanges HTTP/1.1,200,40520\n2015-05-20 06:00:07,210.174.159.227,6180,173.149.119.202,6927,2,GET /twiki/bin/rdiff/TWiki/AlWilliams?rev1=1.2&amp;rev2=1.1 HTTP/1.1,200,5234\n2015-05-20 06:00:07,210.174.159.227,10124,16.194.51.72,10512,69,GET /twiki/bin/rdiff/TWiki/AlWilliams?rev1=1.2&amp;rev2=1.1 HTTP/1.1,200,5234\n2015-05-20 06:00:07,60.48.99.15,12333,85.183.139.166,12020,64,GET /robots.txt HTTP/1.1,200,68\n\n\n로그 데이터의 각각의 필드 값을 확인하고 테이블을 생성한다. machsql 상에서 ‘CREATE TABLE’ 구문을 이용하여 생성하면 된다.\n\nCREATE TABLE SAMPLE_TABLE\n(\n    srcip        IPV4,\n    srcport      INTEGER,\n    dstip        IPV4,\n    dstport      INTEGER,\n    protocol     SHORT,\n    eventlog     VARCHAR(1204),\n    eventcode    SHORT,\n    eventsize    LONG\n);\n\n\n또는 테이블 생성 스크립트 파일을 만들어서 OS 커맨드 라인상에서 machsql 을 실행해도 된다.\n\n[machbase@localhost csvload]$ machsql -s localhost -u sys -p manager -f create_sample_table.sql\n=================================================================\n     Machbase Client Query Utility\n     Release Version x.x.x.official\n     Copyright 2014 MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMACHBASE_CONNECT_MODE=INET, PORT=5656\nType 'help' to display a list of available commands.\nMach&gt; CREATE TABLE SAMPLE_TABLE\n(\n    srcip        IPV4,\n    srcport      INTEGER,\n    dstip        IPV4,\n    dstport      INTEGER,\n    protocol     SHORT,\n    eventlog     VARCHAR(1204),\n    eventcode    SHORT,\n    eventsize    LONG\n);\nCreated successfully. \n\n\n로그 데이터 입력\n\n로그 데이터는 csv 포맷 파일이므로 csvimport 를 이용하여 로딩하면 된다.\n\n로그 파일의 첫번째 필드가 날짜인데, 이 값을 _arrival_time 컬럼에 입력하도록  옵션을 지정한다.\n\n[machbase@localhost csvload]$ csvimport -t sample_table -d sample_data.csv -a -F \"_arrival_time YYYY-MM-DD HH24:MI:SS\"\n-----------------------------------------------------------------\n     Machbase Data Import/Export Utility.\n     Release Version x.x.x.official\n     Copyright 2014, MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n-----------------------------------------------------------------\nNLS            : US7ASCII            EXECUTE MODE   : IMPORT\nTARGET TABLE   : sample_table        DATA FILE      : sample_data.csv\nIMPORT_MODE    : APPEND              FILED TERM     : ,\nROW TERM       :\n                   ENCLOSURE      : \"\nESCAPE         : \"                   ARRIVAL_TIME   : TRUE\nENCODING       : NONE                HEADER         : FALSE\nCREATE TABLE   : FALSE\n \n Progress bar                       Imported records        Error records\n                                             1000000                    0\n \nImport time         :  0 hour  0 min  5.728 sec\nLoad success count  : 1000000\nLoad fail count     : 0\n \n[machbase@localhost csvload]$\n\n\n로그 데이터 조회\n\n데이터 조회는 machsql 상에서 확인한다.\n\n[machbase@localhost csvload]$ machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version x.x.x.official\n     Copyright 2014 MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase server address (Default:127.0.0.1) :\nMachbase user ID  (Default:SYS)\nMachbase User Password :\nMACHBASE_CONNECT_MODE=INET, PORT=5656\nType 'help' to display a list of available commands.\nMach&gt; show tables;\nNAME                                                                              TYPE\n-----------------------------------------------------------------------------------------------\nSAMPLE_TABLE                                                                      LOG\n[1] row(s) selected.\n \nMach&gt; desc sample_table;\n[ COLUMN ]\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH\n----------------------------------------------------------------\nSRCIP                         ipv4                15\nSRCPORT                       integer             11\nDSTIP                         ipv4                15\nDSTPORT                       integer             11\nPROTOCOL                      short               6\nEVENTLOG                      varchar             1204\nEVENTCODE                     short               6\nEVENTSIZE                     long                20\n \nMach&gt; SELECT COUNT(*) FROM SAMPLE_TABLE;\nCOUNT(*)\n-----------------------\n1000000\n[1] row(s) selected.\n \nMach&gt; SELECT SRCIP, COUNT(*) FROM SAMPLE_TABLE GROUP BY SRCIP ORDER BY 2 DESC LIMIT 10;\nSRCIP           COUNT(*)\n----------------------------------------\n96.128.212.177  13594\n173.149.119.202 13546\n219.229.142.218 13537\n69.99.246.62    13511\n239.81.105.222  13501\n86.45.186.17    13487\n231.146.69.51   13483\n248.168.229.34  13472\n105.9.103.49    13472\n115.18.128.171  13468\n[10] row(s) selected.\nMach&gt;\n\n\n인덱스 생성 및 조회\n\n생성된 sample_table 컬럼 중에서 varchar 형인 eventlog 컬럼에 대해서 keyword 인덱스를 생성하고 텍스트 검색을 해본다.\n\n-- eventlog_index 인덱스를 생성한다.\nMach&gt; CREATE INDEX eventlog_index ON SAMPLE_TABLE( eventlog) INDEX_TYPE KEYWORD;\nCreated successfully.\nElapsed time: 0.442\n \n-- 생성된 인덱스를 확인한다.\nMach&gt; desc sample_table;\n[ COLUMN ]\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH\n----------------------------------------------------------------\nSRCIP                         ipv4                15\nSRCPORT                       integer             11\nDSTIP                         ipv4                15\nDSTPORT                       integer             11\nPROTOCOL                      short               6\nEVENTLOG                      varchar             1204\nEVENTCODE                     short               6\nEVENTSIZE                     long                20\n \n[ INDEX ]\n----------------------------------------------------------------\nNAME                          TYPE                COLUMN\n----------------------------------------------------------------\nEVENTLOG_INDEX                KEYWORD_LSM         EVENTLOG\n \n \n-- SEARCH 구문을 이용하여 'view' 가 들어간 데이터를 검색한다.\nMach&gt; SELECT EVENTLOG FROM SAMPLE_TABLE WHERE EVENTLOG SEARCH 'view' LIMIT 10;\nEVENTLOG\n------------------------------------------------------------------------------------\nGET /twiki/bin/view/TWiki/ManagingWebs?skin=print HTTP/1.1\nGET /twiki/bin/view/Main/TokyoOffice HTTP/1.1\nGET /twiki/bin/view/TWiki/ManagingWebs?rev=1.22 HTTP/1.1\nGET /twiki/bin/view/Main/DCCAndPostFix HTTP/1.1\nGET /twiki/bin/view/TWiki/WebTopicEditTemplate HTTP/1.1\nGET /twiki/bin/view/Main/TokyoOffice HTTP/1.1\nGET /twiki/bin/view/TWiki/WikiCulture HTTP/1.1\nGET /twiki/bin/view/Main/MikeMannix HTTP/1.1\nGET /twiki/bin/view/TWiki/WikiCulture HTTP/1.1\nGET /twiki/bin/view/TWiki/WikiCulture HTTP/1.1\n[10] row(s) selected.\n \n-- 'robots.txt'가 포함된 데이터 건수를 구한다.\nMach&gt; SELECT COUNT(*) FROM SAMPLE_TABLE WHERE EVENTLOG SEARCH 'robots.txt';\nCOUNT(*)\n-----------------------\n40283\n[1] row(s) selected.\n \n-- 'robots.txt'가 포함된 데이터를 SRCIP 별로 집계해서 상위 10개만 출력한다.\nMach&gt; SELECT SRCIP, COUNT(*) FROM SAMPLE_TABLE WHERE EVENTLOG SEARCH 'robots.txt' GROUP BY SRCIP ORDER BY 2 DESC LIMIT 10;\nSRCIP           COUNT(*)\n----------------------------------------\n81.227.25.139   616\n162.80.44.96    596\n7.234.88.67     595\n227.106.13.91   578\n220.192.100.45  570\n46.201.48.18    570\n231.146.69.51   564\n185.22.195.164  564\n64.58.31.79     561\n50.5.206.126    561\n[10] row(s) selected.\n\n\n시계열 데이터 조회\n\n마크베이스는 시계열 데이터를 조회하는데 편리한 구문을 제공하고 있다. DURATION을 이용하여 빠른 데이터를 조회하는 방법을 알아본다.\n\n-- _arrival_time 컬럼에 입력된 최대,최소값을 확인한다.\nMach&gt; SELECT MIN(_ARRIVAL_TIME), MAX(_ARRIVAL_TIME) FROM SAMPLE_TABLE;\nMIN(_ARRIVAL_TIME)              MAX(_ARRIVAL_TIME)\n-------------------------------------------------------------------\n2015-05-20 06:00:00 000:000:000 2015-05-20 06:40:10 000:000:000\n[1] row(s) selected.\n \n-- DATE_TRUNC() 함수를 이용하여 분당 건수를 구한다.\nMach&gt; SELECT DATE_TRUNC('minute', _ARRIVAL_TIME) as TIME, COUNT(*) as COUNT FROM SAMPLE_TABLE GROUP BY TIME ORDER BY TIME;\nTIME                            COUNT\n--------------------------------------------------------\n2015-05-20 06:00:00 000:000:000 32001\n2015-05-20 06:01:00 000:000:000 28000\n2015-05-20 06:02:00 000:000:000 24000\n2015-05-20 06:03:00 000:000:000 32000\n2015-05-20 06:04:00 000:000:000 16000\n2015-05-20 06:05:00 000:000:000 16000\n2015-05-20 06:06:00 000:000:000 32000\n2015-05-20 06:07:00 000:000:000 32000\n2015-05-20 06:08:00 000:000:000 20000\n2015-05-20 06:09:00 000:000:000 24000\n2015-05-20 06:10:00 000:000:000 20000\n2015-05-20 06:11:00 000:000:000 20000\n2015-05-20 06:12:00 000:000:000 24000\n2015-05-20 06:13:00 000:000:000 20000\n2015-05-20 06:14:00 000:000:000 32000\n2015-05-20 06:15:00 000:000:000 24000\n2015-05-20 06:16:00 000:000:000 32000\n2015-05-20 06:17:00 000:000:000 28000\n2015-05-20 06:18:00 000:000:000 32000\n2015-05-20 06:19:00 000:000:000 12000\n2015-05-20 06:20:00 000:000:000 24000\n2015-05-20 06:21:00 000:000:000 28000\n2015-05-20 06:22:00 000:000:000 28000\n2015-05-20 06:23:00 000:000:000 24000\n2015-05-20 06:24:00 000:000:000 28000\n2015-05-20 06:25:00 000:000:000 28000\n2015-05-20 06:26:00 000:000:000 32000\n2015-05-20 06:27:00 000:000:000 20000\n2015-05-20 06:28:00 000:000:000 20000\n2015-05-20 06:29:00 000:000:000 20000\n2015-05-20 06:30:00 000:000:000 28000\n2015-05-20 06:31:00 000:000:000 32000\n2015-05-20 06:32:00 000:000:000 32000\n2015-05-20 06:33:00 000:000:000 28000\n2015-05-20 06:34:00 000:000:000 20000\n2015-05-20 06:35:00 000:000:000 24000\n2015-05-20 06:36:00 000:000:000 24000\n2015-05-20 06:37:00 000:000:000 16000\n2015-05-20 06:38:00 000:000:000 24000\n2015-05-20 06:39:00 000:000:000 16000\n2015-05-20 06:40:00 000:000:000 3999\n[41] row(s) selected.\n \n-- DURATION 구문을 이용하여 특정시각 기준 1분 이전 시간 범위를 지정하여 조회한다.\nMach&gt; SELECT MIN(_ARRIVAL_TIME), MAX(_ARRIVAL_TIME), COUNT(*) as COUNT FROM SAMPLE_TABLE DURATION 1 MINUTE BEFORE TO_DATE('2015-05-20 06:30:00');\nMIN(_ARRIVAL_TIME)              MAX(_ARRIVAL_TIME)              COUNT\n-----------------------------------------------------------------------------------------\n2015-05-20 06:29:05 000:000:000 2015-05-20 06:29:45 000:000:000 20000\n[1] row(s) selected.\n \n-- DURATION 구문을 이용하여 특정시각 기준 1분 이후 시간 범위를 지정하여 조회한다.\nMach&gt; SELECT MIN(_ARRIVAL_TIME), MAX(_ARRIVAL_TIME), COUNT(*) as COUNT FROM SAMPLE_TABLE DURATION 1 MINUTE AFTER TO_DATE('2015-05-20 06:30:00');\nMIN(_ARRIVAL_TIME)              MAX(_ARRIVAL_TIME)              COUNT\n-----------------------------------------------------------------------------------------\n2015-05-20 06:30:04 000:000:000 2015-05-20 06:30:57 000:000:000 28000\n[1] row(s) selected.\n \n \n-- DURATION 구문을 이용하여 FROM ~ TO 시간 범위를 지정하여 조회한다.\nMach&gt; SELECT MIN(_ARRIVAL_TIME), MAX(_ARRIVAL_TIME), COUNT(*) as COUNT FROM SAMPLE_TABLE DURATION FROM TO_DATE('2015-05-20 06:20:00') TO TO_DATE('2015-05-20 06:30:00');\nMIN(_ARRIVAL_TIME)              MAX(_ARRIVAL_TIME)              COUNT\n-----------------------------------------------------------------------------------------\n2015-05-20 06:20:03 000:000:000 2015-05-20 06:29:45 000:000:000 252000\n[1] row(s) selected.\n\n\n인터넷 주소형 데이터 조회\n\n마크베이스는 인터넷 주소에 대해서 데이터 타입으로 제공하고 편리하게 검색할 수 있다.\n\n-- Netmask 형식으로 IP 대역을 설정하여 조회한다.\nMach&gt; SELECT COUNT(*) FROM SAMPLE_TABLE WHERE SRCIP CONTAINED '100.195.159.0/24';\nCOUNT(*)\n-----------------------\n13097\n[1] row(s) selected.\n \n \n-- '*' 를 이용하여 Equal(=) 검색도 가능하다.\nMach&gt; SELECT COUNT(*) FROM SAMPLE_TABLE WHERE SRCIP = '100.195.159.*';\nCOUNT(*)\n-----------------------\n13097\n[1] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-table-tag-ex-html": {
						"id": "feature-table-tag-ex-html",
						"title": "태그 테이블 활용 샘플 예제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/ex.html",
						"content": "목차\n\n\n  개요\n  데이터 변환 순서도\n  태그 테이블 생성 및 태그 메타 로딩\n  PLC 데이터 로딩을 위한 테이블 생성\n  PLC 데이터 로딩\n  태그 메타 이름 생성 규칙\n  태그 테이블 데이터 로딩\n\n\n개요\n\n태그 테이블은 일반적인 센서 데이터가 저장된 파일의 구조 형태를 로딩할 수 있다.\n\n가장 흔히 볼 수 있는 형태의 텍스트 저장 파일은 아무런 설명 없이, (콤마)나 나뉘어진 다수의 숫자형 값을 그냥 나열한 무작위의 파일 내용인 &lt;값,값,값&gt;&lt;값,값,값&gt; 형태가 대표적이고, 시간을 포함한 파일의 경우에는 &lt;시간,값,값,값&gt; &lt;시간, 값,값,값&gt;이 있다.\n\n이런 파일의 데이터는 PLC (programmable Logic Controller)라고 불리는 장비에서 1개 이상의 센서 값을 지속적으로 입력된 데이터를 오랜 기간동안 수집했을 경우에 만들어진다. 아래의 사진이 그 예이다.\n\n\n\n이제 이 파일을 어떻게 마크베이스의 태그 테이블로 한꺼번에 배치 형태로 로딩하겠다.\n\n데이터 변환 순서도\n\n\n\n위의 그림에서 볼 수 있듯이 원시 CSV 파일을 마크베이스의 로그 테이블로 한꺼번에 로딩을 한 이후, 이를 태그 테이블로 변환할 것이다.\n\n태그 테이블 생성 및 태그 메타 로딩\n\n아래와 같이 태그 테이블을 생성하고 tagmetaimport라는 도구를 이용해서 CSV 파일에 저장된 태그 이름(태그 메타)들을 한꺼번에 로딩한다.\n\n아래 기술된 Option이외에도 machloader에서 사용할 수 있는 옵션을 모두 사용 가능하다.\n\nMach&gt; create tag table tag (name varchar(32) primary key, time datetime basetime, value double\nsummarized);\nExecuted successfully.\nElapsed time: 3.032\n \n$ cat tag_meta.csv\nMTAG_V00\nMTAG_V01\nMTAG_C00\nMTAG_C01\nMTAG_C02\nMTAG_C03\nMTAG_C04\nMTAG_C05\nMTAG_C06\nMTAG_C07\nMTAG_C08\nMTAG_C09\nMTAG_C10\nMTAG_C11\nMTAG_C12\nMTAG_C13\nMTAG_C14\nMTAG_C15\n \n$ tagmetaimport -d tag_meta.csv\nImport time : 0 hour 0 min 0.340 sec\nLoad success count : 18\n\n\n(Machbase 포트번호를 기본값에서 변경했으면, tagmetaimport에 -P 옵션을 사용해서 변경된 포트번호를 사용해야한다.)\n위와 같이 성공적으로 태그 메타 정보(이름) 18개가 로딩되었다.\n\nPLC 데이터 로딩을 위한 테이블 생성\n\n아래의 쿼리를 수행해 로그 테이블을 생성한다.\n\ncreate table plc_tag_table(\n    tm datetime,\n    V0 DOUBLE ,\n    V1 DOUBLE ,\n    C0 DOUBLE ,\n    C1 DOUBLE ,\n    C2 DOUBLE ,\n    C3 DOUBLE ,\n    C4 DOUBLE ,\n    C5 DOUBLE,\n    C6 DOUBLE ,\n    C7 DOUBLE ,\n    C8 DOUBLE ,\n    C9 DOUBLE ,\n    C10 DOUBLE ,\n    C11 DOUBLE ,\n    C12 DOUBLE ,\n    C13 DOUBLE ,\n    C14 DOUBLE ,\n    C15 DOUBLE\n);\n\n\n\n  주의할 점은 이 테이블은 로그 테이블 타입이라는 것이다(파일명 때문에 헷갈리지 않도록 하자). 마크베이스에서는 별도의 테이블 지정자를 명시하지 않으면, 로그 테이블로 생성된다.\n\n\nPLC 데이터 로딩\n\n아래와 같이 machloader를 사용해 200만 건의 원시 PLC 데이터가 저장된 plc_tag.csv 파일을 위에서 생성한 로그 테이블 plc_tag_table에 PLC 입력 형태로 입력한다. plc_tag.csv 파일은 첫 컬럼은 시간이며, 이후 순서대로 V0, V1, …C15 까지 컬럼이 나뉘어져 있다. 데이터의 패턴은 1초에 0~99mili second까지 약 100개의 데이터가 입력되고, 100 mili second에서 999까지는 입력이 없다가, 다음 1초 동안 동일한 패턴으로 입력된다.\n\n$ machloader -t plc_tag_table -i -d plc_tag.csv -F \"tm YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn\"\n-----------------------------------------------------------------\nMachbase Data Import/Export Utility.\nRelease Version 5.5.0.official\nCopyright 2014, MACHBASE Corporation or its subsidiaries.\nAll Rights Reserved.\n-----------------------------------------------------------------\nNLS : US7ASCII EXECUTE MODE : IMPORT\nTARGET TABLE : plc_tag_table DATA FILE : 4_plc_tag.csv\nIMPORT MODE : APPEND FIELD TERM : ,\nROW TERM : \\n ENCLOSURE : \"\nESCAPE : \\ ARRIVAL_TIME : FALSE\nENCODING : NONE HEADER : FALSE\nCREATE TABLE : FALSE\nProgress bar Imported records Error records\n============================== 2000000 0\nImport time : 0 hour 0 min 26.544 sec\nLoad success count : 2000000\nLoad fail count : 0\n\n\n태그 메타 이름 생성 규칙\n\n이제 태그 테이블에 데이터를 넣어서 실제로 Tag Analyzer를 통해서 데이터를 확인할 수 있도록 한다. 이를 위해서 plc_tag_table 의 정보를 모두 태그 테이블에 넣어야 하는데, 이를 위해서 insert-select 구문을 이용해서 한꺼번에 넣는다. 그리고, 각 컬럼의 값이 모든 태그 테이블의 이름과 맵핑이 되어야 하기 때문에 다음과 같이 매타 태그의 이름 정보를 미리 결정하였다.\n\n\n  \n    \n      로그 테이블의 컬럼명\n      태그 테이블의 Name 컬럼에 입력되는 이름\n    \n  \n  \n    \n      V0\n      MTAG_V00\n    \n    \n      V1\n      MTAG_V01\n    \n    \n      C0\n      MTAG_C00\n    \n    \n      C1\n      MTAG_C01\n    \n    \n      …\n       \n    \n    \n      C15\n      MTAG_C15\n    \n  \n\n\n태그 테이블 데이터 로딩\n\n이제 마지막으로 실제 데이터를 태그 테이블로 로딩할 차례이다. 아래의 쿼리를 수행하면 하나씩 순차적으로 태그 테이블에 넣는다.\n\nMach&gt; insert into tag select 'MTAG_V00', tm, v0 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 4.898\nMach&gt; insert into tag select 'MTAG_V01', tm, v1 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 5.577\nMach&gt; insert into tag select 'MTAG_C00', tm, c0 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.327\nMach&gt; insert into tag select 'MTAG_C01', tm, c1 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.445\nMach&gt; insert into tag select 'MTAG_C02', tm, c2 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.898\nMach&gt; insert into tag select 'MTAG_C03', tm, c3 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.078\nMach&gt; insert into tag select 'MTAG_C04', tm, c4 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.799\nMach&gt; insert into tag select 'MTAG_C05', tm, c5 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.210\nMach&gt; insert into tag select 'MTAG_C06', tm, c6 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 9.232\nMach&gt; insert into tag select 'MTAG_C07', tm, c7 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.398\nMach&gt; insert into tag select 'MTAG_C08', tm, c8 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.432\nMach&gt; insert into tag select 'MTAG_C09', tm, c9 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.734\nMach&gt; insert into tag select 'MTAG_C10', tm, c10 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.692\nMach&gt; insert into tag select 'MTAG_C11', tm, c11 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 8.628\nMach&gt; insert into tag select 'MTAG_C12', tm, c12 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 8.229\nMach&gt; insert into tag select 'MTAG_C13', tm, c13 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 9.517\nMach&gt; insert into tag select 'MTAG_C14', tm, c14 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.231\nMach&gt; insert into tag select 'MTAG_C15', tm, c15 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.830\n\n\n총 3600 만건의 데이터가 로딩된 것을 확인할 수 있다."
					}
					
				
		
				
					,
					
					"faq-faq-html": {
						"id": "faq-faq-html",
						"title": "쿼리 에러를 Property 수정하여 해결하는 방법",
						"version": "all",
						"categories": "",
						"url": " /faq/faq.html",
						"content": "쿼리를 실행한 후 메모리 부족 에러가 발생하였을 때 Property를 수정하여 해결하는 방법을 설명한다.\n\n목차\n\n\n  쿼리 실행 시 메모리가 부족하여 에러가 발생함\n    \n      Standard Edition\n      Cluster Edition\n    \n  \n\n\n쿼리 실행 시 메모리가 부족하여 에러가 발생함\n아래와 같은 이유로 쿼리를 실행하는데 필요한 메모리를 제한하고 있다.\n\n\n  특정 쿼리가 메모리를 너무 많이 사용하는 경우, 동시에 실행 중인 다른 쿼리가 메모리 부족으로 실행이 안되는 경우가 발생할 수 있다.\n\n\n이를 방지하기 위해 하나의 쿼리가 사용가능한 메모리 최대 사이즈 Property 값을 증가시켜 에러를 해결할 수 있다.\n\nMAX_QPX_MEM Property로 하나의 SQL에서 사용할 수 있는 최대 사용 가능 메모리를 관리한다.\n\n실행 중 설정하는 방법및 메모리가 부족하여 발생하는 에러메세지및 TRC메세지도 SET MAX_QPX_MEM 페이지를 참고한다.\n\nSET 명령어로 Property 값을 설정하면 마크베이스 재시작 시 설정 값이 적용되지 않으므로 machbase.conf 파일도 아래와 같이 함께 수정하여야 한다.\n\nStandard Edition\n\nmachbase.conf의 MAX_QPX_MEM 를 보다 큰 값으로 수정한다.\n\nCluster Edition\n\nStandard edtion과 동일하다. 단, 모든 클러스터 노드의 machbase.conf 를 수정해야 한다."
					}
					
				
		
				
					,
					
					"intro-features-html": {
						"id": "intro-features-html",
						"title": "마크베이스 특징",
						"version": "all",
						"categories": "",
						"url": " /intro/features.html",
						"content": "목차\n\n  다양한 테이블 구조 지원\n  다양한 크기의 하드웨어 지원\n  Tag analyzer - 데이터 시각화 솔루션 지원\n  Lock-free 아키텍쳐 지원\n  초고속 데이터 저장\n  STREAM 기능 지원\n  실시간 인덱스 구성\n  실시간 데이터 압축\n  탁월한 질의 성능\n  시계열 데이터 특성 SQL 구문 지원\n  텍스트 검색 기능 지원\n  선택적 삭제 지원\n  자동화된 데이터 수집\n\n\n다양한 테이블 구조 지원\n\n마크베이스는 사용자의 용도에 따라 네가지 형태의 테이블을 각각 제공한다. (Tag, Log, Volatile, Lookup)\n\n이는 센서데이터를 저장하는 고객의 요구 사항이 매우 다양하고, 하나의 비지니스가 하나의 특정 데이터 패턴 만을 담고 있지 않기 때문이기도 하다.\n\n따라서, 그러한 비지니스 요구 사항을 잘 이해하고, 적절한 테이블을 선택하는 것이 중요하다.\n\n아래는 각 테이블의 특성을 구분하여 나타낸 것이다.\n\n\n  \n    \n      테이블 종류\n      Tag Table\n      Log Table\n      Volatile Table\n      Lookup Table\n    \n  \n  \n    \n      목적\n      &lt;센서명, 시간, 센서값&gt; 형태의 센서 시계열 데이터 처리에 최적화\n      PLC 형태 로그 시계열 데이터 처리에 최적화(텍스트 포함)\n      휘발성 메모리 데이터 실시간 처리\n      영구 저장 가능한 마스터 데이터 관리\n    \n    \n      설명\n      고속으로 센서 데이터를 저장하고, 고속으로 해당 데이터를 추출하거나, 실시간으로 통계 테이블을 생성할 경우 활용  주로, 실시간 센서 데이터를 저장\n      텍스트를 포함한 로그성 데이터를 저장하고,\n       \n       \n    \n    \n      이를 일반 DBMS 형태로 분석하고자 할 경우 활용주로, 히스토리성 사용자 데이터를 저장\n      Insert, Delete, Update, Select 가 메모리 기반의 성능으로 필요한 경우 사용 (초당 수만건)  시스템 종료시 모든 데이터가 사라지며, 주로 key-value 기반의 모니터링 용도로 활용함.\n      사용자의 변경 가능한 주요 마스터 데이터를 영구 저장할 목적으로 사용함.SELECT 성능은 고속이지만,\n       \n       \n    \n    \n      나머지 INSERT, UPDATE, DELETE는 디스크 기반의 성능을 제공함.\n       \n       \n       \n       \n    \n    \n      테이블 구조\n      &lt;센서명,시간,센서값&gt; 이 기본형이며, 추가로 컬럼을 지정할 수 있다.\n      임의의 스키마 가능\n      임의의 스키마 가능 (Primary Key 지정 가능)\n       \n    \n    \n      INSERT(입력) 성능\n      초당 수백만건\n      초당 수백만건\n      초당 수만건\n      초당 수백건\n    \n    \n      SELECT(질의)\n      센서명 + 시간 범위 한정\n      모든 질의 가능\n       \n       \n    \n    \n      DELETE(삭제)\n      임의 시점 이전 데이터 실시간 삭제 가능\n      임의 시점/구간 데이터 실시간 삭제 가능\n      Primary Key 기준 record delete 지원 (※ Primary Key 지정 필요)\n       \n    \n    \n      UPDATE(변경)\n      지원 불가 (※ 메타데이터 컬럼 에 한해서 변경 가능)\n      지원 불가\n      Primary Key 기준 Update 지원 (※ Primary Key 지정 필요)\n       \n    \n    \n      저장소 크기 한계\n      디스크 한계\n      디스크 한계\n      메모리 한계\n       \n    \n    \n      INDEX 구조\n      3단계의 partitioning 실시간 인덱스 (※ 기본 생성)\n      LSM 인덱스\n      Red/Black 메모리 인덱스\n       \n    \n    \n      STREAM 지원\n      타겟 대상으로만 가능 (저장 대상)\n      소스/타겟 대상 모두 가능 (읽기 및 저장대상)\n      불가능\n       \n    \n    \n      고려 사항\n      과거 데이터 삭제를 고려한 충분한 스토리지 확보 고려\n      Tag 입력을 위한 임시 저장소로 고려\n      메모리 한계 고려\n       \n    \n  \n\n\n다양한 크기의 하드웨어 지원\n\n마크베이스는 사용자의  환경에 따른 아래와 같은 다양한 제품 Edition을 제공한다.\n\nStandard Edition\n\n이 제품은 단일 서버에서 고속의 데이터 처리를 달성하고자 하는 경우 활용된다.\n\n주로 인텔 x86 CPU 기반의 윈도우나 리눅스 운영체제에서 동작하며, 타 DBMS가 제공하지 못하는 매우 빠른 센서 데이터 저장과 분석을 제공한다.\n\n대부분의 경우 수백대 이상의 Edge 장비로부터 실시간으로 입력되는 데이터를 저장하고, 이를 2차로 분석하기 위한 용도로 활용된다.\n\nCluster Edition\n\n이 제품은 거대 제조 공장을 위한 초거대규모의 센서 데이터를 저장하기 위한 목적으로 개발되었다.\n\n반도체 혹은 디스플레이, 발전, 철강 생산 공정에서 발생하는 초당 천만건 이상의 데이터를 저장하기 위해 다수의 물리적 서버가 클러스터 형태로 동작한다.\n\n데이터가 늘어나는 환경에서 처리 용량과 성능을 지속적으로 유지해야 하는 환경에서 활용된다.\n\nTag analyzer - 데이터 시각화 솔루션 지원\n\n마크베이스는 버젼 5부터 마크베이스에 저장된 수백억건의 센서 데이터에 대한 실시간 시각화를 제공한다.\n\n즉, 임의의 태그 아이디를 지정하며, 그 아이디가 입력된 기간동안의 트렌드 차트를 순식간에 웹 기반으로 확인할 수 있도록 한다.\n\n또한, 단순한 태그 데이터 뿐만 아니라 그 기간동안의 통계 챠트도 함께 볼 수 있도록 제공하기 때문에 단순 시각화를 넘어 일정 수준의 통계 분석도 가능하다.\n\n\n\nWrite Once, Read Many\n\n센서 데이터는 일단 데이터베이스에 입력되면 변경 또는 삭제되는 경우가 거의 없다.\n\n따라서, 마크베이스는 머신 데이터에 대한 특성을 최대한 살리기 위해 한번 입력된 주요 시계열 데이터에 대해서는 UPDATE가 발생할 수 없도록 설계 되었다.\n\n한번 입력된 로그 데이터는, 악의적 사용자에 의해 변조되거나 삭제되지 않으므로 걱정할 필요가 없다.\n\nLock-free 아키텍쳐 지원\n\n센서 데이터 처리하는데 가장 중요한 것은 데이터의 입력, 변경, 삭제 연산과 읽기 연산이 서로 충돌하지 않고가능한 독립적으로 처리되어야 한다는 것이다.\n\n이 때문에 마크베이스는 SELECT 연산에 대한 어떠한 Lock도 할당 받지 않도록 설계되었고, 변경 연산인 입력 혹은 삭제와도 서로 절대로 충돌하지 않는 고성능 구조로 설계되었다.\n\n따라서 수십만 건의 데이터가 입력되고, 실시간으로 일부가 삭제되는 상황에서도 SELECT 연산은 수백만 건의 레코드에 대한 통계 연산을 빠른 속도록 진행할 수 있다.\n\n초고속 데이터 저장\n\n마크베이스는 기존의 데이터베이스보다 수십배의 빠른 데이터 저장 성능을 제공한다. 특정 데이블에 인덱스가 다수 존재하는 상황에서도 최소 초당 300,000 건에서 최고 2,000,000 건까지 데이터를 받아들일 수 있다.\n\n이것이 가능한 이유는 마크베이스가 시계열 데이터를 최적화하는 구조로 설계되었기 때문이다.\n\nSTREAM 기능 지원\n\n마크베이스는 버젼 5 부터  Standard Edition에 대해서 실시간 데이터 필터링을 지원하기 위해 STREAM 기능을 제공한다.\n\n이 STREAM은 DBMS 내부에서 실시간으로 입력되는 데이터에 대해 고속으로 조건 평가를 수행하고, 그 결과를 임의의 테이블로 전송하는 역할을 수행한다.\n\n이 기능은 특정 센서의 값이 특정 범위를 넘었을 경우 경고를 발생시키거나 내부적으로 입력된 데이터에 대한 실시간 평가를 하는 경우 매우 유용하다.\n\n실시간 인덱스 구성\n\n마크베이스는 인덱스가 많으면 많을수록 데이터 입력 성능이 비례적으로 느려지는 전통적인 데이터베이스 구조를 혁신적으로 개선해, 초당 수십만건의 데이터가 입력되더라도 거의 실시간으로 인덱스를 구성할 수 있다.\n\n이 특징은 실제 데이터가 발생하는 순간의 즉시 검색할 수 있는 강력한 기능적인 토대를 제공해 주기 때문에 머신 데이터와 같은 시계열 데이터 분석에 있어서는 핵심적인 기술이다.\n\n실시간 데이터 압축\n\n머신 데이터와 같은 시계열 데이터의 특징은 끊임없이 데이터가 발생한다는 것이다. 이 사실은 필연적으로 해당 데이터베이스의 저장 공간이 언젠가는 부족해질 뿐만 아니라, 처리해야 할 데이터를 충분하게 보유하지 못한다는 의미이다.\n\n특히, 전통적인 데이터베이스는 고속 입력 시 데이터 뿐만 아니라 인덱스가 늘어남에 따라 차지하는 데이터 공간 역시 급격하게 증가한다. 이 때문에 머신 데이터의 저장과 분석에 매우 부적절한 구조이다.\n\n마크베이스는 쏟아져 들어오는 데이터에 대해 혁신적인 실시간 압축 기술 2 가지를 통해 성능 저하 없이 적게는 수십배에서 수백배까지 데이터를 압축하여 저장한다.\n\n논리적 실시간 데이터 압축 기술 지원\n첫 번째로 마크베이스는 논리적 실시간 데이터 압축 기술을 지원한다.\n\n이는 컬럼형 데이터베이스에서 유래한, 머신 데이터의 데이터 중복성을 이용한 것으로서 동일한 값을 갖는 데이터가 많으면 많을수록 데이터의 중복을 코드화하여, 데이터 저장공간을 혁신적으로 줄이는 기술이다. 이를 통해 데이터의 중복성이 높은 데이터에 대해 수백배까지 데이터를 압축할 수 있다.\n\n물리적 데이터 압축 기술 (특허 기술)\n두 번째는 마크베이스의 특허 기술인 물리적 데이터 압축기술이다.\n\n이는 디스크에 저장될 물리적인 데이터 블럭을 미리 일정한 크기의 파티션으로 나누어 압축하여 디스크에 별도로 내림으로써 저장될 물리적 데이터의 량을 줄이고, 더불어 시스템이 유발시키는 I/O 비용을 급격하게 낮추는 기술이다. 이를 통해 실제 논리적으로 압축된 데이터를 다시한번 더 압축하여 저장 공간의 효율성을 높이는데 일조한다.\n\n탁월한 질의 성능\n\n마크베이스의 혁신적인 기술적 우월성은 초당 수십만 건의 데이터를 입력하는 와중에도 이미 저장된 과거의 수백만 혹은 수천만 건의 데이터에 대한 검색 및 통계 분석 성능이 매우 빠르다는 것이다.\n\n삽입과 분석 모두에 탁월한 성능을 제공하는 마크베이스 만의 인덱싱 기술 때문에 가능한 것이며 실시간 비즈니스 의사 결정에 핵심적인 역할을 수행할 것이다.\n\n전통적인 데이터베이스와 달리, 마크베이스는 두 개 이상의 인덱스를 하나의 질의문에서 처리할 수 있기 때문에 병렬로 데이터를 처리할 경우 몇 배나 더 빠른 성능을 기대할 수 있다.\n\n아래는 다음과 같은 질의문에 대해 두 개 이상의 인덱스를 활용하는 경우를 나타낸 것이다.\n\nSELECT * FROM table1 WHERE c1 = 1 and c2 = 2;\n\n\n시계열 데이터 특성 SQL 구문 지원\n\n센서 데이터의 경우 최신 데이터가 예전의 데이터보다 몇 배 더 가치가 있으며 데이터의 접근 빈도도 최근 데이터가 예전 데이터보다 몇 배 더 많은 특징이 있다.\n\n이런 이유로 마크베이스는 두 종류의 테이블 즉, 태그 (Tag) Table과 로그 (Log) Table을 통해  시계열 데이터 특징을 지원한다.\n\n로그 테이블\n\n마크베이스에서 지원하는 로그 테이블 (Log Table) 은 다음 특징을 가지고 있다.\n\n첫째, 입력 시간을 자동으로 저장\n\n레코드가 데이터베이스에 저장되는 순간 나노 세컨드 단위의 timestamp를 _arrival_time이라는 필드로 저장한다.\n\n이 의미는 마크베이스가 저장하는 모든 레코드는 시간을 기준으로 검색하거나 조건을 줄 수 있다는 것이다.\n\n둘째, 최근 데이터 우선 조회\n\n데이터 검색시 최근 시간이 예전 시간 보다 먼저 출력된다. 즉, SELECT를 수행할 때 최근 데이터가 먼저 출력된다는 것이다.\n\n앞에서 언급한 _arrival_time 컬럼 기준으로 내림차순 정렬 (descending sort) 를 한 것과 같은 결과이다.\n\n셋째, DURATION 키워드\n\nDURATION 키워드를 제공해, 특정 시간 범위 데이터를 입력 시간 기준으로 빠르게 조회할 수 있는 기능을 탑재했다.\n\n머신 데이터 분석의 경우 특정 시간 범위를 지정하는 경우가 많기 때문에 SQL 레벨에서 이러한 특성을 제공한다.\n\n이를 통해 복잡한 시간 연산자를 where 절에 주지 않더라도 편리하게 데이터를 분석할 수 있다.\n\n-- 예1) 지금 부터 10분 전까지의 데이터 통계 조회\nSELECT SUM(traffic) FROM t1 DURATION 10 MINUTE;\n \n-- 예2) 지금 부터 1시간 전에 30분간의 데이터 통계 조회\nSELECT SUM(traffic) FROM t1 DURATION 30 MINUTE BEFORE 1 HOUR;\n\n\n태그 테이블\n\n마크베이스 5.0 부터 지원하는 태그 테이블 (Tag Table) 은 다음 특징을 가지고 있다.\n\n첫째, 고속 TAGID/시간 조건 검색 성능\n\n태그 테이블은 임의의 시간 및 임의의 ID 기반  검색 성능이 탁월하다.\n\n기존의 RDBMS로는 도달할 수 없는 초고속의 데이터 추출 성능을 자랑하며, 이는 수십억건의 센서 데이터가 저장된 상황에서도 동일한 속도를 보장한다.\n\n둘째, 고속 태그 데이터 입력\n\n태그 테이블은 고속의 데이터 입력을 지원한다.\n\n앞의 로그 테이블과 마찬가지로, 초당 수십만건의 센서데이터 입력에 있어서도 무리없이 데이터를 입력할 수 있다.\n\n셋째, 실시간 통계 기능\n\n태그 테이블은 실시간 통계 기능을 지원한다.\n\n마크베이스는 이 태그 테이블에 저장된 데이터의 경우 실시간으로 다섯 종류의 통계를 자동적으로 생성하고, 이를 실시간으로 접근할 수 있는 기능을 제공한다.\n\n텍스트 검색 기능 지원\n\n로그성 시계열 데이터를 저장하고 활용하는 사용자의 가장 중요한 실제 용도 중 하나는 특정 시점에 특정 event 가 발생했는지를 확인하는 것이다.\n\n특정 시점의 경우 시계열 데이터 처리로 가능하지만, 특정 event가 발생한 것은 대부분의 경우 특정 컬럼에 저장된 text field에서 특정 “단어”를 찾는 행위가 필요하다.\n\n그러나, 전통적 데이터베이스에서는 특정 필드의 단어를 검색하기 위해서는 B+ Tree를 통해 exact match 혹은 LIKE 절을 통해 최초 일부 캐릭터의 조건을 검사하게 되는데, 대부분의 경우 이는 매우 느린 응답 결과를 초래한다.\n\n그런 이유로 전통적인 데이터베이스에서 특정 단어에 대한 검색은 매우 취약하다.\n\n반면, 마크베이스에서는 로그 테이블 기반의 SEARCH라는 SQL 키워드를 제공함으로써 실시간 단어 검색이 가능하도록 하였다.\n\n이를 통해 장비로부터 발생된 임의의 에러 텍스트를 순식간에 검색할 수 있게 되었다.\n\n-- 예1) msg 필드에 Error 혹은 102를 포함하는 레코드를 출력\nSELECT id, ipv4 FROM devices WHERE msg SEARCH 'Error' or msg SEARCH '102';\n \n-- 예2) msg 필드에 Error 그리고 102를 포함하는 레코드를 출력\nSELECT id, ipv4 FROM devices WHERE msg SEARCH 'Error 102';\n\n\n선택적 삭제 지원\n\n센서 데이터의 경우에는 삽입 이후에 삭제 연산이 거의 발생하지 않는 것이 현실이다.\n\n그러나 embedded 장비의 경우에는 저장 공간의 제약이 분명히 존재하고, 사용자에 의해 주의 깊게 관리되지 않는 것이 그 특징이다.\n\n이 경우 혹시나 머신 데이터에 의해 Disk full이 발생하거나 장애가 발생하게 되면, 기업 입장에서 많은 손해를 감수해야 한다.\n\n마크베이스는 이런 환경에서 주어진 특정 조건에 레코드를 삭제할 수 있도록 기능을 제공한다.\n\n따라서 embedded 개발사는 CRON 혹은 주기적인 프로그램을 통해서 마크베이스가 일정 크기 이상의 데이터를 유지하지 않도록 손쉽게 관리할 수 있다.\n\n로그 테이블의 경우\n\n아래 모든 문법 지원\n\n-- 사용 예1) 가장 오래된 마지막 100건을 삭제하라.\nDELETE FROM devices OLDEST 100 ROWS;\n \n-- 사용 예2) 최근 1000건을 제외하고 모두 삭제하라.\nDELETE FROM devices EXCEPT 1000 ROWS;\n \n-- 사용 예3) 지금부터 하루치를 남기고 모두 삭제하라.\nDELETE FROM devices EXCEPT 1 DAY;\n \n-- 사용 예4) 2014년 6월 1일 이전의 데이터를 모두 삭제하라.\nDELETE FROM devices BEFORE TO_DATE('2014-06-01', 'YYYY-MM-DD');\n\n\n태그 테이블의 경우\n\n아래 한가지 문법 지원\n\n-- 2016년 6월 15일 이전의 데이터를 모두 삭제하라.\nDELETE FROM tag BEFORE TO_DATE('2016-06-15', 'YYYY-MM-DD');\n\n\n자동화된 데이터 수집\n\n마크베이스는 산재해 있는 머신 데이터 로그 파일로부터 데이터를 읽어 자동으로 전송해주는 기능인 “컬렉터 (Collector)” 기능을 제공한다.\n\n이를 통해 syslog 나 웹서버 로그 등의 이미 정형화된 데이터를 수집할 수 있을 뿐만 아니라 사용자가 임의로 정의한 로그 포맷의 경우에도 매우 쉽게 변환하여 자동으로 수집할 수 있는 기능을 제공한다."
					}
					
				
		
				
					,
					
					"feed-xml": {
						"id": "feed-xml",
						"title": "",
						"version": "all",
						"categories": "",
						"url": " /feed.xml",
						"content": "&lt;img src=&quot;/kr/assets/img/logo.png&quot; alt=&quot;Machbase manual&quot;&gt;\n    machbase-manual documents.\n\n    /kr/\n    \n    Wed, 19 Jul 2023 09:27:08 +0900\n    Wed, 19 Jul 2023 09:27:08 +0900\n    Jekyll v3.9.3"
					}
					
				
		
				
					,
					
					"sql-ref-func-html": {
						"id": "sql-ref-func-html",
						"title": "지원 함수",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/func.html",
						"content": "목차\n\n  [ABS]\n  [ADD_TIME]\n  [AVG]\n  [BITAND / BITOR]\n  [COUNT]\n  [DATE_TRUNC]\n  [DAYOFWEEK]\n  [DECODE]\n  [FIRST / LAST]\n  [FROM_UNIXTIME]\n  [FROM_TIMESTAMP]\n  [GROUP_CONCAT]\n  [INSTR]\n  [LEAST / GREATEST]\n  [LENGTH]\n  [LOWER]\n  [LPAD / RPAD]\n  [LTRIM / RTRIM]\n  [MAX]\n  [MIN]\n  [NVL]\n  [ROUND]\n  [ROWNUM]\n  [SERIESNUM]\n  [STDDEV / STDDEV_POP]\n  [SUBSTR]\n  [SUBSTRING_INDEX]\n  [SUM]\n  [SUMSQ]\n  [SYSDATE / NOW]\n  [TO_CHAR]\n  [TO_DATE]\n  [TO_DATE_SAFE]\n  [TO_HEX]\n  [TO_IPV4 / TO_IPV4_SAFE]\n  [TO_IPV6 / TO_IPV6_SAFE]\n  [TO_NUMBER / TO_NUMBER_SAFE]\n  [TO_TIMESTAMP]\n  [TRUNC]\n  [TS_CHANGE_COUNT]\n  [UNIX_TIMESTAMP]\n  [UPPER]\n  [VARIANCE / VAR_POP]\n  [YEAR / MONTH / DAY]\n  [ISNAN / ISINF]\n  [내장 함수 지원 타입]\n  [JSON 관련 함수]\n  [JSON Operator]\n\n\nABS\n\n이 함수는 숫자형 컬럼에 대해 동작하고, 양의 값으로 변환해서 실수형으로 값을 리턴한다.\n\nABS(column_expr)\n\nMach&gt; CREATE TABLE abs_table (c1 INTEGER, c2 DOUBLE, c3 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO abs_table VALUES(1, 1.0, '');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO abs_table VALUES(2, 2.0, 'sqltest');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO abs_table VALUES(3, 3.0, 'sqltest');\n1 row(s) inserted.\n \nMach&gt; SELECT ABS(c1), ABS(c2) FROM abs_table;\nSELECT ABS(c1), ABS(c2) from abs_table;\nABS(c1)                     ABS(c2)\n-----------------------------------------------------------\n3                           3\n2                           2\n1                           1\n[3] row(s) selected.\n\n\nADD_TIME\n\n이 함수는 주어진 datetime 컬럼에 대해 날짜 가감연산을 수행한다.\n\n연, 월, 일, 시간, 분, 초 까지의 가감 연산을 지원하며, 밀리, 마이크로, 나노초에 대한 연산은 지원하지 않는다. Diff format은 다음과 같다.\n\n“Year/Month/Day Hour:Minute:Second” 각 항목은 양수 혹은 음수 값을 가진다.\n\nADD_TIME(column,time_diff_format)\n\nMach&gt; CREATE TABLE add_time_table (id INTEGER, dt DATETIME);\nCreated successfully.\n \nMach&gt; INSERT INTO  add_time_table VALUES(1, TO_DATE('1999-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(2, TO_DATE('2000-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(3, TO_DATE('2012-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(4, TO_DATE('2013-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(5, TO_DATE('2014-12-30 11:22:33 444:555:666'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(6, TO_DATE('2014-12-30 23:22:33 444:555:666'));\n1 row(s) inserted.\n \nMach&gt; SELECT ADD_TIME(dt, '1/0/0 0:0:0') FROM add_time_table;\nADD_TIME(dt, '1/0/0 0:0:0')\n----------------------------------\n2015-12-30 23:22:33 444:555:666\n2015-12-30 11:22:33 444:555:666\n2014-11-11 01:02:03 004:005:006\n2013-11-11 01:02:03 004:005:006\n2001-11-11 01:02:03 004:005:006\n2000-11-11 01:02:03 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '0/0/0 1:1:1') FROM add_time_table;\nADD_TIME(dt, '0/0/0 1:1:1')\n----------------------------------\n2014-12-31 00:23:34 444:555:666\n2014-12-30 12:23:34 444:555:666\n2013-11-11 02:03:04 004:005:006\n2012-11-11 02:03:04 004:005:006\n2000-11-11 02:03:04 004:005:006\n1999-11-11 02:03:04 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '1/1/1 0:0:0') FROM add_time_table;\nADD_TIME(dt, '1/1/1 0:0:0')\n----------------------------------\n2016-01-31 23:22:33 444:555:666\n2016-01-31 11:22:33 444:555:666\n2014-12-12 01:02:03 004:005:006\n2013-12-12 01:02:03 004:005:006\n2001-12-12 01:02:03 004:005:006\n2000-12-12 01:02:03 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '-1/0/0 0:0:0') FROM add_time_table;\nADD_TIME(dt, '-1/0/0 0:0:0')\n----------------------------------\n2013-12-30 23:22:33 444:555:666\n2013-12-30 11:22:33 444:555:666\n2012-11-11 01:02:03 004:005:006\n2011-11-11 01:02:03 004:005:006\n1999-11-11 01:02:03 004:005:006\n1998-11-11 01:02:03 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '0/0/0 -1:-1:-1') FROM add_time_table;\nADD_TIME(dt, '0/0/0 -1:-1:-1')\n----------------------------------\n2014-12-30 22:21:32 444:555:666\n2014-12-30 10:21:32 444:555:666\n2013-11-11 00:01:02 004:005:006\n2012-11-11 00:01:02 004:005:006\n2000-11-11 00:01:02 004:005:006\n1999-11-11 00:01:02 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '-1/-1/-1 0:0:0') FROM add_time_table;\nADD_TIME(dt, '-1/-1/-1 0:0:0')\n----------------------------------\n2013-11-29 23:22:33 444:555:666\n2013-11-29 11:22:33 444:555:666\n2012-10-10 01:02:03 004:005:006\n2011-10-10 01:02:03 004:005:006\n1999-10-10 01:02:03 004:005:006\n1998-10-10 01:02:03 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT * FROM add_time_table WHERE dt &gt; ADD_TIME(TO_DATE('2014-12-30 11:22:33 444:555:666'), '-1/-1/-1 0:0:0');\nID          DT\n-----------------------------------------------\n6           2014-12-30 23:22:33 444:555:666\n5           2014-12-30 11:22:33 444:555:666\n[2] row(s) selected.\n \nMach&gt; SELECT * FROM add_time_table WHERE dt &gt; ADD_TIME(TO_DATE('2014-12-30 11:22:33 444:555:666'), '-1/-2/-1 0:0:0');\nID          DT\n-----------------------------------------------\n6           2014-12-30 23:22:33 444:555:666\n5           2014-12-30 11:22:33 444:555:666\n4           2013-11-11 01:02:03 004:005:006\n[3] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(TO_DATE('2000-12-01 00:00:00 000:000:001'), '-1/0/0 0:0:-1') FROM add_time_table;\nADD_TIME(TO_DATE('2000-12-01 00:00:00 000:000:001'), '-1/0/0 0:0:-1')\n------------------------------------------\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n[6] row(s) selected.\n \nMach&gt; SELECT * FROM add_time_table WHERE dt &gt; ADD_TIME(TO_DATE('2014-12-30 11:22:33 444:555:666'), '-1/-2/-1 0:0:0');\nID          DT\n-----------------------------------------------\n6           2014-12-30 23:22:33 444:555:666\n5           2014-12-30 11:22:33 444:555:666\n4           2013-11-11 01:02:03 004:005:006\n[3] row(s) selected.\n\n\nAVG\n\n이 함수는 집계 함수로써, 숫자형 컬럼에 대해 동작하고 해당 컬럼의 평균 값을 출력한다.\n\nAVG(column_name)\n\nMach&gt; CREATE TABLE avg_table (id1 INTEGER, id2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO avg_table VALUES(1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(1, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(2, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(2, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(null, 4);\n1 row(s) inserted.\n \nMach&gt; SELECT id1, AVG(id2) FROM avg_table GROUP BY id1;\nid1         AVG(id2)\n-------------------------------------------\n2                2\nNULL             4\n1                2\n\n\nBITAND / BITOR\n\n이 함수는 두 입력 값을 64-bit 의 부호 있는 정수로 변환한 뒤, 비트별 and/or 을 수행한 결과를 반환한다. 입력 값은 반드시 정수형이어야 하며, 출력 값은 64비트 부호 있는 정수가 된다.\n\n0보다 작은 정수값에 대해서는 플랫폼에 따라 다른 결과를 얻을 수 있으므로 uinteger, ushort 타입만 사용하기를 권장한다.\n\nBITAND (&lt;expression1&gt;, &lt;expression2&gt;)\nBITOR (&lt;expression1&gt;, &lt;expression2&gt;)\n\nMach&gt; CREATE TABLE bit_table (i1 INTEGER, i2 UINTEGER, i3 FLOAT, i4 DOUBLE, i5 SHORT, i6 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO bit_table VALUES (-1, 1, 1, 1, 2, 'aaa');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO bit_table VALUES (-2, 2, 2, 2, 3, 'bbb');\n1 row(s) inserted.\n \nMach&gt; SELECT BITAND(i1, i2) FROM bit_table;\nBITAND(i1, i2)\n-----------------------\n2\n1\n[2] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITAND(i2, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n-1          1           1                           1                           2           aaa\n[1] row(s) selected.\n \nMach&gt; SELECT BITOR(i5, 1) FROM bit_table WHERE BITOR(i5, 1) = 3;\nBITOR(i5, 1)\n-----------------------\n3\n3\n[2] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITOR(i2, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n-1          1           1                           1                           2           aaa\n[1] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITAND(i3, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n[ERR-02037 : Function [BITAND] argument data type is mismatched.]\n[0] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITAND(i4, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n[ERR-02037 : Function [BITAND] argument data type is mismatched.]\n[0] row(s) selected.\n \nMach&gt; SELECT BITAND(i5, 1) FROM bit_table WHERE BITAND(i5, 1) = 1;\nBITAND(i5, 1)\n-----------------------\n1\n[1] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITOR(i6, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n[ERR-02037 : Function [BITOR] argument data type is mismatched.]\n[0] row(s) selected.\n \nMach&gt; SELECT BITOR(i1, i2) FROM bit_table;\nBITOR(i1, i2)\n-----------------------\n-2\n-1\n[2] row(s) selected.\n \nMach&gt; SELECT BITAND(i1, i3) FROM bit_table;\nBITAND(i1, i3)\n-----------------------\n[ERR-02037 : Function [BITAND] argument data type is mismatched.]\n[0] row(s) selected.\n \nMach&gt; SELECT BITOR(i1, i6) FROM bit_table;\nBITOR(i1, i6)\n-----------------------\n[ERR-02037 : Function [BITOR] argument data type is mismatched.]\n[0] row(s) selected.\n\n\nCOUNT\n\n이 함수는 집계 함수로써, 해당 컬럼의 레코드 개수를 구하는 함수이다.\n\nCOUNT(column_name)\n\nMach&gt; CREATE TABLE count_table (id1 INTEGER, id2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO count_table VALUES(1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(1, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(2, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(2, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(null, 4);\n1 row(s) inserted.\n \nMach&gt; SELECT COUNT(*) FROM count_table;\nCOUNT(*)\n-----------------------\n7\n[1] row(s) selected.\n \nMach&gt; SELECT COUNT(id1) FROM count_table;\nCOUNT(id1)\n-----------------------\n6\n[1] row(s) selected.\n\n\nDATE_TRUNC\n\n이 함수는 주어진 datetime 값을 ‘시간 단위’와 ‘시간 범위’까지만 표시된 새로운 datetime 값으로 반환한다.\n\nDATE_TRUNC (field, date_val [, count])\n\nMach&gt; CREATE TABLE trunc_table (i1 INTEGER, i2 DATETIME);\nCreated successfully.\n \nMach&gt; INSERT INTO trunc_table VALUES (1, TO_DATE('1999-11-11 1:2:0 4:5:1'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (2, TO_DATE('1999-11-11 1:2:0 5:5:2'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (3, TO_DATE('1999-11-11 1:2:1 6:5:3'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (4, TO_DATE('1999-11-11 1:2:1 7:5:4'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (5, TO_DATE('1999-11-11 1:2:2 8:5:5'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (6, TO_DATE('1999-11-11 1:2:2 9:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (7, TO_DATE('1999-11-11 1:2:3 10:5:7'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (8, TO_DATE('1999-11-11 1:2:3 11:5:8'));\n1 row(s) inserted.\n \nMach&gt; SELECT COUNT(*), DATE_TRUNC('second', i2) tm FROM trunc_table group by tm ORDER BY 2;\nCOUNT(*)             tm\n--------------------------------------------------------\n2                    1999-11-11 01:02:00 000:000:000\n2                    1999-11-11 01:02:01 000:000:000\n2                    1999-11-11 01:02:02 000:000:000\n2                    1999-11-11 01:02:03 000:000:000\n[4] row(s) selected.\n \nMach&gt; SELECT COUNT(*), DATE_TRUNC('second', i2, 2) tm FROM trunc_table group by tm ORDER BY 2;\nCOUNT(*)             tm\n--------------------------------------------------------\n4                    1999-11-11 01:02:00 000:000:000\n4                    1999-11-11 01:02:02 000:000:000\n[2] row(s) selected.\n \nMach&gt; SELECT COUNT(*), DATE_TRUNC('nanosecond', i2, 2) tm FROM trunc_table group by tm ORDER BY 2;\nCOUNT(*)             tm\n--------------------------------------------------------\n1                    1999-11-11 01:02:00 004:005:000\n1                    1999-11-11 01:02:00 005:005:002\n1                    1999-11-11 01:02:01 006:005:002\n1                    1999-11-11 01:02:01 007:005:004\n1                    1999-11-11 01:02:02 008:005:004\n1                    1999-11-11 01:02:02 009:005:006\n1                    1999-11-11 01:02:03 010:005:006\n1                    1999-11-11 01:02:03 011:005:008\n[8] row(s) selected.\n \nMach&gt; SELECT COUNT(*), DATE_TRUNC('nsec', i2, 1000000000) tm FROM trunc_table group by tm ORDER BY 2; //DATE_TRUNC('sec', i2, 1) 과같음\nCOUNT(*)             tm\n--------------------------------------------------------\n2                    1999-11-11 01:02:00 000:000:000\n2                    1999-11-11 01:02:01 000:000:000\n2                    1999-11-11 01:02:02 000:000:000\n2                    1999-11-11 01:02:03 000:000:000\n[4] row(s) selected.\n\n\n시간 단위와, 시간 단위별 허용되는 시간 범위는 다음과 같다.\n\n\n  nanosecond, microsecond, milisecond 단위와 축약어는, 5.5.6 부터 사용 가능하다.\n\n\n\n  \n    \n      시간 단위 (축약어)\n      시간 범위\n    \n  \n  \n    \n      nanosecond (nsec)\n      1000000000 (1초)\n    \n    \n      microsecond (usec)\n      60000000 (60초)\n    \n    \n      milisecond (msec)\n      60000 (60초)\n    \n    \n      second (sec)\n      86400 (1일)\n    \n    \n      minute (min)\n      1440 (1일)\n    \n    \n      hour\n      24 (1일)\n    \n    \n      day\n      1\n    \n    \n      month\n      1\n    \n    \n      year\n      1\n    \n  \n\n\n예를 들어, DATE_TRUNC(‘second’, time, 120) 으로 입력하면, 반환되는 값은 2분 간격으로 표시될 것이며 이는 DATE_TRUNC(‘minute’, time, 2) 와 동일하다.\n\nDAYOFWEEK\n\n이 함수는 주어진 datetime 값의 요일을 나타내는 자연수를 반환한다.\n\nTO_CHAR(time, ‘DAY’) 와 의미상 비슷한 값을 반환하지만, 여기서는 정수를 반환한다.\n\nDAYOFWEEK(date_val)\n\n\n반환되는 자연수는 다음의 요일을 나타낸다.\n\n\n  \n    \n      반환값\n      요일\n    \n  \n  \n    \n      0\n      일요일\n    \n    \n      1\n      월요일\n    \n    \n      2\n      화요일\n    \n    \n      3\n      수요일\n    \n    \n      4\n      목요일\n    \n    \n      5\n      금요일\n    \n    \n      6\n      토요일\n    \n  \n\n\nDECODE\n\n이 함수는 주어진 Column 값을 Search와 같은지 비교하고, 같으면 바로 다음의 Return 값을 되돌린다. 만일 만족하는 Search 값이 없을 경우에는 Default 값을 리턴한다. Default가 생략되었을 경우에는 NULL이 리턴된다.\n\nDECODE(column, [search, return],.. default)\n\nMach&gt; CREATE TABLE decode_table (id1 VARCHAR(11));\nCreated successfully.\n \nMach&gt; INSERT INTO decode_table VALUES('decodetest1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO decode_table VALUES('decodetest2');\n1 row(s) inserted.\n \nMach&gt; SELECT id1, DECODE(id1, 'decodetest1', 'result1', 'decodetest2', 'result2', 'DEFAULT') FROM decode_table;\nid1          DECODE(id1, 'decodetest1', 'result1', 'decodetest2', 'result2', 'DEFAULT')\n---------------------------------------------------------\ndecodetest2  result2\ndecodetest1  result1\n[2] row(s) selected.\n \nMach&gt; SELECT id1, DECODE(id1, 'codetest', 2, 99) FROM decode_table;\nid1          DECODE(id1, 'codetest', 2, 99)\n-----------------------------------------------\ndecodetest2  99\ndecodetest1  99\n[2] row(s) selected.\n \nMach&gt; SELECT DECODE(id1, 'decodetest1', 2) FROM decode_table;\nDECODE(id1, 'decodetest1', 2)\n--------------------------------\nNULL\n2\n[2] row(s) selected.\n \nMach&gt; SELECT DECODE(id1, 'codetest', 2) FROM decode_table;\nDECODE(id1, 'codetest', 2)\n-----------------------------\nNULL\nNULL\n[2] row(s) selected.\n\n\nFIRST / LAST\n\n이 함수는 집계 함수로써, 각 Group 에서 ‘기준 값’이 순서상 가장 앞선 (또는 가장 나중의) 레코드의 특정 값을 반환한다.\n\n  FIRST : 순서상 가장 앞선 레코드에서 특정 값을 반환\n  LAST : 순서상 가장 나중의 레코드에서 특정 값을 반환\n\n\nFIRST(sort_expr, return_expr)\nLAST(sort_expr, return_expr)\n\nMach&gt; create table firstlast_table (id integer, name varchar(20), group_no integer);\nCreated successfully.\nMach&gt; insert into firstlast_table values (1, 'John', 0);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (2, 'Grey', 1);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (5, 'Ryan', 0);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (4, 'Andrew', 0);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (7, 'Kyle', 1);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (6, 'Ross', 1);\n1 row(s) inserted.\n \nMach&gt; select group_no, first(id, name) from firstlast_table group by group_no;\ngroup_no    first(id, name)\n-------------------------------------\n1           Grey\n0           John\n[2] row(s) selected.\n \n \nMach&gt; select group_no, last(id, name) from firstlast_table group by group_no;\ngroup_no    last(id, name)\n-------------------------------------\n1           Kyle\n0           Ryan\n\n\nFROM_UNIXTIME\n\n이 함수는 정수형으로 입력된 32비트 UNIXTIME 값을 datetime 자료형의 값으로 변환한다. (UNIX_TIMESTAMP 는 datetime 자료형을 32비트 UNIXTIME 정수형 데이터로 변환한다.)\n\nFROM_UNIXTIME(unix_timestamp_value)\n\nMach&gt; SELECT FROM_UNIXTIME(315540671) FROM TEST;\nFROM_UNIXTIME(315540671)\n----------------------------------\n1980-01-01 11:11:11 000:000:000\n \nMach&gt; SELECT FROM_UNIXTIME(UNIX_TIMESTAMP('2001-01-01')) FROM unix_table;\nFROM_UNIXTIME(UNIX_TIMESTAMP('2001-01-01'))\n------------------------------------------\n2001-01-01 00:00:00 000:000:000\n\n\nFROM_TIMESTAMP\n\n이 함수는 1970-01-01 09:00 부터 경과된 nanosecond 값을 입력받아 datetime 자료형의 값으로 변환한다. (TO_TIMESTAMP() 는 datetime 자료형을 1970-01-01 09:00 부터 경과된 nanosecond 데이터로 변환한다.)\n\nFROM_TIMESTAMP(nanosecond_time_value)\n\nMach&gt; SELECT FROM_TIMESTAMP(1562302560007248869) FROM TEST;\nFROM_TIMESTAMP(1562302560007248869)\n--------------------------------------\n2019-07-05 13:56:00 007:248:869\n\n\nsysdate, now 는 모두 현재 시각의 1970-01-01 09:00 부터 경과된 nanosecond 값을 나타내므로, 곧바로 FROM_TIMESTAMP() 를 사용해도 된다.\n\n물론, 사용하지 않아도 결과는 같다. sysdate 와 now 에 nanosecond 단위로 연산을 한 경우에 유용하게 사용할 수 있다.\n\nMach&gt; select sysdate, from_timestamp(sysdate) from test_tbl;\nsysdate                         from_timestamp(sysdate)\n-------------------------------------------------------------------\n2019-07-05 14:00:59 722:822:443 2019-07-05 14:00:59 722:822:443\n[1] row(s) selected.\n \nMach&gt; select sysdate, from_timestamp(sysdate-1000000) from test_tbl;\nsysdate                         from_timestamp(sysdate-1000000)\n-------------------------------------------------------------------\n2019-07-05 14:01:05 130:939:525 2019-07-05 14:01:05 129:939:525      -- 1 ms (1,000,000 ns) 차이가 발생함\n[1] row(s) selected.\n\n\nGROUP_CONCAT\n\n이 함수는 집계 함수로써, 그룹 안에 존재하는 해당 컬럼의 값을 문자열로 이어 붙여서 출력한다.\n\n\n  Cluster Edition 에서는 사용할 수 없는 함수이다.\n\n\nGROUP_CONCAT(\n     [DISTINCT] column\n     [ORDER BY { unsigned_integer | column }\n     [ASC | DESC] [, column ...]]\n     [SEPARATOR str_val]\n)\n\n\n\n  DISTINCT: 이어 붙일 컬럼의 값이 중복되는 경우, 중복된 값은 이어 붙이지 않는다.\n  ORDER BY: 지정된 컬럼 값들에 따라, 이어 붙이는 컬럼 값의 순서를 정렬한다.\n  SEPARATOR: 컬럼 값을 이어 붙일 때 사용하는 구분자 문자열로, 기본 값은 콤마(,)이다.\n\n\n문법에 대한 주의사항은 아래와 같다.\n\n\n  \n    \n      \n        \n          이어 붙일 컬럼은 1개만 지정할 수 있으며, 2개 이상 지정하고자 하는 경우에는 TO_CHAR() 함수와 CONCAT 연산자 (\n           \n          )를 활용해서 1개의 표현식으로 만들어서 입력해야 한다.\n        \n      \n    \n  \n  ORDER BY 에는 이어 붙이는 컬럼 외에 다른 컬럼을 지정할 수 있으며, 여러 컬럼을 지정할 수 있다.\n  SEPARATOR 에는 반드시 문자열 상수를 입력해야 하며, 문자열 컬럼은 입력할 수 없다.\n\n\nMach&gt; CREATE TABLE concat_table(id1 INTEGER, id2 DOUBLE, name VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO concat_table VALUES (1, 2, 'John');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (2, 1, 'Ram');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (3, 2, 'Zara');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (4, 2, 'Jill');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (5, 1, 'Jack');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (6, 1, 'Jack');\n1 row(s) inserted.\n \n \nMach&gt; SELECT GROUP_CONCAT(name) AS G_NAMES FROM concat_table GROUP BY id2;\nG_NAMES                                                                                                          \n------------------------------------------------------------------------------------\nJack,Jack,Ram                                                                                                    \nJill,Zara,John                                                                                                   \n[2] row(s) selected.\n \nMach&gt; SELECT GROUP_CONCAT(DISTINCT name) AS G_NAMES FROM concat_table GROUP BY Id2;\nG_NAMES                                                                                                          \n------------------------------------------------------------------------------------\nJack,Ram                                                                                                         \nJill,Zara,John                                                                                                   \n[2] row(s) selected.\n \nMach&gt; SELECT GROUP_CONCAT(name SEPARATOR '.') G_NAMES FROM concat_table GROUP BY Id2;\nG_NAMES                                                                                                          \n------------------------------------------------------------------------------------\nJack.Jack.Ram                                                                                                    \nJill.Zara.John                                                                                                   \n[2] row(s) selected.\n \nMach&gt; SELECT GROUP_CONCAT(name ORDER BY id1) G_NAMES, GROUP_CONCAT(id1 ORDER BY id1) G_SORTID FROM concat_table GROUP BY id2;\nG_NAMES                                                                                                          \n------------------------------------------------------------------------------------\nG_SORTID                                                                                                         \n------------------------------------------------------------------------------------\nRam,Jack,Jack                                                                                                    \n2,5,6                                                                                                            \nJohn,Zara,Jill                                                                                                   \n1,3,4                                                                                                            \n[2] row(s) selected.\n\n\nINSTR\n\n이 함수는 입력된 문자열 패턴이, 함께 입력된 문자열에서 몇 번째 문자에 있는지 그 인덱스를 반환한다. 인덱스 시작은 1 이다.\n\n\n  문자열 패턴을 찾을 수 없는 경우, 0 이 반환된다.\n  찾고자 하는 문자열 패턴의 길이가 0이거나 NULL 인 경우, NULL 이 반환된다.\n\n\nINSTR(target_string, pattern_string)\n\nMach&gt; CREATE TABLE string_table(c1 VARCHAR(20));\nCreated successfully.\n \nMach&gt; INSERT INTO string_table VALUES ('abstract');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO string_table VALUES ('override');\n1 row(s) inserted.\n \nMach&gt; SELECT c1, INSTR(c1, 'act') FROM string_table;\nc1                    INSTR(c1, 'act')\n------------------------------------------\noverride              0\nabstract              6\n[2] row(s) selected.\n\n\nLEAST / GREATEST\n\n두 함수는 입력 매개변수로 여러 개의 컬럼 또는 값들을 지정하면 그중 가장 작은 값(LEAST) 또는 가장 큰 값(GREATEST)을 반환한다.\n\n만약 입력값이 1개 또는 없는 경우에는 오류로 처리되며 입력값들 중에 NULL이 있는 경우에는 NULL을 반환하므로 입력값이 컬럼인 경우 함수등을 이용하여 미리 변환하여야 한다.\n\n입력값의 비교가 불가능한 컬럼(BLOB, TEXT) 등이 포함되어 있거나 대소비교를 위한 형 변환이 불가능한 경우 오류로 처리된다.\n\nLEAST(value_list, value_list,...)\nGREATEST(value_list, value_list,...)\n\nMach&gt; CREATE TABLE lgtest_table(c1 INTEGER, c2 LONG, c3 VARCHAR(10), c4 VARCHAR(5));\nCreated successfully.\n \nMach&gt; INSERT INTO lgtest_table VALUES (1, 2, 'abstract', 'ace');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lgtest_table VALUES (null, 100, null, 'bag');\n1 row(s) inserted.\n \nMach&gt; SELECT LEAST (c1, c2) FROM lgtest_table;\nLEAST (c1, c2)\n-----------------------\nNULL\n1\n[2] row(s) selected.\n \nMach&gt; SELECT LEAST (c1, c2, -1) FROM lgtest_table;\nLEAST (c1, c2, -1)\n-----------------------\nNULL\n-1\n[2] row(s) selected.\n \nMach&gt; SELECT GREATEST(c3, c4) FROM lgtest_table;\nGREATEST(c3, c4)\n--------------------\nNULL\nace\n[2] row(s) selected.\n \nMach&gt; SELECT LEAST(c3, c4) FROM lgtest_table;\nLEAST(c3, c4)\n-----------------\nNULL\nabstract\n[2] row(s) selected.\n \nMach&gt; SELECT LEAST(NVL(c3, 'aa'), c4) FROM lgtest_table;\nLEAST(NVL(c3, 'aa'), c4)\n----------------------------\naa\nabstract\n[2] row(s) selected.\n\n\nLENGTH\n\n이 함수는 문자열 컬럼의 길이를 구한다. 구해진 값은 영문 기준으로 바이트 수를 출력한다.\n\nLENGTH(column_name)\n\nMach&gt; CREATE TABLE length_table (id1 INTEGER, id2 DOUBLE, name VARCHAR(15));\nCreated successfully.\n \nMach&gt; INSERT INTO length_table VALUES(1, 10, 'Around the Horn');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO length_table VALUES(NULL, 20, 'Alfreds Futterkiste');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO length_table VALUES(3, NULL, 'Antonio Moreno');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO length_table VALUES(4, 40, NULL);\n1 row(s) inserted.\n \nMach&gt; select * FROM length_table;\nID1         ID2                         NAME\n-------------------------------------------------------------\n4           40                          NULL\n3           NULL                        Antonio Moreno\nNULL        20                          Alfreds Futterk\n1           10                          Around the Horn\n[4] row(s) selected.\n \nMach&gt; select id1 * 10 FROM length_table;\nid1 * 10\n-----------------------\n40\n30\nNULL\n10\n[4] row(s) selected.\n \nMach&gt; select * FROM length_table Where id1 &gt; 1 and id2 &lt; 50;\nID1         ID2                         NAME\n-------------------------------------------------------------\n4           40                          NULL\n[1] row(s) selected.\n \nMach&gt; select name || ' with null concat' FROM length_table;\nname || ' with null concat'\n------------------------------------\nNULL\nAntonio Moreno with null concat\nAlfreds Futterk with null concat\nAround the Horn with null concat\n[4] row(s) selected.\n \nMach&gt; select LENGTH(name) FROM length_table;\nLENGTH(name)\n---------------\nNULL\n14\n15\n15\n[4] row(s) selected.\n\n\nLOWER\n\n이 함수는 영문 문자열을 소문자로 변환한다.\n\nLOWER(column_name)\n\nMach&gt; CREATE TABLE lower_table (name VARCHAR(20));\nCreated successfully.\n \nMach&gt; INSERT INTO lower_table VALUES('');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lower_table VALUES('James Backley');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lower_table VALUES('Alfreds Futterkiste');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lower_table VALUES('Antonio MORENO');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lower_table VALUES (NULL);\n1 row(s) inserted.\n \nMach&gt; SELECT LOWER(name) FROM lower_table;\nLOWER(name)\n------------------------\nNULL\nantonio moreno\nalfreds futterkiste\njames backley\nNULL\n[5] row(s) selected.\n\n\nLPAD / RPAD\n\n이 함수는 입력 값을 주어진 길이(length)가 될때까지 문자(char)를 왼쪽(LPAD) 또는 오른쪽(RPAD)에 덧붙이는 함수이다.\n\n마지막 매개변수인 char는 생략이 가능하며 생략된 경우에는 공백 문자 ‘ ‘ 를 이용한다.\n입력된 컬럼값이 length로 주어진 길이보다 긴 경우에는 문자를 덧붙이지 않고 앞에서부터 length 만큼만 가져온다.\n\nLPAD(str, len, padstr)\nRPAD(str, len, padstr)\n\nMach&gt; CREATE TABLE pad_table (c1 integer, c2 varchar(15));\nCreated successfully.\n \nMach&gt; INSERT INTO pad_table VALUES (1, 'Antonio');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO pad_table VALUES (25, 'Johnathan');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO pad_table VALUES (30, 'M');\n1 row(s) inserted.\n \nMach&gt; SELECT LPAD(to_char(c1), 5, '0') FROM pad_table;\nLPAD(to_char(c1), 5, '0')\n-----------------------------\n00030\n00025\n00001\n[3] row(s) selected.\n \nMach&gt; SELECT RPAD(to_char(c1), 5, '0') FROM pad_table;\nRPAD(to_char(c1), 5, '0')\n-----------------------------\n30000\n25000\n10000\n[3] row(s) selected.\n \nMach&gt; SELECT LPAD(c2, 5) FROM pad_table;\nLPAD(c2, 5)\n---------------\n    M\nJohna\nAnton\n[3] row(s) selected.\n \nMach&gt; SELECT RPAD(c2, 5) FROM pad_table;\nRPAD(c2, 5)\n---------------\nM\nJohna\nAnton\n[3] row(s) selected.\n \nMach&gt; SELECT RPAD(c2, 10, '***') FROM pad_table;\nRPAD(c2, 10, '***')\n-----------------------\nM*********\nJohnathan*\nAntonio***\n[3] row(s) selected.\n\n\nLTRIM / RTRIM\n\n이 함수는 첫 번째 매개변수에서 pattern 문자열에 해당하는 값을 제거하는 역할을 수행한다. LTRIM 함수는 컬럼 값의 왼쪽에서 오른쪽으로, RTRIM 함수는 오른쪽에서 왼쪽으로 문자들이 pattern에 있는지 확인하고 pattern에 없는 문자를 만날 때까지 잘라낸다. 만약 모든 문자열이 pattern에 존재한다면 NULL을 리턴한다.\n\nPattern 표현식을 명시하지 않은 경우 공백 문자 ‘ ‘를 기본으로 사용하여 공백 문자를 제거한다.\n\nLTRIM(column_name, pattern)\nRTRIM(column_name, pattern)\n\nMach&gt; CREATE TABLE trim_table1(name VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO trim_table1 VALUES ('   smith   ');\n1 row(s) inserted.\n \nMach&gt; SELECT ltrim(name) FROM trim_table1;\nltrim(name)\n---------------\nsmith\n[1] row(s) selected.\n \nMach&gt; SELECT rtrim(name) FROM trim_table1;\nrtrim(name)\n---------------\n   smith\n[1] row(s) selected.\n \nMach&gt; SELECT ltrim(name, ' s') FROM trim_table1;\nltrim(name, ' s')\n---------------------\nmith\n[1] row(s) selected.\n \nMach&gt; SELECT rtrim(name, 'h ') FROM trim_table1;\nrtrim(name, 'h ')\n---------------------\n   smit\n[1] row(s) selected.\n \nMach&gt; CREATE TABLE trim_table2 (name VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO trim_table2 VALUES ('ddckaaadkk');\n1 row(s) inserted.\n \nMach&gt; SELECT ltrim(name, 'dc') FROM trim_table2;\nltrim(name, 'dc')\n---------------------\nkaaadkk\n[1] row(s) selected.\n \nMach&gt; SELECT rtrim(name, 'dk') FROM trim_table2;\nrtrim(name, 'dk')\n---------------------\nddckaaa\n[1] row(s) selected.\n \nMach&gt; SELECT ltrim(name, 'dckak') FROM trim_table2;\nltrim(name, 'dckak')\n------------------------\nNULL\n[1] row(s) selected.\n \nMach&gt; SELECT rtrim(name, 'dckak') FROM trim_table2;\nrtrim(name, 'dckak')\n------------------------\nNULL\n[1] row(s) selected.\n\n\nMAX\n\n이 함수는 집계 함수로써, 해당 숫자 컬럼의 최대 값을 구하는 함수이다.\n\nMAX(column_name)\n\nMach&gt; CREATE TABLE max_table (c INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO max_table VALUES(10);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO max_table VALUES(20);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO max_table VALUES(30);\n1 row(s) inserted.\n \nMach&gt; SELECT MAX(c) FROM max_table;\nMAX(c)\n--------------\n30\n[1] row(s) selected.\n\n\nMIN\n\n이 함수는 집계 함수로써, 해당 숫자 컬럼의 최소값을 구하는 함수이다.\n\nMIN(column_name)\n\nMach&gt; CREATE TABLE min_table(c1 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO min_table VALUES(1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO min_table VALUES(22);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO min_table VALUES(33);\n1 row(s) inserted.\n \nMach&gt; SELECT MIN(c1) FROM min_table;\nMIN(c1)\n--------------\n1\n[1] row(s) selected.\n\n\nNVL\n\n이 함수는 컬럼의 값이 NULL이면 value를 리턴하고, NULL이 아니면 원래 컬럼의 값을 출력한다.\n\nNVL(string1, replace_with)\n\nMach&gt; CREATE TABLE nvl_table (c1 varchar(10));\nCreated successfully.\n \nMach&gt; INSERT INTO nvl_table VALUES ('Johnathan');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO nvl_table VALUES (NULL);\n1 row(s) inserted.\n \nMach&gt; SELECT NVL(c1, 'Thomas') FROM nvl_table;\nNVL(c1, 'Thomas')\n---------------------\nThomas\nJohnathan\n\n\nROUND\n\n이 함수는 정수형 입력 값에서 (입력된 자릿수+1) 의 자릿수에서 반올림한 결과를 반환한다. 자릿수가 입력되지 않은 경우, 반올림은 0의 자리에서 이뤄진다. 소수점 자리를 반올림하기 위해서 decimals 자리에 음수를 입력하는 것이 가능하다.\n\nROUND(column_name, [decimals])\n\nMach&gt; CREATE TABLE round_table (c1 DOUBLE);\nCreated successfully.\n \nMach&gt; INSERT INTO round_table VALUES (1.994);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO round_table VALUES (1.995);\n1 row(s) inserted.\n \nMach&gt; SELECT c1, ROUND(c1, 2) FROM round_table;\nc1                          ROUND(c1, 2)\n-----------------------------------------------------------\n1.995                       2\n1.994                       1.99\n\n\nROWNUM\n\n이 함수는 SELECT 쿼리 결과 Row에 번호를 부여하는 함수이다.\n\nSELECT 쿼리 내부에 사용되는 Subquery 또는 Inline View 내부에서도 사용이 가능하며, Inline View 에서 ROWNUM() 함수를 Target List에 사용하는 경우엔 Alias를 부여해야 외부에서 참조가 가능하다.\n\nROWNUM()\n\n\n사용 가능한 절\n\n해당 함수는 SELECT 쿼리의 Target List, GROUP BY, 또는 ORDER BY 절에서 사용이 가능하다. 하지만 SELECT 쿼리의 WHERE와 HAVING 절에서는 사용할 수 없다. ROWNUM() 결과 번호로 WHERE나 HAVING 절을 통해 제어하고자 한다면, ROWNUM() 을 포함한 SELECT 쿼리를 Inline View로 사용한 다음 외부에 있는 WHERE나 HAVING 절에서 참조하면 된다.\n\n\n  \n    \n      사용할 수 있는 절\n      사용할 수 없는 절\n    \n  \n  \n    \n      Target List / GROUP BY / ORDER BY\n      WHERE / HAVING\n    \n  \n\n\nMach&gt; CREATE TABLE rownum_table(c1 INTEGER, c2 DOUBLE, c3 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO rownum_table VALUES(1, 1.0, '');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(2, 2.0, 'Second Row');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(3, 3.3, 'Third Row');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(4, 4.3, 'Fourth Row');\n1 row(s) inserted.\n \nMach&gt; SELECT INNER_RANK, c3 AS NAME\n    2 FROM   (SELECT ROWNUM() AS INNER_RANK, * FROM rownum_table)\n    3 WHERE  INNER_RANK &lt; 3;\nINNER_RANK           NAME\n------------------------------------\n1                    Fourth Row\n2                    Third Row\n[2] row(s) selected.\n\n\n정렬로 인한 결과 변화\n\nSELECT 쿼리에 ORDER BY 절이 존재하는 경우, Target List에 있는 ROWNUM()의 결과 번호가 순차적으로 부여되지 않는 경우가 발생할 수 있다. 이는 ROWNUM() 연산이 ORDER BY 절의 연산 이전에 이루어지기 때문이다. 순차적으로 부여하고자 할 경우, ORDER BY 절을 포함한 쿼리를 Inline View로 사용한 다음 ROWNUM()을 외부 SELECT 문에서 호출하면 된다.\n\nMach&gt; CREATE TABLE rownum_table(c1 INTEGER, c2 DOUBLE, c3 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO rownum_table VALUES(1, 1.0, '');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(2, 2.0, 'John');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(3, 3.3, 'Sarah');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(4, 4.3, 'Micheal');\n1 row(s) inserted.\n \nMach&gt; SELECT ROWNUM(), c2 AS SORT, c3 AS NAME\n    2 FROM   ( SELECT * FROM rownum_table ORDER BY c3 );\nROWNUM()             SORT                        NAME\n-----------------------------------------------------------------\n1                    1                           NULL\n2                    2                           John\n3                    4.3                         Micheal\n4                    3.3                         Sarah\n[4] row(s) selected.\n\n\nSERIESNUM\n\n각 레코드가, SERIES BY 로 그룹지어진 시리즈의 몇 번째에 속해있는지를 나타낸 번호를 반환한다. 반환형은 BIGINT 형이며, SERIES BY 절이 사용되지 않았을 경우엔 항상 1을 반환한다.\n\nSERIESNUM()\n\nMach&gt; CREATE TABLE T1 (C1 INTEGER, C2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO T1 VALUES (0, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (2, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (3, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (4, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (5, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (6, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (7, 1);\n1 row(s) inserted.\n \n \nMach&gt; SELECT SERIESNUM(), C1, C2 FROM T1 ORDER BY C1 SERIES BY C2 &gt; 1;\nSERIESNUM() C1 C2\n-------------------------------------------------\n1 1 2\n1 2 3\n1 3 2\n2 5 2\n2 6 3\n[5] row(s) selected.\n\n\nSTDDEV / STDDEV_POP\n\n이 함수는 집계 함수로써, 입력된 컬럼의 (샘플) 표준 편차와 모집단 표준 편차를 반환한다. 각각 VARIANCE 와 VAR_POP 값의 제곱근과 같다.\n\nSTDDEV(column)\nSTDDEV_POP(column)\n\nMach&gt; CREATE TABLE stddev_table(c1 INTEGER, C2 DOUBLE);\n \nMach&gt; INSERT INTO stddev_table VALUES (1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO stddev_table VALUES (2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO stddev_table VALUES (3, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO stddev_table VALUES (4, 2);\n1 row(s) inserted.\n \nMach&gt; SELECT c2, STDDEV(c1) FROM stddev_table GROUP BY c2;\nc2                          STDDEV(c1)\n-----------------------------------------------------------\n1                           0.707107\n2                           0.707107\n[2] row(s) selected.\n \nMach&gt; SELECT c2, STDDEV_POP(c1) FROM stddev_table GROUP BY c2;\nc2                          STDDEV_POP(c1)\n-----------------------------------------------------------\n1                           0.5\n2                           0.5\n[2] row(s) selected.\n\n\nSUBSTR\n\n이 함수는 가변 문자열 컬럼의 데이터를 START 부터 SIZE 만큼 잘라낸다.\n\n\n  START 는 1부터 시작하며, 0일 경우에는 NULL을 리턴한다.\n  SIZE가 만일 해당 문자열의 크기보다 클 경우에는 그 문자열의 최대값까지만 되돌린다.\n\n\nSIZE는 생략 가능하며, 생략할 경우에는 해당 문자열 크기만큼 내부적으로 지정된다.\n\nSUBSTRING(column_name, start, [length])\n\nMach&gt; CREATE TABLE substr_table (c1 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO substr_table values('ABCDEFG');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO substr_table values('abstract');\n1 row(s) inserted.\n \nMach&gt; SELECT SUBSTR(c1, 1, 1) FROM substr_table;\nSUBSTR(c1, 1, 1)\n--------------------\na\nA\n[2] row(s) selected.\n \nMach&gt; SELECT SUBSTR(c1, 3, 3) FROM substr_table;\nSUBSTR(c1, 3, 3)\n--------------------\nstr\nCDE\n[2] row(s) selected.\n \nMach&gt; SELECT SUBSTR(c1, 2) FROM substr_table;\nSUBSTR(c1, 2)\n-----------------\nbstract\nBCDEFG\n[2] row(s) selected.\n \nMach&gt; drop table substr_table;\nDropped successfully.\n \nMach&gt; CREATE TABLE substr_table (c1 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO substr_table values('ABCDEFG');\n1 row(s) inserted.\n \nMach&gt; SELECT SUBSTR(c1, 1, 1) FROM substr_table;\nSUBSTR(c1, 1, 1)\n--------------------\nA\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTR(c1, 3, 3) FROM substr_table;\nSUBSTR(c1, 3, 3)\n--------------------\nCDE\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTR(c1, 2) FROM substr_table;\nSUBSTR(c1, 2)\n-----------------\nBCDEFG\n[1] row(s) selected.\n\n\nSUBSTRING_INDEX\n\n주어진 구분자(delim)가 입력한 count만큼 발견될 때까지 복제한 문자열을 반환한다. 만약 count를 음수값으로 입력하면 입력한 문자열의 끝에서부터 구분자를 검사해서 구분자가 발견된 위치에서 문자열의 끝까지 반환한다.\n\n만약 count를 0으로 입력하거나 문자열에 구분자가 존재하지 않는다면 함수는 NULL을 리턴할 것이다.\n\nSUBSTRING_INDEX(expression, delim, count)\n\nMach&gt; CREATE TABLE substring_table (url VARCHAR(30));\nCreated successfully.\n \nMach&gt; INSERT INTO substring_table VALUES('www.machbase.com');\n1 row(s) inserted.\n \nMach&gt; SELECT SUBSTRING_INDEX(url, '.', 1) FROM substring_table;\nSUBSTRING_INDEX(url, '.', 1)\n----------------------------------\nwww\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTRING_INDEX(url, '.', 2) FROM substring_table;\nSUBSTRING_INDEX(url, '.', 2)\n----------------------------------\nwww.machbase\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTRING_INDEX(url, '.', -1) FROM substring_table;\nSUBSTRING_INDEX(url, '.', -1)\n----------------------------------\ncom\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTRING_INDEX(SUBSTRING_INDEX(url, '.', 2), '.', -1) FROM substring_table;\nSUBSTRING_INDEX(SUBSTRING_INDEX(url, '.', 2), '.', -1)\n-------------------------------------------\nmachbase\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTRING_INDEX(url, '.', 0) FROM substring_table;\nSUBSTRING_INDEX(url, '.', 0)\n----------------------------------\nNULL\n[1] row(s) selected.\n\n\nSUM\n\n이 함수는 집계 함수로써, 숫자형 컬럼의 총합을 나타낸다.\n\nSUM(column_name)\n\nMach&gt; CREATE TABLE sum_table (c1 INTEGER, c2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO sum_table VALUES(1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(1, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(2, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(2, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(3, 4);\n1 row(s) inserted.\n \nMach&gt; SELECT c1, SUM(c1) from sum_table group by c1;\nc1          SUM(c1)\n------------------------------------\n2           6\n3           3\n1           3\n[3] row(s) selected.\n \nMach&gt; SELECT c1, SUM(c2) from sum_table group by c1;\nc1          SUM(c2)\n------------------------------------\n2           6\n3           4\n1           6\n[3] row(s) selected.\n\n\nSUMSQ\n\nSUMSQ 함수는 숫자형 컬럼값에 대한 제곱 합을 반환한다.\n\nSUMSQ(value)\n\nMach&gt; CREATE TABLE sumsq_table (c1 INTEGER, c2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO sumsq_table VALUES (1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sumsq_table VALUES (1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sumsq_table VALUES (1, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sumsq_table VALUES (2, 4);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sumsq_table VALUES (2, 5);\n1 row(s) inserted.\n \nMach&gt; SELECT c1, SUMSQ(c2) FROM sumsq_table GROUP BY c1;\nc1          SUMSQ(c2)           \n------------------------------------\n2           41                  \n1           14                  \n[2] row(s) selected.\n\n\nSYSDATE / NOW\n\nSYSDATE 는 함수가 아니라 의사컬럼 (pseudocolumn) 으로, 시스템의 현재 시각을 반환한다.\n\nNOW 는 SYSDATE 와 동일한 함수이며, 사용자 편의를 위해 같이 제공한다.\n\nSYSDATE\nNOW\n\nMach&gt; SELECT SYSDATE, NOW FROM t1;\n \nSYSDATE                         NOW\n-------------------------------------------------------------------\n2017-01-16 14:14:53 310:973:000 2017-01-16 14:14:53 310:973:000\n\n\nTO_CHAR\n\n이 함수는 주어진 데이터 타입을 문자열 타입으로 변환하는 함수이다. 타입에 따라 format_string을 지정할 수도 있으며, Binary 타입에 대해서는 지원하지 않는다.\n\nTO_CHAR(column)\n\nTO_CHAR : 기본 자료형\n\n아래와 같이 기본 자료형에서는 그대로 문자열 형식의 데이터로 변환된다.\n\nMach&gt; CREATE TABLE fixed_table (id1 SHORT, id2 INTEGER, id3 LONG, id4 FLOAT, id5 DOUBLE, id6 IPV4, id7 IPV6, id8 VARCHAR (128));\nCreated successfully.\n \nMach&gt; INSERT INTO fixed_table values(200, 19234, 1234123412, 3.14, 7.8338, '192.168.0.1', '::127.0.0.1', 'log varchar');\n1 row(s) inserted.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id1) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id1) || ' ]'\n------------------------------------------------------------------------------------\n[ 200 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id2) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id2) || ' ]'\n------------------------------------------------------------------------------------\n[ 19234 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id3) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id3) || ' ]'\n------------------------------------------------------------------------------------\n[ 1234123412 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id4) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id4) || ' ]'\n------------------------------------------------------------------------------------\n[ 3.140000 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id5) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id5) || ' ]'\n------------------------------------------------------------------------------------\n[ 7.833800 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id6) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id6) || ' ]'\n------------------------------------------------------------------------------------\n[ 192.168.0.1 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id7) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id7) || ' ]'\n------------------------------------------------------------------------------------\n[ 0000:0000:0000:0000:0000:0000:7F00:0001 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id8) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id8) || ' ]'\n------------------------------------------------------------------------------------\n[ log varchar ]\n[1] row(s) selected.\n\n\nTO_CHAR : 부동소수형\n\n\n  5.5.6 부터 지원됩니다.\n\n\nfloat, double 컬럼의 값을 임의의 문자열로 변환하는 함수이다.\n\n포맷 표현은 중복해서 사용할 수 없고, ‘[문자][숫자]’ 의 형태로 입력해야 한다.\n\n\n  \n    \n      포맷 표현식\n      설명\n    \n  \n  \n    \n      F / f\n      컬럼 값의 소수점 자릿수를 지정한다. 입력 최대 숫자값은 30이다.\n    \n    \n      N / n\n      컬럼 값의 소수점 자릿수를 지정하고, 정수 부분은 세자리마다 쉼표 (,) 를 입력한다. 입력 최대 숫자값은 30이다.\n    \n  \n\n\nMach&gt; create table float_table (i1 float, i2 double);\nCreated successfully.\n \nMach&gt; insert into float_table values (1.23456789, 1234.5678901234567890);\n1 row(s) inserted.\n \nMach&gt; select TO_CHAR(i1, 'f8'), TO_CHAR(i2, 'N9') from float_table;\nTO_CHAR(i1, 'f8')       TO_CHAR(i2, 'N9')\n--------------------------------------------------------------\n1.23456788              1,234.567890123\n[1] row(s) selected.\n\n\nTO_CHAR : DATETIME 형\n\ndatetime 컬럼의 값을 임의의 문자열로 변환하는 함수이다. 이 함수를 이용하여 다양한 형태의 문자열을 만들고 조합할 수 있다.\n\nformat_string이 생략된 경우에는 기본값으로 “YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn” 을 사용해서 변환한다.\n\n\n  \n    \n      포맷 표현식\n      설명\n    \n  \n  \n    \n      YYYY\n      연도를 4자리 숫자로 변환한다.\n    \n    \n      YY\n      연도를 2자리 숫자로 변환한다.\n    \n    \n      MM\n      해당 월을 2자리 숫자로 변환한다.\n    \n    \n      MON\n      해당 월을 3자리 축약 알파벳으로 변환한다. (e.g. JAN, FEB, MAY, …)\n    \n    \n      DD\n      해당 일을 2자리 숫자로 변환한다.\n    \n    \n      DAY\n      해당 요일을 3자리 축약 알파벳으로 변환한다. (e.g. SUN, MON, …)\n    \n    \n      IW\n      ISO 8601 규칙에 의해 (요일을 고려한), 1부터 53까지의 특정 연도의 주 수 (Week Number) 를 변환한다.한 주의 시작은 월요일이다.첫 주는 이전 연도의 마지막 주로 간주할 수 있다. 마찬가지로, 마지막 주는 다음 연도의 첫 주로 간주할 수 있다.자세한 내용은 ISO 8601 규칙을 참고한다.\n    \n    \n      WW\n      요일을 고려하지 않은, 1부터 53까지의 특정 연도의 주 수 (Week Number) 를 변환한다.즉, 1월 1일부터 1월 7일까지는 1로 변환된다.\n    \n    \n      W\n      요일을 고려하지 않은, 1부터 5까지의 특정 달의 주 수 (Week Number) 를 변환한다.즉, 3월 1일부터 3월 7일까지는 1로 변환된다.\n    \n    \n      HH\n      해당 시간을 2자리 숫자로 변환한다.\n    \n    \n      HH12\n      해당 시간을 2자리 숫자로 변환하되, 1~12까지 표현한다.\n    \n    \n      HH24\n      해당 시간을 2자리 숫자로 변환하되, 1~23까지 표현한다.\n    \n    \n      HH2, HH3, HH6\n      해당 시간을 HH 다음에 오는 숫자로 절삭한다.즉, HH6을 사용한 경우 0시부터 5시까지는 0으로 6~11시까지는 6으로 표현한다.이 표현은 시계열 상의 특정 시간 단위 통계를 계산할 때 유용하다.이 값은 24시 기준으로 표현된다.\n    \n    \n      MI\n      해당 분을 두자리 숫자로 표현한다.\n    \n    \n      MI2, MI5, MI10, MI20, MI30\n      해당 분을 MI 다음에 오는 숫자로 절삭한다.즉, MI30을 사용한 경우에는 0분에서 29분까지는 0으로 표현되고, 30~59분까지는 30으로 표현된다.이 표현은 시계열 상의 특정 시간 단위 통계를 계산할 때 유용하다.\n    \n    \n      SS\n      해당 초를 두자리 숫자로 표현한다.\n    \n    \n      SS2, SS5, SS10, SS20, SS30\n      해당 초를 이어지는 숫자로 절삭한다.즉, SS30을 사용한 경우에는 0초에서 29초까지는 0으로 표현되고, 30~59초까지는 30으로 표현된다.이 표현은 시계열 상의 특정 시간 단위 통계를 계산할 때 유용하다.\n    \n    \n      AM\n      현재 시간이 오전, 오후에 따라 각각 AM 혹은 PM으로 표현한다.\n    \n    \n      mmm\n      해당 시간의 mili second를 3자리 숫자로 표현한다.값의 범위는 0~999이다.\n    \n    \n      uuu\n      해당 시간의 micro second를 3자리 숫자로 표현한다.값의 범위는 0~999이다.\n    \n    \n      nnn\n      해당 시간의 nano second를 3자리 숫자로 표현한다.값의 범위는 0~999이다.\n    \n  \n\n\nMach&gt; CREATE TABLE datetime_table (id integer, dt datetime);\nCreated successfully.\n \nMach&gt; INSERT INTO  datetime_table values(1, TO_DATE('1999-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  datetime_table values(2, TO_DATE('2012-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  datetime_table values(3, TO_DATE('2013-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  datetime_table values(4, TO_DATE('2014-12-30 11:22:33 444:555:666'));\n1 row(s) inserted.\n \nMach&gt; SELECT id, dt FROM datetime_table WHERE dt &gt; TO_DATE('2000-11-11 1:2:3 4:5:0');\nid          dt\n-----------------------------------------------\n4           2014-12-30 11:22:33 444:555:666\n3           2013-11-11 01:02:03 004:005:006\n2           2012-11-11 01:02:03 004:005:006\n[3] row(s) selected.\n \nMach&gt; SELECT id, dt FROM datetime_table WHERE dt &gt; TO_DATE('2013-11-11 1:2:3') and dt &lt; TO_DATE('2014-11-11 1:2:3');\nid          dt\n-----------------------------------------------\n3           2013-11-11 01:02:03 004:005:006\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt) FROM datetime_table;\nid          TO_CHAR(dt)\n-------------------------------------------------------------------------------------------------\n4           2014-12-30 11:22:33 444:555:666\n3           2013-11-11 01:02:03 004:005:006\n2           2012-11-11 01:02:03 004:005:006\n1           1999-11-11 01:02:03 004:005:006\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY')\n-------------------------------------------------------------------------------------------------\n4           2014\n3           2013\n2           2012\n1           1999\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM')\n-------------------------------------------------------------------------------------------------\n4           2014-12\n3           2013-11\n2           2012-11\n1           1999-11\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM-DD') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM-DD')\n-------------------------------------------------------------------------------------------------\n4           2014-12-30\n3           2013-11-11\n2           2012-11-11\n1           1999-11-11\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM-DD TO_CHAR') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM-DD TO_CHAR')\n-------------------------------------------------------------------------------------------------\n4           2014-12-30 TO_CHAR\n3           2013-11-11 TO_CHAR\n2           2012-11-11 TO_CHAR\n1           1999-11-11 TO_CHAR\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM-DD HH24:MI:SS') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM-DD HH24:MI:SS')\n-------------------------------------------------------------------------------------------------\n4           2014-12-30 11:22:33\n3           2013-11-11 01:02:03\n2           2012-11-11 01:02:03\n1           1999-11-11 01:02:03\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM-DD HH24:MI:SS mmm.uuu.nnn') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM-DD HH24:MI:SS mmm.\n-------------------------------------------------------------------------------------------------\n4           2014-12-30 11:22:33 444.555.666\n3           2013-11-11 01:02:03 004.005.006\n2           2012-11-11 01:02:03 004.005.006\n1           1999-11-11 01:02:03 004.005.006\n[4] row(s) selected.\n\n\nTO_CHAR : 지원하지 않는 타입\n\n현재 Binary 타입에 대해서는 TO_CHAR를 지원하지 않는다.\n\n왜냐하면, 일반 텍스트로 변환이 불가능하기 때문이다. 만일 이를 화면에 출력하고자 할 경우에는 TO_HEX() 함수를 통해 16진수 값을 출력해서 확인할 수 있다.\n\nTO_DATE\n\n이 함수는 주어진 포맷 문자열로 표현된 문자열을 datetime 타입으로 변환한다.\n\nformat_string이 생략된 경우에는 기본값으로 “YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn” 을 사용해서 변환한다.\n\n-- default format is \"YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn\" if no format exists.\nTO_DATE(date_string [, format_string])\n\n\nMach&gt; CREATE TABLE to_date_table (id INTEGER, dt datetime);\nCreated successfully.\n \nMach&gt; INSERT INTO  to_date_table VALUES(1, TO_DATE('1999-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  to_date_table VALUES(2, TO_DATE('2012-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  to_date_table VALUES(3, TO_DATE('2014-12-30 11:22:33 444:555:666'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  to_date_table VALUES(4, TO_DATE('2014-12-30 23:22:34 777:888:999', 'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn'));\n1 row(s) inserted.\n \nMach&gt; SELECT id, dt FROM to_date_table WHERE dt &gt; TO_DATE('1999-11-11 1:2:3 4:5:0');\nid          dt\n-----------------------------------------------\n4           2014-12-30 23:22:34 777:888:999\n3           2014-12-30 11:22:33 444:555:666\n2           2012-11-11 01:02:03 004:005:006\n1           1999-11-11 01:02:03 004:005:006\n[4] row(s) selected.\n \nMach&gt; SELECT id, dt FROM to_date_table WHERE dt &gt; TO_DATE('2000-11-11 1:2:3 4:5:0');\nid          dt\n-----------------------------------------------\n4           2014-12-30 23:22:34 777:888:999\n3           2014-12-30 11:22:33 444:555:666\n2           2012-11-11 01:02:03 004:005:006\n[3] row(s) selected.\n \nMach&gt; SELECT id, dt FROM to_date_table WHERE dt &gt; TO_DATE('2012-11-11 1:2:3','YYYY-MM-DD HH24:MI:SS') and dt &lt; TO_DATE('2014-11-11 1:2:3','YYYY-MM-DD HH24:MI:SS');\nid          dt\n-----------------------------------------------\n2           2012-11-11 01:02:03 004:005:006\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999', 'YYYY') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999', 'YYYY')\n-----------------------------------------------\n4           1999-01-01 00:00:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12', 'YYYY-MM') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12', 'YYYY-MM')\n-----------------------------------------------\n4           1999.12.01 00:00:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999', 'YYYY') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999', 'YYYY')\n-----------------------------------------------\n4           1999-01-01 00:00:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12', 'YYYY-MM') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12', 'YYYY-MM')\n-----------------------------------------------\n4           1999-12-01 00:00:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12', 'YYYY-MM-DD HH24:MI') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12-31 13:12', 'YYYY-MM-DD HH24:MI')\n-------------------------------------------------------\n4           1999-12-31 13:12:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12:32', 'YYYY-MM-DD HH24:MI:SS') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12-31 13:12:32', 'YYYY-MM-DD HH24:MI:SS')\n-------------------------------------------------------\n4           1999-12-31 13:12:32 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12:32 123', 'YYYY-MM-DD HH24:MI:SS mmm') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12-31 13:12:32 123', 'YYYY-MM-DD HH24:MI:SS mmm')\n-------------------------------------------------------\n4           1999-12-31 13:12:32 123:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12:32 123:456', 'YYYY-MM-DD HH24:MI:SS mmm:uuu') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12-31 13:12:32 123:456', 'YYYY-MM-DD HH24:MI:SS mmm:uuu')\n-------------------------------------------------------\n4           1999-12-31 13:12:32 123:456:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12:32 123:456:789', 'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn') FROM to_date_table LIMIT 1;\nid           TO_DATE('1999-12-31 13:12:32 123:456:789', 'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn')\n-------------------------------------------------------\n4           1999-12-31 13:12:32 123:456:789\n[1] row(s) selected.\n\n\nTO_DATE_SAFE\n\nTO_DATE() 와 비슷하지만, 변환에 실패할 경우 에러를 내지 않고 NULL 을 반환한다.\n\nTO_DATE_SAFE(date_string [, format_string])\n\nMach&gt; CREATE TABLE date_table (ts DATETIME);\nCreated successfully.\n \nMach&gt; INSERT INTO date_table VALUES (TO_DATE_SAFE('2016-01-01', 'YYYY-MM-DD'));\n1 row(s) inserted.\nMach&gt; INSERT INTO date_table VALUES (TO_DATE_SAFE('2016-01-02', 'YYYY'));\n1 row(s) inserted.\nMach&gt; INSERT INTO date_table VALUES (TO_DATE_SAFE('2016-12-32', 'YYYY-MM-DD'));\n1 row(s) inserted.\n \nMach&gt; SELECT ts FROM date_table;\nts\n----------------------------------\nNULL\nNULL\n2016-01-01 00:00:00 000:000:000\n[3] row(s) selected.\n\n\nTO_HEX\n\n이 함수는 컬럼의 값이 NULL이면 value를 리턴하고, NULL이 아니면 원래 컬럼의 값을 출력한다. 출력의 일관성을 보장하기 위해 short, int, long 타입의 경우에는 BIG ENDIAN 형태로 변환해서 출력해 준다.\n\nTO_HEX(column)\n\nMach&gt; CREATE TABLE hex_table (id1 SHORT, id2 INTEGER, id3 VARCHAR(10), id4 FLOAT, id5 DOUBLE, id6 LONG, id7 IPV4, id8 IPV6, id9 TEXT, id10 BINARY,\nid11 DATETIME);\nCreated successfully.\n \nMach&gt; INSERT INTO hex_table VALUES(256, 65535, '0123456789', 3.141592, 1024 * 1024 * 1024 * 3.14, 13513135446, '192.168.0.1', '::192.168.0.1', 'textext',\n'binary', TO_DATE('1999', 'YYYY'));\n1 row(s) inserted.\n \nMach&gt; SELECT TO_HEX(id1), TO_HEX(id2), TO_HEX(id3), TO_HEX(id4), TO_HEX(id5), TO_HEX(id6), TO_HEX(id7), TO_HEX(id8), TO_HEX(id9), TO_HEX(id10), TO_HEX(id11)\nFROM hex_table;\nTO_HEX(id1)  TO_HEX(id2)  TO_HEX(id3)            TO_HEX(id4)  TO_HEX(id5)        TO_HEX(id6)        TO_HEX(id7)\n-------------------------------------------------------------------------------------------------------------------------\nTO_HEX(id8)                          TO_HEX(id9)\n--------------------------------------------------------------------------------------------------------------------------\nTO_HEX(id10)                                                                      TO_HEX(id11)\n--------------------------------------------------------------------------------------------------------\n0100   0000FFFF   30313233343536373839   D80F4940   1F85EB51B81EE941   0000000325721556   04C0A80001\n06000000000000000000000000C0A80001   74657874657874\n62696E617279                                                                      0CB325846E226000\n[1] row(s) selected.\n\n\nTO_IPV4 / TO_IPV4_SAFE\n\n이 함수는 주어진 문자열을 IP 버전4 타입으로 변환한다. 만일 해당 문자열이 숫자형으로 변환이 불가능할 경우에는 TO_IPV4() 함수는 에러를 리턴하고, 동작을 종료한다.\n\n그러나, TO_IPV4_SAFE() 함수의 경우에는 에러 상황일 경우 NULL을 리턴하고, 동작을 계속할 수 있게 되어 있다.\n\nTO_IPV4(string_value)\nTO_IPV4_SAFE(string_value)\n\nMach&gt; CREATE TABLE ipv4_table (c1 varchar(100));\nCreated successfully.\n \nMach&gt; INSERT INTO ipv4_table VALUES('192.168.0.1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv4_table VALUES('     192.168.0.2    ');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv4_table VALUES(NULL);\n1 row(s) inserted.\n \nMach&gt; SELECT c1 FROM ipv4_table;\nc1\n------------------------------------------------------------------------------------\nNULL\n     192.168.0.2\n192.168.0.1\n[3] row(s) selected.\n \nMach&gt; SELECT TO_IPV4(c1) FROM ipv4_table;\nTO_IPV4(c1)\n------------------\nNULL\n192.168.0.2\n192.168.0.1\n[3] row(s) selected.\n \nMach&gt; INSERT INTO ipv4_table VALUES('192.168.0.1.1');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_IPV4(c1) FROM ipv4_table limit 1;\nTO_IPV4(c1)\n------------------\n[ERR-02068 : Invalid IPv4 address format (192.168.0.1.1).]\n[0] row(s) selected.\n \nMach&gt; SELECT TO_IPV4_SAFE(c1) FROM ipv4_table;\nTO_IPV4_SAFE(c1)\n-------------------\nNULL\nNULL\n192.168.0.2\n192.168.0.1\n[4] row(s) selected.\n\n\nTO_IPV6 / TO_IPV6_SAFE\n\n이 함수는 주어진 문자열을 IP 버전6 타입으로 변환한다. 만일 해당 문자열이 숫자형으로 변환이 불가능할 경우에는 TO_IPV6() 함수는 에러를 리턴하고, 동작을 종료한다.\n\n그러나, TO_IPV6_SAFE() 함수의 경우에는 에러 상황일 경우 NULL을 리턴하고, 동작을 계속할 수 있게 되어 있다.\n\nTO_IPV6(string_value)\nTO_IPV6_SAFE(string_value)\n\nMach&gt; CREATE TABLE ipv6_table (id varchar(100));\nCreated successfully.\n \nMach&gt; INSERT INTO ipv6_table VALUES('::0.0.0.0');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('::127.0.0.1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('::127.0' || '.0.2');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('   ::127.0.0.3');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('::127.0.0.4  ');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('   ::FFFF:255.255.255.255   ');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('21DA:D3:0:2F3B:2AA:FF:FE28:9C5A');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_IPV6(id) FROM ipv6_table;\nTO_IPV6(id)\n---------------------------------------------------------------\n21da:d3::2f3b:2aa:ff:fe28:9c5a\n::ffff:255.255.255.255\n::127.0.0.4\n::127.0.0.3\n::127.0.0.2\n::127.0.0.1\n::\n[7] row(s) selected.\n \nMach&gt; INSERT INTO ipv6_table VALUES('127.0.0.10.10');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_IPV6(id) FROM ipv6_table limit 1;\nTO_IPV6(id)\n---------------------------------------------------------------\n[ERR-02148 : Invalid IPv6 address format.(127.0.0.10.10)]\n[0] row(s) selected.\n \nMach&gt; SELECT TO_IPV6_SAFE(id) FROM ipv6_table;\nTO_IPV6_SAFE(id)\n---------------------------------------------------------------\nNULL\n21da:d3::2f3b:2aa:ff:fe28:9c5a\n::ffff:255.255.255.255\n::127.0.0.4\n::127.0.0.3\n::127.0.0.2\n::127.0.0.1\n::\n[8] row(s) selected.\n\n\nTO_NUMBER / TO_NUMBER_SAFE\n\n이 함수는 주어진 문자열을 숫자형 double 타입으로 변환한다. 만일 해당 문자열이 숫자형으로 변환이 불가능할 경우에는 TO_NUMBER() 함수는 에러를 리턴하고, 동작을 종료한다.\n\n그러나, TO_NUMBER_SAFE() 함수의 경우에는 에러 상황일 경우 NULL을 리턴하고, 동작을 계속할 수 있게 되어 있다.\n\nTO_NUMBER(string_value)\nTO_NUMBER_SAFE(string_value)\n\n\nMach&gt; CREATE TABLE number_table (id varchar(100));\nCreated successfully.\n \nMach&gt; INSERT INTO number_table VALUES('10');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO number_table VALUES('20');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO number_table VALUES('30');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_NUMBER(id) from number_table;\nTO_NUMBER(id)\n------------------------------\n30\n20\n10\n[3] row(s) selected.\n \nMach&gt; CREATE TABLE safe_table (id varchar(100));\nCreated successfully.\n \nMach&gt; INSERT INTO safe_table VALUES('invalidnumber');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_NUMBER(id) from safe_table;\nTO_NUMBER(id)\n------------------------------\n[ERR-02145 : The string cannot be converted to number value.(invalidnumber)]\n[0] row(s) selected.\n \nMach&gt; SELECT TO_NUMBER_SAFE(id) from safe_table;\nTO_NUMBER_SAFE(id)\n------------------------------\nNULL\n[1] row(s) selected.\n\n\nTO_TIMESTAMP\n\n이 함수는 datetime 자료형을 1970-01-01 09:00 부터 경과된 nanosecond 데이터로 변환한다.\n\nTO_TIMESTAMP(datetime_value)\n\nMach&gt; create table datetime_tbl (c1 datetime);\nCreated successfully.\n \nMach&gt; insert into datetime_tbl values ('2010-01-01 10:10:10');\n1 row(s) inserted.\n \nMach&gt; select to_timestamp(c1) from datetime_tbl;\nto_timestamp(c1)\n-----------------------\n1262308210000000000\n[1] row(s) selected.\n\n\nTRUNC\n\nTRUNC 함수는 소수점 아래 n번째 자리에서 버림한 number를 반환한다.\n\nn이 생략될 경우 이를 0으로 취급하여 소수점 아래 자리는 모두 삭제한다. n이 음수일 경우 소수점 앞 n자리에서 버림한 값을 반환한다.\n\nTRUNC(number [, n])\n\nMach&gt; CREATE TABLE trunc_table (i1 DOUBLE);\nCreated successfully.\n \nMach&gt; INSERT INTO trunc_table VALUES (158.799);\n1 row(s) inserted.\n \nMach&gt; SELECT TRUNC(i1, 1), TRUNC(i1, -1) FROM trunc_table;\nTRUNC(i1, 1)                TRUNC(i1, -1)\n-----------------------------------------------------------\n158.7                       150\n[1] row(s) selected.\n \nMach&gt; SELECT TRUNC(i1, 2), TRUNC(i1, -2) FROM trunc_table;\nTRUNC(i1, 2)                TRUNC(i1, -2)\n-----------------------------------------------------------\n158.79                      100\n[1] row(s) selected.\n\n\nTS_CHANGE_COUNT\n\n\n  Cluster Edition 에서는 사용할 수 없는 함수이다.\n\n\n이 함수는 집계 함수로써, 특정 컬럼 값의 변경 횟수를 얻는다.\n\n입력되는 데이터의 순서가 시간 순으로 입력된다는 것을 보장할 수 있어야 원하는 결과를 얻을 수가 있기 때문에 이 함수는 1) Join을 사용하거나 2) Inline view 를 사용한 경우에는 사용할 수가 없다.\n\n현재 버전에서는 varchar를 제외한 타입만 지원한다.\n\nTS_CHANGE_COUNT(column)\n\nMach&gt; CREATE TABLE ipcount_table (id INTEGER, ip IPV4);\nCreated successfully.\n \nMach&gt; INSERT INTO ipcount_table VALUES (1, '192.168.0.1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (1, '192.168.0.2');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (1, '192.168.0.1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (1, '192.168.0.2');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (2, '192.168.0.3');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (2, '192.168.0.3');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (2, '192.168.0.4');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (2, '192.168.0.4');\n1 row(s) inserted.\n \nMach&gt; SELECT id, TS_CHANGE_COUNT(ip) from ipcount_table GROUP BY id;\nid          TS_CHANGE_COUNT(ip)\n------------------------------------\n2           2\n1           4\n[2] row(s) selected.\n\n\nUNIX_TIMESTAMP\n\nUNIX_TIMESTAMP는 date타입의 값을 unix의 time() 시스템 호출이 변환하는 32비트 정수 값으로 변환하는 함수이다. (FROM_UNIXTIME은 반대로 정수형 데이터를 date 타입 값으로 변환하는 함수이다.)\n\nUNIX_TIMESTAMP(datetime_value)\n\nMach&gt; CREATE table unix_table (c1 int);\nCreated successfully.\n \nMach&gt; INSERT INTO unix_table VALUES (UNIX_TIMESTAMP('2001-01-01'));\n1 row(s) inserted.\n \nMach&gt; SELECT * FROM unix_table;\nC1\n--------------\n978274800\n[1] row(s) selected.\n\n\nUPPER\n\n이 함수는 주어진 영문 컬럼의 내용을 대문자로 변환한다.\n\nUPPER(string_value)\n\nMach&gt; CREATE TABLE upper_table(id INTEGER,name VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO upper_table VALUES(1, '');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO upper_table VALUES(2, 'James');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO upper_table VALUES(3, 'sarah');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO upper_table VALUES(4, 'THOMAS');\n1 row(s) inserted.\n \nMach&gt; SELECT id, UPPER(name) FROM upper_table;\nid          UPPER(name)\n----------------------------\n4           THOMAS\n3           SARAH\n2           JAMES\n1           NULL\n[4] row(s) selected.\n\n\nVARIANCE / VAR_POP\n\n이 함수는 집계 함수로써, 주어진 숫자형 컬럼 값들의 분산값을 리턴한다. Variance함수는 샘플에 대한 분산을, VAR_POP함수는 모집단에 대한 분산값을 리턴한다.\n\nVARIANCE(column_name)\nVAR_POP(column_name)\n\nMach&gt; CREATE TABLE var_table(c1 INTEGER, c2 DOUBLE);\nCreated successfully.\n \nMach&gt; INSERT INTO var_table VALUES (1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO var_table VALUES (2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO var_table VALUES (1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO var_table VALUES (2, 2);\n1 row(s) inserted.\n \nMach&gt; SELECT VARIANCE(c1) FROM var_table;\nVARIANCE(c1)\n------------------------------\n0.333333\n[1] row(s) selected.\n \nMach&gt; SELECT VAR_POP(c1) FROM var_table;\nVAR_POP(c1)\n------------------------------\n0.25\n[1] row(s) selected.\n\n\nYEAR / MONTH / DAY\n\n이 함수들은 입력된 datetime 컬럼값에서 해당하는 연, 월, 일을 추출하여 정수형 값으로 반환하는 함수이다.\n\nYEAR(datetime_col)\nMONTH(datetime_col)\nDAY(datetime_col)\n\nMach&gt; CREATE TABLE extract_table(c1 DATETIME, c2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO extract_table VALUES (to_date('2001-01-01 12:30:00 000:000:000'), 1);\n1 row(s) inserted.\n \nMach&gt; SELECT YEAR(c1), MONTH(c1), DAY(c1) FROM extract_table;\nyear(c1)    month(c1)   day(c1)\n----------------------------------------\n2001        1           1\n\n\nISNAN / ISINF\n\n이 함수는 인자로 받은 숫자형 값이 NaN / Inf 값인지를 판단한다. NaN / Inf 값인 경우 1 그렇지 않은 경우 0 을 반환한다.\n\nISNAN(number)\nISINF(number)\n\nMach&gt; SELECT * FROM test;\nI1                          I2                          I3    \n------------------------------------------------------------------------\n1                           1                           1   \nnan                         inf                         0    \nNULL                        NULL                        NULL    \n[3] row(s) selected.\n \n \nMach&gt; SELECT ISNAN(i1), ISNAN(i2), ISNAN(i3), i3 FROM test ;\nISNAN(i1)   ISNAN(i2)   ISNAN(i3)   i3\n-----------------------------------------------------\n0           0           0           1\n1           0           0           0\nNULL        NULL        NULL        NULL\n[3] row(s) selected.\n \nMach&gt; SELECT * FROM test WHERE ISNAN(i1) = 1;\nI1                          I2                          I3\n------------------------------------------------------------------------\nnan                         inf                         0\n[1] row(s) selected.\n\n\n내장 함수 지원 타입\n\n\n  \n    \n       \n      Short\n      Integer\n      Long\n      Float\n      Double\n      Varchar\n      Text\n      Ipv4\n      Ipv6\n      Datetime\n      Binary\n    \n  \n  \n    \n      ABS\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      ADD_TIME\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n    \n    \n      AVG\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      BITAND / BITOR\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      COUNT\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      DATE_TRUNC\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n    \n    \n      DECODE\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      x\n      o\n      x\n    \n    \n      FIRST / LAST\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      FROM_UNIXTIME\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      FROM_TIMESTAMP\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      GROUP_CONCAT\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      INSTR\n      x\n      x\n      x\n      x\n      x\n      o\n      o\n      x\n      x\n      x\n      x\n    \n    \n      LEAST / GREATEST\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      LENGTH\n      x\n      x\n      x\n      x\n      x\n      o\n      o\n      x\n      x\n      x\n      o\n    \n    \n      LOWER\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      LPAD / RPAD\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      LTRIM / RTRIM\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      MAX\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      MIX\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      NVL\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      o\n      x\n      x\n      x\n    \n    \n      ROUND\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      ROWNUM\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n    \n    \n      SERIESNUM\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n    \n    \n      STDDEV / STDDEV_POP\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      SUBSTR\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      SUBSTRING_INDEX\n      x\n      x\n      x\n      x\n      x\n      o\n      o\n      x\n      x\n      x\n      x\n    \n    \n      SUM\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      SYSDATE / NOW\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_CHAR\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      TO_DATE / TO_DATE_SAFE\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_HEX\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n    \n    \n      TO_IPV4 / TO_IPV4_SAFE\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_IPV6 / TO_IPV6_SAFE\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_NUMBER / TO_NUMBER_SAFE\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_TIMESTAMP\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n    \n    \n      TRUNC\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TS_CHANGE_COUNT\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      o\n      o\n      o\n      x\n    \n    \n      UNIX_TIMESTAMP\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      UPPER\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      VARIANCE / VAR_POP\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      YEAR / MONTH / DAY\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n    \n    \n      ISNAN / ISINF\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n  \n\n\nJSON 관련 함수\n\n이 함수들은 입력된 JSON 데이터 타입을 인자로 받아 사용한다.\n\n\n  \n    \n      함수명\n      설명\n      비고\n    \n  \n  \n    \n      JSON_EXTRACT(JSON 칼럼명, ‘json path’)\n      해당 값을 string type으로 출력한다.(해당 객체가 없을 경우 ERROR를 출력한다.)\n      - JSON 객체 혹은 배열형 : 포함된 모든 객체를 문자열로 변환해서 리턴 - 문자열 : 그대로 리턴- 숫자형 :문자열로 변환하여 리턴- boolean 형 : “True” or “False” 리턴\n    \n    \n      JSON_EXTRACT_DOUBLE(JSON 칼럼명, ‘json path’)\n      해당 값을 부동소수점 64비트 double type으로 출력한다.(해당 객체가 없을 경우 NULL을 출력한다.)\n      - JSON 객체 혹은 배열형 : NULL 리턴- 문자열 : 변환하여 리턴하고, 변환 실패시 NULL 리턴- 숫자형 : 64비트 실수 리턴- boolean 형 : “True”는 1.0, “False”는 0.0 리턴\n    \n    \n      JSON_EXTRACT_INTEGER(JSON 칼럼명, ‘json path’)\n      해당 값을 64비트 integer type으로 출력한다.(해당 객체가 없을 경우 NULL을 출력한다.)\n      - JSON 객체 혹은 배열형 : NULL 리턴- 문자열 : 변환하여 리턴하고, 변환 실패시 NULL 리턴- 숫자형 : 64비트 정수 리턴- boolean 형 : “True”는 1 , “False”는 0 리턴\n    \n    \n      JSON_EXTRACT_STRING(JSON 칼럼명, ‘json path’)\n      해당 값을 string type으로 출력한다.(해당 객체가 없을 경우 NULL을 출력한다.)operator(-&gt;)와 같은 결과를 출력한다.\n      - JSON 객체 혹은 배열형 : 포함된 모든 객체를 문자열로 변환해서 리턴- 문자열 : 그대로 리턴- 숫자형 :문자열로 변환하여 리턴- Boolean 형 : “True”, “Fase” 리턴\n    \n    \n      JSON_IS_VALID(‘json string’)\n      json string이 json format에 유효한지 확인한다.\n      - 0 : False- 1 : True\n    \n    \n      JSON_TYPEOF(JSON 칼럼명, ‘json path’)\n      해당 값의 타입을 반환한다.\n      - None : 해당 키가 존재하지 않음- Object : 객체형- Integer : 정수형- Real : 실수형- String : 문자형- True/False : Boolean- Array :배열형- Null : NULL\n    \n  \n\n\nMach&gt; CREATE TABLE jsontbl (name VARCHAR(20), jval JSON);\nCreated successfully.\n \nMach&gt; INSERT INTO jsontbl VALUES(\"name1\", '{\"name\":\"test1\"}');\n1 row(s) inserted.\nMach&gt; INSERT INTO jsontbl VALUES(\"name2\", '{\"name\":\"test2\", \"value\":123}');\n1 row(s) inserted.\nMach&gt; INSERT INTO jsontbl VALUES(\"name3\", '{\"name\":{\"class1\": \"test3\"}}');\n1 row(s) inserted.\nMach&gt; INSERT INTO jsontbl VALUES(\"name4\", '{\"myarray\": [1, 2, 3, 4]}');\n1 row(s) inserted.\nMach&gt; INSERT INTO jsontbl VALUES(\"name5\", '{\"name\":\"error\"');\n[ERR-02233: Error occurred at column (2): (Error in json load.)]\n \nMach&gt; SELECT name, JSON_EXTRACT_STRING(jval, '$.name') FROM jsontbl;\nname                  JSON_EXTRACT_STRING(jval, '$.name')                                              \n-----------------------------------------------------------------------------------------------------------\nname4                 NULL                                                                             \nname3                 {\"class1\": \"test3\"}                                                              \nname2                 test2                                                                            \nname1                 test1                                                                            \n[4] row(s) selected.\n \nMach&gt; SELECT name, JSON_EXTRACT_INTEGER(jval, '$.myarray[1]') FROM jsontbl;\nname                  JSON_EXTRACT_INTEGER(jval, '$.myarray[1]')\n--------------------------------------------------------------------\nname4                 2                                         \nname3                 NULL                                      \nname2                 NULL                                      \nname1                 NULL                                      \n[4] row(s) selected.\n \nMach&gt; SELECT name, JSON_TYPEOF(jval, '$.name') FROM jsontbl;\nname                  JSON_TYPEOF(jval, '$.name')                                                      \n-----------------------------------------------------------------------------------------------------------\nname4                 None                                                                             \nname3                 Object                                                                           \nname2                 String                                                                           \nname1                 String                                                                           \n[4] row(s) selected.\n\n\nJSON Operator\n\nJSON 데이터의 object에 접근할 때 ‘-&gt;’ operator를 사용할 수 있다.\n\nJSON_EXTRACT_STRING과 같은 결과를 반환한다.\n\njson_col -&gt; 'json path'\n\nMach&gt; SELECT name, jval-&gt;'$.name' FROM jsontbl;\nname                  JSON_EXTRACT_STRING(jval, '$.name')                                              \n-----------------------------------------------------------------------------------------------------------\nname4                 NULL                                                                             \nname3                 {\"class1\": \"test3\"}                                                              \nname2                 test2                                                                            \nname1                 test1                                                                            \n[4] row(s) selected.\n \nMach&gt; SELECT name, jval-&gt;'$.myarray[1]' FROM jsontbl;\nname                  JSON_EXTRACT_INTEGER(jval, '$.myarray[1]')\n--------------------------------------------------------------------\nname4                 2                                         \nname3                 NULL                                      \nname2                 NULL                                      \nname1                 NULL                                      \n[4] row(s) selected.\n \nMach&gt; SELECT name, jval-&gt;'$.name.class1' FROM jsontbl;\nname                  jval-&gt;'$.name.class1'                                                            \n-----------------------------------------------------------------------------------------------------------\nname4                 NULL                                                                             \nname3                 test3                                                                            \nname2                 NULL                                                                             \nname1                 NULL                                                                             \n[4] row(s) selected"
					}
					
				
		
				
					,
					
					"feature-table-log-insert-import-data-html": {
						"id": "feature-table-log-insert-import-data-html",
						"title": "데이터 불러오기 : import",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/insert/import-data.html",
						"content": "machloader 도구를 이용하여, CSV 또는 다른 구분자로 구별된 텍스트 파일을 입력할 수 있다.\n\nmachloader 도구에 대한 자세한 설명은 machloader 문서를 참조한다."
					}
					
				
		
				
					,
					
					"": {
						"id": "",
						"title": "마크베이스 매뉴얼",
						"version": "all",
						"categories": "",
						"url": " /",
						"content": "개요\n\n\n  마크베이스 소개\n  마크베이스 특징\n  마크베이스 제품군 소개\n\n\n설치\n\n\n  패키지 개요\n  리눅스 설치\n  Windows 설치\n  라이선스 설치\n  Cluster Edition 설치\n\n\n업그레이드 / 복구\n\n\n  Cluster Edition 업그레이드 및 복구\n\n\n주요기능 / 테이블\n\n\n  태그 테이블 (Tag Table)\n  로그 테이블 (Log Table)\n  휘발성 테이블 (Volatile Table)\n  참조 테이블 (Lookup Table)\n  스트림 (Stream)\n  백업 및 마운트\n  데이터 자동 삭제\n\n\n도구\n\n\n  유틸리티 모음\n  machcoordinatoradmin\n  machdeployeradmin\n\n\n설정 / 모니터링\n\n  Meta Table\n  Virtual Table\n  Property\n  Property (Cluster)\n\n\nSQL Reference\n\n  자료형\n  DDL\n  DML\n  SELECT\n  SELECT Hint\n  사용자 관리\n  지원 함수\n  시스템/세션 관리\n\n\nSDK\n\n\n  CLI/ODBC\n  CLI/ODBC 예제\n  JDBC\n  Python\n  RESTful API\n  .NET Connector\n  Timezone\n\n\nRelease Note\n\n\n  7.0 Release Note\n  7.5 Release Note\n\n\n자주 묻는 질문(FAQ)\n\n\n  쿼리 에러를 Property 수정하여 해결하는 방법"
					}
					
				
		
				
					,
					
					"feature-table-log-insert-insert-data-html": {
						"id": "feature-table-log-insert-insert-data-html",
						"title": "데이터 입력 : Insert",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/insert/insert-data.html",
						"content": "다른 상용 RDBMS와 유사하게, 테이블을 먼저 생성하고 데이터는 INSERT INTO 문을 이용하여 데이터를 입력할 수 있다.\n\n마크베이스는 ‘machsql’ 도구를 대화형 질의 처리기로 제공한다.\n\n목차\n\n  테이블 생성\n  데이터 입력\n  데이터 입력 확인\n  전체 과정\n\n\n테이블 생성\n\nCREATE TABLE table_name (\n    column1 datatype,\n    column2 datatype,\n    column3 datatype,\n    ....\n);\nCREATE TABLE sensor_data\n(\n    id VARCHAR(32),\n    val DOUBLE\n);\n\n\n데이터 입력\n\nINSERT INTO table_name\nVALUES (value1, value2, value3, ...);\n\n\nINSERT INTO sensor_data VALUES('sensor1', 10.1);\nINSERT INTO sensor_data VALUES('sensor2', 20.2);\nINSERT INTO sensor_data VALUES('sensor3', 30.3);\n\n\n데이터 입력 확인\n\nSELECT column1, column2, ...\nFROM table_name;\n\n\nSELECT * FROM sensor_data;\n\n\n전체 과정\n\n아래는 machsql 을 사용한 예제이다.\n\nMach&gt; CREATE TABLE sensor_data (id VARCHAR(32), val DOUBLE);\nCreated successfully.\nMach&gt; INSERT INTO sensor_data VALUES('sensor1', 10.1);\n1 row(s) inserted.\nMach&gt; INSERT INTO sensor_data VALUES('sensor2', 20.2);\n1 row(s) inserted.\nMach&gt; INSERT INTO sensor_data VALUES('sensor3', 30.3);\n1 row(s) inserted.\nMach&gt; SELECT * FROM sensor_data;\nID                                VAL\n-----------------------------------------------------------------\nsensor3                           30.3\nsensor2                           20.2\nsensor1                           10.1\n[3] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-table-volatile-insert-update-html": {
						"id": "feature-table-volatile-insert-update-html",
						"title": "휘발성 테이블의 입력 및 갱신",
						"version": "all",
						"categories": "",
						"url": " /feature-table/volatile/insert-update.html",
						"content": "데이터 입력 (Insert)\n\n휘발성 테이블의 데이터 입력(Insert)은 다음과 같다.\n\nMach&gt; create volatile table vtable (id integer, name varchar(20));\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(3, 'north device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(4, 'south device');\n1 row(s) inserted.\n\n\n데이터 입력 (Append)\n\n마크베이스에서 제공하는 빠른 실시간 데이터 입력 API이다.\n\nC, C++, C#, Java, Python, PHP, Javascript 를 이용하여 입력할 수 있다.\n\nMach&gt; create volatile table vtable (id integer, value double);\n\nSQL_APPEND_PARAM sParam[2];\nfor(int i=0; i&lt;10000; i++)\n{\n    sParam[0].mInteger  = i;\n    sParam[1].mDouble   = i;\n    SQLAppendDataV2(stmt, sParam) != SQL_SUCCESS)\n}\n\n\nCluster Edition의 경우 Append 입력은 Leader Broker에서 수행해야 한다.\n\n세부 내용은 SDK 가이드를 참고한다.\n\n데이터 갱신\n\n휘발성 테이블의 데이터 입력 시, ON DUPLICATE KEY UPDATE 절을 사용해 중복된 기본 키 값을 가진 데이터의 갱신을 할 수 있다.\n\n삽입할 데이터 값으로 갱신\nINSERT 구문에서 삽입할 데이터를 지정했지만, 삽입 데이터의 기본 키 값과 일치하는 다른 데이터가 존재하는 경우에는 INSERT 구문이 실패하게 되고 해당 데이터는 삽입되지 않는다. 삽입 데이터의 기본 키 값과 일치하는 다른 데이터가 존재하는 경우에, 삽입이 아닌 해당 데이터를 갱신하고자 하는 경우에는 ON DUPLICATE KEY UPDATE 절을 추가할 수 있다.\n\n\n  기본 키 중복 데이터가 존재하지 않는 경우, 삽입할 데이터 내용이 그대로 삽입된다.\n  기본 키 중복 데이터가 존재하는 경우, 삽입할 데이터 내용으로 기존의 데이터가 갱신된다.\n\n\n이 기능을 사용하기 위한 제약 조건은 다음과 같다.\n\n  휘발성 테이블에 기본 키가 지정되어 있어야 한다.\n  삽입하고자 하는 값에, 기본 키 값이 반드시 포함되어야 한다.\n\n\nMach&gt; create volatile table vtable (id integer primary key, direction varchar(10), refcnt integer);\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west', 0);\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east', 0);\n1 row(s) inserted.\nMach&gt; select * from vtable;\nID          DIRECTION   REFCNT     \n----------------------------------------\n1           west       0          \n2           east        0          \n[2] row(s) selected.\n \nMach&gt; insert into vtable values(1, 'south', 0);\n[ERR-01418 : The key already exists in the unique index.]\nMach&gt; insert into vtable values(1, 'south', 0) on duplicate key update;\n1 row(s) inserted.\n \nMach&gt; select * from vtable;\nID          DIRECTION   REFCNT     \n----------------------------------------\n1           south        0          \n2           east        0          \n[2] row(s) selected.\n\n\n갱신할 데이터 값을 지정\n\n위와 비슷하지만, 삽입할 데이터 값과 다른 컬럼 값으로 갱신해야 하는 경우에는 ON DUPLICATE KEY UPDATE SET 절을 통해 지정할 수 있다. SET 절 아래에 갱신할 데이터 값을 지정할 수 있다.\n\n\n  기본 키 중복 데이터가 존재하지 않는 경우, 삽입 데이터 내용이 그대로 삽입된다.\n  기본 키 중복 데이터가 존재하는 경우, SET 절에 명시된 갱신 데이터만으로 기존의 데이터가 갱신된다.\n  기본 키 값을 갱신할 데이터 값으로 지정할 수 없다.\n  SET 절에서 명시되지 않은 컬럼들의 값은 갱신되지 않는다.\n\n\nMach&gt; create volatile table vtable (id integer primary key, direction varchar(10), refcnt integer);\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west', 0);\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east', 0);\n1 row(s) inserted.\nMach&gt; select * from vtable;\nID          DIRECTION   REFCNT     \n----------------------------------------\n1           west        0          \n2           east        0          \n[2] row(s) selected.\n \nMach&gt; insert into vtable values(1, 'west', 0) on duplicate key update set refcnt = 1;\n1 row(s) inserted.\n \nMach&gt; select * from vtable;\nID          DIRECTION   REFCNT     \n----------------------------------------\n1           west        1          \n2           east        0          \n[2] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-table-lookup-insert-html": {
						"id": "feature-table-lookup-insert-html",
						"title": "참조 데이터의 입력",
						"version": "all",
						"categories": "",
						"url": " /feature-table/lookup/insert.html",
						"content": "휘발성 테이블의 입력 및 갱신 방법과 대부분 동일하다.\n\n한가지 차이점이 있다면 참조 테이블에 Append를 통해 데이터를 입력할 경우 ‘LOOKUP_APPEND_UPDATE_ON_DUPKEY ‘ Property 를 설정해 Primary Key가 중복일 경우에 해당 Row를 Update 하도록 설정 가능하다.\n\n‘LOOKUP_APPEND_UPDATE_ON_DUPKEY’에 대한 세부 내용은 Property 가이드를 참고한다.\n\n참조 테이블 재로딩\n\n마크베이스 6.7 부터 참조 테이블 데이터를 룩업 노드가 관리하도록 수정되었다.\n\n룩업 노드로부터 참조 테이블 데이터를 다시 불러오고 싶다면\n\nEXEC TABLE_REFRESH 명령어를 사용하면된다.\n\nEXEC TABLE_REFRESH(lktable);"
					}
					
				
		
				
					,
					
					"feature-table-log-insert-html": {
						"id": "feature-table-log-insert-html",
						"title": "로그 데이터의 입력",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/insert.html",
						"content": "마크베이스에 로그 데이터를 입력하는 방법은 여러 가지가 있다.\n\n\n  데이터 입력 : Insert\n  데이터 입력 : Append\n  데이터 불러오기 : Export\n  데이터 불러오기 : Load"
					}
					
				
		
				
					,
					
					"feature-table-tag-manipulate-insert-html": {
						"id": "feature-table-tag-manipulate-insert-html",
						"title": "태그 데이터의 입력",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/manipulate/insert.html",
						"content": "목차\n\n\n  INSERT 구문을 통해 입력하기\n  CSV 파일을 통해 한꺼번에 로딩하기\n  RESTful API를 통해 입력하기\n  SDK를 통해 데이터 입력하기\n\n\n태그 데이터를 입력하기 위해서는 아래와 같은 다양한 방법을 활용할 수 있다.\n\nINSERT 구문을 통해 입력하기\n\n가장 간단한 방법으로 아래와 같이 INSERT 구문을 통해 입력할 수 있다.\n\n간단하게 테스트 용도로 할 수 있는 방법이고, 만일 대량의 데이터를 빨리 넣고자 할 경우에는 다른 방법을 활용한다.\n\nMach&gt; create tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\nExecuted successfully.\n \nMach&gt;  insert into tag metadata values ('TAG_0001');\n1 row(s) inserted.\n \nMach&gt; insert into tag values('TAG_0001', now, 0);\n1 row(s) inserted.\n \nMach&gt; insert into tag values('TAG_0001', now, 1);\n1 row(s) inserted.\n \nMach&gt; insert into tag values('TAG_0001', now, 2);\n1 row(s) inserted.\n \nMach&gt; select * from tag where name = 'TAG_0001';\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0001              2018-12-19 17:41:37 806:901:728 0                          \nTAG_0001              2018-12-19 17:41:42 327:839:368 1                          \nTAG_0001              2018-12-19 17:41:43 812:782:202 2                          \n[3] row(s) selected.\n\n\n위와 같이 3개의 TAG 값을 현재의 시간으로 넣어 보았다.\n\nCSV 파일을 통해 한꺼번에 로딩하기\n\n마크베이스는 csvimport 라는 도구를 통해서 CSV 파일 대량으로 로딩할 수 있도록 해 준다.\n\n더 자세한 내용은 실제 예제를 통해서 파악할 수 있으며, 아래에 간단하게 기술한다.\n\nCSV 파일 형태 (data.csv)\n\nTAG_0001, 2009-01-28 07:03:34 0:000:000, -41.98\nTAG_0001, 2009-01-28 07:03:34 1:000:000, -46.50\nTAG_0001, 2009-01-28 07:03:34 2:000:000, -36.16\n....\n\n\n위와 같이 &lt;태그명, 시간, 값&gt; 으로 구성된 csv 파일 준비한다.\n\n물론, 태그명 TAG_0001이 존재해야 한다.\n\n## 로딩 프로그램 csvimport  사용\n\n```bash\ncsvimport -t TAG -d data.csv -F \"time YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn\" -l error.log\n\n\nTAG라는 테이블에 data.csv를 로딩한다.\n\n그리고, -F 옵션은 data.csv에 저장된 시간 포맷을 지정하는 것인데, 현재 파일은 나노 단위까지 값을 넣을 수 있도록 되어 있다.\n\n또한, -l error.log 는 입력시 발생한 에러에 대해 별도의 파일로 기록하는 것이다.\n\nRESTful API를 통해 입력하기\n\nRESTful API의 더 자세한 사용법은 다음의 활용 예제를 참고하도록 한다.\n\n입력 API 문법\n\n마크베이스는 다음과 같이 RESTful API를 제공한다.\n\n{\n \"values\":[\n     [TAG_NAME, TAG_TIME, VALUE],  # 태그명,시간,값을 입력한다. TAG 형태에 따라 부가 컬럼 추가 필요\n     [ .... ]....\n ],\n \"date_format\":\"Date Format\"       # date_format은 생략시 'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn' 로 설정된다.\n}\n\n\n정의된 TAG 스키마의  컬럼 갯수만큼의 값을 위의 구조와 일치되도록 요청한다.\n\nSDK를 통해 데이터 입력하기\n\n마크베이스는 아래와 같은 다양한 언어의 표준 개발 툴을 제공하고 있다.\n\n\n  C/C++ library\n  Java library\n  Python library\n  C# library\n\n\n이러한 라이브러리를 통해서 사용자는 자신의 환경에 따라 다양한 형태의 응용 프로그램을 작성하여 마크베이스에 대한 데이터 입력이 가능하다."
					}
					
				
		
				
					,
					
					"intro-introduction-html": {
						"id": "intro-introduction-html",
						"title": "마크베이스 소개",
						"version": "all",
						"categories": "",
						"url": " /intro/introduction.html",
						"content": "마크베이스(Machbase)는 다양한 IoT 환경에서 발생하는 대량의 “센서 데이터”를 실시간으로 저장할 뿐만 아니라, 실시간 데이터 분석이 가능한 시계열 데이터베이스이다.\n\n마크베이스는 기존의 솔루션으로는 해결할 수 없었던 센서 데이터에 대한 데이터 저장과 처리에 대한 대량의 부하 문제를 해결하였으며, 다양한 기능을 통해 앞으로 폭증하는 센서 데이터에 대한 훌륭한 솔루션을 제공한다.\n\n새로운 데이터의 출현\n\n최근 정보통신 분야가 유무선 통신에 기반한 사물인터넷(IoT: Internet of Things) 환경으로 발전하고 있으며, 이를 통해 다양하고 방대한 양의 데이터가 축적되고 있다.\n\n따라서 이런 데이터를 실시간으로 분석할 수 있다면 이전에 발생하였거나 앞으로 발생할 수 있는 수많은 장애들을 파악하고 방지할 수 있을 것이다.\n\n특히 몇 년 사이에 데이터를 생성하는 원천 디바이스의 숫자가 급격하게 늘어나고, 이에 뒤따르는 데이터의 발생량도 몇 년 만에 수십 배로 증가하고 있다.\n\n반면 데이터를 처리하는 전통적인 소프트웨어에는 이에 대한 적절한 해결책을 제시하지 못하고 있다.\n\n전통적인 솔루션이 현재 데이터의 처리에 적합하지 않은 이유는 다음과 같다.\n\n첫째, 데이터 발생 속도가 어마어마하게 증가하고 있다.\n\n데이터 원천 소스가 늘어나고 처리해야 할 정보의 종류가 증가함에 따라 서버에 데이터가 저장되는 속도가 기존과는 완전히 다른 속도로 도착하고 있다.\n\n초당 수만 건에서 수십만 건의 데이터를 저장하기 위해 최적화된 소프트웨어 솔루션은 파일 시스템에 일반 Text 파일로 저장하는 것 외에는 해결책이 없는 실정이다.\n\n둘째, 데이터의 발생 속도와 비례하여 실시간 데이터 분석의 수요가 증가하고 있다.\n\n이런 빅데이터를 활용하여 의사 결정에 도움을 주는 것이 매우 중요하지만 기존 솔루션은 초당 수십만 건에 이르는 데이터를 실시간으로 인덱싱하고 검색 및 분석을 통해 결과를 사용자에게 전달하는 기술적 성숙도가 높지 않다.\n\n셋째, 앞에서 언급했던 “센서  데이터”의 데이터 특성이 완전히 다르다.\n\n전통적인 데이터베이스는 센서형 머신 데이터를 처리하기에 부적당한 아키텍쳐를 가지고 있으며, 이를 해결하기 위해서는 기술적으로 새로운 접근이 필요하다.\n\n이런 이유로 마크베이스는 새로운 형태의 “센서형 머신 데이터”를 처리하는 최고의 아키텍쳐를 통해 새롭게 개발되었으며, 전통적인 기술로 해결하지 못했던 실시간 데이터에 대한 저장과 처리 및 분석이 가능한 유일한 데이터베이스 솔루션이다.\n\n센서 데이터의 형태\n\n\n  \n    \n      ID\n      Time\n      Data\n    \n  \n  \n    \n      Temperature01\n      2020-01-01 00:00:00\n      20\n    \n    \n      Pressure01\n      2020-01-01 00:00:00\n      0.98\n    \n  \n\n\nID\n\n이 값은, 해당 머신 데이터가 발생한 디바이스(원천 소스)의 유일성을 나타내는 기호 및 숫자를 나타낸다. 해당 머신이나 센서의 일련 번호로 구성되며 32비트 혹은 64비트의 정수로 표현된다\n\nTIME\n\n이 값은, 해당 머신 데이터가 발생한 순간의 시간을 나타낸다. 이 시간은 지속적으로 증가하는 경향이 있으며, 일반적으로 초 단위까지 표현하고 특별한 경우 나노초(nanosecond)까지 표현되기도 한다.\n\nDATA\n\n이 값은, 2진 데이터로 주로 정수형, 실수형 숫자 혹은 IP 주소값 등의 형식의 데이터이다. 대표적으로 이 영역에는 특정 센서에서 나오는 온도, 가속도, 밝기 등의 숫자형 값이나, IPv4 혹은 IPv6와 같은 4바이트 혹은 16 바이트의 고정 데이터가 포함된다.\n\n마크베이스는 Tag Table을 제공하여, 위 형태의 센서 데이터를 초당 수십만건을 받아들일 수 있는 최적화된 아키텍처를 제공한다.\n\nPLC에서의 센서 데이터 구조\n\ndata from PLC\nTime          SN01M  SN02M  SN03M  SN04M  SN05M  SN06M  SN07M  SN08M  SN09M  SN10M\n04:01:56.005    11.1      1      0      0      0      1      0      0      0      0\n04:01:56.057    11.3      1      0      0      0      1      0      0      0      1\n04:01:56.109    11.1      1      0      1      0      1      0      1      1      0\n04:01:56.161    12.3      1      0      0      0      1      0      0      0      1\n04:01:56.213     9.1      1      1      0      0      1      0      0      0      0\n04:01:56.266     0.9      1      0      0      1      1      0      0      0      0\n04:01:56.319     8.9      1      0      1      0      1      0      1      0      1\n04:01:56.370     1.3      1      0      0      0      1      0      0      0      0\n04:01:56.422    33.1      1      0      0      0      1      0      0      0      0\n04:01:56.474     3.3      1      0      0      0      1      0      0      0      0\n04:01:56.526     5.6      1      0      0      1      1      0      0      0      1\n04:01:56.578     5.5      1      0      0      0      1      0      0      1      0\n04:01:56.630     4.5      1      0      0      0      1      0      0      0      0\n04:01:56.682     5.3      1      0      0      0      1      0      0      0      0\n04:01:56.733     1.2      1      0      0      0      1      0      0      0      1\n04:01:56.785     3.4      1      0      0      1      1      0      0      0      0\n04:01:56.838    11.3      1      0      1      0      1      0      1      0      0\n04:01:56.890    11.2      1      0      0      0      1      0      0      0      0\n04:01:56.942     9.9      1      0      0      0      1      0      0      0      1\n04:01:56.994     8.4      1      0      0      0      1      0      0      0      0\n04:01:57.046     8.4      1      0      1      0      1      0      0      0      0\n04:01:57.097     1.2      1      0      0      1      1      0      1      1      1\n04:01:57.149     1.3      1      0      0      0      1      0      0      0      0\n04:01:57.200    11.2      1      0      0      0      1      0      0      0      0\n04:01:57.252     3.1      1      0      0      0      1      0      0      0      0\n\n\n마크베이스는 Log Table을 제공하여, 위와 같은 형태의 PLC 데이터를 저장할 수 있는 구조 또한 제공한다.\n\n이 테이블 역시 초당 수만건의 데이터를 입력하고, 분석할 수 있는 능력을 제공한다."
					}
					
				
		
				
					,
					
					"sdk-jdbc-html": {
						"id": "sdk-jdbc-html",
						"title": "JDBC",
						"version": "all",
						"categories": "",
						"url": " /sdk/jdbc.html",
						"content": "목차\n\n\n  목차\n  JDBC 개요\n  표준 JDBC 함수\n  확장 JDBC 함수\n    \n      setIPv4\n      setIPv6\n      executeAppendOpen\n      executeAppendData\n      executeAppendDataByTime\n      executeAppendClose\n      executeSetAppendErrorCallback\n      getAppendSuccessCount\n      getAppendFailCount\n    \n  \n  응용 프로그램 개발\n    \n      JDBC 라이브러리 설치 확인\n      Makefile 작성 가이드\n      컴파일 및 링크\n    \n  \n  JDBC 샘플\n    \n      접속 예제\n      데이터 입력 및 출력 예제 (1) 직접 입/출력\n      데이터 입력 및 출력 예제 (2) PreparedStatement 이용한 입력\n      확장 함수 Append 예제\n    \n  \n\n\nJDBC 개요\n\n자바 프로그래밍 언어로 만들어진 데이터베이스 조작 인터페이스의 집합을 JDBC(Java DataBase Connectivity)라고 한다. 다양한 관계형 데이터베이스를 위해 일관된 인터페이스를 제공하는 API 집합으로서 프로그래머가 SQL 요구를 만드는데 사용할 일련의 객체지향 프로그램의 클래스들을 정의하고 있다. 즉, 어떤 데이터베이스를 사용하더라도 JDBC 드라이버만 제공된다면 코드 수정 없이 바로 적용 가능한 장점이 있다.\n\n표준 JDBC 함수\n\n표준 함수 스펙 4.0\n\n확장 JDBC 함수\n\nsetIPv4\n\nvoid setIpv4(int ind, String ipString)\n\n\nPrepareStatement에서 IPv4 주소 타입을 입력하기 위한 함수이다.\n\n컬럼 인덱스와 IPv4 문자열을 인자로 받는다.\n\nsetIPv6\n\nvoid setIpv6(int ind, String ipString)\n\nPrepareStatement에서 IPv6 주소 타입을 입력하기 위한 함수이다.\n\n컬럼 인덱스와 IPv6 문자열을 인자로 받는다.\n\nexecuteAppendOpen\n\nResultSet executeAppendOpen(String aTableName, int aErrorCheckCount)\n\n\nStatement에서 Append 프로토콜을 쓰기 위한 것으로 프로토콜을 오픈한다.\n\n테이블 이름과 오류 검사 간격을 인자로 받는다. 결과값으로 ResultSet을 리턴한다.\n\nexecuteAppendData\n\nint executeAppendData(ResultSetMetaData rsmd, ArrayList aData)\n\n\nStatement에서 Append 프로토콜을 위한 것으로 실제 데이터를 입력한다.\n\nexecuteAppendOpen의 결과값인 ResultSet의 메타데이터와 입력하고자 하는 데이터를 인자로 받는다. 결과값이 전송 버퍼에 저장되면 1이 리턴되고, 전송 버퍼가 차서 마크베이스로 전송되면 2가 리턴된다. 따라서 1 또는 2가 리턴되면 성공으로 판단하면 된다.\n\nexecuteAppendDataByTime\n\nint executeAppendDataByTime(ResultSetMetaData rsmd, long aTime, ArrayList aData)\n\n\nStatement에서 Append 프로토콜을 위한 것으로 실제 데이터를 시간 기준으로 입력한다.\n\nexecuteAppendOpen의 결과값인 ResultSet의 메타데이터와 설정하고자 하는 특정 시간대의 시간 값, 입력하고자 하는 데이터를 인자로 받는다. 결과값이 전송 버퍼에 저장되면 1이 리턴된다.\n\nexecuteAppendClose\n\nint executeAppendClose()\n\n\nStatement에서 Append 프로토콜을 위한 것으로 statement를 종료한다.\n\n결과값으로 성공하면 1을 리턴한다.\n\nexecuteSetAppendErrorCallback\n\nint executeSetAppendErrorCallback(MachAppendCallback aCallback)\n\n\nAppend 수행하는 도중에 에러가 발생하는 경우 에러를 출력하는 콜백 함수를 설정한다.\n\n에러 로그를 출력하는 콜백 함수를 인자로 받는다. 결과값으로 성공하면 1이 리턴된다.\n\ngetAppendSuccessCount\n\nlong getAppendSuccessCount()\n\n\nStatement에서 Append 프로토콜을 위한 것으로 성공한 개수를 리턴한다.\n\n결과값으로 성공한 개수를 리턴한다.\n\ngetAppendFailCount\n\nlong getAppendFailCount()\n\nStatement에서 Append 프로토콜을 위한 것으로 실패한 개수를 리턴한다.\n\n결과값으로 실패한 개수를 리턴한다.\n\n응용 프로그램 개발\n\nJDBC 라이브러리 설치 확인\n\n$MACHBASE_HOME/lib 디렉터리에 machbase.jar 파일이 있는지 확인한다.\n\n[mach@localhost ~]$ cd $MACHBASE_HOME/lib\n[mach@localhost lib]$ ls -l machbase.jar\n-rw-rw-r-- 1 mach mach 78599 Jun 18 10:00 machbase.jar\n[mach@localhost lib]$\n\n\nMakefile 작성 가이드\n\n$(MACHBASE_HOME)/lib/machbase.jar를 classpath에 지정해주어야 한다. 다음은 Makefile 예시이다.\n\nCLASSPATH=\".:$(MACHBASE_HOME)/lib/machbase.jar\"\n \nSAMPLE_SRC = Sample1Connect.java Sample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n \nall: build\n \nbuild:\n    -@rm -rf *.class\n    javac -classpath $(CLASSPATH) -d . $(SAMPLE_SRC)\n \ncreate_table:\n    machsql -s localhost -u sys -p manager -f createTable.sql\n \nselect_table:\n    machsql -s localhost -u sys -p manager -f selectTable.sql\n \nrun_sample1:\n    java -classpath $(CLASSPATH) Sample1Connect\n \nrun_sample2:\n    java -classpath $(CLASSPATH) Sample2Insert\n \nrun_sample3:\n    java -classpath $(CLASSPATH) Sample3PrepareStmt\n \nrun_sample4:\n    java -classpath $(CLASSPATH) Sample4Append\n \nclean:\n    rm -rf *.class\n\n\n컴파일 및 링크\n\n다음과 같이 make 명령어를 수행하여 컴파일 및 링크를 수행한다.\n\n[mach@localhost jdbc]$ make\njavac -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" -d . Sample1Connect.java Sample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n[mach@localhost jdbc]$\n\n\nJDBC 샘플\n\n접속 예제\n\n마크베이스 JDBC 드라이버를 이용하여 마크베이스 서버에 접속하는 예제 프로그램을 작성해 보기로 한다. 소스 파일명을 Sample1Connect.java로 한다.\n\n_arrival_time 컬럼은 디폴트로 표시되지 않는다. 따라서 _arrival_time 컬럼을 표시하려면, 연결 문자열에 show_hidden_cols=1 을 추가하면 된다.\n\n아래 예제 소스에서 접속 문자열을 다음과 같이 수정하면 된다.\n\nString sURL = “jdbc:machbase://localhost:5656/mhdb?show_hidden_cols=1”;\n\nimport java.util.*;\nimport java.sql.*;\nimport com.machbase.jdbc.*;\n \npublic class Sample1Connect\n{\n    public static Connection connect()\n    {\n        Connection conn = null;\n        try\n        {\n            String sURL = \"jdbc:machbase://localhost:5656/mhdb\";\n \n            Properties sProps = new Properties();\n            sProps.put(\"user\", \"sys\");\n            sProps.put(\"password\", \"manager\");\n \n            Class.forName(\"com.machbase.jdbc.driver\");\n            conn = DriverManager.getConnection(sURL, sProps);\n        }\n        catch ( ClassNotFoundException ex )\n        {\n            System.err.println(\"Exception : unable to load mach jdbc driver class\");\n        }\n        catch ( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        return conn;\n    }\n \n    public static void main(String[] args) throws Exception\n    {\n        Connection conn = null;\n \n        try\n        {\n            conn = connect();\n            if( conn != null )\n            {\n                System.out.println(\"mach JDBC connected.\");\n            }\n        }\n        catch( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        finally\n        {\n            if( conn != null )\n            {\n                conn.close();\n                conn = null;\n            }\n        }\n    }\n}\n\n\n이제 소스 코드를 컴파일하고 실행한다. 이미 작성한 Makefile을 이용한다.\n\n[mach@localhost jdbc]$ make\njavac -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" -d . Sample1Connect.java Sample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n[mach@localhost jdbc]$ make run_sample1\njava -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" Sample1Connect\nmach JDBC connected.\n\n\n데이터 입력 및 출력 예제 (1) 직접 입/출력\n\n마크베이스 JDBC 드라이버를 이용하여 데이터를 입력하고 출력하는 예제를 작성하여 보기로 한다.\n\n소스 파일명은 Sample2Insert.java 라고 한다.\n\n먼저, machsql 프로그램을 이용하여 필요한 테이블을 생성하여야 한다.\n예제에서는 sample_table이라는 테이블을 미리 생성한 뒤에 샘플 코드를 이용하는 방식을 사용했다.\n\n[mach@localhost jdbc]$ machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version 3.5.0.826b8f2.official\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase server address (Default:127.0.0.1):\nMachbase rser ID  (Default:SYS)\nMachbase user password: MANAGER\nMACHBASE_CONNECT_MODE=INET, PORT=5656\nmach&gt; create table sample_table(d1 short, d2 integer, d3 long, f1 float, f2 double, name varchar(20), text text, bin binary, v4 ipv4, v6 ipv6, dt datetime);\nCreated successfully.\nmach&gt; exit\n[mach@localhost jdbc]$\n\n\nimport java.util.*;\nimport java.sql.*;\nimport com.machbase.jdbc.*;\n \npublic class Sample2Insert\n{\n    public static Connection connect()\n    {\n        Connection conn = null;\n        try\n        {\n \n            String sURL = \"jdbc:machbase://localhost:5656/mhdb\";\n \n            Properties sProps = new Properties();\n            sProps.put(\"user\", \"sys\");\n            sProps.put(\"password\", \"manager\");\n \n            Class.forName(\"com.machbase.jdbc.driver\");\n \n            conn = DriverManager.getConnection(sURL, sProps);\n \n        }\n        catch ( ClassNotFoundException ex )\n        {\n            System.err.println(\"Exception : unable to load mach jdbc driver class\");\n        }\n        catch ( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n \n        return conn;\n    }\n \n \n    public static void main(String[] args) throws Exception\n    {\n        Connection conn = null;\n        Statement stmt = null;\n        String sql;\n \n        try\n        {\n            conn = connect();\n            if( conn != null )\n            {\n                System.out.println(\"mach JDBC connected.\");\n \n                stmt = conn.createStatement();\n \n                for(int i=1; i&lt;10; i++)\n                {\n                    sql = \"INSERT INTO SAMPLE_TABLE VALUES (\";\n                    sql += (i - 5) * 6552;//short\n                    sql += \",\"+ ((i - 5) * 429496728);//integer\n                    sql += \",\"+ ((i - 5) * 922337203685477580L);//long\n                    sql += \",\"+ 1.23456789+\"e\"+((i&lt;=5)?\"\":\"+\")+((i-5)*7);//float\n                    sql += \",\"+ 1.23456789+\"e\"+((i&lt;=5)?\"\":\"+\")+((i-5)*61);//double\n                    sql += \",'id-\"+i+\"'\";//varchar\n                    sql += \",'name-\"+i+\"'\";//text\n                    sql += \",'aabbccddeeff'\";//binary\n                    sql += \",'192.168.0.\"+i+\"'\";//ipv4\n                    sql += \",'::192.168.0.\"+i+\"'\";\n                    sql += \",TO_DATE('2014-08-0\"+i+\"','YYYY-MM-DD')\";//dt\n                    sql += \")\";\n \n                    stmt.execute(sql);\n \n                    System.out.println( i+\" record inserted.\");\n                }\n \n                String query = \"SELECT d1, d2, d3, f1, f2, name, text, bin, to_hex(bin), v4, v6, to_char(dt,'YYYY-MM-DD') as dt from SAMPLE_TABLE\";\n                ResultSet rs = stmt.executeQuery(query);\n                while( rs.next () )\n                {\n                    short d1 = rs.getShort(\"d1\");\n                    int d2 = rs.getInt(\"d2\");\n                    long d3 = rs.getLong(\"d3\");\n                    float f1 = rs.getFloat(\"f1\");\n                    double f2 = rs.getDouble(\"f2\");\n                    String name = rs.getString(\"name\");\n                    String text = rs.getString(\"text\");\n                    String bin = rs.getString(\"bin\");\n                    String hexbin = rs.getString(\"to_hex(bin)\");\n                    String v4 = rs.getString(\"v4\");\n                    String v6 = rs.getString(\"v6\");\n                    String dt = rs.getString(\"dt\");\n \n                    System.out.print(\"d1: \" + d1);\n                    System.out.print(\", d2: \" + d2);\n                    System.out.print(\", d3: \" + d3);\n                    System.out.print(\", f1: \" + f1);\n                    System.out.print(\", f2: \" + f2);\n                    System.out.print(\", name: \" + name);\n                    System.out.print(\", text: \" + text);\n                    System.out.print(\", bin: \" + bin);\n                    System.out.print(\", hexbin: \"+hexbin);\n                    System.out.print(\", v4: \" + v4);\n                    System.out.print(\", v6: \" + v6);\n                    System.out.println(\", dt: \" + dt);\n \n                }\n                rs.close();\n            }\n        }\n        catch( SQLException se )\n        {\n            System.err.println(\"SQLException : \" + se.getMessage());\n        }\n        catch( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        finally\n        {\n            if( stmt != null )\n            {\n                stmt.close();\n                stmt = null;\n            }\n            if( conn != null )\n            {\n                conn.close();\n                conn = null;\n            }\n        }\n    }\n}\n\n이제 소스 코드를 컴파일하고 실행한다. 이미 작성한 Makefile을 이용한다.\n\n[mach@localhost jdbc]$ make\njavac -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" -d . Sample1Connect.java Sample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n[mach@localhost jdbc]$ make run_sample2\nmake run_sample2\njava -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" Sample2Insert\nmach JDBC connected.\n1 record inserted.\n2 record inserted.\n3 record inserted.\n4 record inserted.\n5 record inserted.\n6 record inserted.\n7 record inserted.\n8 record inserted.\n9 record inserted.\nd1: 26208, d2: 1717986912, d3: 3689348814741910320, f1: 1.2345679E28, f2: 1.23456789E244, name: id-9, text: name-9, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.9, v6: 0:0:0:0:0:0:c0a8:9, dt: 2014-08-09\nd1: 19656, d2: 1288490184, d3: 2767011611056432740, f1: 1.2345678E21, f2: 1.23456789E183, name: id-8, text: name-8, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.8, v6: 0:0:0:0:0:0:c0a8:8, dt: 2014-08-08\nd1: 13104, d2: 858993456, d3: 1844674407370955160, f1: 1.23456788E14, f2: 1.23456789E122, name: id-7, text: name-7, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.7, v6: 0:0:0:0:0:0:c0a8:7, dt: 2014-08-07\nd1: 6552, d2: 429496728, d3: 922337203685477580, f1: 1.2345679E7, f2: 1.23456789E61, name: id-6, text: name-6, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.6, v6: 0:0:0:0:0:0:c0a8:6, dt: 2014-08-06\nd1: 0, d2: 0, d3: 0, f1: 1.2345679, f2: 1.23456789, name: id-5, text: name-5, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.5, v6: 0:0:0:0:0:0:c0a8:5, dt: 2014-08-05\nd1: -6552, d2: -429496728, d3: -922337203685477580, f1: 1.2345679E-7, f2: 1.23456789E-61, name: id-4, text: name-4, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.4, v6: 0:0:0:0:0:0:c0a8:4, dt: 2014-08-04\nd1: -13104, d2: -858993456, d3: -1844674407370955160, f1: 1.2345679E-14, f2: 1.23456789E-122, name: id-3, text: name-3, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.3, v6: 0:0:0:0:0:0:c0a8:3, dt: 2014-08-03\nd1: -19656, d2: -1288490184, d3: -2767011611056432740, f1: 1.2345679E-21, f2: 1.23456789E-183, name: id-2, text: name-2, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.2, v6: 0:0:0:0:0:0:c0a8:2, dt: 2014-08-02\nd1: -26208, d2: -1717986912, d3: -3689348814741910320, f1: 1.2345679E-28, f2: 1.23456789E-244, name: id-1, text: name-1, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.1, v6: 0:0:0:0:0:0:c0a8:1, dt: 2014-08-01\n\n\n데이터 입력 및 출력 예제 (2) PreparedStatement 이용한 입력\n\nPreparedStatement를 이용하여 데이터를 입력하고 출력하는 예제를 작성하여 보기로 한다.\n\n소스 파일명은 Sample3PrepareStmt.java 로 한다.\n\nimport java.util.*;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport com.machbase.jdbc.*;\n \npublic class Sample3PrepareStmt\n{\n    public static Connection connect()\n    {\n        Connection conn = null;\n        try\n        {\n            String sURL = \"jdbc:machbase://localhost:5656/mhdb\";\n \n            Properties sProps = new Properties();\n            sProps.put(\"user\", \"sys\");\n            sProps.put(\"password\", \"manager\");\n \n            Class.forName(\"com.machbase.jdbc.driver\");\n \n            conn = DriverManager.getConnection(sURL, sProps);\n \n        }\n        catch ( ClassNotFoundException ex )\n        {\n            System.err.println(\"Exception : unable to load mach jdbc driver class\");\n        }\n        catch ( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        return conn;\n    }\n \n    public static void main(String[] args) throws Exception\n    {\n        Connection conn = null;\n        Statement stmt = null;\n        machPreparedStatement preStmt = null;\n        SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss SSS\");\n \n        try\n        {\n            conn = connect();\n            if( conn != null )\n            {\n                System.out.println(\"mach JDBC connected.\");\n \n                stmt = conn.createStatement();\n                preStmt = (machPreparedStatement)conn.prepareStatement(\"INSERT INTO SAMPLE_TABLE VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n \n                String ipStr = null;\n                String dateStr = null;\n                for(int i=1; i&lt;10; i++)\n                {\n                    ipStr = String.format(\"172.16.0.%d\",i);\n                    dateStr = String.format(\"2014-08-09 12:23:34 %03d\", i);\n                    byte[] bin = new byte[20];\n                    for(int j=0;j&lt;20;j++){\n                        bin[j]=(byte)(Math.random()*255);\n                    }\n                    java.util.Date day = sdf.parse(dateStr);\n                    java.sql.Date sqlDate = new java.sql.Date(day.getTime());\n \n                    preStmt.setShort(1, (i-5) * 3276 );\n                    preStmt.setInt(2, (i-5) * 214748364 );\n                    preStmt.setLong(3, (i-5) * 922337203685477580L );\n                    preStmt.setFloat(4, 1.23456789101112131415*Math.pow(10,i));\n                    preStmt.setDouble(5, 1.23456789101112131415*Math.pow(10,i*10));\n                    preStmt.setString(6, String.format(\"varchar-%d\",i));\n                    preStmt.setString(7, String.format(\"text-%d\",i));\n                    preStmt.setBytes(8, bin);\n                    preStmt.setIpv4(9, ipStr);\n                    preStmt.setIpv6(10, \"::\"+ipStr);\n                    preStmt.setDate(11, sqlDate);\n                    preStmt.executeUpdate();\n \n                    System.out.println( i+\" record inserted.\");\n                }\n \n                //date type format : YYYY-MM-DD HH24:MI:SS mmm:uuu:nnnn\n                String query = \"SELECT d1, d2, d3, f1, f2, name, text, bin, to_hex(bin), v4, v6, to_char(dt,'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn') as dt from SAMPLE_TABLE\";\n                ResultSet rs = stmt.executeQuery(query);\n                while( rs.next () )\n                {\n                    short d1 = rs.getShort(\"d1\");\n                    int d2 = rs.getInt(\"d2\");\n                    long d3 = rs.getLong(\"d3\");\n                    float f1 = rs.getFloat(\"f1\");\n                    double f2 = rs.getDouble(\"f2\");\n                    String name = rs.getString(\"name\");\n                    String text = rs.getString(\"text\");\n                    String bin = rs.getString(\"bin\");\n                    String hexbin = rs.getString(\"to_hex(bin)\");\n                    String v4 = rs.getString(\"v4\");\n                    String v6 = rs.getString(\"v6\");\n                    String dt = rs.getString(\"dt\");\n \n                    System.out.print(\"d1: \" + d1);\n                    System.out.print(\", d2: \" + d2);\n                    System.out.print(\", d3: \" + d3);\n                    System.out.print(\", f1: \" + f1);\n                    System.out.print(\", f2: \" + f2);\n                    System.out.print(\", name: \" + name);\n                    System.out.print(\", text: \" + text);\n                    System.out.print(\", bin: \" + bin);\n                    System.out.print(\", hexbin: \"+hexbin);\n                    System.out.print(\", v4: \" + v4);\n                    System.out.print(\", v6: \" + v6);\n                    System.out.println(\", dt: \" + dt);\n                }\n                rs.close();\n            }\n        }\n        catch( SQLException se )\n        {\n            System.err.println(\"SQLException : \" + se.getMessage());\n        }\n        catch( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        finally\n        {\n            if( stmt != null )\n            {\n                stmt.close();\n                stmt = null;\n            }\n            if( conn != null )\n            {\n                conn.close();\n                conn = null;\n            }\n        }\n    }\n}\n\n이제 소스 코드를 컴파일하고 실행본다. 이미 작성한 Makefile을 이용한다.\n\nSample2Insert.java에서 입력한 데이터가 함께 출력되고 있다는 점에 유의해야 한다.\n\n[mach@localhost jdbc]$ make\njavac -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" -d . Sample1Connect.java\nSample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n[mach@localhost jdbc]$ make run_sample3\nmake run_sample3\njava -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" Sample3PrepareStmt\nMach JDBC connected.\n1 record inserted.\n2 record inserted.\n3 record inserted.\n4 record inserted.\n5 record inserted.\n6 record inserted.\n7 record inserted.\n8 record inserted.\n9 record inserted.\nd1: 13104, d2: 858993456, d3: 3689348814741910320, f1: 754454.6, f2: 453821.380752063, name:\nvarchar-9, text: text-9, bin: ?+,??r?J?????S)n?, hexbin:\nA4C9A8D491D6728B4AACB39EE5FC5300296EFA9F, v4: 172.16.0.9, v6: 0:0:0:0:0:0:ac10:9, dt:\n2014-08-09 12:23:34 009:000:000\n?h???a?, hexbin: 6C20F09329ABBA3E7DE501C30DA368D6EFC961EF, v4: 172.16.0.8, v6:\n0:0:0:0:0:0:ac10:8, dt: 2014-08-09 12:23:34 008:000:000\nd1: 6552, d2: 429496728, d3: 1844674407370955160, f1: 2664182.0, f2: 1357910.1926900472, name:\nvarchar-7, text: text-7, bin: ????Uls?q?H?I?&amp;(?, hexbin:\nB5A0A2EFA185556C73BF719448BD49C92628F8C6, v4: 172.16.0.7, v6: 0:0:0:0:0:0:ac10:7, dt:\n2014-08-09 12:23:34 007:000:000\nd1: 3276, d2: 214748364, d3: 922337203685477580, f1: 443847.1, f2: 9342855.256576871, name:\nvarchar-6, text: text-6, bin: ??&gt;x??Eu?? ?Iw??+n, hexbin:\nBC973E78F5B44575D6CC15F94977DAE62B6E1D0E, v4: 172.16.0.6, v6: 0:0:0:0:0:0:ac10:6, dt:\n2014-08-09 12:23:34 006:000:000\nd1: 0, d2: 0, d3: 0, f1: 1283723.1, f2: 1771261.2019240903, name: varchar-5, text: text-5,\nbin: &amp;== j?j3?? T??y?\n??, hexbin: 263D3D1C6AF56A33F79D0C54A5C479A4030AFE8B, v4: 172.16.0.5, v6: 0:0:0:0:0:0:ac10:5,\ndt: 2014-08-09 12:23:34 005:000:000\nd1: -3276, d2: -214748364, d3: -922337203685477580, f1: 9447498.0, f2: 7529392.937964935,\nname: varchar-4, text: text-4, bin: ?Sw ??)? ?h2?E??/?, hexbin:\nC653771DD2DF29CDB30ED96832E745D3D7A52FD2, v4: 172.16.0.4, v6: 0:0:0:0:0:0:ac10:4, dt:\n2014-08-09 12:23:34 004:000:000\nd1: -6552, d2: -429496728, d3: -1844674407370955160, f1: 9589634.0, f2: 5994172.201347323,\nname: varchar-3, text: text-3, bin: 9aB,.????L/?=3,?`?f, hexbin:\n3961422C2EA39BE6F2964C2FCD3D332C8960A466, v4: 172.16.0.3, v6: 0:0:0:0:0:0:ac10:3, dt:\n2014-08-09 12:23:34 003:000:000\nd1: -9828, d2: -644245092, d3: -2767011611056432740, f1: 7409537.5, f2: 2313739.6613546023,\nname: varchar-2, text: text-2, bin: _? N?3 ?? ??~H ??= 8, hexbin:\n5F84144EF63320F3C718B0FD7E4809A4CB3D1838, v4: 172.16.0.2, v6: 0:0:0:0:0:0:ac10:2, dt:\n2014-08-09 12:23:34 002:000:000\nd1: -13104, d2: -858993456, d3: -3689348814741910320, f1: 596626.75, f2: 2649492.1936065694,\nname: varchar-1, text: text-1, bin: ???d??Wu$v? 7m?-, hexbin:\nE8D0C564B4EB57E59B08752476FC07376DBF2D14, v4: 172.16.0.1, v6: 0:0:0:0:0:0:ac10:1, dt:\n2014-08-09 12:23:34 001:000:000\nd1: 26208, d2: 1717986912, d3: 3689348814741910320, f1: 1.2345679E28, f2: 1.23456789E244,\nname: id-9, text: name-9, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.9, v6: 0:0:0:0:0:0:c0a8:9, dt: 2014-08-09 00:00:00 000:000:000\nd1: 19656, d2: 1288490184, d3: 2767011611056432740, f1: 1.2345678E21, f2: 1.23456789E183,\nname: id-8, text: name-8, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.8, v6: 0:0:0:0:0:0:c0a8:8, dt: 2014-08-08 00:00:00 000:000:000\nd1: 13104, d2: 858993456, d3: 1844674407370955160, f1: 1.23456788E14, f2: 1.23456789E122,\nname: id-7, text: name-7, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.7, v6: 0:0:0:0:0:0:c0a8:7, dt: 2014-08-07 00:00:00 000:000:000\nd1: 6552, d2: 429496728, d3: 922337203685477580, f1: 1.2345679E7, f2: 1.23456789E61, name:\nid-6, text: name-6, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.6, v6:\n0:0:0:0:0:0:c0a8:6, dt: 2014-08-06 00:00:00 000:000:000\nd1: 0, d2: 0, d3: 0, f1: 1.2345679, f2: 1.23456789, name: id-5, text: name-5, bin:\naabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.5, v6: 0:0:0:0:0:0:c0a8:5, dt:\n2014-08-05 00:00:00 000:000:000\nd1: -6552, d2: -429496728, d3: -922337203685477580, f1: 1.2345679E-7, f2: 1.23456789E-61,\nname: id-4, text: name-4, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.4, v6: 0:0:0:0:0:0:c0a8:4, dt: 2014-08-04 00:00:00 000:000:000\nd1: -13104, d2: -858993456, d3: -1844674407370955160, f1: 1.2345679E-14, f2: 1.23456789E-122,\nname: id-3, text: name-3, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.3, v6: 0:0:0:0:0:0:c0a8:3, dt: 2014-08-03 00:00:00 000:000:000\nd1: -19656, d2: -1288490184, d3: -2767011611056432740, f1: 1.2345679E-21, f2: 1.23456789E-183,\nname: id-2, text: name-2, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.2, v6: 0:0:0:0:0:0:c0a8:2, dt: 2014-08-02 00:00:00 000:000:000\nd1: -26208, d2: -1717986912, d3: -3689348814741910320, f1: 1.2345679E-28, f2: 1.23456789E-244,\nname: id-1, text: name-1, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.1, v6: 0:0:0:0:0:0:c0a8:1, dt: 2014-08-01 00:00:00 000:000:000\n\n\n확장 함수 Append 예제\n\n마크베이스 JDBC 드라이버는 많은 건수의 데이터를 빠르게 업로드하기 위한 Append 프로토콜을 지원한다.\n\n다음은 Append 프로토콜 사용 예제이다. \n이전 예제에 사용된 sample_table을 그대로 이용한다.\n\n소스 파일명은 Sample4Append.java 라고 한다.\ndata.txt에 있는 내용을 sample_table에 입력한다.\nCLI append 예제에 이용한 data.txt 파일을 복사하여 사용하기로 한다.\n\nimport java.util.*;\nimport java.sql.*;\nimport java.io.*;\nimport java.text.SimpleDateFormat;\nimport java.math.BigDecimal;\nimport com.machbase.jdbc.*;\n \n \npublic class Sample4Append\n{\n    protected static final String sTableName = \"sample_table\";\n    protected static final int sErrorCheckCount = 100;\n \n    public static Connection connect()\n    {\n        Connection conn = null;\n        try\n        {\n            String sURL = \"jdbc:machbase://localhost:5656/mhdb\";\n \n            Properties sProps = new Properties();\n            sProps.put(\"user\", \"sys\");\n            sProps.put(\"password\", \"manager\");\n \n            Class.forName(\"com.machbase.jdbc.driver\");\n \n            conn = DriverManager.getConnection(sURL, sProps);\n \n        }\n        catch ( ClassNotFoundException ex )\n        {\n            System.err.println(\"Exception : unable to load mach jdbc driver class\");\n        }\n        catch ( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        return conn;\n    }\n \n    public static void main(String[] args) throws Exception\n    {\n        Connection conn = null;\n        MachStatement stmt = null;\n        SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        Calendar cal = Calendar.getInstance();\n        String filename = \"data.txt\";\n \n        try\n        {\n            conn = connect();\n            if( conn != null )\n            {\n                System.out.println(\"Mach JDBC connected.\");\n \n                stmt = (MachStatement)conn.createStatement();\n \n                ResultSet rs = stmt.executeAppendOpen(sTableName, sErrorCheckCount);\n                ResultSetMetaData rsmd = rs.getMetaData();\n \n                System.out.println(\"append open ok\");\n \n                MachAppendCallback cb = new MachAppendCallback() {\n                        @Override\n                        public void onAppendError(long aErrNo, String aErrMsg, String aRowMsg) {\n                             System.out.format(\"Append Error : [%05d - %s]\\n%s\\n\", aErrNo, aErrMsg, aRowMsg);\n                        }\n                    };\n \n                stmt.executeSetAppendErrorCallback(cb);\n \n                System.out.println(\"append data start\");\n                BufferedReader in = new BufferedReader(new FileReader(filename));\n                String buf = null;\n                int cnt = 0;\n                long dt;\n \n                long startTime = System.nanoTime();\n \n                while( (buf = in.readLine()) != null )\n                {\n                    ArrayList&lt;Object&gt; sBuf = new ArrayList&lt;Object&gt;();\n                    StringTokenizer st = new StringTokenizer(buf,\",\");\n                    for(int i=0; st.hasMoreTokens() ;i++ )\n                    {\n                        switch(i){\n                            case 7://binary case\n                                sBuf.add(new ByteArrayInputStream(st.nextToken().getBytes())); break;\n                            case 10://date case\n                                java.util.Date day = sdf.parse(st.nextToken());\n                                cal.setTime(day);\n                                dt = cal.getTimeInMillis()*1000000; //make nanotime\n                                sBuf.add(dt);\n                                break;\n                            default:\n                                sBuf.add(st.nextToken()); break;\n                        }\n                    }\n \n                    if( stmt.executeAppendData(rsmd, sBuf) != 1 )\n                    {\n                        System.err.println(\"Error : AppendData error\");\n                        break;\n                    }\n \n                    if( (cnt++%10000) == 0 )\n                    {\n                        System.out.print(\".\");\n                    }\n                    sBuf = null;\n \n                }\n                System.out.println(\"\\nappend data end\");\n \n                long endTime = System.nanoTime();\n                stmt.executeAppendClose();\n                System.out.println(\"append close ok\");\n                System.out.println(\"Append Result : success = \"+stmt.getAppendSuccessCount()+\", failure = \"+stmt.getAppendFailureCount());\n                System.out.println(\"timegap \" + ((endTime - startTime)/1000) + \" in microseconds, \" + cnt + \" records\" );\n \n                try {\n                    BigDecimal records = new BigDecimal( cnt );\n                    BigDecimal gap = new BigDecimal( (double)(endTime - startTime)/1000000000 );\n                    BigDecimal rps = records.divide(gap, 2, BigDecimal.ROUND_UP );\n \n                    System.out.println( rps + \" records/second\" );\n                } catch(ArithmeticException ae) {\n                    System.out.println( cnt + \" records/second\");\n                }\n \n                rs.close();\n            }\n        }\n        catch( SQLException se )\n        {\n            System.err.println(\"SQLException : \" + se.getMessage());\n        }\n        catch( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        finally\n        {\n            if( stmt != null )\n            {\n                stmt.close();\n                stmt = null;\n            }\n            if( conn != null )\n            {\n                conn.close();\n                conn = null;\n            }\n        }\n    }\n}\n\n\nAppend를 할 때 date 타입 데이터는 반드시 long 타입의 나노초 단위 시간으로 변환하여 전송하여야 한다.\n\n[mach@localhost jdbc]$ make run_sample4\nmake run_sample4\njava -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" Sample4Append;\nMach JDBC connected.\nappend open ok\nappend data start\n......\nappend data end\nappend close ok\nAppend Result : success = 60000, failure = 0\ntimegap 6905594 in microseconds, 60000 records\n8688.61 records/second\n\n\n10,000건마다 점(.)을 표시하고 있으며, 입력 소요 시간을 알 수 있다.\n\n# machsql을 이용하여 실제 입력된 건수를 확인해보자.\n# Sample2Insert,Sample3PrepareStmt에서 입력한 건수와 함께 60018건이 입력된 것을 확인한다.\n \n \n[mach@localhost jdbc]$ machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version 3.0.0\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase server address (Default:127.0.0.1):\nMachbase user ID  (Default:SYS)\nMachbase user password: MANAGER\nMACH_CONNECT_MODE=INET, PORT=5656\nmach&gt; select count(*) from sample_table;\ncount(*)            \n-----------------------\n60018               \n[1] row(s) selected."
					}
					
				
		
				
					,
					
					"install-license-html": {
						"id": "install-license-html",
						"title": "라이선스 설치",
						"version": "all",
						"categories": "",
						"url": " /install/license.html",
						"content": "라이선스 키 설치는 일반적으로 마크베이스 설치가 끝난 후에 수행한다. 설치 이후에 라이선스를 설치하지 않았다면 일부 제약이 있는 상태로 마크베이스의 사용이 가능하다. 이 장은 마크베이스의 라이선스 정책과 구조 및 설치 방법 등에 대해 기술한다.\n\n라이선스 파일 구조\n\n마크베이스의 라이선스는 license.dat 파일로 관리된다. 제품을 구매하거나 혹은 테스트를 위해 받는 라이선스는 텍스트 파일의 형태이다.\n\nmach@localhost:~$ cat license.dat \n \n#Company\\#ID-ProjectName: test\\#0-Machbase \n#License Policy: SIZE4DAY \n#License Type \\(Version 2\\): OFFICIAL \n#Issue DATE: 20160216 \n#Expiry DATE: 20160319 \nBz5h4TC-d3+Bf3Efkpdp/Tx873PpZA-78LRSdrxbPY-xhGf4355/iXaY5/jfnn+Jdpjn+N+ef4l2mOf4355/iXaY5/jfnn+Jdpjn+N+ef4l2mOf4355/iXaY5/jfnn+Jdpjn+N+ef4l2mOf4355/iXaY5/jfnn+Jdpjn+\n\n\n라이선스 파일이 없는 경우\n\n라이선스가 없는 경우에도 서버가 구동되지만 일부 제약 사항이 있다. 평가 용도로만 사용할 수 있으며, 정식으로 사용하려는 경우에는 적법한 절차로 라이선스를 취득하여야 한다.\n\n라이선스 파일이 없을 경우에는 아래와 같은 기능적인 제약이 존재한다.\n\n\n  한 세션에서 Append 프로토콜을 통해 1억건 이상의 레코드를 입력할 경우 경고 메시지가 출력된다. 이후 Append 입력이 정지된다. 서버를 재시작할 경우에만 입력 제한 상태가 해제된다.\n  테이블스페이스를 생성할 때 2개 이상의 디스크 디렉터리에 대해 생성할 수 없으며, 만약 2개 이상 사용할 경우 아래와 같은 오류가 출력된다. 즉, 고성능 데이터 입력을 위한 병렬 I/O 기능을 사용할 수 없다.\n\n\nCREATE TABLESPACE tbs1 DATADISK disk1 (disk_path=\"tbs1_disk1\"), disk2 (disk_path=\"tbs1_disk2\"), disk3 (disk_path=\"tbs1_disk3\");\n[ERR-00867 : Error in adding disk to tablespace. You cannot use multiple disks for tablespace without valid license.]\n\n\n라이선스 설치\n\n마크베이스의 라이선스는 반드시 $MACHBASE_HOME/conf 에 설치하고, license.dat를 기본 이름으로 한다. 라이선스를 설치하는 방법에는 아래처럼 세 가지가 있다.\n\n라이선스 파일을 $MACHBASE_HOME/conf에 복사\n이때 발급 받은 라이선스 파일의 이름을 반드시 license.dat 로 바꾼 후 복사해야 한다. 이후 서버 구동시 해당 라이선스가 적합한지 판별하여 서버를 구동한다.\n\nmachadmin -t ‘licensefile_path’ 실행\n이 방법의 장점은 라이선스 파일 이름이나 위치를 맞춰줄 필요가 없이 명령어로 손쉽게 설치가 가능하다.\n\n쿼리문으로 설치: 이 방법은 서버 구동 중에 쿼리문을 이용하여 라이선스를 설치하는 방법이다.\n\n라이선스 설치 확인\n\n라이선스가 설치된 경우\n\n라이선스 파일이 설치된 경우 서버 구동 이후에 machbase.trc에 아래와 같이 출력된다.\n\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] LICENSE [License Type (Version 2)][OFFICIAL]\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] LICENSE [License Policy] [CORE]\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] LICENSE [Host ID] [FFFFFFFFFFFFFFF]\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] LICENSE [Expiry DATE] [25300318]\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] Machbase Logs Signature! : OFFICIAL:CORE:FFFFFFFFFFFFFFF:25300318-3.5.0.826b8f2.official-LINUX-X86-64-release\n\n\n그리고 machadmin -f 명령어로도 확인 할 수 있다.\n\n라이선스가 없는 경우\n\n라이선스 파일이 설치되지 않았거나 비정상 파일을 사용할 경우에는 아래와 같이 출력된다.\n\n[2016-02-17 14:49:54 P-6620 T-140539052701440][INFO] LICENSE [License Type(Version 2)][Only for evaluation (No license)]\n[2016-02-17 14:49:54 P-6620 T-140539052701440][INFO] LICENSE [License Policy] [None]\n[2016-02-17 14:49:54 P-6620 T-140539052701440][INFO] LICENSE [Host ID] [Unknown]\n[2016-02-17 14:49:54 P-6620 T-140539052701440][INFO] LICENSE [Expiry DATE] [N/A]"
					}
					
				
		
				
					,
					
					"install-linux-linux-env-html": {
						"id": "install-linux-linux-env-html",
						"title": "Linux 환경 설치 준비",
						"version": "all",
						"categories": "",
						"url": " /install/linux/linux-env.html",
						"content": "파일 최대 개수 확인 및 설정\n\n\n  리눅스 파일 최대 개수를 아래 명령어로 확인한다.\n\n\n[machbase@localhost ~] ulimit -Sn\n1024\n\n\n\n  결과값이 65535보다 작다면, limit.conf 와 user.conf 를 아래와 같이 수정하고 서버를 재시작 한다.\n\n\n[machbase@localhost ~] sudo vi /etc/security/limits.conf\n \n \n#&lt;domain&gt;      &lt;type&gt;  &lt;item&gt;         &lt;value&gt;\n#\n \n*               hard    nofile          65535\n*               soft    nofile          65535\n \n \n[machbase@localhost ~] sudo vi /etc/systemd/user.conf\n \nDefaultLimitNOFILE=65535\n\n\n\n  서버를 재시작하고 다시 값을 확인한다.\n\n\n[machbase@localhost ~] ulimit -Sn\n65535\n\n\n서버의 시간 확인 및 설정\n\nMachbase는 시계열 데이터를 다루는 데이터베이스이므로 Machbase가 설치될 서버의 시간 값을 정확하게 설정해야 한다.\n\n타임존 설정하기\n\nMachbase는 모든 데이터를 해당 서버가 위치한 곳의 지역 시간을 이용하기 때문에 현재 서버의 시간과 Timezone이 맞는지 꼭 확인해야 한다.\n\n아래 명령어로 자신이 위치한 Timezone과 맞는지 확인한다. 만약 다르다면, /usr/share/zoneinfo 에서 맞는 지역을 선택하여 링크한다.\n\n[machbase@localhost ~] ls -l /etc/localtime\nlrwxrwxrwx 1 root root 32 Sep 27 14:08 /etc/localtime -&gt; ../usr/share/zoneinfo/Asia/Seoul\n \n \n# date 명령어를 통해 설정된 Timezone을 확인할 수 있다.\n[machbase@localhost ~] date\nWed Jan  2 11:12:44 KST 2019\n\n\n시간 설정하기\n\n만약 현재의 지역 시간이 맞지 않다면 다음 명령을 통해 시간을 재설정한다.\n\n[machbase@localhost ~] sudo date -s '2018/12/25 12:34:56'\n\n\n포트 설정\n\nMachbase가 사용할 운영체제 포트는 다른 프로그램에서 사용이 안되도록 포트 예약을 사용해야 한다.\n\n아래 명령어로 예약 포트를 설정하면 운영체제가 다른 프로그램에 해당 포트를 할당하지 않게 되어 포트 충돌 문제를 피할 수 있다.\n\n[machbase@localhost ~] sudo echo 예약포트범위~에약포트범위 &gt; /proc/sys/net/ipv4/ip_local_reserved_ports\n\n\n위 방법은 일시적인 방법으로 영구적으로 설정하려면 /etc/sysctl.conf 파일을 수정해야 한다.\n\n[machbase@localhost ~] sudo vim /etc/sysctl.conf\n\n# 해당 내용 추가\nnet.ipv4.ip_local_reserved_ports = 예약포트범위-예약포트범위```"
					}
					
				
		
				
					,
					
					"install-linux-html": {
						"id": "install-linux-html",
						"title": "리눅스 설치",
						"version": "all",
						"categories": "",
						"url": " /install/linux.html",
						"content": "Linux 환경 설치 준비\n  Tarball 설치\n  Docker 설치"
					}
					
				
		
				
					,
					
					"feature-table-log-insert-load-data-html": {
						"id": "feature-table-log-insert-load-data-html",
						"title": "데이터 불러오기 : Load",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/insert/load-data.html",
						"content": "‘Load Data’ 문은 csv 파일을 마크베이스에 입력한다.\n\n먼저 데이터를 저장할 테이블을 생성하는데, csv 파일의 첫 번째 라인을 이용하여 칼럼들을 생성한다.\n\n\n  생성된 칼럼들의 데이터 타입은 VARCHAR(32768)이다.\n  데이터 파일 경로는 $MACHBASE_HOME을 기준으로 한 상대 경로이다. 절대경로로 설정할 수도 있다.\n테이블의 데이터들을 csv 파일로 저장하기 위해서는 SAVE DATA 문을 이용한다.\n\n\n만약 csv파일의 각 필드에 대한 데이터 타입을 미리 알고 있다면, 테이블을 미리 생성하여 데이터를 입력할 수 있다.\n\n‘load_sample.csv’ 파일을 LOAD DATA 문으로 입력하면, 테이블 ‘load_sample’ 이 자동으로 생성된다.\n\n목차\n\n  데이터 불러오기\n  입력 데이터 확인\n  샘플 예제\n\n\n데이터 불러오기\nLOAD DATA INFILE 'sample/quickstart/load_sample.csv' INTO TABLE load_sample AUTO HEADUSE;\n\n\n입력 데이터 확인\nSELECT * FROM load_sample;\n\n\n샘플 예제\n\n샘플 파일을 이용하여, 다음과 같이 실행할 수 있다.\n\n[mach@localhost ~]$ cd $MACHBASE_HOME/sample/quickstart\n[mach@localhost ~]$ ls -l load_sample.csv\n-rw-r\n--r--- 1 root root 2827 2017-02-23 15:01 load_sample.csv\n \n[mach@localhost ~]$ machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version x.x.x.official\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved\n=================================================================\nMachbase server address (Default:127.0.0.1) :\nMachbase user ID  (Default:SYS)\nMachbase User Password :\nMACH_CONNECT_MODE=INET, PORT=5656\n \nMach&gt; LOAD DATA INFILE 'sample/quickstart/load_sample.csv' INTO TABLE load_sample AUTO HEADUSE;\n50 row(s) loaded. Failed to load 0 row(s).\nMach&gt; DESC load_sample;\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH\n----------------------------------------------------------------\nSENSOR_ID                     varchar             32767\nEPOCH_TIME                    varchar             32767\nE_YEAR                        varchar             32767\nE_MONTH                       varchar             32767\nE_DAY                         varchar             32767\nE_HOUR                        varchar             32767\nE_MINUTE                      varchar             32767\nE_SECOND                      varchar             32767\nVALUE                         varchar             32767\nMach&gt; SELECT COUNT(*) FROM load_sample;\nCOUNT(*)\n-----------------------\n50\n[1] row(s) selected.\nMach&gt;"
					}
					
				
		
				
					,
					
					"feature-table-log-log-index-html": {
						"id": "feature-table-log-log-index-html",
						"title": "로그 인덱스 생성 및 관리",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/log-index.html",
						"content": "마크베이스의 로그테이블에는 2가지 인덱스 타입을 생성할 수 있다.\n\n자세한 내용은 SQL 레퍼런스의 DDL 페이지의 CREATE INDEX 문단을 참조하면 된다.\n\n\n  BITMAP 인덱스 : BITMAP인덱스는 Text, Binary 타입을 제외한 모든 컬럼에 생성할 수 있다.\n  KEYWORD 인덱스 : Varchar, Text 컬럼에만 생성 가능하며 문자열을 검색할 때 사용한다.\n\n\n목차\n\n  인덱스 생성\n  인덱스 변경\n  인덱스 삭제\n\n\n인덱스 생성\n\nCREATE INDEX 구문을 이용하여 특정 컬럼에 대해서 인덱스를 생성한다.\n\nCREATE INDEX index_name ON table_name (column_name) [index_type] [tablespace] [index_prop_list]\n    index_type ::= INDEX_TYPE { BITMAP | KEYWORD }\n    tablespace ::= TABLESPACE tablesapce_name\n    index_prop_list ::= value_pair, value_pair, ...\n    value_pair ::= property_name = property_value\nMach&gt; CREATE INDEX id_index ON log_data(id) INDEX_TYPE BITMAP TABLESPACE tbs_data MAX_LEVEL=3;\nCreated successfully.\n\n\n인덱스 변경\n\nALTER INDEX 구문을 이용하여 인덱스 속성을 변경한다.\n\nALTER INDEX index_name SET KEY_COMPRESS = { 0 | 1 }\nMach&gt; ALTER INDEX id_index SET KEY_COMPRESS = 1;\n\n\n인덱스 삭제\n\nDROP INDEX 구문을 이용하여 지정된 인덱스를 삭제한다. 단, 해당 테이블을 검색 중인 다른 세션이 존재할 경우에는 에러를 내면서 실패한다.\n\nDROP INDEX index_name;\nMach&gt; DROP INDEX id_index;\nDropped successfully."
					}
					
				
		
				
					,
					
					"feature-table-log-html": {
						"id": "feature-table-log-html",
						"title": "로그 테이블 (Log Table)",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log.html",
						"content": "개념\n\n로그 테이블은 입력되는 데이터가 시계열 데이터인 머신 로그 데이터를 저장할 수 있는 테이블이다.\n\n이 테이블에는 데이터가 무한히 입력되며, 각 필드의 값에는 고유한 의미가 있다.\n또한 텍스트 필드(varchar 혹은 text) 를 통해 데이터를 검색할 수 있으며, 빠른 통계 연산이 가능하다.\n\n마크베이스에서 테이블이라고 하면 기본적으로 “로그 테이블”을 가리킨다.\n\n로그 테이블의 특징은 다음과 같다.\n\n숨은 시간 컬럼 존재\n\n모든 로그 테이블에는 _arrival_time이라는 숨겨진 컬럼이 있다.\n이 컬럼에는 해당 레코드가 생성된 시간이 저장되어 있으며, 나노초 단위 정밀도를 지원한다.\n\n시간 역순 검색\n\n일반 데이터베이스는 데이터 검색 시 입력 순서와 무관하게 출력된다.\n그러나 Machbase의 로그 테이블은 별도의 ORDER BY를 통한 정렬 옵션을 주지 않는 한 언제나 최신 데이터가 먼저 출력된다.\n이는 _arrival_time 컬럼을 통해서도 확인할 수 있다.\n\n이렇게 설계된 이유는 머신 로그 데이터에서는 최근 데이터의 중요도가 이전의 데이터에 비해 훨씬 높기 때문이다.\n\n입력 후 조회 전용\n\nMachbase의 로그 테이블은 변경(Update) 연산이 존재하지 않는다. 다시 말해 사용자 로그 데이터가 일단 Machbase에 저장되고 나면, 해당 데이터에 대한 변경 연산을 불허함으로써 데이터의 안정성과 로그 데이터 자체의 무결성을 엔진 레벨에서 지원하는 것이다.\n\n제한된 삭제 허용\n\nMachbase가 비록 데이터의 변경은 불허하더라도 특수한 상황에서 필요한 데이터의 삭제는 허용한다.\n그러나 전통적인 데이터베이스에서처럼 임의의 데이터를 삭제할 수는 없으며, 가장 오래된 데이터부터 순차적으로 삭제하는 것만 가능하다.\n이 기능을 통해 저장공간에 제약이 있는 임베디드 장비나 관리가 쉽지 않은 형태의 장비에서 편리하게 주기적으로 데이터를 삭제하여 관리할 수 있다.\n\n텍스트 검색 기능 지원\n\nMachbase는 일반 데이터베이스의 문자열을 취급하는 방식에서 한걸음 더 나아가 단어 기반의 검색 기능을 제공한다.\n이 기능은 머신 로그 데이터의 용도에 가장 잘 부합하는 것으로서, 특정 시간대에 저장된 로그 데이터에 대한 검색이 비즈니스 상황에서 주요한 기능으로 사용된다.\n\n이를 위해서 Machbase는 실시간 역인덱스를 제공함으로써 데이터의 삽입과 동시에 실시간으로 텍스트 검색이 가능하게 함으로써 빠른 장애 진단 및 장애 상황 해결에 큰 도움을 줄 수 있다.\n\n특수 데이터 타입 지원\n\nMachbase는 IPv4, IPv6를 지원한다. 이는 인터넷 주소를 나타내는 특수한 타입으로서 수많은 머신 로그 데이터가 주로 표현하는 주소 체계를 반영한 것이다.\n이 데이터 타입을 활용하여 특정 주소의 검색과 추출을 손쉽게 할 수 있다.\n\n또한, select * from t1 where ipaddr = ‘192.168.0.*’ 과 같은 확장 문법을 활용하여, 특정 주소 체계의 일부 주소 범위를 검색하거나 추출할 수 있는 기능도 함께 제공한다.\n또한 netmask 연산자를 제공하여 특정 인터넷 주소가 특정한 주소 범위에 포함되는지 쉽게 판단할 수 있다.\n\nLOB(Large Object) 데이터 지원\n\n로그 테이블은 64MB 바이트까지 저장 가능한 Text 및 Binary 타입을 제공한다.\n\n만일 해당 데이터가 텍스트 문서 형태로서 검색이 필요한 경우에는 Text 타입으로 저장하고 데이터를 검색할 수 있다.\n만일 해당 데이터가 그림이나 음악과 같은 2진 데이터 형태인 경우 binary 타입으로 저장할 수 있다.\n\n시간 기반 파티셔닝\n로그 테이블은 시간 축을 기준으로 특정 개수의 레코드 및 인덱스를 유지하고 있는 파티션 파일의 연속체이다.\n다시 말해 데이터가 계속 입력됨에 따라 새로운 파티션 파일이 생성되고, 그 파티션에 특정 개수의 레코드가 모두 차게 되면 다음 파티션이 생성된다는 의미이다.\n\n파티션으로 관리하는 이유는 주로 시간을 기준으로 검색이 발생하는 로그 데이터의 특성을 반영한 것이며 데이터 입력 성능에 대단히 큰 장점이 있다.\n통계 분석을 위한 초고속 데이터 접근에 용이한 구조이기 때문이다.\n\n작업 방법\n\n  로그 테이블 생성 및 관리\n  로그 데이터의 입력\n  로그 데이터의 추출\n  로그 데이터의 삭제\n  로그 인덱스 생성 및 관리\n  로그 테이블 활용 샘플 예제"
					}
					
				
		
				
					,
					
					"feature-table-lookup-lookup-ex-html": {
						"id": "feature-table-lookup-lookup-ex-html",
						"title": "참조 테이블 활용 샘플 예제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/lookup/lookup-ex.html",
						"content": "참조 테이블은 갱신이 가능하며, 원본 로그 데이터가 갖고 있지 않은 데이터를 join을 통하여 추가하는데 사용된다. 아래 예제는 로그 테이블과 참조 테이블을 생성하는 예를 보여준다.\n\n-- 로그 테이블의 생성\ncreate table weblog (addr ipv4, msg varchar(100));\n-- 샘플 데이터 입력\ninsert into weblog values ('127.0.0.1', 'a test msessage');\n-- 참조 테이블의 생성\ncreate lookup table dnslookup (addr ipv4 primary key, hostname varchar (100));\n\n\n참조 테이블에 데이터를 삽입 혹은 갱신해 보자.\n\ninsert into dnslookup values ('127.0.0.1', 'localhost') on duplicate key update set hostname = '127.0.0.1'\n\n\n참조 테이블과 로그 테이블에 join을 통하여 데이터를 검색할 수 있다.\n\nselect msg, hostname from weblog, dnslookup where weblog.addr = dnslookup.addr;"
					}
					
				
		
				
					,
					
					"feature-table-lookup-lookup-index-html": {
						"id": "feature-table-lookup-lookup-index-html",
						"title": "참조 인덱스 생성 및 관리",
						"version": "all",
						"categories": "",
						"url": " /feature-table/lookup/lookup-index.html",
						"content": "휘발성 테이블과 마찬가지로 RED-BLACK 인덱스를 기본으로 제공하고 있으며, 사용 방법은 휘발성 테이블과 동일하다.\n\n휘발성 테이블과 같이 RED-BLACK인덱스만 생성 가능하며 keyword, LSM인덱스의 생성은 불가능하다."
					}
					
				
		
				
					,
					
					"feature-table-lookup-html": {
						"id": "feature-table-lookup-html",
						"title": "참조 테이블 (Lookup Table)",
						"version": "all",
						"categories": "",
						"url": " /feature-table/lookup.html",
						"content": "개념\n\n참조 테이블은 휘발성 테이블과 마찬가지로 모든 데이터를 메모리에 상주시킴으로써 빠른 질의 처리를 수행할 수 있다.\n\n또한 데이터의 입력 및 변경에 대해서 디스크에 반영함으로써 데이터의 영구성을 보장한다. 휘발성 테이블과 비교하여 질의 처리 성능은 동일하지만 데이터 입력 및 변경 성능은 다소 떨어진다.\n\n이 테이블의 특성은 다음과 같다.\n\n낮은 입력/갱신 성능\n\n휘발성 테이블과는 달리, 디스크 데이터 이미지 유지에 따른 입력 및 갱신 성능이 낮으므로 대시보드 (Dashboard) 등 실시간 데이터 표현을 위한 테이블에는 부적합하다.\n\n스키마 보존\n\n역시 휘발성 테이블과는 달리, 참조 테이블의 구조 (스키마) 정보는 서버 재시작 후에도 유지된다. 해당 테이블을 삭제하기 위해서는 명시적으로 DROP TABLE 를 수행해야 한다.\n\n데이터 보존\n\n참조 테이블은 휘발성 테이블과 달리 서버 재시작 시 데이터가 서버 종료 직전의 상태로 복구된다.\n\n인덱스 제공\n\n휘발성 테이블과 마찬가지로 RED-BLACK 인덱스를 제공한다. 따라서 검색 과정이나 로그 테이블과의 Join 과정에서 효율적으로 활용될 수 있다.\n\n작업 방법\n\n  참조 테이블 생성 및 관리\n  참조 데이터의 입력\n  참조 데이터의 추출\n  참조 데이터의 삭제\n  참조 인덱스 생성 및 관리\n  참조 테이블 활용 샘플 예제"
					}
					
				
		
				
					,
					
					"tools-utilities-machadmin-html": {
						"id": "tools-utilities-machadmin-html",
						"title": "MACHADMIN",
						"version": "all",
						"categories": "",
						"url": " /tools/utilities/machadmin.html",
						"content": "마크베이스 서버를 시작, 종료하거나 생성, 삭제 및 실행 상태를 체크하기 위해서는 machadmin을 사용한다.\n\n옵션 및 기능\n\nmachadmin의 옵션은 아래와 같다.  앞의 설치 절에서 설명한 기능은 생략한다.\n\nmach@localhost:~$ machadmin -h\n\n\n\n  \n    \n      옵션\n      설명\n    \n  \n  \n    \n      -u, –startup/ –recovery[=simple,complex,reset]\n      마크베이스 서버 시작/ 복구 mode (기본: simple)\n    \n    \n      -s, –shutdown\n      마크베이스 서버 정상 종료\n    \n    \n      -c, –createdb\n      마크베이스 데이터베이스 생성\n    \n    \n      -d, –destroydb\n      마크베이스 데이터베이스 삭제\n    \n    \n      -k, –kill\n      마크베이스 서버 강제 종료\n    \n    \n      -i, –silence\n      출력 없이 실행\n    \n    \n      -r, –restore\n      백업에서 데이터베이스 복구\n    \n    \n      -x, –extract\n      백업 파일을 백업 디렉터리로 변환\n    \n    \n      -e, –check\n      마크베이스 서버 실행 체크\n    \n    \n      -t, –licinstall\n      라이선스 파일 설치\n    \n    \n      -f, –licinfo\n      설치된 라이선스 정보 출력\n    \n  \n\n\n복구 모드\n\nSyntax:\n\nmachadmin -u --recovery=[simple | complex | reset]\n\n\n복구 모드는 다음과 같다.\n\n\n  simple: 서버가 동작중일때 전원이 끊어지는 문제가 발생하지 않았다면, simple recovery 모드가 기본 실행된다.\n  complex: complex recovery 모드는 simple 모드에 비해서 실행시간이 더 오래 걸린다. 전원이 끊어진 이후 재시작시에 기본으로 실행된다.\n  reset: simple 혹은 complex 모드로 복구가 수행되지 않을 때, 모든 테이블의 모든 데이터를 검사하여 데이터베이스를 복구한다. 이 경우, 데이터의 일부 유실이 발생할 수 있다.\n\n\n서버 정상 종료\n\nExample:\n\nmach@localhost:~$ machadmin -c\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDatabase created successfully.\n\n\n데이터베이스 삭제\n\nExample:\n\nmach@localhost:~$ machadmin -d\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDestroy Machbase database- Are you sure?(y/N) y\nDatabase destroyed successfully.\n\n\n서버 강제 종료\n\nSyntax:\n\nmachadmin -k\n\n\nExample:\n\nmach@localhost:~$ machadmin -k\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for Machbase terminated...\nServer terminated successfully.\n\n\n침묵 모드 실행\n\nmachadmin 실행시 출력되는 메시지를 없앤다.\n\nSyntax:\n\nmachadmin -i\n\n\n데이터베이스 복구\n\nSyntax:\n\nmachadmin -r backup_database_path\n\n\nExample:\n\nmach@localhost:~$ machadmin -r 'backup'\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nBacked up database restored successfully.\n\n\n서버 실행 유무 확인\n\nSyntax:\n\nmachadmin -e\n\n\n서버가 실행중이지 않을 때의 출력 예\n\nmach@localhost:~$ machadmin -e\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\n[ERR] Server is not running.\n\n\n서버가 실행중일 때의 출력 예\n\nmach@localhost:~$ machadmin -e\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nMachbase server is already running with PID (14098).\n\n\n라이선스 파일 설치\n\nSyntax:\n\nmachadmin -t license_file\n\n\nExample:\n\nmach@localhost:~$ machadmin -t license.dat\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nLicense installed successfully.\n\n\n라이선스 확인\n\nExample:\n\nmach@localhost:~$ machadmin -f\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\n                   INFORMATION\nInstall Date                      : 2018-12-20 11:34:43\nCompany#ID-ProjectName            : machbase\nLicense Policy                    : CORE\nLicense Type(Version 2)           : OFFICIAL\nHost ID                           : FFFFFFFFFFFFFFF\nIssue Date                        : 2013-03-25\nExpiry Date                       : 2037-03-18\nMax Data Size For a Day(GB)       : 0\nPercentage Of Data Addendum(%)    : 0\nOverflow Action                   : 0\nOverflow Count to Stop Per Month  : 0\nStop Action                       : 0\nReset Flag                        : 0\n-----------------------------------------------------------------\n                   STATUS\nUsage Of Data(GB)                 : 0.000000\nPrevious Checked Date             : 2018-12-22\nViolation Count                   : 0\nStop Enabled                      : 0\n-----------------------------------------------------------------\nLicense information displayed successfully."
					}
					
				
		
				
					,
					
					"tools-machcoordinatoradmin-html": {
						"id": "tools-machcoordinatoradmin-html",
						"title": "machcoordinatoradmin",
						"version": "all",
						"categories": "",
						"url": " /tools/machcoordinatoradmin.html",
						"content": "machcoordinatoradmin\nCoordinator 에서 클러스터 전체에 대한 관리 도구이다.\n\nCluster Edition 패키지에만 존재한다.\n\n옵션 및 기능\nmachcoordinatoradmin의 옵션은 아래와 같다.  앞의 설치 절에서 설명한 기능은 생략한다.\n\nmach@localhost:~$ machcoordinatoradmin -h\n\n\n\n  \n    \n      옵션\n      설명\n       \n    \n  \n  \n    \n      -u, –startup\n      Coordinator 프로세스를 구동\n       \n    \n    \n      -s, –shutdown\n      Coordinator 프로세스를 종료\n       \n    \n    \n      -k, –kill\n      Coordinator 프로세스를 중단\n       \n    \n    \n      -c, –createdb\n      Coordinator의 메타를 생성\n       \n    \n    \n      -d, –destroydb\n      Coordinator의 메타를 삭제하고,$MACHBASE_COORDINATOR_HOME/package에 있는 패키지 파일들을 삭제\n       \n    \n    \n      -e, –check\n      Coordinator 프로세스가 작동 중인지 확인\n       \n    \n    \n      -i, –silence\n      출력 없이 실행\n       \n    \n    \n      --configuration[=name]\n      configuration 설정에서의 키와 값 출력(특정 키만 출력 가능)\n       \n    \n    \n      --activate\n      Cluster status를 Service로 전환\n       \n    \n    \n      --deactivate\n      Cluster status를 Deactivate로 전환\n       \n    \n    \n      --list-package[=package]\n      등록한 Package들의 정보를 나열(특정 Package만 출력 가능)\n       \n    \n    \n      --add-package=package\n      Package를 추가\n       \n    \n    \n      --remove-package=package\n      Package를 삭제\n       \n    \n    \n      --list-node[=node]\n      Node들의 정보를 나열(특정 Node만 출력 가능)\n       \n    \n    \n      --add-node=node\n      Node를 추가\n       \n    \n    \n      --remove-node=node\n      Node를 삭제\n       \n    \n    \n      --upgrade-node=node\n      Node를 업그레이드\n       \n    \n    \n      --startup-node=node\n      Node를 구동\n       \n    \n    \n      --shutdown-node=node\n      Node를 종료\n       \n    \n    \n      --kill-node=node\n      Node를 중단\n       \n    \n    \n      --cluster-status\n      Cluster의 각 Node 상태를 출력\n       \n    \n    \n      --cluster-status-full\n      Cluster의 각 Node 상태를 상세하게 출력\n       \n    \n    \n      --cluster-node\n      Cluster의 정보를 출력\n       \n    \n    \n      --set-group-state=[normal\n      readonly]\n      특정 warehouse 그룹의 상태를 변경\n    \n    \n      --get-host-resource\n      각 Node가 위치한 Host 자원 정보를 출력\n       \n    \n    \n      --host-resource-enable\n      각 노드의 Host 자원 정보 수집을 시작\n       \n    \n    \n      --host-resource-disable\n      각 노드의 Host 자원 정보 수집을 멈춤\n       \n    \n  \n\n\n\n  \n    \n      부가 옵션\n      설명\n      필수 옵션\n       \n       \n       \n    \n  \n  \n    \n      --file-name=filename\n      파일 이름\n      --add-package\n       \n       \n       \n    \n    \n      --port-no=portno\n      포트 번호\n      --add-node\n       \n       \n       \n    \n    \n      --deployer=node\n      Deployer의 노드 이름\n      --add-node\n       \n       \n       \n    \n    \n      --package-name=packagename\n      설치 원본이 될 Package 이름\n      --add-package\n       \n       \n       \n    \n    \n      --home-path=path\n      Deployer 서버 기준, 현재 Node의 설치 경로\n      --add-node\n       \n       \n       \n    \n    \n      --node-type=[broker\n      warehouse]\n      설치할 노드의 타입(broker / warehouse 중 선택)\n      --add-node\n       \n       \n    \n    \n      --group=groupname\n      설치할 노드의 그룹 이름\n      --add-node\n       \n       \n       \n    \n    \n      --replication=host:port\n      Replication을 주고 받을 host:port\n      --add-node\n       \n       \n       \n    \n    \n      --no-replicate\n      설치할 노드의 Replication을 사용하지 않음\n      --add-node\n       \n       \n       \n    \n    \n      --primary=host:port\n      Secondary Coordinator 설치 시 Primary Coordinator의 노드 이름을 지정\n      -u, –startup\n       \n       \n       \n    \n    \n      --host=host\n      Host 자원 정보를 출력할 특정 Host 지정\n      --get-host-resource\n       \n       \n       \n    \n    \n      --metric=[cpu\n      memory\n      disk\n      network]\n      Host 자원 정보를 출력할 특정 Metric 지정\n      --get-host-resource\n    \n  \n\n\n동작 여부 확인\nExample:\nmach@localhost:~$ machcoordinatoradmin -e\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nMachbase Coordinator is running with pid(29245)!\n\n\n메타 생성 / 삭제\nExample:\nmach@localhost:~$ machcoordinatoradmin -c\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nCoordinator metadata created successfully.\n  \nmach@localhost:~$ machcoordinatoradmin -d\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nCoordinator metadata destroyed successfully.\n\n\nConfiguration 설정 출력\nSyntax:\nmachcoordinatoradmin --configuration[=name]\n\n\nExample:\nmach@localhost:~$ machcoordinatoradmin --configuration\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nName  : CLUSTER\nValue : 3\n \nName  : DECISION\nValue : ON\n \nName  : HOST-RESOURCE\nValue : OFF\n  \nmach@localhost:~$ machcoordinatoradmin --configuration=decision\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : DECISION\n             Value : ON\n            Format : text/plain\n\n\nluster Status 변경\nExample:\nmach@localhost:~$ machcoordinatoradmin --activate\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : CLUSTER\n             Value : 3\n            Format : text/plain\n \n \nmach@localhost:~$ machcoordinatoradmin --deactivate\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : CLUSTER\n             Value : 0\n            Format : text/plain\n\n\n패키지 정보 나열\nSyntax:\nmachcoordinatoradmin --list-package[=package]\n\n\nExample:\nmach@localhost:~$ machcoordinatoradmin --list-package\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nPackage Name : machbase\nFile Name    : machbase-cluster-6bab497c9.develop-LINUX-X86-64-release-lightweight.tgz\nFile Size    : 64630670 bytes\n \nPackage Name : machbase2\nFile Name    : machbase-cluster-e3c0717.develop-LINUX-X86-64-release-lightweight.tgz\nFile Size    : 64677030 bytes\n \n \nmach@localhost:~$ machcoordinatoradmin --list-package=machbase\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nPackage Name : machbase\nFile Name    : machbase-cluster-6bab497c9.develop-LINUX-X86-64-release-lightweight.tgz\nFile Size    : 64630670 bytes\n\n\n노드 정보 나열\nSyntax:\nmachcoordinatoradmin --list-node[=node]\n\n\nExample:\nmach@localhost:~$  machcoordinatoradmin --list-node\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nNode Name             : 192.168.0.32:5101\nNode Type             : coordinator\nHTTP Admin Port       : 5102\nGroup Name            : Coordinator\nDesired State         : primary\nActual State          : primary\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 497590\nLast Modify Time      : 421020408\nLast Response Elapsed : 1006148\n \nNode Name             : 192.168.0.32:5201\nNode Type             : deployer\nGroup Name            : Deployer\nDesired State         : normal\nActual State          : normal\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 497594\nLast Modify Time      : 404915419\nLast Response Elapsed : 1006128\n \nNode Name             : 192.168.0.32:5301\nNode Type             : broker\nPort Number           : 5757\nDeployer              : 192.168.0.32:5201\nPackage Name          : machbase\nHome Path             : /home/machbase/broker1\nGroup Name            : Broker\nDesired State         : leader\nActual State          : leader\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 497544\nLast Modify Time      : 353606480\nLast Response Elapsed : 1006157\n \nNode Name             : 192.168.0.32:5401\nNode Type             : warehouse\nPort Number           : 5400\nDeployer              : 192.168.0.32:5201\nPackage Name          : machbase\nHome Path             : /home/machbase/warehouse_a1\nGroup Name            : Group1\nDesired State         : normal\nActual State          : normal\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 497556\nLast Modify Time      : 332480933\nLast Response Elapsed : 1006160\n  \nmach@localhost:~$  machcoordinatoradmin --list-node=192.168.0.32:5401\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nNode Name             : 192.168.0.32:5401\nNode Type             : warehouse\nPort Number           : 5400\nDeployer              : 192.168.0.32:5201\nPackage Name          : machbase\nHome Path             : /home/cumulus/warehouse_a1\nGroup Name            : Group1\nDesired State         : normal\nActual State          : normal\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 648879\nLast Modify Time      : 419153148\nLast Response Elapsed : 1005962\n\n\nCluster의 Node 상태 출력\nExample:\nmach@localhost:~$ machcoordinatoradmin --cluster-status\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n+-------------+-------------------+-------------------+-------------------+--------------+\n|  Node Type  |     Node Name     |    Group Name     |    Group State    |     State    |\n+-------------+-------------------+-------------------+-------------------+--------------+\n| coordinator | 192.168.0.32:5101 | Coordinator       | normal            | primary      |\n| deployer    | 192.168.0.32:5201 | Deployer          | normal            | normal       |\n| broker      | 192.168.0.32:5301 | Broker            | normal            | leader       |\n| warehouse   | 192.168.0.32:5401 | Group1            | normal            | normal       |\n+-------------+-------------------+-------------------+-------------------+--------------+\n \nmach@localhost:~$ machcoordinatoradmin --cluster-status-full\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n+-------------+-------------------+-------------------+-------------------+-------------------------------+-------------+\n|  Node Type  |     Node Name     |    Group Name     |    Group State    |    Desired &amp; Actual State     |  RP State   |\n+-------------+-------------------+-------------------+-------------------+-------------------------------+-------------+\n| coordinator | 192.168.0.32:5101 | Coordinator       | normal            | primary       | primary       | ----------- |\n| deployer    | 192.168.0.32:5201 | Deployer          | normal            | normal        | normal        | ----------- |\n| broker      | 192.168.0.32:5301 | Broker            | normal            | leader        | leader        | ----------- |\n| warehouse   | 192.168.0.32:5401 | Group1            | normal            | normal        | normal        | ----------- |\n+-------------+-------------------+-------------------+-------------------+-------------------------------+-------------+\n\n\nCluster 정보 출력\nExample:\nmach@localhost:~$ machcoordinatoradmin --cluster-node\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nToken Pid      : 29245\nToken Time     : 1553153902646178\nModify Time    : 1553154010296715\nModify Count   : 8\nCluster Status : Service\nBroker         : 192.168.0.32:5301\nWarehouse      : 192.168.0.32:5401\n\n\nGroup State 변경\nSyntax:\nmachcoordinatoradmin --set-group-state=[ normal | readonly ] --group=group\n\n\nExample:\nmach@localhost:~$ machcoordinatoradmin --set-group-state=readonly --group=Group1\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nGroup Name: Group1\nFlag      : 1\n  \nmach@localhost:~$ machcoordinatoradmin --cluster-status\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n+-------------+-------------------+-------------------+-------------------+--------------+\n|  Node Type  |     Node Name     |    Group Name     |    Group State    |     State    |\n+-------------+-------------------+-------------------+-------------------+--------------+\n| coordinator | 192.168.0.32:5101 | Coordinator       | normal            | primary      |\n| deployer    | 192.168.0.32:5201 | Deployer          | normal            | normal       |\n| broker      | 192.168.0.32:5301 | Broker            | normal            | leader       |\n| warehouse   | 192.168.0.32:5401 | Group1            | readonly          | normal       |\n+-------------+-------------------+-------------------+-------------------+--------------+\n\n\nHost Resource 출력\nSyntax:\nmachcoordinatoradmin --host-resource-enable [--metric=metric] [host=host]\n\n\nExample:\nmach@localhost:~$ machcoordinatoradmin --host-resource-enable\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : HOST-RESOURCE\n             Value : ON\n            Format : text/plain\n  \nmach@localhost:~$ machcoordinatoradmin --get-host-resource\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nHost Name : 192.168.0.32\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 14.0%\n      CPU IOWait Ratio    : 0.0%\n   Memory Info :\n      Physical Memory Utilization : 99.1%\n      Virtual Memory Utilization  : 98.6%\n   Network Info :\n      Receive Bytes(per second)    : 42809\n      Receive Packets(per second)  : 337\n      Transmit Bytes(per second)   : 42885\n      Transmit Packets(per second) : 332\n   Disk Info :\n      /dev/sda1 : 87.4%\n         |-&gt; 192.168.0.32:5101   /home/cumulus/coordinator1\n         |-&gt; 192.168.0.32:5301   /home/cumulus/broker1\n         |-&gt; 192.168.0.32:5401   /home/cumulus/warehouse_a1\nHost Name : 192.168.0.33\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 2.0%\n      CPU IOWait Ratio    : 0.0%\n   Memory Info :\n      Physical Memory Utilization : 46.9%\n      Virtual Memory Utilization  : 22.8%\n   Network Info :\n      Receive Bytes(per second)    : 12336\n      Receive Packets(per second)  : 103\n      Transmit Bytes(per second)   : 13500\n      Transmit Packets(per second) : 103\n   Disk Info :\n      /dev/sda1 : 64.2%\n         |-&gt; 192.168.0.33:5101   /home/cumulus/coordinator2\n         |-&gt; 192.168.0.33:5401   /home/cumulus/warehouse_a2\n  \nmach@localhost:~$ machcoordinatoradmin --get-host-resource --metric=cpu\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nHost Name : 192.168.0.32\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 13.9%\n      CPU IOWait Ratio    : 0.0%\nHost Name : 192.168.0.33\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 1.9%\n      CPU IOWait Ratio    : 0.0%\n  \nmach@localhost:~$ machcoordinatoradmin --get-host-resource --host=192.168.0.33\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nHost Name : 192.168.0.33\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 2.0%\n      CPU IOWait Ratio    : 0.0%\n   Memory Info :\n      Physical Memory Utilization : 46.9%\n      Virtual Memory Utilization  : 22.8%\n   Network Info :\n      Receive Bytes(per second)    : 12588\n      Receive Packets(per second)  : 106\n      Transmit Bytes(per second)   : 13330\n      Transmit Packets(per second) : 100\n   Disk Info :\n      /dev/sda1 : 64.2%\n         |-&gt; 192.168.0.33:5101   /home/cumulus/coordinator2\n         |-&gt; 192.168.0.33:5401   /home/cumulus/warehouse_a2\n  \nmach@localhost:~$ machcoordinatoradmin --host-resource-disable\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : HOST-RESOURCE\n             Value : OFF\n            Format : text/plain"
					}
					
				
		
				
					,
					
					"tools-machdeployeradmin-html": {
						"id": "tools-machdeployeradmin-html",
						"title": "machdeployeradmin",
						"version": "all",
						"categories": "",
						"url": " /tools/machdeployeradmin.html",
						"content": "machdeployeradmin\nDeployer의 상태를 확인하거나, Deployer의 구동/종료/중단 명령을 직접 내릴 수 있다.\n\n보통은 machcoordinatoradmin 을 통해서 내리는 편이 가장 빠르지만, 불가능한 경우에는 아래와 같이 수행해야 한다.\n\nCluster Edition 패키지에만 존재한다.\n\n옵션 및 기능\nmachdeployeradmin의 옵션은 아래와 같다.  앞의 설치 절에서 설명한 기능은 생략한다.\n\nmach@localhost:~$ machdeployeradmin -h\n\n\n\n  \n    \n      옵션\n      설명\n    \n  \n  \n    \n      -u, –startup\n      Deployer 프로세스 구동\n    \n    \n      -s, –shutdown\n      Deployer 프로세스 종료\n    \n    \n      -k, –kill\n      Deployer 프로세스 중단\n    \n    \n      -c, –createdb\n      Deployer 메타 생성\n    \n    \n      -d, –destroydb\n      Deployer 메타 삭제\n    \n    \n      -i, –silence\n      출력 없이 실행\n    \n    \n      -e, –check\n      Deployer 프로세스가 작동중인지 확인\n    \n  \n\n\n동작 여부 확인\nExample:\nmach@localhost:~$ machdeployeradmin -e\n-------------------------------------------------------------------------\n     Machbase Deployer Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nMachbase Deployer is running with pid(29373)!"
					}
					
				
		
				
					,
					
					"tools-utilities-machloader-html": {
						"id": "tools-utilities-machloader-html",
						"title": "MACHLOADER",
						"version": "all",
						"categories": "",
						"url": " /tools/utilities/machloader.html",
						"content": "마크베이스 서버에 텍스트 파일 데이터를 import/export하기 위해서 machloader를 사용한다. 기본적으로 CSV 파일을 이용하여 동작하지만, 다른 포맷도 지원한다.\n\nmachloader의 특징은 다음과 같다.\n\n\n  machloader는 datetime 형식을 스키마 파일에서 지정할 수 있다. 지정하는 datetime 형식은 마크베이스 서버에서 지원하는 형식이어야 한다. 하나의 datetime 형식을 모든 필드에 적용할 수도 있고, 각 필드마다 다른 형식을 지정할 수도 있다.\n  입력 대상 테이블의 데이터를 삭제하고 입력하려면 “-m replace” 옵션을 사용하면 된다.\n  machloader는 스키마와 데이터 파일의 정합성을 검증하지 않는다. 사용자는 스키마, 테이블, 데이터 파일이 정합성을 만족하는지 검사해야만 한다.\n  machloader는 APPEND 모드를 기본으로 지원한다.\n  \n    machloader는 기본적으로는 “_ARRIVAL_TIME” 컬럼을 사용하지 않는다. 해당 컬럼 데이터를 import/export하려면 “-a” 옵션을 사용하여야 한다.\n  \n  machloader의 옵션은 다음 명령으로 볼 수 있다.\n\n\n[mach@localhost]$ machloader -h\n\n\n\n  \n    \n      옵션\n      설명\n       \n    \n  \n  \n    \n      -s, –server=SERVER\n      마크베이스 서버의 IP 주소를 입력한다.(default : 127.0.0.1)\n       \n    \n    \n      -u, –user=USER\n      접속할 사용자명을 입력한다.(default : SYS)\n       \n    \n    \n      -p, –password=PASSWORD\n      접속할 사용자의 패스워드 (default : MANAGER)\n       \n    \n    \n      -P, –port=PORT\n      마크베이스 서버의 포트 번호 (default : 5656)\n       \n    \n    \n      -i, –import\n      데이터 import 명령 옵션\n       \n    \n    \n      -o, –export\n      데이터 export 명령 옵션\n       \n    \n    \n      -c, –schema\n      데이터베이스의 테이블 정보를 이용하여 스키마 파일을 만드는 명령 옵션\n       \n    \n    \n      -t, –table=TABLE_NAME\n      스키마 파일을 생성할 테이블 명을 설정\n       \n    \n    \n      -f, –form=SCHEMA_FORM_FILE\n      스키마 파일명을 지정\n       \n    \n    \n      -d, –data=DATA_FILE\n      데이터 파일명을 지정\n       \n    \n    \n      -l, –log=LOG_FILE\n      machloader 실행 로그 파일을 지정\n       \n    \n    \n      -b, –bad=BAD_FILE\n      -i 옵션 실행시 입력 오류가 발생한 데이터를 기록하며, 에러 설명을 기록하는 파일명을 지정한다.\n       \n    \n    \n      -m, –mode=MODE\n      -i 옵션 실행시 import 방법을 지시한다. append 또는 replace 옵션이 사용가능 하다. append는 기존 데이터\n      이후에 데이터를 입력하고 replace는 기존 데이터를 삭제하고 데이터를 입력한다.\n    \n    \n      -D, –delimiter=DELIMITER\n      각 필드 구분자를 설정한다. 기본값은 ‘,’이다.\n       \n    \n    \n      -n, –newline=NEWLINE\n      각 레코드 구분자를 설정한다. 기본값은 ‘\\n’이다.\n       \n    \n    \n      -e, –enclosure=ENCLOSURE\n      각 필드의 enclosing 구분자를 설정한다.\n       \n    \n    \n      -r, –format=FORMAT\n      파일 입력/출력 시 포맷을 지정한다. (default : csv)\n       \n    \n    \n      -a, –atime\n      내장 컬럼 “_ARRIVAL_TIME”을 사용할 것인지를 결정한다. 기본값은 사용하지 않는 것이다.\n       \n    \n    \n      -z, –timezone\n      Set timezone ex) +0900 -1230\n       \n    \n    \n      -I, –silent\n      저작권 관련 출력 및 import/export 상태 정보를 표시하지 않는다.\n       \n    \n    \n      -h, –help\n      옵션 리스트를 표시한다.\n       \n    \n    \n      -F, –dateformat=DATEFORMAT\n      컬럼 dateformat을 설정한다. (“_arrival_time YYYY-MM-DD HH24:MI:SS”)* dateformat 대신 ‘unixtimestamp’ 을 설정하면, 입력되는 값을 unix timestamp 값으로 간주한다. (“time_column unixtimestamp”)* dateformat 대신 ‘nanotimestamp’ 을 설정하면, 입력되는 값을 nanosecond 단위의 timestamp 값으로 간주한다. (“time_column nanotimestamp”)unixtimestamp, nanotimestamp format 은 5.7 부터 지원합니다.\n       \n    \n    \n      -E, –encoding=CHARACTER_SET\n      입/출력하는 파일의 인코딩을 설정한다. 지원되는 인코딩은 UTF8(기본값), ASCII, MS949, KSC5601, EUCJP, SHIFTJIS, BIG5, GB231280, UTF16이다.\n       \n    \n    \n      -C, –create\n      import시에 table이 없으면 생성한다.\n       \n    \n    \n      -H, –header\n      import/export시에 헤더 정보의 유무를 설정한다. 기본값은 미설정이다.\n       \n    \n    \n      -S, –slash\n      backslash 구분자를 지정한다.\n       \n    \n  \n\n\n기본 사용법\n\n아래 사용법을 실행하기 전에 테이블을 먼저 생성해야 한다.\n\nCSV 파일 Import\n\n마크베이스 서버에 CSV 파일을 import한다.\n\nOption:\n-i: import 지정 옵션\n-d: 데이터 파일명 지정 옵션\n-t: 테이블명 지정 옵션\n\n\nExample:\nmachloader -i -d data.csv -t table_name\n\n\nCSV 파일 Export\n데이터를 CSV 파일에 기록한다.\n\nOption:\n-o: export 지정 옵션\n-d: 데이터 파일명 지정 옵션\n-t: 테이블명 지정 옵션\n\nExample:\nmachloader -o -d data.csv -t table_name\n\n\nCSV 파일 헤더 사용\n\nCSV 파일의 헤더 관련 설정이다.\n\nOption:\n-i -H: import 할 때 csv 파일의 첫번째 라인을 헤더로 인식한다. 따라서 첫번째 라인은 입력에서 제외된다.\n-o -H: export 할 때 테이블의 컬럼명으로 csv 헤더를 생성한다.\n\n\nExample:\nmachloader -i -d data.csv -t table_name -H\nmachloader -o -d data.csv -t table_name -H\n\n\n테이블 자동 생성\n\n테이블 자동 생성에 관련한 내용이다.\n\nOption:\n-C: import할 때 테이블을 자동 생성한다. 컬럼명은 c0, c1, ... 자동으로 생성된다. 생성되는 컬럼은 varchar(32767) 타입이다.\n-H: import할 때 csv 헤더명으로 컬럼명을 생성한다.\n\n\nExample:\nmachloader -i -d data.csv -t table_name -C\nmachloader -i -d data.csv -t table_name -C -H\n\n\nCSV 포맷 이외 파일\n\nCSV 포맷이 아닌 파일에 대해서 구분자를 설정하여 사용한다.\n\nOption:\n-D: 각 필드의 구분자 지정 옵션\n-n: 각 레코드 구분자 지정 옵션\n-e: 각 필드의 enclosing character 지정 옵션\n\n\nExample:\nmachloader -i -d data.txt -t table_name -D '^' -n '\\n' -e '\"'\nmachloader -o -d data.txt -t table_name -D '^' -n '\\n' -e '\"'\n\n\n입력 모드 지정\n\nimport 시 (-i 옵션 설정 시) REPLACE와 APPEND의 두 가지 모드가 있다. APPEND가 기본값이다. REPLACE 모드인 경우, 기존 데이터를 삭제하므로 주의해야 한다.\n\nOption:\n-m: import 모드 지정\n\n\nExample:\nmachloader -i -d data.csv -t table_name -m replace\n\n\n접속 정보 지정\n\n서버 IP, 사용자, 패스워드를 별도로 지정한다.\n\nOption:\n-s: 서버 IP 주소 지정 (default: 127.0.0.1)\n-P: 서버 포트 번호 지정 (default: 5656)\n-u: 접속할 사용자명 지정 (default: SYS)\n-p: 접속할 사용자의 패스워드 지정 (default: MANAGER)\n\n\nExample:\nmachloader -i -s 192.168.0.10 -P 5656 -u mach -p machbase -d data.csv -t table_name\n\n\n로그 파일 생성\n\nmachloader의 실행 로그 파일을 생성한다.\n\nOption:\n-b: import할 때 입력되지 않은 데이터를 생성할 로그 파일명 설정한다.\n-l: import할 때 입력되지 않은 데이터와 에러 메시지를 생성할 로그 파일명을 설정한다.\n\n\nExample:\nmachloader -i -d data.csv -t table_name -b table_name.bad -l table_name.log\n\n\n스키마 파일 생성\n\nmachloader의 스키마 파일을 생성할 수 있다. 스키마 파일을 이용하여  데이터 타입 형식을 바꾸거나 테이블과 데이터 파일의 컬럼 수가 다른 경우에도 import/export가 가능하다.\n\nOption:\n-c: 스키마 파일 생성 옵션\n-t: 테이블명 지정 옵션\n-f: 생성될 스키마 파일명 지정 옵션\n\n\nExample:\nmachloader -c -t table_name -f table_name.fmt\nmachloader -c -t table_name -f table_name.fmt -a\n\n\n스키마 파일에서 datetime 형식 설정\n\nDATEFORMAT 옵션으로 dateformat을 원하는 대로 설정할 수 있다.\n\nSyntax:\n\n# 모든 datetime 컬럼에 대해서 설정한다.\nDATEFORMAT &lt;dateformat&gt;\n\n# 개별 datetime 컬럼에 대해서 설정한다.\nDATEFORMAT &lt;column_name&gt; &lt;format&gt;\n\n\nExample:\n-- 스키마 파일(datetest.fmt)에 datetest.csv 파일의 각 필드에 맞게 dateformat을 설정한다.\ndatetest.fmt\ntable datetest\n{\nINS_DT datetime;\nUPT_DT datetime;\n}\nDATEFORMAT ins_dt \"YYYY/MM/DD HH12:MI:SS\"\nDATEFORMAT upt_dt \"YYYY DD MM HH12:MI:SS\"\n \ndatetest.csv\n2017/02/20 11:05:23,2017 20 02 11:05:23\n2017/02/20 11:06:34,2017 20 02 11:06:34\n \n-- datetest.csv 파일을 import 하고 입력된 데이터를 확인한다.\nmachloader -i -f datetest.fmt -d datetest.csv\n-----------------------------------------------------------------\nMachbase Data Import/Export Utility.\nRelease Version 5.1.9.community\nCopyright 2014, MACHBASE Corporation or its subsidiaries.\nAll Rights Reserved.\n-----------------------------------------------------------------\nImport time : 0 hour 0 min 0.39 sec\nLoad success count : 2\nLoad fail count : 0\n \nmach&gt; SELECT * FROM datetest;\nINS_DT UPT_DT\n-------------------------------------------------------------------\n2017-02-20 11:06:34 000:000:000 2017-02-20 11:06:34 000:000:000\n2017-02-20 11:05:23 000:000:000 2017-02-20 11:05:23 000:000:000\n[2] row(s) selected.\nElapsed time: 0.000\n\n\nIGNORE\n\nCSV 파일의 특정 필드를 입력하려 하지 않을 때, IGNORE 옵션을 fmt 파일에 설정할 수 있다.\n\nignoretest.csv 파일은 세 개의 필드를 갖지만, 마지막 필드가 필요 없을 경우, fmt 파일에 필요 없는 컬럼에 IGNORE를 명시한다.\n\nExample:\n-- ignoretest.fmt 파일에 마지막 필드에 대해서 ignore 옵션을 설정한다.\nignoretest.fmt\ntable ignoretest\n{\nID integer;\nMSG varchar(40);\nSUB_ID integer IGNORE;\n}\n \nignoretest.csv\n1, \"msg1\", 3\n2, \"msg2\", 4\n \n-- ignoretest.csv 파일을 import 하고 입력된 데이터를 확인한다.\nmachloader -i -f ignoretest.fmt -d ignoretest.csv\n-----------------------------------------------------------------\nMachbase Data Import/Export Utility.\nRelease Version 5.1.9.community\nCopyright 2014, MACHBASE Corporation or its subsidiaries.\nAll Rights Reserved.\n-----------------------------------------------------------------\nNLS : US7ASCII EXECUTE MODE : IMPORT\nSCHMEA FILE : ignoretest.fmt DATA FILE : ignoretest.csv\nIMPORT_MODE : APPEND FILED TERM : ,\nROW TERM : \\n ENCLOSURE : \"\nARRIVAL_TIME : FALSE ENCODING : NONE\nHEADER : FALSE CREATE TABLE : FALSE\n \nProgress bar Imported records Error records\n2 0\n \nImport time : 0 hour 0 min 0.39 sec\nLoad success count : 2\nLoad fail count : 0\n \n \nmach&gt; SELECT * FROM ignoretest;\nID MSG\n---------------------------------------------------------\n2 msg2\n1 msg1\n[2] row(s) selected.\nElapsed time: 0.000\n\n\n컬럼 개수가 필드 개수보다 많은 경우\n\n테이블의 컬럼 개수가 데이터 파일의 필드 개수보다 많은 경우에는 스키마 파일에 지정된 컬럼만 입력되고 다른 컬럼은 NULL로 입력된다.\n\n컬럼 개수가 필드 개수보다 적은 경우\n\n테이블의 컬럼 개수가 데이터 파일의 필드 개수보다 적은 경우에는 테이블에 없는 필드는 IGNORE 옵션을 제외하고 입력하여야 한다.\n\nExample:\n-- 마지막 필드에 대해서 ignore 옵션을 설정해서 제외한다.\nloader_test.fmt\ntable loader_test\n{\nID integer;\nMSG varchar (40);\nSUB_ID integer IGNORE;\n}"
					}
					
				
		
				
					,
					
					"tools-utilities-machsql-html": {
						"id": "tools-utilities-machsql-html",
						"title": "MACHSQL",
						"version": "all",
						"categories": "",
						"url": " /tools/utilities/machsql.html",
						"content": "MACHSQL은 터미널 화면을 통해 SQL질의를 수행하는 대화형 도구이다.\n\n구동 옵션 설명\n\n[mach@localhost]$ machsql -h\n\n\n\n  \n    \n      짧은 옵션\n      긴 옵션\n      설명\n    \n  \n  \n    \n      -s\n      –server\n      접속할 서버의 IP 주소 (default : 127.0.0.1)\n    \n    \n      -u\n      –user\n      사용자명 (default : SYS)\n    \n    \n      -p\n      –password\n      사용자 패스워드 (default : MANAGER)\n    \n    \n      -P\n      –port\n      서버의 포트 번호 (default : 5656)\n    \n    \n      -n\n      –nls\n      NLS 설정\n    \n    \n      -f\n      –script\n      실행할 SQL 스크립트 파일\n    \n    \n      -z\n      –timezone=+-HHMM\n      Timezone 설정 ex) +0900   -1230\n    \n    \n      -o\n      –output\n      질의 결과를 저장할 파일명\n    \n    \n      -i\n      –silent\n      저작권 출력 없이 실행\n    \n    \n      -v\n      –verbose\n      상세 출력\n    \n    \n      -r\n      –format\n      출력 파일 포맷 지정 (default: csv)\n    \n    \n      -h\n      –help\n      옵션 출력\n    \n    \n      -c\n      N/A\n      Connection 매개변수 추가(6.1이후 버전부터 지원)\n    \n  \n\n\nExample:\n\nmachsql -s localhost -u sys -p manager\nmachsql --server=localhost --user=sys --password=manager\nmachsql -s localhost -u sys -p manager -f script.sql\n# 6.1 이후버전부터 지원\nmachsql -s 127.0.0.1 -u sys -p manager -P 8888 -c ALTERNATIVE_SERVERS=192.168.0.147:9209;CONNECTION_TIMEOUT=10\n\n\n환경변수 MACHBASE_CONNECTION_STRING\n\n기본 접속  매개변수를 지정한다. 예를 들어 CONNECTION_TIMEOUT 값 설정 및 ALTERNATIVE_SERVERS 설정을 추가하기 위해 다음의 환경변수를 설정할 수 있다.\n\nexport MACHBASE_CONNECTION_STRING=ALTERNATIVE_SERVERS=192.168.0.148:8888;CONNECTION_TIMEOUT=3\n\n-c 옵션으로 접속 매개변수를 지정하면 환경변수보다 우선하여 수행된다. 이 기능은 6.1 이후 버전부터 지원한다.\n\nSHOW 명령어\n\n테이블, 테이블스페이스, 인덱스 등의 정보를 출력한다.\n\nSHOW 명령어 목록\n\n\n  SHOW INDEX\n  SHOW INDEXES\n  SHOW INDEXGAP\n  SHOW LSM\n  SHOW LICENSE\n  SHOW STATEMENTS\n  SHOW STORAGE\n  SHOW TABLE\n  SHOW TABLES\n  SHOW TABLESPACE\n  SHOW TABLESPACES\n  SHOW USERS\n\n\nSHOW INDEX\n\n인덱스 정보를 출력한다.\n\nSyntax:\n\nSHOW INDEX index_name\n\n\nExample:\n\nMach&gt; CREATE TABLE t1 (c1 INTEGER, c2 VARCHAR(10));\nCreated successfully.\nMach&gt; CREATE VOLATILE TABLE t2 (c1 INTEGER, c2 VARCHAR(10));\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx1 ON t1(c1) INDEX_TYPE LSM;\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx2 ON t1(c1) INDEX_TYPE BITMAP;\nCreated successfully.\nMach&gt; CREATE INDEX t2_idx1 ON t2(c1) INDEX_TYPE REDBLACK;\nCreated successfully.\nMach&gt; CREATE INDEX t2_idx2 ON t2(c2) INDEX_TYPE REDBLACK;\nCreated successfully.\n \nMach&gt; SHOW INDEX t1_idx2;\nTABLE_NAME                                COLUMN_NAME                               INDEX_NAME                      \n----------------------------------------------------------------------------------------------------------------------------------\nINDEX_TYPE   BLOOM_FILTER  KEY_COMPRESS  MAX_LEVEL   PART_VALUE_COUNT BITMAP_ENCODE\n--------------------------------------------------------------------------------------------\nT1                                        C1                                        T1_IDX2                         \nLSM          ENABLE   COMPRESSED    2           100000      EQUAL\n[1] row(s) selected.\n\n\nSHOW INDEX\n\n인덱스 전체 리스트를 출력한다.\n\nSyntax:\n\nSHOW INDEXES\n\n\nExample:\n\nMach&gt; CREATE TABLE t1 (c1 INTEGER, c2 VARCHAR(10));\nCreated successfully.\nMach&gt; CREATE VOLATILE TABLE t2 (c1 INTEGER, c2 VARCHAR(10));\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx1 ON t1(c1) INDEX_TYPE LSM;\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx2 ON t1(c1) INDEX_TYPE BITMAP;\nCreated successfully.\nMach&gt; CREATE INDEX t2_idx1 ON t2(c1) INDEX_TYPE REDBLACK;\nCreated successfully.\nMach&gt; CREATE INDEX t2_idx2 ON t2(c2) INDEX_TYPE REDBLACK;\nCreated successfully.\n \nMach&gt; SHOW INDEXES;\nTABLE_NAME                                COLUMN_NAME                               INDEX_NAME                      \n----------------------------------------------------------------------------------------------------------------------------------\nINDEX_TYPE\n---------------\nT1                                        C1                                        T1_IDX1                         \nLSM\nT1                                        C1                                        T1_IDX2                         \nLSM\nT2                                        C2                                        T2_IDX2                         \nREDBLACK\nT2                                        C1                                        T2_IDX1                         \nREDBLACK\n[4] row(s) selected.\n\n\nSHOW INDEXGAP\n\n인덱스 생성 GAP 정보를 출력한다.\n\nExample:\n\nMach&gt; SHOW INDEXGAP\nTABLE_NAME                                INDEX_NAME                                GAP\n-------------------------------------------------------------------------------------------------------------\nINDEX_TABLE                               T1_IDX1                                   0\nINDEX_TABLE                               T1_IDX2                                   0\n\n\nSHOW LSM\n\nLSM 인덱스 생성 정보를 출력한다.\n\nExample:\n\nMach&gt; SHOW LSM;\nTABLE_NAME                                INDEX_NAME                                LEVEL       COUNT\n--------------------------------------------------------------------------------------------------------------------------\nT1                                        IDX1                                      0           0\nT1                                        IDX1                                      1           100000\nT1                                        IDX1                                      2           0\nT1                                        IDX1                                      3           0\nT1                                        IDX2                                      0           100000\nT1                                        IDX2                                      1           0\n[6] row(s) selected.\n\n\nSHOW LICENSE\n\n라이선스 정보를 출력한다.\n\nExample:\n\nMach&gt; SHOW LICENSE\nINSTALL_DATE          ISSUE_DATE            EXPIRY_DATE  TYPE        POLICY    \n---------------------------------------------------------------------------------------\n2016-07-01 10:24:37   20160325              20170325    2           0         \n[1] row(s) selected.\n\n\nSHOW STATEMENTS\n\n서버에 등록(Prepare, Execute, Fetch)된 모든 질의문을 출력한다.\n\nExample:\n\nMach&gt; SHOW STATEMENTS\nUSER_ID     SESSION_ID  QUERY                                                                           \n--------------------------------------------------------------------------------------------------------------\n0           2           SELECT ID USER_ID, SESS_ID SESSION_ID, QUERY FROM V$STMT                        \n[1] row(s) selected.\n\n\nSHOW STORAGE\n\n사용자가 생성한 테이블 별 디스크 사용량을 출력한다.\nSyntax:\n\nSHOW STORAGE\n\n\nExample:\n\nMach&gt; CREATE TAGDATA TABLE TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\nCreated successfully.\n \nMach&gt; SHOW STORAGE\nTABLE_NAME                                          DATA_SIZE            INDEX_SIZE           TOTAL_SIZE          \n------------------------------------------------------------------------------------------------------------------------  \n_TAG_DATA_0                                         50335744             0                    50335744            \n_TAG_DATA_1                                         50335744             0                    50335744            \n_TAG_DATA_2                                         50335744             0                    50335744            \n_TAG_DATA_3                                         50335744             0                    50335744            \n_TAG_META                                           0                    0                    0\n\n\nSHOW TABLE\n\n사용자가 생성한 테이블의 정보를 출력한다.\n\nSyntax:\n\nSHOW TABLE table_name\n\n\nExample:\n\nMach&gt; CREATE TABLE t1 (c1 INTEGER, c2 VARCHAR(10));\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx1 ON t1(c1) INDEX_TYPE LSM;\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx2 ON t1(c1) INDEX_TYPE BITMAP;\nCreated successfully.\n \nMach&gt; SHOW TABLE T1\n[ COLUMN ]\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH\n----------------------------------------------------------------\nC1                            integer             11\nC2                            varchar             10\n \n[ INDEX ]\n----------------------------------------------------------------\nNAME                          TYPE                COLUMN\n----------------------------------------------------------------\nT1_IDX1                       LSM                 C1\nT1_IDX2                       LSM                 C1\n\n\nSHOW TABLES\n\n사용자가 생성한 테이블 전체 목록을 출력한다.\n\nExample:\n\nMach&gt; SHOW TABLES\nNAME                                    \n--------------------------------------------\nBONUS                                   \nDEPT                                    \nEMP                                     \nSALGRADE                                \n[4] row(s) selected.\n\n\nSHOW TABLESPACE\n\n테이블 스페이스 정보를 출력한다.\n\nExample:\n\nMach&gt; CREATE TABLE t1 (id integer);\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx_id ON t1(id);\nCreated successfully.\n \nMach&gt; SHOW TABLESPACE SYSTEM_TABLESPACE;\n[TABLE]\nNAME                                      TYPE\n-------------------------------------------------------\nT1                                        LOG\n[1] row(s) selected.\n \n[INDEX]\nTABLE_NAME                                COLUMN_NAME                               INDEX_NAME                      \n----------------------------------------------------------------------------------------------------------------------------------\nT1                                        ID                                        T1_IDX_ID                   \n[1] row(s) selected.\n\n\nSHOW TABLESPACES\n\n테이블스페이스 전체 목록을 출력한다.\n\nExample:\n\nMach&gt; CREATE TABLESPACE tbs1 DATADISK disk1 (DISK_PATH=\"tbs1_disk1\"), disk2 (DISK_PATH=\"tbs1_disk2\"), disk3 (DISK_PATH=\"tbs1_disk3\");\nCreated successfully.\n \n-- 데이터를 입력한다\n...\n...\n \n \nMach&gt; SHOW TABLESPACES;\nNAME                                                                              DISK_COUNT  USAGE\n-----------------------------------------------------------------------------------------------------------------------\nSYSTEM_TABLESPACE                                                                 1           0\nTBS1                                                                              3           25824256\n[2] row(s) selected.\n\n\nSHOW USERS\n\n사용자 목록을 출력한다.\n\nExample:\n\nMach&gt; CREATE USER testuser IDENTIFIED BY 'test1234';\nCreated successfully.\n \nMach&gt; SHOW USERS;\nUSER_NAME                               \n--------------------------------------------\nSYS                                     \nTESTUSER\n[2] row(s) selected."
					}
					
				
		
				
		
				
					,
					
					"assets-js-main-js": {
						"id": "assets-js-main-js",
						"title": "",
						"version": "all",
						"categories": "",
						"url": " /assets/js/main.js",
						"content": "(function($) {\n    'use strict';\n    $(function() {\n        $('[data-toggle=\"tooltip\"]').tooltip();\n        $('[data-toggle=\"popover\"]').popover();\n        $('.popover-dismiss').popover({\n            trigger: 'focus'\n        })\n    });\n\n    function bottomPos(element) {\n        return element.offset().top + element.outerHeight();\n    }\n    $(function() {\n        var promo = $(\".js-td-cover\");\n        if (!promo.length) {\n            return\n        }\n        var promoOffset = bottomPos(promo);\n        var navbarOffset = $('.js-navbar-scroll').offset().top;\n        var threshold = Math.ceil($('.js-navbar-scroll').outerHeight());\n        if ((promoOffset - navbarOffset) < threshold) {\n            $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n        }\n        $(window).on('scroll', function() {\n            var navtop = $('.js-navbar-scroll').offset().top - $(window).scrollTop();\n            var promoOffset = bottomPos($('.js-td-cover'));\n            var navbarOffset = $('.js-navbar-scroll').offset().top;\n            if ((promoOffset - navbarOffset) < threshold) {\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n            } else {\n                $('.js-navbar-scroll').removeClass('navbar-bg-onscroll');\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll--fade');\n            }\n        });\n    });\n}(jQuery));\n(function($) {\n    'use strict';\n    var Search = {\n        init: function() {\n            $(document).ready(function() {\n                $(document).on('keypress', '.td-search-input', function(e) {\n                    if (e.keyCode !== 13) {\n                        return\n                    }\n                    var query = $(this).val();\n                    var searchPage = \"/kr/search/?q=\" + query;\n                    document.location = searchPage;\n                    return false;\n                });\n            });\n        },\n    };\n    Search.init();\n}(jQuery));"
					}
					
				
		
				
					,
					
					"feature-table-tag-manipulate-html": {
						"id": "feature-table-tag-manipulate-html",
						"title": "태그 데이터 조작",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/manipulate.html",
						"content": "태그 데이터의 입력\n  태그 데이터의 추출\n  태그 데이터의 삭제"
					}
					
				
		
				
					,
					
					"config-monitor-meta-table-html": {
						"id": "config-monitor-meta-table-html",
						"title": "Meta Table",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/meta-table.html",
						"content": "메타 테이블은 마크베이스의 스키마 정보를 제시해 주는 테이블들로 테이블 명이 M$로 시작된다.\n\n이 테이블들은 테이블의 이름과, 컬럼 정보, 인덱스 정보들을 유지하고 있고, DDL문에 의해서 생성, 변경, 삭제된 상황을 반영한다.\n이 메타 테이블은 사용자에 의해서 추가, 삭제, 변경될 수 없다.\n\nUser Object\nM$SYS_TABLES\n\n사용자가 생성한 테이블을 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      NAME\n      테이블 이름\n    \n    \n      TYPE\n      테이블 유형0: Log1: Fixed3: Volatile4: Lookup5: Key Value6: Tag\n    \n    \n      DATABASE_ID\n      데이터베이스 식별자\n    \n    \n      ID\n      테이블 식별자\n    \n    \n      USER ID\n      테이블을 생성한 사용자\n    \n    \n      COLCOUNT\n      컬럼의 개수\n    \n    \n      FLAG\n      테이블 타입 구분1 : Tag Data Table2 : Rollup Table4 : Tag Meta Table8 : Tag Stat Table\n    \n  \n\n\nM$SYS_TABLE_PROPERTY\n\n각 테이블에 적용된 테이블 프로퍼티 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      ID\n      대상 테이블 ID\n    \n    \n      NAME\n      프로퍼티 이름\n    \n    \n      VALUE\n      프로퍼티 값\n    \n  \n\n\nM$SYS_COLUMNS\n\nM$SYS_TABLES 에 표시된 사용자 테이블의 컬럼 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      NAME\n      컬럼 이름\n    \n    \n      TYPE\n      컬럼 타입\n    \n    \n      DATABASE_ID\n      데이터베이스 식별자\n    \n    \n      ID\n      컬럼 식별자\n    \n    \n      LENGTH\n      컬럼 길이\n    \n    \n      TABLE_ID\n      컬럼이 속한 테이블의 식별자\n    \n    \n      FLAG\n      (서버 내부 사용을 위한 정보)\n    \n    \n      PART_PAGE_COUNT\n      파티션 당 페이지 수\n    \n    \n      PAGE_VALUE_COUNT\n      페이지 당 데이터의 수\n    \n    \n      MINMAX_CACHE_SIZE\n      MIN-MAX 캐쉬의 크기\n    \n    \n      MAX_CACHE_PART_COUNT\n      파티션 캐쉬의 최대 개수\n    \n  \n\n\nM$SYS_INDEXES\n\n사용자가 생성한 인덱스 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      NAME\n      인덱스의 이름\n    \n    \n      TYPE\n      인덱스의 종류\n    \n    \n      DATABASE_ID\n      데이터베이스 식별자\n    \n    \n      ID\n      인덱스 식별자\n    \n    \n      TABLE_ID\n      인덱스를 생성한 테이블 식별자\n    \n    \n      COLCOUNT\n      인덱스를 생성한 컬럼의 수\n    \n    \n      PART_VALUE_COUNT\n      인덱스가 속한 테이블의 파티션당 데이터 수\n    \n    \n      BLOOM_FILTER\n      Bloom Filter 사용 가능 여부\n    \n    \n      KEY_COMPRESS\n      키값의 압축 여부\n    \n    \n      MAX_LEVEL\n      인덱스의 최대 레벨 (LSM 만 가능)\n    \n    \n      PAGE_SIZE\n      페이지 크기\n    \n    \n      MAX_KEYWORD_SIZE\n      최대 키워드 길이 (Keyword 만 가능)\n    \n    \n      BITMAP_ENCODE\n      Bitmap 인코딩 유형 (RANGE / EQUAL)\n    \n  \n\n\nM$SYS_INDEX_COLUMNS\n\nM$SYS_INDEXES 에 표시된 사용자 인덱스의 컬럼 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      INDEX_ID\n      인덱스 식별자\n    \n    \n      INDEX_TYPE\n      인덱스 종류\n    \n    \n      NAME\n      컬럼의 이름\n    \n    \n      COL_ID\n      컬럼 식별자\n    \n    \n      DATABASE_ID\n      데이터베이스 식별자\n    \n    \n      TABLE_ID\n      테이블 식별자\n    \n    \n      TYPE\n      컬럼의 데이터타입\n    \n  \n\n\nM$SYS_TABLESPACES\n\n사용자가 생성한 테이블스페이스 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      NAME\n      테이블스페이스 이름\n    \n    \n      ID\n      테이블스페이스 식별자\n    \n    \n      DISK_COUNT\n      테이블스페이스에 속한 디스크의 수\n    \n  \n\n\nM$SYS_TABLESPACE_DISKS\n\n테이블스페이스가 사용하는 디스크 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      NAME\n      디스크 이름\n    \n    \n      ID\n      디스크 식별자\n    \n    \n      TABLESPACE_ID\n      디스크가 속한 테이블스페이스 식별자\n    \n    \n      PATH\n      디스크의 경로\n    \n    \n      IO_THREAD_COUNT\n      이 디스크에 할당된 IO 스레드의 수\n    \n    \n      VIRTUAL_DISK_COUNT\n      이 디스크에 할당된 Virtual Disk 단위 개수\n    \n  \n\n\nM$SYS_USERS\n\n마크베이스에 등록된 사용자 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      USER_ID\n      사용자의 식별자\n    \n    \n      NAME\n      사용자의 이름\n    \n  \n\n\nOthers\nM$TABLES\n\nM$로 시작하는 모든 메타테이블을 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      NAME\n      메타 테이블의 이름\n    \n    \n      TYPE\n      테이블 유형\n    \n    \n      DATABASE_ID\n      데이터베이스 식별자\n    \n    \n      ID\n      메타 테이블의 식별자\n    \n    \n      USER ID\n      테이블을 생성한 사용자 (여기서는 SYS)\n    \n    \n      COLCOUNT\n      컬럼의 개수\n    \n  \n\n\nM$COLUMNS\n\nM$TABLES 에 표시된 메타테이블의 컬럼 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      NAME\n      컬럼 이름\n    \n    \n      TYPE\n      컬럼 타입\n    \n    \n      DATABASE_ID\n      데이터베이스 식별자\n    \n    \n      ID\n      컬럼 식별자\n    \n    \n      LENGTH\n      컬럼 길이\n    \n    \n      TABLE_ID\n      컬럼이 속한 테이블의 식별자\n    \n    \n      FLAG\n      (서버 내부 사용을 위한 정보)\n    \n    \n      PART_PAGE_COUNT\n      파티션당 페이지 수\n    \n    \n      PAGE_VALUE_COUNT\n      페이지 당 데이터의 수\n    \n    \n      MINMAX_CACHE_SIZE\n      MIN-MAX 캐쉬의 크기\n    \n    \n      MAX_CACHE_PART_COUNT\n      파티션 캐쉬의 최대 개수\n    \n  \n\n\nM%RETENTION\n\nRETENTION POLICY 정보를 표시한다.\n\n\n  \n    \n      컬럼 이름\n      설명\n    \n  \n  \n    \n      USER_ID\n      사용자 ID\n    \n    \n      POLICY_NAME\n      policy 이름\n    \n    \n      DURATION\n      만료 기간(sec)\n    \n    \n      INTERVAL\n      갱신 주기(sec)"
					}
					
				
		
				
					,
					
					"install-windows-msi-install-html": {
						"id": "install-windows-msi-install-html",
						"title": "MSI 설치",
						"version": "all",
						"categories": "",
						"url": " /install/windows/msi-install.html",
						"content": "설치하기\n\n\n  \n    마크베이스 다운로드 사이트에서 설치 파일을 다운로드 받아서 실행한다.\n설치파일을 더블 클릭하여 실행하면, 아래처럼 보안 경고 창이 나타난다. 실행 버튼을 클릭한다.\n\n  \n  \n    설치 시작 화면이 표시되면, Next 버튼을 클릭한다.\n\n  \n  \n    설치할 디렉터리를 선택하는 화면이 표시되는데, 기본적으로 “C:\\Machbase-5.1\" 디렉터리에 설치가 된다. \n다른 디렉터리에 설치하려면, 해당 경로 변경하여 설치한다.\n지정 완료 후 Next 버튼을 클릭한다.\n\n  \n  \n    설치 진행 화면이 표시된다. 설치가 완료되면 Next 버튼이 활성화되는데, 이 버튼을 클릭한다.\n\n  \n  \n    설치 완료 화면이 표시된다. Close 버튼을 클릭한다.\n\n  \n\n\n실행하기\n\n\n  \n    마크베이스 설치가 완료되면, 바탕화면에 마크베이스 실행 아이콘이 표시된다.\n더블 클릭하면 마크베이스 서버가 실행된다.\n\n  \n  \n    마크베이스 서버를 관리하는 윈도우 인터페이스 화면이다.\n메뉴를 클릭해서 마크베이스 서버와 MWA 웹서버를 제어할 수 있다."
					}
					
				
		
				
					,
					
					"feature-table-log-select-network-type-html": {
						"id": "feature-table-log-select-network-type-html",
						"title": "네트워크 데이터 타입 / 연산자",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/select/network-type.html",
						"content": "마크베이스는 네트워크 데이터 타입을 지원함과 동시에 SELECT 문에서 사용할 수 있는 함수들을 지원한다.\n\n\n  IPv4 형식 : 4바이트 주소 타입\n  IPv6 형식 : 16바이트 주소 타입\n  네트워크 마스크 : IPv4  또는 IPv6 에 대한 네트워크 마스크 지정 형식(/비트수)\n\n\n목차\n\n  IPv4\n    \n      INSERT\n      SELECT\n    \n  \n  IPv6\n    \n      INSERT\n      SELECT\n    \n  \n  네트워크 마스크\n    \n      마스크의 표현 형태\n      마스크 연산자\n      마스크 사용 예제\n    \n  \n\n\nIPv4\n\nINSERT\n\nINSERT INTO table_name VALUES (value1,value2,value3,...);\n\nCREATE TABLE addrtable (addr IPV4);\nINSERT  INTO addrtable VALUES ('127.0.0.1');\nINSERT  INTO addrtable VALUES ('127.0' || '.0.2');\nINSERT  INTO addrtable VALUES ('127.0.0.3');\nINSERT  INTO addrtable VALUES ('127.0.0.4');\nINSERT  INTO addrtable VALUES ('127.0.0.5');\nINSERT  INTO addrtable VALUES ('255.255.255.255');\n\n\nSELECT\nSELECT  column_name,column_name FROM    table_name;\n\nMach&gt; SELECT addr FROM addrtable WHERE addr = '127.0.0.3' or addr = '127.0.0.5';\naddr           \n------------------\n127.0.0.5      \n127.0.0.3      \n[2] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable WHERE addr &gt; '127.0.0.3' AND addr &lt; '127.0.0.5';\naddr           \n------------------\n127.0.0.4      \n[1] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable WHERE addr &lt;&gt; '127.0.0.3';\naddr           \n------------------\n255.255.255.255\n127.0.0.5      \n127.0.0.4      \n127.0.0.2      \n127.0.0.1      \n[5] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable WHERE addr = '127.0.0.*';\naddr           \n------------------\n127.0.0.5      \n127.0.0.4      \n127.0.0.3      \n127.0.0.2      \n127.0.0.1      \n[5] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable WHERE addr = '*.0.0.*';\naddr           \n------------------\n127.0.0.5      \n127.0.0.4      \n127.0.0.3      \n127.0.0.2      \n127.0.0.1      \n[5] row(s) selected.\n\n\nIPv6\n\nINSERT\n\nINSERT  INTO    table_name  VALUES  (value1,value2,value3,...);\n\nCREATE TABLE addrtable6 (addr ipv6);\nINSERT INTO addrtable6 VALUES ('::0.0.0.0');\nINSERT INTO addrtable6 VALUES ('::127.0' || '.0.1');\nINSERT INTO addrtable6 VALUES ('::127.0.0.3');\nINSERT INTO addrtable6 VALUES ('::127.0.0.4');\nINSERT INTO addrtable6 VALUES ('21DA:D3:0:2F3B:2AA:FF:FE28:9C5A');\nINSERT INTO addrtable6 VALUES ('::FFFF:255.255.255.255');\n\n\nSELECT\n\nSELECT  column_name,column_name FROM    table_name;\n\nMach&gt; SELECT addr FROM addrtable6 WHERE addr = '::127.0.0.3' or addr = '::127.0.0.5';\naddr                                                        \n---------------------------------------------------------------\n::127.0.0.3                  \n[1] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable6 WHERE addr &gt; '::127.0.0.3' and addr &lt; '::127.0.0.5';\naddr                                                        \n---------------------------------------------------------------\n::127.0.0.4                     \n[1] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable6 WHERE addr &lt;&gt; '::127.0.0.3';\naddr                                                        \n---------------------------------------------------------------\n::ffff:255-255.255.255\n21da:d3::2f3b:2aa:ff:fe28:9c5a\n::127.0.0.4\n::127.0.0.1\n::                     \n[5] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable6 WHERE addr &gt;= '21DA::';\naddr                                                        \n---------------------------------------------------------------\n21da:d3::2f3b:2aa:ff:fe28:9c5a                   \n[1] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable6 order by addr desc;\naddr                                                        \n---------------------------------------------------------------\n21da:d3::2f3b:2aa:ff:fe28:9c5a\n::ffff:255.255.255.255\n::127.0.0.4\n::127.0.0.3\n::127.0.0.1\n::                   \n[6] row(s) selected.\n\n\n네트워크 마스크\n\n네트워크 마스크는 특정 주소가 특정한 네트워크에 포함되는지를 지정하는 표현 형식이다. 마크베이스는 네트워크 마스크 타입과 관련 연산자를 지원한다.\n\n마스크의 표현 형태\n\n일반 네트워크 표현과 마찬가지로 네트워크 주소 마지막에 / 기호와 비트 개수를 표현하는 형식으로 나타낸다.\n\n'192.128.0.0/16'\n'FFFF::192.128.99.0/32'\n\n\n마스크 연산자\n\nCONTAINS\n\n이 연산자는 왼쪽에 네트워크 마스크와 오른쪽에 네트워크 주소 데이터 타입이 나와야 한다. 즉  입력된 주소가 주어진 네트워크 마스크에 포함되는지를 검사한다. NOT 연산자도 함께 사용할 수 있다.\n\nSELECT addr FROM addrtable  WHERE '192.0.0.0/16'    CONTAINS    addr;\nSELECT addr FROM addrtable  WHERE '192.128.99.0/32' NOT CONTAINS    addr;\n\n\nCONTAINED\n\nCONSTAINS와 반대로, 네트워크 주소가 왼쪽, 네트워크 마스크가 오른쪽이다. 왼쪽 주소가 오른쪽 마스크의 일부인지를 검사한다.\n\nSELECT addr FROM addrtable  WHERE addr CONTAINED '192.0.0.0/16';\nSELECT addr FROM addrtable  WHERE addr NOT CONTAINED '192.128.99.0/32';\n\n\n마스크 사용 예제\n\n네트워크 마스크 타입을 이용한 검색의 예는 다음과 같다.\n\nCREATE TABLE ip_table (addr4 IPV4, addr6 IPV6);\n \nINSERT INTO ip_table VALUES ('192.0.0.1','FFFF::192.0.0.1');\nINSERT INTO ip_table VALUES ('192.0.10.1','FFFF::192.0.10.1');\nINSERT INTO ip_table VALUES ('192.128.0.1','FFFF::192.128.0.1');\nINSERT INTO ip_table VALUES ('192.128.99.128','FFFF::192.128.99.128');\nINSERT INTO ip_table VALUES ('192.128.99.64','FFFF::192.128.99.64');\nINSERT INTO ip_table VALUES ('192.128.99.32','FFFF::192.128.99.32');\nINSERT INTO ip_table VALUES ('192.128.99.16','FFFF::192.128.99.16');\nINSERT INTO ip_table VALUES ('192.128.99.8','FFFF::192.128.99.8');\nINSERT INTO ip_table VALUES ('192.128.99.4','FFFF::192.128.99.4');\nINSERT INTO ip_table VALUES ('192.128.99.2','FFFF::192.128.99.2');\nINSERT INTO ip_table VALUES ('192.128.99.1','FFFF::192.128.99.1');\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.0.0.0/16' CONTAINS addr4;\naddr4\n-----------\n192.0.10.1\n192.0.0.1\n[2] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.128.0.0/16' CONTAINS addr4;\naddr4\n-----------\n192.128.99.1\n192.128.99.2\n192.128.99.4\n192.128.99.8\n192.128.99.16\n192.128.99.32\n192.128.99.64\n192.128.99.128\n192.128.0.1\n[9] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.0.10.0/24' CONTAINS addr4;\naddr4\n--------------------------------------------------------------------\n192.0.10.1\n[1] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.128.99.0/31' CONTAINS addr4;\naddr4\n-------------------------------------------------------\n192.128.99.1\n[1] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.128.99.0/32' NOT CONTAINS addr4;\naddr4\n-----------\n192.128.99.1\n192.128.99.2\n192.128.99.4\n192.128.99.8\n192.128.99.16\n192.128.99.32\n192.128.99.64\n192.128.99.128\n192.128.0.1\n192.0.10.1\n192.0.0.1\n[11] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE addr4 CONTAINED '192.0.0.0/16';\naddr4\n-------------------------------------\n192.0.10.1\n192.0.0.1\n[2] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE addr4 CONTAINED '192.128.0.0/16';\naddr4\n-------------------------------------\n192.128.99.1\n192.128.99.2\n192.128.99.4\n192.128.99.8\n192.128.99.16\n192.128.99.32\n192.128.99.64\n192.128.99.128\n192.128.0.1\n[9] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE addr4 CONTAINED '192.0.10.0/24';\naddr4\n----------------------------\n192.0.10.1\n[1] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE addr4 not CONTAINED '192.128.99.0/32';\naddr4\n-------------------------------------------------\n192.128.99.1\n192.128.99.2\n192.128.99.4\n192.128.99.8\n192.128.99.16\n192.128.99.32\n192.128.99.64\n192.128.99.128\n192.128.0.1\n192.0.10.1\n192.0.0.1\n[11] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.0.0.0/104' CONTAINS addr6;\naddr6\n-------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\nffff::c000:a01\nffff::c000:1\n[11] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.128.0.0/112' CONTAINS addr6;\naddr6\n------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\n[9] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.0.10.0/120' CONTAINS addr6;\naddr6\n------------------------------------------------\nffff::c000:a01\n[1] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.128.99.0/31' CONTAINS addr6;\naddr6\n---------------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\nffff::c000:a01\nffff::c000:1\n[11] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.128.99.0/32' not CONTAINS addr6;\naddr6\n-------------------------------------\n[0] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE addr6 CONTAINED 'FFFF::192.0.0.0/104';\naddr6\n-------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\nffff::c000:a01\nffff::c000:1\n[11] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE addr6 CONTAINED 'FFFF::192.128.0.0/112';\naddr6\n-------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\n[9] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE addr6 CONTAINED 'FFFF::192.0.10.0/120';\naddr6\n-------------------------------------\nffff::c000:a01\n[1] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE addr6 not CONTAINED 'FFFF::192.128.99.0/32';\naddr6\n-------------------------------------\n[0] row(s) selected."
					}
					
				
		
				
					,
					
					"install-package-html": {
						"id": "install-package-html",
						"title": "패키지 개요",
						"version": "all",
						"categories": "",
						"url": " /install/package.html",
						"content": "패키지 종류\n\nMACHBASE 는 매뉴얼 설치, 패키지 설치 파일을 제공한다.\n\n\n  \n    \n      설치 형태\n      특징\n      비고\n    \n  \n  \n    \n      매뉴얼 설치\n      압축파일 형태를 띄고 있으며, 유닉스의 경우에는 확장자가 tgz 이다.사용자는 tar 및 GNU gzip 을 이용하여 압축을 풀어서 설치를 진행해야 한다.\n      콘솔 환경에서만 설치 가능\n    \n    \n      패키지 설치\n      각 운영체제 환경에 맞는 설치 패키지를 제공한다. - Windows: msi  -  Linux: tgz\n      콘솔 환경에서 설치 가능\n    \n  \n\n\n패키지 파일명 구조\n\n패키지 파일명은 다음과 같이 구성된다.\n\n_machbase-EDITION_VERSION-OS-CPU-BIT-MODE-OPTIONAL.EXT_\n\n\n\n  \n    \n      항목\n      설명\n    \n  \n  \n    \n      EDITION\n      패키지의 에디션을 나타낸다. - standard: Standard Edition - cluster: Cluster Edition\n    \n    \n      VERSION\n      버전을 나태낸다.세부적으로는 MajorVersion.MinorVersion.FixVersion.AUX 로 숫자 및 문자로 구분된다.- Major Version: 제품 메인버전- Minor Version: 같은 메인버전에서 비교적 큰 기능이 추가된 버전. DB 파일/프로토콜 호환을 보장하지 않는다.- Fix Version: 같은 메인 버전에서 버그/사소한 기능이 추가된 버전. DB 파일/프로토콜 호환을 보장한다. - AUX: 패키지의 구분을 나타낸다.– official: 일반 패키지 – community: 커뮤니티 에디션 용 패키지 – develop: 내부 개발자용 패키지(비공개)\n    \n    \n      OS\n      운영채제 명을 나타낸다. (예: LINUX, WINDOWS)\n    \n    \n      CPU\n      해당 운영체제에 설치된 CPU 의 타입을 나타낸다. (예: X86, IA64)\n    \n    \n      BIT\n      컴파일된 바이너리가 32비트 혹은 64비트 인지 나타낸다. (예: 32, 64)\n    \n    \n      MODE\n      컴파일 시 해당 바이너리의 릴리즈 모드를 나타낸다. (예: release, debug, prerelease)\n    \n    \n      OPTIONAL\n      lightweight: coordinator 에 추가할 경량화 package 를 나타낸다.\n    \n    \n      EXT\n      패키지 파일 확장자이다. 패키지에 따라 tgz, rpm, deb, msi 로 제공된다."
					}
					
				
		
				
		
				
					,
					
					"config-monitor-property-cl-html": {
						"id": "config-monitor-property-cl-html",
						"title": "Property (Cluster)",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/property-cl.html",
						"content": "Property와 별개로, Cluster Edition 에서만 사용 가능한 Property 를 정리한다.\n\n목차\n\n  CLUSTER_LINK_ACCEPT_TIMEOUT\n  CLUSTER_LINK_BUFFER_SIZE\n  CLUSTER_LINK_CHECK_INTERVAL\n  CLUSTER_LINK_CONNECT_RETRY_TIMEOUT\n  CLUSTER_LINK_CONNECT_TIMEOUT\n  CLUSTER_LINK_ERROR_ADD_ORIGIN_HOST\n  CLUSTER_LINK_HANDSHAKE_TIMEOUT\n  CLUSTER_LINK_HOST\n  CLUSTER_LINK_LONG_TERM_CALLBACK_INTERVAL\n  CLUSTER_LINK_LONG_WAIT_INTERVAL\n  CLUSTER_LINK_MAX_LISTEN\n  CLUSTER_LINK_MAX_POLL\n  CLUSTER_LINK_PORT_NO\n  CLUSTER_LINK_RECEIVE_TIMEOUT\n  CLUSTER_LINK_REQUEST_TIMEOUT\n  CLUSTER_LINK_SEND_RETRY_COUNT\n  CLUSTER_LINK_SEND_TIMEOUT\n  CLUSTER_LINK_SESSION_TIMEOUT\n  CLUSTER_LINK_THREAD_COUNT\n  CLUSTER_QUERY_STAT_LOG_ENABLE\n  CLUSTER_REPLICATION_BLOCK_SIZE\n  CLUSTER_WAREHOUSE_DIRECT_DML_ENABLE\n  COORDINATOR_DBS_PATH\n  COORDINATOR_DDL_REQUEST_TIMEOUT\n  COORDINATOR_DDL_TIMEOUT\n  COORDINATOR_DECISION_DELAY\n  COORDINATOR_DECISION_INTERVAL\n  COORDINATOR_HOST_RESOURCE_ENABLE\n  COORDINATOR_HOST_RESOURCE_COLLECT_INTERVAL\n  COORDINATOR_HOST_RESOURCE_INTERVAL\n  COORDINATOR_HOST_RESOURCE_REQUEST_TIMEOUT\n  COORDINATOR_NODE_REQUEST_TIMEOUT\n  COORDINATOR_NODE_TIMEOUT\n  COORDINATOR_STARTUP_DELAY\n  COORDINATOR_STATUS_NODE_INTERVAL\n  COORDINATOR_STATUS_NODE_REQUEST_TIMEOUT\n  COORDINATOR_DISK_FULL_UPPER_BOUND_RATIO\n  COORDINATOR_DISK_FULL_LOWER_BOUND_RATIO\n  DEPLOYER_DBS_PATH\n  EXECUTION_STAGE_MEMORY_MAX\n  HTTP_ADMIN_PORT\n  HTTP_CONNECT_TIMEOUT\n  HTTP_RECEIVE_TIMEOUT\n  HTTP_SEND_TIMEOUT\n  INSERT_BULK_DATA_MAX_SIZE\n  INSERT_RECORD_COUNT_PER_NODE\n  LOOKUPNODE_COMMAND_RETRY_MAX_COUNT\n  STAGE_RESULT_BLOCK_SIZE\n\n\nCLUSTER_LINK_ACCEPT_TIMEOUT\n\n특정 Node와 연결할 때, Accept 후 Handshake 메시지를 수신할 때까지의 Timeout.\n\nTimeout 이후까지 수신에 실패하면, 해당 연결은 실패한다.\n\n기본값은 5초.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      5000000\n    \n  \n\n\nCLUSTER_LINK_BUFFER_SIZE\n\n\n  5.6 부터 사용 가능합니다.\n\n\n이 크기가 모자라면 송신시 버퍼가 비워질 때 까지 재시도하게 된다.송신/수신 버퍼의 크기.\n\n기본값은 32M.\n\n\n  \n    \n      (byte)\n      Value\n    \n  \n  \n    \n      최소값\n      1024768\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      33554432 (32M)\n    \n  \n\n\nCLUSTER_LINK_CHECK_INTERVAL\n\n특정 Node와 연결된 Socket들을 검사하는, Timeout Thread의 검사 주기.\n\nRECEIVE_TIMEOUT, SESSION_TIMEOUT 을 검사하는 Timeout Thread가 존재한다.\n주기를 짧게 할 수록, 자주 검사하지만 Timeout 판단은 아래의 값에 따라 이루어진다.\n\n기본값은 1초.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1000000\n    \n  \n\n\nCLUSTER_LINK_CONNECT_RETRY_TIMEOUT\n\n특정 Node와 연결이 실패한 이후, 재연결 시도를 반복하는 Timeout\n\nTimeout 이후까지 재연결되지 않는다면, 완전히 단절되었다고 판단한다.\n\n기본값은 1분.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      60000000\n    \n  \n\n\nCLUSTER_LINK_CONNECT_TIMEOUT\n\n특정 Node와 연결을 시도할 때, 기다리는 시간.\n\nTimeout 이후까지 연결되지 않는다면, CLUSTER_LINK_CONNECT_RETRY_TIMEOUT 이 지나기 전 까지 재연결을 시도한다.\n\n기본값은 5초.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      5000000\n    \n  \n\n\nCLUSTER_LINK_ERROR_ADD_ORIGIN_HOST\n\nCluster 간 통신 중 발생하는 에러 메시지에, 오류가 발생한 호스트 이름을 추가할지 여부를 선택할 수 있다.\n\n자세한 에러 메시지를 표시하고자 한다면, 해당 Property를 켜야 한다.\n\n기본값은 ‘No’ (=0). 호스트 이름이 출력되지 않는다.\n\n\n  \n    \n      (boolean)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nCLUSTER_LINK_HANDSHAKE_TIMEOUT\n\n특정 Node와 Cluster Socket으로 연결된 상태에서, Handshake 메시지를 수신할 때까지의 Timeout\n\n연결이 막 완료된 두 Node는, 연결 상태를 점검하는 차원에서 작은 크기의 Handshake 메시지를 주고 받는다.\nAccept한 Node가 Handshake 메시지를 먼저 보내는데, 그 응답을 기다리는 시간을 여기서 설정한다.\n\n기본값은 5초.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      5000000\n    \n  \n\n\nCLUSTER_LINK_HOST\n\n특정 Node와 Cluster Socket 을 연결하기 위한, 현재 Node의 호스트 이름\n\n\n  \n    \n      (string)\n      Value\n    \n    \n      기본값\n      localhost\n    \n  \n\n\nCLUSTER_LINK_LONG_TERM_CALLBACK_INTERVAL\n\nCluster Socket 으로 수신 되는 메시지를 처리할 Receive Callback 이 수행한 시간을 Long-Term Callback 으로 인식할 시간\n\n수신 Thread의 개수가 제한적이므로, 가급적이면 Receive Callback은 오랜 시간 동안 메시지를 처리하고 있으면 안 된다.\n이 시간이 지나도록 Receive Callback이 메시지를 처리하고 있다면, Long-Term Callback 으로 인식하고 Trace Log에 그 기록을 남긴다.\n\n기본값은 1초.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1000000\n    \n  \n\n\nCLUSTER_LINK_LONG_WAIT_INTERVAL\n\nCluster Socket 으로 수신 되는 메시지가 도착할 때 까지의 시간을 Long-Wait Message 로 인식할 시간\n\n수신 시작~수신 종료 까지의 시간이 길면 네트워크 환경의 문제로 볼 수 있다.\n이 시간이 지나도록 수신 메시지가 도착하지 않는다면, Long-Wait Message 로 인식하고 Trace Log에 그 기록을 남긴다.\n\n기본값은 1초.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1000000\n    \n  \n\n\nCLUSTER_LINK_MAX_LISTEN\n\n특정 Node와 연결할 때, Socket의 Accept Queue 의 최대 숫자\n\n\n  \n    \n      (count)\n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      512\n    \n  \n\n\nCLUSTER_LINK_MAX_POLL\n\n특정 Node와 통신할 때, Poll에 의헤서 한번에 조회할 수 있는 최대 Event 수\n\n\n  \n    \n      (count)\n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      4096\n    \n  \n\n\nCLUSTER_LINK_PORT_NO\n\n특정 Node와 Cluster Socket 을 연결하기 위한, 현재 Node의 포트 번호\n\n\n  \n    \n      (port)\n      Value\n    \n  \n  \n    \n      최소값\n      1024\n    \n    \n      최대값\n      65535\n    \n    \n      기본값\n      3868\n    \n  \n\n\nCLUSTER_LINK_RECEIVE_TIMEOUT\n\nTimeout Thread가, 마지막 수신 이후로 연결이 끊긴 것을 판단할 때 까지의 Timeout\n\nCluster Node 간 연결은, 수신이 완료되면 종료되기 때문에 ‘연결 리스트’ 에 존재하는 연결들은 지속적으로 수신을 받고 있어야 한다.\n이 시간이 지나도록 마지막 수신 시각이 갱신되지 않으면, Timeout Thread는 Trace Log에 기록을 남기고 해당 Socket을 닫는다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      30000000\n    \n  \n\n\nCLUSTER_LINK_REQUEST_TIMEOUT\n\nCluster Socket에서 요청 메시지를 보냈을 때, 요청에 대한 응답이 올 때 까지의 Timeout\n\n특정 메시지의 경우 Request 이후 Answer 전송까지 대기할 수 있는 시간을 따로 지정한다.\n이 시간이 지나도록 응답 메시지가 도착하지 않으면, Trace Log에 기록을 남기고 해당 Socket을 닫는다.\n\n기본값은 60초. 메시지 종류와 수신 처리가 어떻게 될지 모르므로, Timeout이 길다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      60000000\n    \n  \n\n\nCLUSTER_LINK_SEND_RETRY_COUNT\n\n\n  5.6 부터 사용 가능합니다.\n\n\n재시도를 할 때 마다 1ms 씩 쉬게 된다. 이 회수를 넘어서서 재시도하게 될 경우 연결을 해제하게 된다.송신 버퍼가 비워질 때 까지 송신을 재시도하는 회수.\n\n기본값은 5000.\n\n\n  \n    \n      (count)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      5000\n    \n  \n\n\nCLUSTER_LINK_SEND_TIMEOUT\n\nCluster Socket을 통해 메시지를 송신할 때 설정하는 Timeout\n\n송신할 때 해당 Timeout 을 설정하며, \nTimeout 까지 송신이 완료되지 않으면 Trace Log에 그 기록을 남긴다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      30000000\n    \n  \n\n\nCLUSTER_LINK_SESSION_TIMEOUT\n\nTimeout Thread가, 특정 세션에서 마지막 수신 이후로 연결이 끊긴 것을 판단할 때 까지의 Timeout\n\nCluster 연결은, 내부적으로 모든 메시지의 세션을 관리하고 있다. 갑자기 세션 정리를 하지 못하게 된 상황에서 필요한 Property 이다.\n이 시간이 지나도록 해당 세션에 대한 마지막 수신 시각이 갱신되지 않으면, Timeout Thread는 Trace Log에 기록을 남기고 해당 세션을 닫는다.\n\n기본값은 1시간.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      3600000000\n    \n  \n\n\nCLUSTER_LINK_THREAD_COUNT\n\n특정 Node와 통신할 때, 수신된 메시지를 처리할 Thread의 수\n\nCluster의 규모가 커지거나, 처리해야 할 연산의 개수가 많아져서 수신 가능한 Thread 가 여유가 없을 때 늘릴 수 있다.\n\n\n  \n    \n      (count)\n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      4096\n    \n    \n      기본값\n      8\n    \n  \n\n\nCLUSTER_QUERY_STAT_LOG_ENABLE\n\n수행한 질의에 대한 통계정보를 trace log에 출력한다.\n\n\n  \n    \n      (boolean)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nCLUSTER_REPLICATION_BLOCK_SIZE\n\nCluster Edition 에서, Node 추가로 Replication 을 진행할 때, 한번에 실어 보내는 데이터 크기.\n\nReplication Active 가 되는 Warehouse (=전송을 하는 Warehouse) 에 직접 Property 를 적용해야 한다.\n\n기본값은 640KB 이다.\n\n\n  \n    \n      (size)\n      Value\n    \n  \n  \n    \n      최소값\n      64 * 1024\n    \n    \n      최대값\n      100 * 1024 * 1024\n    \n    \n      기본값\n      640 * 1024 (655360)\n    \n  \n\n\nCLUSTER_WAREHOUSE_DIRECT_DML_ENABLE\n\nCluster Edition 에서, Warehouse 에 곧바로 접속해 DML 을 수행할 수 있도록 한다.\n\n\n  1 : 수행 가능\n  2 : 수행 불가능. 에러가 반환된다.\n\n\nWarehouse 에 직접 DML 을 수행할 경우 Broker 를 통한 것보다 성능 이점이 있지만, 동일 Group 에 DML 이 전파되지 않는 문제가 있다. \n따라서, 데이터 불일치로 인한 비상 복구용 혹은 Group 의 데이터 불일치를 감안해도 되는 경우에 한해 사용한다.\n\n원하는 특정 Warehouse 에 직접 Property 를 적용해야 한다.\n\n기본값은 0이다.\n\n\n  해당 Property 를 켠 채로 Group 내 Warehouse 간의 데이터 차이가 발생하더라도, Coordinator 는 데이터 불일치 여부를 별도로 검사하지 않는다.\n\n\n\n  \n    \n      (boolean)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nCOORDINATOR_DBS_PATH\n\nCoordinator 의 데이터 파일이 생성될 디렉터리를 지정한다.\n\n기본값은 ?/dbs 로 설정되어 있으며, ? 는 $MACHBASE_COORDINATOR_HOME 환경변수로 치환된다.\n이는 환경변수 $MACHBASE_COORDINATOR_HOME/dbs 디렉터리라는 의미이다.\n\nCoordinator 에 적용해야 하며, 다른 Node 에는 아무런 효과가 없다.\n\n\n  \n    \n      (path)\n      Value\n    \n  \n  \n    \n      기본값\n      ?/dbs\n    \n  \n\n\nCOORDINATOR_DDL_REQUEST_TIMEOUT\n\nCoordinator가 Node에게 DDL 수행을 요청한 후 대기할 때 까지의 Timeout\n\n이 값은 Coordinator가 각 Node에게 DDL 수행을 요청한 후 대기할 때 까지를 말한다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      300000000\n    \n  \n\n\nCOORDINATOR_DDL_TIMEOUT\n\nBroker가 CC를 통해 Coordinator에게 DDL 수행을 요청한 후 대기할 때 까지의 Timeout\n\n이 값은 Broker가 Cluster 전체에 대한 DDL 수행을 Coordinator에게 요청한 후 대기할 때 까지를 말한다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      3600000000\n    \n  \n\n\nCOORDINATOR_DECISION_DELAY\n\nCoordinator가 상태 변경을 요청하고 실제로 반영할 때 까지의 Timeout.\n\n이 시간이 지나도록 실제로 상태가 변경되지 않는 경우, Cluster 상태를 비활성화시킨다.\n만약 Warehouse Active의 상태가 변경되지 않았는데 연결된 Standby가 존재하는 경우, Fail-Over 작업을 시작한다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1000000\n    \n  \n\n\nCOORDINATOR_DECISION_INTERVAL\n\nCoordinator가 상태 변경을 얼마나 자주 할지 결정할 시간.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1000000\n    \n  \n\n\nCOORDINATOR_HOST_RESOURCE_ENABLE\n\nCoordinator가 Cluster Node들의 Host Resource 수집 여부\n\n\n  \n    \n      (boolean)\n      Value\n    \n  \n  \n    \n      최소값\n      0(false)\n    \n    \n      최대값\n      1(true)\n    \n    \n      기본값\n      0(false)\n    \n  \n\n\nCOORDINATOR_HOST_RESOURCE_COLLECT_INTERVAL\n\nCluster Node들이 Host Resource를 수집하는 주기\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1000000\n    \n  \n\n\nCOORDINATOR_HOST_RESOURCE_INTERVAL\n\nCoordinator가 Node들과 Host Resource를 주고받는 주기\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1000000\n    \n  \n\n\nCOORDINATOR_HOST_RESOURCE_REQUEST_TIMEOUT\n\nCoordinator가 Node들에게 Host Resource 정보를 요청한 이후 대기할 시간\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      10000000\n    \n  \n\n\nCOORDINATOR_NODE_REQUEST_TIMEOUT\n\nCoordinator가 Node에게 명령을 수행하도록 요청한 후 대기할 때 까지의 Timeout\n\nAdd/Remove-node, Add/Remove-Package 등의 Node 명령 수행이 포함되어 있어, 짧은 시간으로 잡을 경우 해당 명령 처리가 완료되지 못할 수 있다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      600000000\n    \n  \n\n\nCOORDINATOR_NODE_TIMEOUT\n\nCoordinator가 Node의 장애를 판단하기 까지 기다릴 시간.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      30000000\n    \n  \n\n\nCOORDINATOR_STARTUP_DELAY\n\nCoordinator 시작 직후 Decision Thread를 작동시킬 때 까지의 유예 시간.\n\nCluster 전체 구동에 오랜 시간이 소요되는 경우, 해당 값을 크게 설정해서 Coordinator의 Node 제어를 더욱 늦게 시작할 수 있다.\n전체 구동도 하기 전에 Decision Thread가 작동하는 경우, Coordinator가 오판할 가능성이 높아진다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      3000000\n    \n  \n\n\nCOORDINATOR_STATUS_NODE_INTERVAL\n\nCoordinator가 Node들과 상태 조회 메시지를 주고 받을 주기\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1000000\n    \n  \n\n\nCOORDINATOR_STATUS_NODE_REQUEST_TIMEOUT\n\nCoordinator가 Node들에게 상태 조회 요청을 한 이후 대기할 시간.\n\n해당 시간동안 상태 조회 응답이 없으면, Coordinator는 해당 Node의 상태를 갱신하지 않고 계속 진행한다.\n네트워크 상황이 좋지 않은데 상태 갱신을 반드시 해야 하는 경우엔, 값을 늘리는 것을 고려해 볼 수 있다.\n대신, 상태 조회 응답이 없을 경우엔 값을 늘린 만큼 Coordinator에서 반드시 기다리게 된다.\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      5000000\n    \n  \n\n\nCOORDINATOR_DISK_FULL_UPPER_BOUND_RATIO\n\nCluster 로 구성중인 일부 서버의 디스크 사용량이 프로퍼티 값을 넘어가면 해당 host 가 속한 group 이 DISKFULL 상태로 전환된다.\nDISKFULL 상태의 group 에 대해서는 입력이 제한되고 조회 및 삭제만 가능하다.\n\n프로퍼티 값이 0 인 경우 해당 기능이 disable 된다.\n\n\n  \n    \n      (percent)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      99\n    \n    \n      기본값\n      0\n    \n  \n\n\nCOORDINATOR_DISK_FULL_LOWER_BOUND_RATIO\n\nDISKFULL 상태로 동작중인 서버의  디스크 사용량이 프로퍼티 값 이하로 떨어질 경우 해당 group 이 normal 상태로 전환된다.\n\n프로퍼티 값이 0 인 경우 해당 기능이 disable 된다.\n\n\n  \n    \n      (percent)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      99\n    \n    \n      기본값\n      0\n    \n  \n\n\nDEPLOYER_DBS_PATH\n\nDeployer 의 데이터 파일이 생성될 디렉터리를 지정한다.\n\n기본값은 ?/dbs 로 설정되어 있으며, ? 는 $MACHBASE_DEPLOYER_HOME 환경변수로 치환된다.\n이는 환경변수 $MACHBASE_DEPLOYER_HOME /dbs 디렉터리라는 의미이다.\n\nDeployer 에 적용해야 하며, 다른 Node 에는 아무런 효과가 없다.\n\n\n  \n    \n      (path)\n      Value\n    \n  \n  \n    \n      기본값\n      ?/dbs\n    \n  \n\n\nEXECUTION_STAGE_MEMORY_MAX\n\nCluster Edition 에서, SELECT 쿼리를 수행하는 Stage Thread 가 사용하는 Memory 의 최대 크기.\n\n각 Stage 의 최대 크기이므로, Stage 개수가 늘어나는 복잡한 SELECT 쿼리의 경우 요구 메모리가 더 커질 수 있다.\n최대 크기를 넘는 Stage 가 존재하는 경우, 해당 Stage 는 취소되고 Query 역시 에러와 함께 취소된다.\n\n원하는 특정 Warehouse 에 직접 Property 를 적용해야 한다.\n\n기본값은 1GB 이다.\n\n\n  \n    \n      (size)\n      Value\n    \n  \n  \n    \n      최소값\n      1024\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1024 *1024 * 1024\n    \n  \n\n\nHTTP_ADMIN_PORT\n\nMWA 또는 machcoordinatoradmin 으로부터 요청을 받을 port number\n\n\n  \n    \n      (port)\n      Value\n    \n  \n  \n    \n      최소값\n      1024\n    \n    \n      최대값\n      65535\n    \n    \n      기본값\n      5779\n    \n  \n\n\nHTTP_CONNECT_TIMEOUT\n\nmachcoordinatoradmin 과 연결할 때 사용하는 timeout\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      30000000\n    \n  \n\n\nHTTP_RECEIVE_TIMEOUT\n\nmachcoordinatoradmin 과 통신할 때 사용하는 timeout\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      3600000000\n    \n  \n\n\nHTTP_SEND_TIMEOUT\n\nmachcoordinatoradmin 과 통신할 때 사용하는 timeout\n\n\n  \n    \n      (usec)\n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      60000000\n    \n  \n\n\nINSERT_BULK_DATA_MAX_SIZE\n\nAppend 또는 INSERT-SELECT 수행 시 입력 data block의 최대 크기\n\n\n  \n    \n      (size)\n      Value\n    \n  \n  \n    \n      최소값\n      1024\n    \n    \n      최대값\n      10 * 1024 * 1024\n    \n    \n      기본값\n      1024 * 1024\n    \n  \n\n\nINSERT_RECORD_COUNT_PER_NODE\n\n입력 수행시 warehouse group 전환을 유도하는 data 입력 개수.\n\n\n  \n    \n      (count)\n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      10000\n    \n  \n\n\nLOOKUPNODE_COMMAND_RETRY_MAX_COUNT\n\nLookup 노드에 명령 및 접속 실패시 재시도 횟수\n\n\n  \n    \n      (count)\n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      3600\n    \n    \n      기본값\n      30\n    \n  \n\n\nSTAGE_RESULT_BLOCK_SIZE\n\n하나의 stage 에서 만드는 최대 block 크기\n\n\n  \n    \n      (size)\n      Value\n    \n  \n  \n    \n      최소값\n      1024\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      1024 * 1024"
					}
					
				
		
				
					,
					
					"config-monitor-property-html": {
						"id": "config-monitor-property-html",
						"title": "Property",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/property.html",
						"content": "프로퍼티란 $MACHBASE_HOME/conf/machbase.conf 파일에 정의되어 있는 키-값 의 쌍을 의미한다.\n\n이 값들은 마크베이스 서버가 시작할 때 설정되고 실행시 지속적으로 이용된다. 성능 튜닝을 위해서 이 값을 변경하려면 이 값들에 대한 의미를 이해하고, 주의 깊게 설정하여야 한다.\n\n목차\n\n  CPU_AFFINITY_BEGIN_ID\n  CPU_AFFINITY_COUNT\n  CPU_COUNT\n  CPU_PARALLEL\n  DBS_PATH\n  DEFAULT_LSM_MAX_LEVEL\n  DISK_BUFFER_COUNT\n  DISK_COLUMNAR_INDEX_CHECKPOINT_INTERVAL_SEC\n  DISK_COLUMNAR_INDEX_FDCACHE_COUNT\n  DISK_COLUMNAR_PAGE_CACHE_MAX_SIZE\n  DISK_COLUMNAR_TABLE_CHECKPOINT_INTERVAL_SEC\n  DISK_COLUMNAR_TABLE_COLUMN_FDCACHE_COUNT\n  DISK_COLUMNAR_TABLE_COLUMN_MINMAX_CACHE_SIZE\n  DISK_COLUMNAR_TABLE_COLUMN_PART_FLUSH_MODE\n  DISK_COLUMNAR_TABLE_COLUMN_PART_IO_INTERVAL_MIN_SEC\n  DISK_COLUMNAR_TABLE_COLUMN_PARTITION_PRECREATE_COUNT\n  DISK_COLUMNAR_TABLE_TIME_INVERSION_MODE\n  DISK_COLUMNAR_TABLESPACE_DWFILE_EXT_SIZE\n  DISK_COLUMNAR_TABLESPACE_DWFILE_INT_SIZE\n  DISK_COLUMNAR_TABLESPACE_MEMORY_EXT_SIZE\n  DISK_COLUMNAR_TABLESPACE_MEMORY_MAX_SIZE\n  DISK_COLUMNAR_TABLESPACE_MEMORY_MIN_SIZE\n  DISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_HIGH_LIMIT_PCT\n  DISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_MSEC\n  DISK_IO_THREAD_COUNT\n  DISK_TABLESPACE_DIRECT_IO_FSYNC\n  DISK_TABLESPACE_DIRECT_IO_READ\n  DISK_TABLESPACE_DIRECT_IO_WRITE\n  DISK_TAG_AUTO_RECLAIM\n  DUMP_APPEND_ERROR\n  DUMP_TRACE_INFO\n  DURATION_BEGIN\n  DURATION_GAP\n  FEEDBACK_APPEND_ERROR\n  GRANT_REMOTE_ACCESS\n  HTTP_THREAD_COUNT\n  INDEX_BUILD_MAX_ROW_COUNT_PER_THREAD\n  INDEX_BUILD_THREAD_COUNT\n  INDEX_FLUSH_MAX_REQUEST_COUNT_PER_INDEX\n  INDEX_LEVEL_PARTITION_AGER_THREAD_COUNT\n  INDEX_LEVEL_PARTITION_BUILD_MEMORY_HIGH_LIMIT_PCT\n  INDEX_LEVEL_PARTITION_BUILD_THREAD_COUNT\n  LOOKUP_APPEND_UPDATE_ON_DUPKEY\n  MAX_QPX_MEM\n  MEMORY_ROW_TEMP_TABLE_PAGESIZE\n  PID_PATH\n  PORT_NO\n  PROCESS_MAX_SIZE\n  QUERY_PARALLEL_FACTOR\n  ROLLUP_FETCH_COUNT_LIMIT\n  RS_CACHE_APPROXIMATE_RESULT_ENABLE\n  RS_CACHE_ENABLE\n  RS_CACHE_MAX_MEMORY_PER_QUERY\n  RS_CACHE_MAX_MEMORY_SIZE\n  RS_CACHE_MAX_RECORD_PER_QUERY\n  RS_CACHE_TIME_BOUND_MSEC\n  SHOW_HIDDEN_COLS\n  TABLE_SCAN_DIRECTION\n  TAGDATA_AUTO_META_INSERT\n  TAG_PARTITION_COUNT\n  TAG_DATA_PART_SIZE\n  TAG_TABLE_META_MAX_SIZE\n  TRACE_LOGFILE_COUNT\n  TRACE_LOGFILE_PATH\n  TRACE_LOGFILE_SIZE\n  UNIX_PATH\n  VOLATILE_TABLESPACE_MEMORY_MAX_SIZE\n\n\nCPU_AFFINITY_BEGIN_ID\n\n마크베이스 서버가 사용할 CPU의 시작 번호이다. 마크베이스 서버의 CPU사용량을 조절하기 위해서 사용한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2 ^ 32 - 1\n    \n    \n      기본값\n      0\n    \n  \n\n\nCPU_AFFINITY_COUNT\n\n마크베이스 서버가 사용할 CPU의 수이다. 0으로 설정하면 마크베이스 서버가 모든 CPU를 사용한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2 ^ 32 - 1\n    \n    \n      기본값\n      0\n    \n  \n\n\nCPU_COUNT\n\n시스템에 설정된 CPU의 수를 지정한다. 이 값을 기반으로 마크베이스의 스레드 수를 결정한다. 0으로 지정한 경우에는 시스템의 모든 CPU를 사용한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (시스템에 물리적으로 설치된 CPU수)\n    \n    \n      최대값\n      2^32 -1\n    \n    \n      기본값\n      0\n    \n  \n\n\nCPU_PARALLEL\n\nCPU당 생성할 스레드의 수를 지정한다. 만약 이 값이 2이고 cpu의 수가 2인 경우, 두개의 CPU마다 병렬 스레드가 2개씩 생성되므로 병렬처리 스레드의 수가 4가 된다. 이 값이 너무 큰 경우, 메모리가 빨리 소모될 수 있다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^32 -1\n    \n    \n      기본값\n      1\n    \n  \n\n\nDBS_PATH\n\n마크베이스 서버의 기본 데이터가 저장될 경로를 지정한다. 기본값은 “?/dbs”로, $MACHBASE_HOME/dbs 를 의미한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      기본값\n      ?/dbs\n    \n  \n\n\nDEFAULT_LSM_MAX_LEVEL\n\nLSM인덱스의 기본 레벨을 설정한다. 인덱스를 생성할 때 MAX_LEVEL값을 입력하지 않으면 이 값이 적용된다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      3\n    \n    \n      기본값\n      2\n    \n  \n\n\nDISK_BUFFER_COUNT\n\n디스크 입출력을 위한 버퍼의 수를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      16\n    \n  \n\n\nDISK_COLUMNAR_INDEX_CHECKPOINT_INTERVAL_SEC\n\n인덱스에 대한 체크포인트 주기를 설정한다. 너무 길게 설정할 경우, 인덱스 빌드에 오류가 발생할 수 있다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1 (sec)\n    \n    \n      최대값\n      2^32 -1 (sec)\n    \n    \n      기본값\n      120 (sec)\n    \n  \n\n\nDISK_COLUMNAR_INDEX_FDCACHE_COUNT\n\n오픈한 인덱스 파티션 파일 디스크립터의 수를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^32 -1\n    \n    \n      기본값\n      0\n    \n  \n\n\nDISK_COLUMNAR_INDEX_SHUTDOWN_BUILD_FINISH\n\n마크베이스 서버를 종료할 때, 인덱스 정보를 디스크에 모두 반영할 것인지를 설정한다. 이 값을 ‘1’로 설정하면 모든 인덱스 정보를 디스크에 반영하고 종료하므로 종료시 대기 시간이 길어질 수 있다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (False)\n    \n    \n      최대값\n      1 (True)\n    \n    \n      기본값\n      0 (False)\n    \n  \n\n\nDISK_COLUMNAR_PAGE_CACHE_MAX_SIZE\n\n페이지 캐쉬의 최대 크기를 설정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      2 * 1024 * 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLE_CHECKPOINT_INTERVAL_SEC\n\n테이블 데이터의 체크포인트 주기를 설정한다. 이 값이 너무 크면 재시작시 복구 시간이 매우 길어지고, 이 값이 너무 작으면 I/O가 자주 발생하여 전체 성능이 저하될 수 있다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1 (sec)\n    \n    \n      최대값\n      2^32 -1 (sec)\n    \n    \n      기본값\n      120 (sec)\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_FDCACHE_COUNT\n\n테이블의 컬럼 데이터에 대한 오픈된 파일 설명자의 최대 수를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      0\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_MINMAX_CACHE_SIZE\n\n_ARRIVAL_TIME 컬럼에 설정되는 기본 MINMAX 캐쉬의 크기를 설정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      100 *1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_PART_FLUSH_MODE\n\n컬럼 파티션 파일의 자동 플러쉬 주기를 설정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (sec)\n    \n    \n      최대값\n      2^32-1 (sec)\n    \n    \n      기본값\n      60 (sec)\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_PART_IO_INTERVAL_MIN_SEC\n\n파티션 파일을 디스크에 반영하는 주기를 설정한다. 파티션이 설정된 갯수보다 더 많은 데이터를 입력받으면 이 주기와 관계없이 디스크에 반영된다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (sec)\n    \n    \n      최대값\n      2^32-1 (sec)\n    \n    \n      기본값\n      3 (sec)\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_PARTITION_PRECREATE_COUNT\n\n테이블에 대해서 사용할 예정인 컬럼 파티션 객체의 사전 생성 수를 정의한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^32-1\n    \n    \n      기본값\n      3\n    \n  \n\n\nDISK_COLUMNAR_TABLE_TIME_INVERSION_MODE\n\n설정된 값 만큼 _ARRIVAL_TIME컬럼의 값이 감소하더라도 입력을 허용한다. 만약 0인 경우 _ARRIVAL_TIME컬럼 값의 최대값보다 작은 값이 입력되면 이는 오류로 처리된다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (False)\n    \n    \n      최대값\n      1 (True)\n    \n    \n      기본값\n      1 (True)\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_DWFILE_EXT_SIZE\n\n시작시 복구를 위해서 사용되는 더블 라이트 파일이 한번에 증가하는 크기를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1024 * 1024\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_DWFILE_INT_SIZE\n\n파일 생성시에 더블라이트 파일이 확보하는 용량을 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1024 * 1024\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      2* 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_EXT_SIZE\n\n컬럼 파티션을 위해서 확보하는 메모리의 블록 크기를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1024 * 1024\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      2* 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_MAX_SIZE\n\n로그 테이블에 의하여 할당된 최대 메모리 크기를 지정한다. 만약 서버가 이 값 이상의 메모리를 할당하게 되면, 메모리 사용량이 이 값 이하로 줄어들 때 까지 메모리 할당이 대기하므로 성능이 저하된다. 이 값은 물리적 메모리의 50~80% 정도로 설정할 것을 추천한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      256 * 1024 * 1024\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      8 * 1024 * 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_MIN_SIZE\n\n마크베이스 서버가 시작할 때, 메모리 할당에 의한 성능 저하를 막기 위해서 이 값 만큼 메모리를 사전 확보한다. 데이터 입력 버퍼로만 이 메모리를 사용하므로, 메모리가 충분할 경우에만 사용할 것을 추천한다.\n\nTable 24. Range of values\n||Value|\n|-|—-|\n|최소값|\t1024 * 1024|\n|최대값|\t2^64 - 1|\n|기본값|\t100 * 1024 * 1024|\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_HIGH_LIMIT_PCT\n\n컬럼 데이터 파일을 위한 메모리 사용량이 제한 값을 이 값을 다음과 같이 이용하여 계산하고, 초과한 경우 입력 성능을 저하시킨다.\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_MAX_SIZE * (DISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_HIGH_LIMIT_PCT / 100)\n\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      100\n    \n    \n      기본값\n      80\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_MSEC\n\n컬럼 데이터 파일을 위한 메모리 사용량이 기준을 초과한 경우, 매 레코드 입력시에 다음의 대기 시간을 설정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (msec)\n    \n    \n      최대값\n      2^32 - 1 (msec)\n    \n    \n      기본값\n      1 (msec)\n    \n  \n\n\nDISK_IO_THREAD_COUNT\n\n데이터를 디스크에 기록하는 입출력 스레드의 수를 설정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^32 -1\n    \n    \n      기본값\n      3\n    \n  \n\n\nDISK_TABLESPACE_DIRECT_IO_FSYNC\n\nDirect I/O를 실행할 경우, 데이터 파일에 대해서 fsync는 불필요하다. Direct I/O 를 사용할 경우 fsync를 사용하지 않도록 하면 데이터 I/O 성능을 향상시킬 수 있다 (0으로 설정).\nFsync를 수행하지 않아도 일반적 상황에서는 데이터 유실이 없으나 전원이 꺼지는 등의 장애 상황이 발생할 수 있는 경우에는 fsync를 수행하도록 설정해야 한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nDISK_TABLESPACE_DIRECT_IO_READ\n\n데이터 읽기 연산에 DIRECT I/O 를 사용할 것인지를 설정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nDISK_TABLESPACE_DIRECT_IO_WRITE\n\n데이터 쓰기 연산에 DIRECT I/O 를 사용할 것인지 설정한다. 파일 시스템에 따라서 DIRECT I/O 지원하지 않는 경우(ex: ZFS), 0으로 설정한다.\n\n참고 : ZFS DIRECT I/O https://github.com/openzfs/zfs/issues/224\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      1\n    \n  \n\n\nDISK_TAG_AUTO_RECLAIM\n\nTAG 데이터에 대해서 사용되지 않는 공간을 자동 확보할 것인지의 여부를 결정한다. 기본값인 1인 경우, 자동 공간 확보 기능이 동작하고 0 인 경우에는 동작하지 않으며 사용자가 ALTER TABLE문을 이용하여 해당 기능을 직접 수행해야 한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      1\n    \n  \n\n\nDUMP_APPEND_ERROR\n\n이 값을 1로 설정하면 Append API 가 실패한 경우 $MACHBASE_HOME/trc/machbase.trc 파일에 에러 내용을 기록한다.\n이 상황에서 append 성능이 매우 저하될 수 있으므로 테스트용으로만 사용할 것을 권장한다.\n\n사용자 application에서 에러를 검사하고 싶으면 SQLAppendSetErrorCallback API 를 사용하는 것이 도움이 된다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nDUMP_TRACE_INFO\n\n서버는 일정한 주기로 DBMS 시스템 상태 정보를 machbase.trc 파일에 주기적으로 기록하는데, 이 주기를 설정한다.\n0으로 설정하면 기록하지 않는다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (sec)\n    \n    \n      최대값\n      2^32 - 1 (sec)\n    \n    \n      기본값\n      60 (sec)\n    \n  \n\n\nDURATION_BEGIN\n\nDURATION 절을 지정하지 않은 SELECT 문에 대해서 기본을 설정하는 duration 값 중 시작시점을 설정한다.\n만약 60을 설정해 두었다면, 현재 시각에서 60초 이전의 데이터를 검색하게 된다.\n\n기본값은 0으로 모든 데이터를 검색한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      0\n    \n  \n\n\nDURATION_GAP\nDURATION 절을 지정하지 않은 SELECT 문에 대해서 기본을 설정하는 duration 값 중 기간을 설정한다.\n\n만약 60을 설정해 두었다면, 현재 시각에서 60초 동안의 데이터를 검색하게 된다.\nDURATION_BEGIN 값도 60이라면, 현재 시각에서 60초 이전부터 60초 동안의 데이터를 검색하게 된다.\n기본값은 0으로 모든 데이터를 검색한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      Non-zero\n    \n    \n      기본값\n      0\n    \n  \n\n\nFEEDBACK_APPEND_ERROR\n\nAppend API 실행시 오류가 발생하였을 경우, 오류 데이터를 클라이언트에 전송할지를 설정한다. 0이면 클라이언트에 오류 데이터를 전송하지 않으며 1이면 클라이언트에 오류 정보를 전송한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      1\n    \n  \n\n\nGRANT_REMOTE_ACCESS\n\n원격지에서 데이터베이스에 접근할 수 있는지를 결정한다. 0이면 원격지 접속이 차단된다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (False)\n    \n    \n      최대값\n      1 (True)\n    \n    \n      기본값\n      1 (True)\n    \n  \n\n\nHTTP_THREAD_COUNT\n\n마크베이스의 웹 서버가 사용할 스레드의 개수를 설정 가능하다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1024\n    \n    \n      기본값\n      32\n    \n  \n\n\nINDEX_BUILD_MAX_ROW_COUNT_PER_THREAD\n\n인덱스 빌드 스레드가 인덱싱 되지 않은 레코드의 수가 이 값 이상이 되면 인덱스를 추가하기 시작한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      100000\n    \n  \n\n\nINDEX_BUILD_THREAD_COUNT\n\n인덱스 생성 스레드의 수를 지정한다. 0으로 설정되면 인덱스를 생성하지 않는다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2 ^ 32 - 1\n    \n    \n      기본값\n      3\n    \n  \n\n\nINDEX_FLUSH_MAX_REQUEST_COUNT_PER_INDEX\n\n인덱스당 최대 flush 요청 수를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2 ^ 32 - 1\n    \n    \n      기본값\n      3\n    \n  \n\n\nINDEX_LEVEL_PARTITION_AGER_THREAD_COUNT\n\nLSM 인덱스 생성시에 필요없는 인덱스 파일의 삭제를 위한 스레드의 갯수를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1024\n    \n    \n      기본값\n      1\n    \n  \n\n\nINDEX_LEVEL_PARTITION_BUILD_MEMORY_HIGH_LIMIT_PCT\n\nLSM 인덱스 생성을 위한 최대 메모리 사용량의 퍼센트로 설정한다. 이 퍼센트는 마크베이스가 사용하는 최대 메모리사용량 대비하여 설정된다. 메모리 사용량이 한도를 초과하면, LSM 파티션 병합은 중지된다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      100\n    \n    \n      기본값\n      70\n    \n  \n\n\nINDEX_LEVEL_PARTITION_BUILD_THREAD_COUNT\n\nLSM 인덱스의 생성을 위한 병합 연산을 수행하는 스레드의 수를 결정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      1024\n    \n    \n      기본값\n      3\n    \n  \n\n\nLOOKUP_APPEND_UPDATE_ON_DUPKEY\n\nLookup 테이블에 Append 할 때 Primary Key가 중복일 경우 어떻게 처리할지 지정한다.\n\n\n  0 : Append 실패\n  1 : 해당 Primary Key 에 대해서 Row를 Update 한다.\n\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nMAX_QPX_MEM\n\nGROUP BY, DISTINCT, ORDER BY 절을 수행하기 위해서 질의처리기가  이용하는 메모리의 최대 양을 설정한다.\n하나의 질의문이 이보다 큰 값으로 메모리를 사용하게 되면 질의는 취소된다. 이때, 에러메시지를 클라이언트에 전송하고, machbase.trc 파일에 관련 내용이 기록된다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1024 * 1024\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      500 * 1024 * 1024\n    \n  \n\n\nMEMORY_ROW_TEMP_TABLE_PAGESIZE\n\nVolatile table및 lookup 테이블을 위한 임시 테이블 스페이스의 페이지 크기를 설정한다. Volatile 테이블 및 lookup 테이블의 레코드들은 페이지에 저장되므로 volatile을 위한 최대 레코드 크기보다 커야 한다.\n페이지에 N개의 레코드를 입력하고 싶으면 이 값을 최대 레코드 크기 * N으로 설정해야 한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      8 * 1024\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      32 * 1024\n    \n  \n\n\nPID_PATH\n\n마크베이스 서버 프로세스의 PID파일이 기록되는 위치를 지정한다. 기본값은 “?/conf”이며 이는 $MACHBASE_HOME/conf 를 의미한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      기본값\n      ?/conf\n    \n  \n\n\n\n  \n    \n      PID_PATH 값\n      PID 파일 위치 경로\n    \n  \n  \n    \n      지정되지 않음\n      $MACHBASE_HOME/conf/machbase.pid\n    \n    \n      ?/test\n      $MACHBASE_HOME/test/machbase.pid\n    \n    \n      /tmp\n      /tmp/machbase.pid\n    \n  \n\n\nPORT_NO\n\n마크베이스 서버 프로세스가 클라이언트와 통신하기 위한 TCP/IP 포트를 지정한다. 기본값은 5656이다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1024\n    \n    \n      최대값\n      65535\n    \n    \n      기본값\n      5656\n    \n  \n\n\nPROCESS_MAX_SIZE\n\n마크베이스 서버 프로세스인 machbased 프로그램이 사용하는 최대 메모리 사이즈를 지정한다. 이 제한값 이상의 메모리를 사용하려고 하면 서버는 다음과 같이 동작하여 메모리의 사용량을 줄이려고 시도한다. 메모리 제한을 초과한 경우, 다음의 방법으로 메모리 사용량을 줄인다.\n\n데이터 입력을 중지하거나 오류로 처리\n인덱스 생성 속도를 떨어뜨림\n이 경우, 성능이 매우 저하되므로, 메모리 과다 사용 원인을 찾아서 해결하여야 한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1024 * 1024 * 1024\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      8 * 1024 * 1024 * 1024\n    \n  \n\n\nQUERY_PARALLEL_FACTOR\n\n병렬 질의 실행기의 실행 스레드의 수를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      100\n    \n    \n      기본값\n      8\n    \n  \n\n\nROLLUP_FETCH_COUNT_LIMIT\n\n롤업 스레드가 한번에 패치해올 데이터양을 제한한다.\n\n0으로 설정할 경우 제한이 없다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      3000000\n    \n  \n\n\nRS_CACHE_APPROXIMATE_RESULT_ENABLE\n\n결과값 캐쉬의 추측 모드(approximate result mode)를 사용할지의 여부를 결정한다. 이 값이 1이면 결과값 캐쉬를 사용할 때, 추측 값을 얻고(매우 빠르지만 데이터가 부정확할 수 있다.) 0 이면 정확한 값을 얻는다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (False)\n    \n    \n      최대값\n      1 (True)\n    \n    \n      기본값\n      0 (False)\n    \n  \n\n\nRS_CACHE_ENABLE\n\n결과값 캐쉬를 사용할 지의 여부를 결정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (False)\n    \n    \n      최대값\n      1 (True)\n    \n    \n      기본값\n      1 (True)\n    \n  \n\n\nRS_CACHE_MAX_MEMORY_PER_QUERY\n\n결과값 캐쉬가 사용할 메모리의 양을 설정한다. 특정 질의 결과의 메모리 사용량이 이 값을 초과하면, 해당 질의의 결과는 결과값 캐쉬에 저장되지 않는다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1024\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      16 * 1024 * 1024\n    \n  \n\n\nRS_CACHE_MAX_MEMORY_SIZE\n\n결과값 캐쉬의 최대 메모리 사용량을 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      32 * 1024\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      512 * 1024 * 1024\n    \n  \n\n\nRS_CACHE_MAX_RECORD_PER_QUERY\n\n결과값 캐쉬에 저장되는 최대 레코드 갯수이다. 만약 질의의 결과 레코드의 수가 이 값 이상이면 해당 질의 결과값은 캐쉬에 저장하지 않는다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      10000\n    \n  \n\n\nRS_CACHE_TIME_BOUND_MSEC\n\n특정 질의가 매우 빠르게 실행된 경우에는 그 결과값을 결과값 캐쉬에 저장하지 않는 것이 메모리 사용량을 줄일 수 있으므로 캐쉬에 저장하지 않는것이 좋다.\n\n이 값은 어느 정도 빨리 실행된 질의를 캐쉬에 저장하지 않을지를 결정한다. 0으로 설정된 경우에는 모든 질의결과를 결과집합캐쉬에 저장한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0 (msec)\n    \n    \n      최대값\n      2^64 - 1 (msec)\n    \n    \n      기본값\n      1000 (msec)\n    \n  \n\n\nSHOW_HIDDEN_COLS\n\n_ARRIVAL_TIME 컬럼은 기본 설정으로는 SELECT * FROM 질의에 의해서 표시되지 않는다. 그러나 이 값이 1로 설정된 경우에는 해당 컬럼을 표시한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nTABLE_SCAN_DIRECTION\n\n태그 테이블의 스캔 방향을 설정할 수 있다. 프로퍼티 값은 -1,0, 1중 택일이며 기본값은 0이다.\n\n\n  -1 : 역방향 스캔\n  0  : Tag Table(정방향 스캔), Log Table(역방향 스캔)\n  1  : 정방향 스캔\n\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      -1\n    \n    \n      최대값\n      1\n    \n    \n      기본값\n      0\n    \n  \n\n\nTAGDATA_AUTO_META_INSERT\n\n5.5 에서는 TAGDATA_AUTO_NAME_INSERT 이다. 값의 범위도 0/1 이다.\n5.7 이하에서는 기본값이 1 이다.\n\nTAGDATA 테이블에 APPEND/INSERT 를 통해 데이터를 입력할 때, 일치하는 TAG_NAME 이 없을 경우 어떻게 처리할 것인지를 정한다.\n\n\n  0 : 입력이 실패한다.\n  1 : 입력을 원하는 TAG_NAME 값을 입력한다. 추가 메타데이터 컬럼이 존재할 경우, 해당 컬럼의 값은 모두 NULL 로 입력된다.\n  2 : 입력을 원하는 TAG_NAME 값과 함께, 추가 메타데이터 컬럼 값도 같이 입력한다.\nAPPEND 에서만 유효한 설정이며, INSERT 는 추가 메타데이터 컬럼 값을 입력할 수 없기 때문에 1과 같이 작동한다.\n이 설정을 한 이후에는, APPEND 에서 반드시 메타데이터 컬럼 값까지 포함시킨 APPEND Parameter 로 입력해야 한다.\n\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2\n    \n    \n      기본값\n      2\n    \n  \n\n\nTAG_TABLE_META_MAX_SIZE\n\nTAGDATA Table 생성 시 Metadata 영역을 보관할 메모리의 최대 크기를 설정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1024*1024\n    \n    \n      최대값\n      2^32-1\n    \n    \n      기본값\n      10010241024\n    \n  \n\n\nTAG_PARTITION_COUNT\n\nTag 테이블을 구성하는 Key Value 테이블의 개수를 지정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      4\n    \n    \n      기본값\n      1024\n    \n  \n\n\nTAG_DATA_PART_SIZE\n\nTag 데이터 저장공간의 파티션 크기를 결정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1048576 (1MB)\n    \n    \n      최대값\n      1073741824 (1GB)\n    \n    \n      기본값\n      16777216 (16MB)\n    \n  \n\n\nTRACE_LOGFILE_COUNT\n\nTRACE_LOGFILE_PATH에 생성되는 로그 트레이스 파일의 최대 수를 지정한다. 디스크 공간을 절약하기 위해서, 최대 개수 이상의 로그파일이 생성되면 가장 오래된 로그파일을 삭제한다.\n\n로그 트레이스 파일의 최대 개수 이상의 로그파일이 생성되어 가장 오래된 파일이 삭제될 경우 삭제된 파일의 이름이 가장 최신의 로그파일로 저장이 된다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      1000\n    \n  \n\n\nTRACE_LOGFILE_PATH\n\n로그 트레이스 파일들(machbase.trc, machadmin.trc, machsql.trc)의 경로를 설정한다.\n이 파일들은 마크베이스의 시작, 종료, 실행시 내부 정보를 지속적으로 기록한다. 기본값인 ?/trc의 의미는 $MACHBASE_HOME/trc 를 의미한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      기본값\n      ?/conf\n    \n  \n\n\n\n  \n    \n      TRACE_LOGFILE_PATH 값\n      trc 디렉터리 위치\n    \n  \n  \n    \n      지정되지 않음\n      $MACHBASE_HOME/trc/\n    \n    \n      ?/test\n      $MACHBASE_HOME/test/\n    \n    \n      /tmp\n      /tmp/\n    \n  \n\n\nTRACE_LOGFILE_SIZE\n\n로그 트레이스 파일의 최대 크기를 설정한다. 만약 크기 이상의 데이터를 기록하여야 한다면, 신규로 log 파일을 생성할 것이다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      10 * 1024 * 1024\n    \n    \n      최대값\n      2^32 - 1\n    \n    \n      기본값\n      10 * 1024 * 1024\n    \n  \n\n\nUNIX_PATH\n\nUnix domain socket 파일의 경로를 설정한다. 사용자가 설정하지 않았을 경우의 기본 값은 ?/conf/machbase-unix 이다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      기본값\n      ?/conf/machbase-unix\n    \n  \n\n\nVOLATILE_TABLESPACE_MEMORY_MAX_SIZE\n\n시스템의 모든 volatile, lookup 테이블의 메모리 사용량 총계의 한도를 설정한다.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      2^64 - 1\n    \n    \n      기본값\n      2 * 1024 * 1024 * 1024"
					}
					
				
		
				
					,
					
					"sdk-python-html": {
						"id": "sdk-python-html",
						"title": "Python",
						"version": "all",
						"categories": "",
						"url": " /sdk/python.html",
						"content": "목차\n\n\n  목차\n  Python 모듈 사용 개요\n  사용 환경 설정\n  Class 생성\n  접속과 접속해제\n    \n      machbase.open(aHost, aUser, aPw, aPort)\n      machbase.close()\n      machbase.isConnected()\n    \n  \n  명령어 실행 및 사용자 편의 함수\n    \n      machbase.execute(aSql)\n      machbase.append(aTableName, aTypes, aValues, aFormat)\n      machbase.tables()\n      machbase.columns(aTableName)\n    \n  \n  결과 확인\n    \n      machbase.result()\n    \n  \n  SELECT 결과 확인\n    \n      machbase.select(aSql)\n      machbase.fetch()\n      machbase.selectClose()\n      machbase.select() 예제\n    \n  \n  예제\n    \n      Connect\n      Simple\n      Append\n    \n  \n\n\nPython 모듈 사용 개요\n\n마크베이스에서는 Python 모듈을 지원한다. 모듈을 설치함으로써 마크베이스 서버와 CLI 방식으로 값을 주고 받을 수 있는 클래스를 제공한다. 이를 활용해서 Python에서 쉽게 마크베이스에 질의 형태로 값을 입력하거나 삭제, 테이블 생성, 삭제 등 다양한 명령어를 사용할 수 있다.\n\n사용 환경 설정\n\n리눅스\n\n이를 사용하기 위해서는 간단한 환경 설정과 라이브러리 파일들이 필요하다. 먼저 $LD_LIBRARY_PATH에 $MACHBASE_HOME/lib 디렉터리가 등록되어있는지 확인한다. CLI를 이용해서 마크베이스에 접속하기 때문에 libmachbasecli_dll.so 파일이 라이브러리 폴더에 존재해야 한다. 그리고 $MACHBASE_HOME/3rd-party/python3-module 폴더에 있는 machbaseAPI-1.0.tar.gz를 압축 해제후 python setup.py install 명령을 통해 사용하려는 Python에 모듈을 설치한다.\n\n윈도우\n\n먼저 %MACHBASE_HOME%/3rd-party/python3-module 폴더에 있는 machbaseAPI-1.0.zip을 압축 해제 후 python setup.py install 명령을 통해 사용하려는 Python에 모듈을 설치한다.\n\nClass 생성\n\n마크베이스 Python 모듈을 사용하기 위해서 해당 클래스를 선언해야 한다.\n\n해당 클래스 명은 machbase이다.\n\nfrom machbaseAPI import machbase\n\n\n접속과 접속해제\n\nmachbase.open(aHost, aUser, aPw, aPort)\n\n마크베이스에 접속하는 함수이다. 알맞은 파라미터 값을 입력했을시에 DB에 접속 성공했는지 실패했는지를 반환한다. 정상 종료시 1, 실패시 0을 반환한다.\n\nmachbase.close()\n\n마크베이스 접속을 해제하는 함수이다. 정상 종료시 1, 실패시 0을 반환한다.\n\nmachbase.isConnected()\n\n선언한 클래스가 해당 서버에 접속중인지 아닌지를 판별하는 함수이다. 접속 중일 때 1, 접속 중이 아닐 때 0을 반환한다.\n\n명령어 실행 및 사용자 편의 함수\n\nmachbase.execute(aSql)\n\n서버에 접속되어 있을 때 해당 서버에 질의문을 전송하는 명령어이다. 정상적으로 실행되었을 때 1, 실패하거나 에러가 발생했을 시에는 0을 반환한다.\n\n마크베이스에서 지원하지 않는 UPDATE를 제외하고 모든 명령어들을 사용할 수 있다.\n\n결과를 한꺼번에 return하는 구조이기 때문에 SELECT 구문을 사용하는 경우 메모리 부족 오류가 발생할 수 있다. 따라서 SELECT 구문은 machbase.select() 함수를 사용한다.\n\nmachbase.append(aTableName, aTypes, aValues, aFormat)\n\n마크베이스에서 지원하는 Append 프로토콜을 사용할 수 있는 함수이다.\n\n데이터를 입력하게 될 테이블명, 각 컬럼들의 타입의 딕셔너리, 그리고 입력할 값들을 JSON 형태로 입력하고 dateformat을 지정해 주면 Append를 실행할 수 있다.\n\n정상적으로 실행하였다면 1, 실패시에는 0을 반환한다.\n\n\n  \n    \n      타입명\n      값\n    \n  \n  \n    \n      short\n      4\n    \n    \n      ushort\n      104\n    \n    \n      integer\n      8\n    \n    \n      uinteger\n      108\n    \n    \n      long\n      12\n    \n    \n      ulong\n      112\n    \n    \n      float\n      16\n    \n    \n      double\n      20\n    \n    \n      datetime\n      6\n    \n    \n      varchar\n      5\n    \n    \n      ipv4\n      32\n    \n    \n      ipv6\n      36\n    \n    \n      text\n      49\n    \n    \n      binary\n      97\n    \n  \n\n\nmachbase.tables()\n\n접속한 서버에 있는 모든 테이블들의 정보를 반환한다. 정상적으로 실행되었다면 1, 실패했을 시에는 0을 반환한다.\n\nmachbase.columns(aTableName)\n\n접속한 서버에 있는 해당 테이블 내의 컬럼들의 정보를 반환한다. 정상적으로 실행되었다면 1, 실패했을 시에는 0을 반환한다.\n\n결과 확인\n\n마크베이스 Python 모듈에서 모든 결과값은 JSON으로 반환된다.\n\n다양한 환경에서 활용하기 쉬운 형태의 결과를 반환하는 것으로 채택하였다.\n\nmachbase.result()\n\n위쪽에서 설명하였던 함수들은 실행 결과들을 해당 함수의 반환값으로 나타내지 않고 성공, 실패 여부만 반환한다. 함수들의 결과값은 이 함수의 반환값으로만 얻을 수 있다.\n\nSELECT 결과 확인\n\nmachbase.execute() 함수로 SELECT 결과를 읽어오면 모든 결과를 한번에 읽기때문에 많은 메모리가 필요하며 경우에 따라서 메모리 부족 오류가 발생할 수 있다.\n\n따라서 SELECT 구문의 경우에는 한 레코드씩 결과를 얻을 수 있는 함수를 사용해야 한다.\n\nmachbase.select(aSql)\n\n서버에 접속되어 있을 때 해당 서버에 SELECT 질의문을 전송하는 명령어이다. 정상적으로 실행되었을 때 1, 실패하거나 에러가 발생했을 시에는 0을 반환한다.\n\nSELECT 구문만 사용이 가능하며, 모든 결과를 한번에 가져오는 machbase.execute() 함수와 달리 machbase.fetch()함수를 사용해서 1 레코드 씩 결과를 얻을 수 있다.\n\nmachbase.fetch()\n\nmachbase.select(aSql) 함수의 결과를 한 레코드 씩 읽어온다.\n\nreturn 값은 성공여부와 결과값으로 성공여부는 1과 0으로 표시되고, 결과값은 한 레코드의 결과가 json 형식으로 return 된다.\n\nmachbase.selectClose()\n\n한번 선택된 machbase.select()의 결과는 다른 machbase.select()나 machbase.execute() 함수가 호출될 때까지 open된 상태로 존재한다.\n\n이 것을 명시적으로 닫아주는 것이 machbase.selectClose() 함수이다.\n\nmachbase.select() 예제\n\ndb = machbase()\nif db.open('127.0.0.1','SYS','MANAGER', 5656) is 0 :\n    return db.result()\n \nquery = 'SELECT * from sample_table'\nwhile True:\n    is_success, result = db.fetch()\n    if is_success is 0:\n        break;\n \n    res = json.loads(result)\n \ndb.selectClose()\nif db.close() is 0 :\n    return db.result()\n\n예제\n\n간단한 예제들을 통해서 마크베이스 Python 모듈을 사용하는 방법을 알아보자\n\n$MACHBASE_HOME/sample/python 파일들을 이용해서 확인할 수 있다. 해당 디렉터리에는 간편하게 테스트를 해 볼 수 있게 해주는 Makefile과 데이터를 만들어주는 MakeData.py 파일이 있다. Makefile 내부 변수 중 PYPATH의 값을 마크베이스 Python 모듈이 설치된 파이썬으로 지정해야 정상 작동한다. 기본값은 마크베이스 패키지에 설치된 파이썬으로 지정되어 있다. 또한 파이썬에서 모듈을 독자적으로 실행하기 위해서는 init.py 파일이 필요하므로 해당 디렉터리에 파일이 존재하는지 확인하도록 하다.\n\n[mach@localhost]$ cd $MACHBASE_HOME/sample/python\n[mach@localhost python]$ ls -l\ntotal 20\n-rw-rw-r-- 1 mach mach    0 Oct  7 14:37 __init__.py\n-rw-rw-r-- 1 mach mach  764 Oct  7 14:37 MakeData.py\n-rw-rw-r-- 1 mach mach  593 Oct  7 14:58 Makefile\n-rw-rw-r-- 1 mach mach  664 Oct  7 14:37 Sample1Connect.py\n-rw-rw-r-- 1 mach mach 2401 Oct  7 14:37 Sample2Simple.py\n-rw-rw-r-- 1 mach mach 1997 Oct  7 14:37 Sample3Append.py\n\n\nConnect\n\n아래의 예제는 서버에 접속해서 질의를 실행하고 결과값을 반환하는 단순한 함수이다. 각각의 함수들이 실패(0)를 반환했을 때에 결과값을 반환하는 경우는 에러 결과를 반환하기 위함이다. 정상적으로 실행된다면 m$tables 테이블의 값들 개수가 반환 된다.\n\n파일 이름은 Sample1Connect.py이다.\n\nfrom machbaseAPI import machbase\ndef connect():\n    db = machbase()\n    if db.open('127.0.0.1','SYS','MANAGER',5656) is 0 :\n        return db.result()\n    if db.execute('select count(*) from m$tables') is 0 :\n        return db.result()\n    result = db.result()\n    if db.close() is 0 :\n        return db.result()\n    return result\nif __name__==\"__main__\":\n    print connect()\n\n\n[mach@localhost python]$ make run_sample1\n/home/machbase/machbase_home/webadmin/flask/Python/bin/python Sample1Connect.py\n{\"count(*)\":\"13\"}\n\n\nSimple\n\n아래 예제를 이용해서 단순하게 마크베이스에 파이썬을 이용해서 테이블을 만들고 값을 입력하고 입력된 값을 추출해서 확인하는 예제이다. 파일 이름은 Sample2Simple.py이다.\n\nimport re\nimport json\nfrom machbaseAPI import machbase\ndef insert():\n    db = machbase()\n    if db.open('127.0.0.1','SYS','MANAGER',5656) is 0 :\n        return db.result()\n    db.execute('drop table sample_table')\n    db.result()\n    if db.execute('create table sample_table(d1 short, d2 integer, d3 long, f1 float, f2 double, name varchar(20), text text, bin binary, v4 ipv4, v6 ipv6, dt datetime)') is 0:\n        return db.result()\n    db.result()\n    for i in range(1,10):\n        sql = \"INSERT INTO SAMPLE_TABLE VALUES (\"\n        sql += str((i - 5) * 6552) #short\n        sql += \",\"+ str((i - 5) * 42949672) #integer\n        sql += \",\"+ str((i - 5) * 92233720368547758L) #long\n        sql += \",\"+ \"1.234\"+str((i-5)*7) #float\n        sql += \",\"+ \"1.234\"+str((i-5)*61) #double\n        sql += \",'id-\"+str(i)+\"'\" #varchar\n        sql += \",'name-\"+str(i)+\"'\" #text\n        sql += \",'aabbccddeeff'\" #binary\n        sql += \",'192.168.0.\"+str(i)+\"'\" #ipv4\n        sql += \",'::192.168.0.\"+str(i)+\"'\" #ipv6\n        sql += \",TO_DATE('2015-08-0\"+str(i)+\"','YYYY-MM-DD')\" #date\n        sql += \")\";\n        if db.execute(sql) is 0 :\n            return db.result()\n        else:\n            print db.result()\n        print str(i)+\" record inserted.\"\n    query = \"SELECT d1, d2, d3, f1, f2, name, text, bin, to_hex(bin), v4, v6, to_char(dt,'YYYY-MM-DD') as dt from SAMPLE_TABLE\";\n    if db.execute(query) is 0 :\n        return db.result()\n    result = db.result()\n    for item in re.findall('{[^}]+}',result):\n        res = json.loads(item)\n        print \"d1 : \"+res.get('d1')\n        print \"d2 : \"+res.get('d2')\n        print \"d3 : \"+res.get('d3')\n        print \"f1 : \"+res.get('f1')\n        print \"f2 : \"+res.get('f2')\n        print \"name : \"+res.get('name')\n        print \"text : \"+res.get('text')\n        print \"bin : \"+res.get('bin')\n        print \"to_hex(bin) : \"+res.get('to_hex(bin)')\n        print \"v4 : \"+res.get('v4')\n        print \"v6 : \"+res.get('v6')\n        print \"dt : \"+res.get('dt')\n    if db.close() is 0 :\n        return db.result()\n    return result\nif __name__==\"__main__\":\n    print insert()\n\n\n[mach@loclahost python]$ make run_sample2\n/home/machbase/machbase_home/webadmin/flask/Python/bin/python Sample2Simple.py\n{\"EXECUTE RESULT\":\"Execute Success\"}\n1 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n2 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n3 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n4 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n5 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n6 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n7 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n8 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n9 record inserted.\nd1 : 26208\nd2 : 171798688\nd3 : 368934881474191032\nf1 : 1.23428\nf2 : 1.23424\nname : id-9\ntext : name-9\nbin : 616162626363646465656666\nto_hex(bin) : 616162626363646465656666\nv4 : 192.168.0.9\nv6 : ::192.168.0.9\n...\n\n\nAppend\n\n마크베이스에서 고속으로 데이터를 입력할 수 있는 Append 방식 또한 파이썬 모듈을 활용해서 사용할 수 있다. 아래의 예제는 고속으로 데이터를 입력하는 예제이다. 컬럼 정보 및 초기화를 위한 접속 클래스 db, Append를 하기 위한 접속용 클래스 db2를 선언하여 각각의 함수를 활용하는 방식을 사용했다. 파일 이름은 Sample3Append.py이다.\n\nimport re\nimport json\nfrom machbaseAPI import machbase\ndef append():\n#init,columns start\n    db = machbase()\n    if db.open('127.0.0.1','SYS','MANAGER',5656) is 0 :\n        return db.result()\n    db.execute('drop table sample_table')\n    db.result()\n    if db.execute('create table sample_table(d1 short, d2 integer, d3 long, f1 float, f2 double, name varchar(20), text text, bin binary, v4 ipv4, v6 ipv6, dt datetime)') is 0:\n        return db.result()\n    db.result()\n    tableName = 'sample_table'\n    db.columns(tableName)\n    result = db.result()\n    if db.close() is 0 :\n        return db.result()\n#init, colums end\n#append start\n    db2 = machbase()\n    if db2.open('127.0.0.1','SYS','MANAGER',5656) is 0 :\n        return db2.result()\n    types = []\n    for item in re.findall('{[^}]+}',result):\n        types.append(json.loads(item).get('type'))\n    values = []\n    with open('data.txt','r') as f:\n        for line in f.readlines():\n            v = []\n            i = 0\n            for l in line[:-1].split(','):\n                t = int(types[i])\n                if t == 4 or t == 8 or t == 12 or t == 104 or t == 108 or t == 112:\n                    #short   integer    long       ushort      uinteger     ulong\n                    v.append(int(l))\n                elif t == 16 or t == 20:\n                    #float      double\n                    v.append(float(l))\n                else:\n                    v.append(l)\n                i+=1\n            values.append(v)\n    db2.append(tableName, types, values, 'YYYY-MM-DD HH24:MI:SS')\n    result = db2.result()\n    if db2.close() is 0 :\n        return db2.result()\n#append end\n    return result\nif __name__==\"__main__\":\n    print append()\n\n\n[mach@localhost python]$ make run_sample3\n/home/machbase/machbase_home/webadmin/flask/Python/bin/python Sample3Append.py\n{\"EXECUTE RESULT\":\"Append success\"}"
					}
					
				
		
				
					,
					
					"sdk-restful-api-html": {
						"id": "sdk-restful-api-html",
						"title": "RESTful API",
						"version": "all",
						"categories": "",
						"url": " /sdk/restful_api.html",
						"content": "Table of contents\n\n\n  Table of contents\n  RESTful API 개요\n  Machbase RestAPI\n    \n      RestAPI 지원 Machbase Edition\n      RestAPI 지원 Table 종류\n      Configuration 설정\n        \n          버전별 .conf 파일의 위치\n          각 .conf 파일 Property 설명\n          각 .conf 파일 Sample\n        \n      \n      RestAPI 사용\n        \n          DDL Sample\n          DML Sample\n          APPEND Sample\n          Binary Append\n          Binary Append Sample\n          HTTP_AUTH Property 사용\n          출력 소수점 Scale 지정 (s 옵션)\n          데이터 Fetch 모드 변경 (m 옵션)\n          NULL 값의 처리\n        \n      \n      RestAPI for Tag Table 사용\n        \n          Raw 데이터 처리 함수\n            \n              Raw Value 입력 API\n              Raw Value 추출 API\n              전체 Tag 기준 Raw Value 삭제 API\n              특정 Tag 기준 Raw Value 삭제 API\n            \n          \n          통계 데이터 처리 함수\n            \n              통계 추출 API\n            \n          \n          태그 메타 데이터 처리 함수\n            \n              태그 정보 INSERT API\n              태그 정보 SELECT API\n              태그 정보 UPDATE API\n              태그 정보 삭제 API\n            \n          \n          기타 함수\n            \n              시간 범위 얻기 API\n              최소 value 얻기 API\n              최대 value 얻기 API\n              최초 row 얻기 API\n              최후 row 얻기 API\n              레코드 갯수 얻기 API\n              디스크 사용량 얻기 API\n              롤업 요청  API\n            \n          \n        \n      \n    \n  \n\n\nRESTful API 개요\n\nRepresentational State Transfer (REST) 는 소프트웨어 구조 스타일 의 일종으로, 확장 가능한 웹 서비스에서 제공하는 인터페이스의 가이드라인과 모범적인 규범들로 구성되어 있다.\n\nHTTP protocol에 정의된 4개의 Method 들이 Resource에 대한 CRUD를 정의한다.\n\n\n  \n    \n      HTTP Method\n      의미\n    \n  \n  \n    \n      POST\n      Create\n    \n    \n      GET\n      Select\n    \n    \n      PUT\n      Update\n    \n    \n      DELETE\n      Delete\n    \n  \n\n\n마크베이스는 표준 RESTful API 방식이 아니라, POST와 GET method만을 이용하여 CRUD를 처리하는 방식으로 RESTful API라고 할 수 있다.\n\n즉, 데이터 입력에는 POST method를 사용하고 나머지는 SQL query를 GET Method parameter로 전달하여 모든 작업을 할 수 있도록 구성되어 있다.\n\nMachbase RestAPI\n\nMachbase에 웹서버를 내장하여 별도의 서버 구동 없이 Machbase에 직접 RestAPI를 수행하는 기능이다.\n\nRestAPI 지원 Machbase Edition\n\nStandard / Cluster\n\nRestAPI 지원 Table 종류\n\nTag Table / Log Table / Lookup Table / Volatile Table\n\nConfiguration 설정\n\nmachbase.conf 와 http.conf 두 가지 설정 파일이 존재한다.\n\n\n  machbase.conf : Rest API를 사용하기 위해 HTTP 서버를 사용할 지와 최대 메모리 사용량 등 HTTP Server에 대한 설정을 저장\n  http.conf : HTTP Web Server 자체에 대한 설정을 저장\n\n\n설정 파일을 수정 하면 machbase 서버를 다시 시작해야 변경 내용이 적용된다.\n\n버전별 .conf 파일의 위치\n\nStandard Edition\n\n$MACHBASE_HOME/conf/machbase.conf\n\n$MACHBASE_HOME/http/conf/http.conf\n\nCluster Edition\n\nEACH_BROKER_HOME/conf/machbase.conf (Broker 별로 모두 수정)\n\nEACH_BROKER_HOME/http/conf/http.conf (Broker 별로 모두 수정)\n\n각 .conf 파일 Property 설명\n\nmachbase.conf (PROPERTY = VALUE 형태로 설정)\n\n\n  \n    \n      Property\n      설명\n    \n  \n  \n    \n      HTTP_ENABLE\n      내장 웹 서버를 구동할 지 여부 0 : 구동 안함, 1 : 구동\n    \n    \n      HTTP_PORT_NO\n      내장 웹 서버 접속 Port 번호 Port 범위 : 0 ~ 65535 Default : 5657\n    \n    \n      HTTP_MAX_MEM\n      하나의 Web Session에서 사용할 최대 메모리 Min : 1048576 (1MB) Default : 536870912 (512MB)\n    \n    \n      HTTP_AUTH\n      내장 웹 서버 사용 시 기본 인증을 사용할 지 여부 0 : 인증 사용 안함, 1 : 인증 사용함\n    \n  \n\n\nhttp.conf (JSON 형식으로 설정)\n\n\n  \n    \n      Property\n      설명\n    \n  \n  \n    \n      document_root\n      $MACHBASE_HOME 기준의 html 파일 위치 Default : http/html ($MACHBASE_HOME/http/html)\n    \n    \n      max_request_size\n      1회 요청의 최대 요청 byte 크기 제한\n    \n    \n      request_timeout_ms\n      1회 요청의 최대 응답 대기 시간 (millisecond)\n    \n    \n      enable_auth_domain_check\n      도메인 인증을 활성화 할지 여부 “yes” or “no” 값으로 설정 Default : “no”\n    \n    \n      reverse_proxy\n      요청 URL을 특정 URL로 변경 참고: https://brainbackdoor.tistory.com/113\n    \n  \n\n\n각 .conf 파일 Sample\n\nmachbase.conf\n#################################################################################\n# Rest-API port\n#################################################################################\nHTTP_PORT_NO = 5657\n  \n#################################################################################\n# Maximum memory per web session.\n# Default Value: 536870912 (512MB)\n#################################################################################\nHTTP_MAX_MEM = 536870912\n  \n#################################################################################\n# Min Value:     0\n# Max Value:     1\n# Default Value: 0\n#\n# Enable REST-API service.\n#################################################################################\nHTTP_ENABLE = 0\n  \n#################################################################################\n# Min Value:     0\n# Max Value:     1\n# Default Value: 0\n#\n# Enable Basic Authentication for Rest-API service\n#################################################################################\nHTTP_AUTH = 0\n\n\nhttp.conf\n{\n    \"document_root\":\"http/html/\",\n    \"max_request_size\": \"100000\",\n    \"request_timeout_ms\": \"10000\",\n    \"enable_auth_domain_check\": \"no\",\n    \"reverse_proxy\" : [[\"/machbase/tables\", \"http://127.0.0.1:55657/machbase\"],\n        [\"/self_machbase_proxy\", \"http://127.0.0.1:55657/machbase\"],\n        [\"/dead_proxy\", \"http://127.0.0.0/machbase\"]]\n}\n\n\nRestAPI 사용\n\nDDL / DML / Append 수행 가능\n\n기본 요청 형식\n \nhttp://addr:port/machbase?q=query&amp;f=dateformat\n \n응답 형식 DDL / Append / DML (except Select)\n \n{\"error_code\":0, \"error_message\" :\"Message\", \"data\":[]}\n \n응답 형식 DML (Select)\n \n{\"error_code\":0, \"error_message\" :\"Message\", \"columns\":[Columns], \"data\":[Data]}\n\n\nDDL Sample\n\n## 테이블 생성 요청\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=create table test_table (name varchar(20), time datetime, value double)'\n  \n## 정상 응답\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[]}\n  \n## 테이블 삭제 요청\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=drop table test_table'\n  \n## 정상 응답\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[]}\n\nDML Sample\n\n## 로그 테이블 입력(Insert) 요청\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=insert into test_table values (\"test\", \"1999-01-01 00:00:00\", 0)'\n  \n## 정상 응답\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[]}\n  \n## 로그 테이블 조회 요청\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from test_table'\n  \n## 정상 응답\n{\"error_code\":0, \"error_message\": \"\", \"columns\" : [{\"name\":\"NAME\", \"type\":5, \"length\":20},{\"name\":\"TIME\", \"type\":6, \"length\":8},{\"name\":\"VALUE\", \"type\":20, \"length\":8}],\"data\" :[{\"NAME\":\"test\", \"TIME\":\"1999-01-01 00:00:00 000:000:000\", \"VALUE\":0.000}]}\n\n\nAPPEND Sample\n\n## 로그 테이블 입력(Append) 요청\ncurl -X POST -H \"Content-Type: application/json\" \"http://127.0.0.1:5657/machbase\" -d '{\"name\":\"test_table\", \"date_format\":\"YYYY-MM-DD\",\"values\":[[\"test\", \"1999-01-01 00:00:01\", 1], [\"test\", \"1999-01-01 00:00:02\", 2], [\"test\", \"1999-01-01 00:00:03\", 3]]}'\n  \n## 정상 응답\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[], \"append_success\":3, \"append_failure\":0}\n\n\nBinary Append\n\nBinary Append의 경우 Binary 데이터를 Base64로 인코딩 후 전송하면 서버에서 디코딩 후 저장하게 된다.\n\n출력 시에는 바이너리 데이터가 Base64로 인코딩되어 반환된다.\n\n입력 : Binary Data » Base64 Encoding » HTTP(POST) » Base64 Decoding » Append(BLOB Binary)\n\n출력 : BLOB Binary » Base64 Encoding » HTTP (GET) » Base64 Decoding » Save or View Binary\n\nBinary Append Sample\n\n## 00 ~ FF까지의 256바이트 바이너리 데이터를 Base64로 인코딩 후 입력 / 출력 예\n  \n## 로그 테이블 입력(Append) 요청\ncurl  -X POST -H \"Content-Type: application/json\" \"http://127.0.0.1:5657/machbase\" -d '{\"name\":\"test_table\", \"date_format\":\"YYYY-MM-DD\",\"values\":[[\"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWprbG1ub3BxcnN0dXZ3eHl6e3x9fn+AgYKDhIWGh4iJiouMjY6PkJGSk5SVlpeYmZqbnJ2en6ChoqOkpaanqKmqq6ytrq+wsbKztLW2t7i5uru8vb6/wMHCw8TFxsfIycrLzM3Oz9DR0tPU1dbX2Nna29zd3t/g4eLj5OXm5+jp6uvs7e7v8PHy8/T19vf4+fr7/P3+/w==\"]]}'\n  \n## 정상 응답\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[], \"append_success\":1, \"append_failure\":0}\n  \n## 로그 테이블 출력 요청\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from test_table';\n  \n## Base64 데이터 출력\n{\"error_code\" :0, \"error_message\": \"No Error\", \"columns\" : [{\"name\":\"V1\", \"type\":57, \"length\":67108864}],\"data\" :[{\"V1\":\"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWprbG1ub3BxcnN0dXZ3eHl6e3x9fn+AgYKDhIWGh4iJiouMjY6PkJGSk5SVlpeYmZqbnJ2en6ChoqOkpaanqKmqq6ytrq+wsbKztLW2t7i5uru8vb6/wMHCw8TFxsfIycrLzM3Oz9DR0tPU1dbX2Nna29zd3t/g4eLj5OXm5+jp6uvs7e7v8PHy8/T19vf4+fr7/P3+/w==\"}]}\n  \n## machsql을 통한 HEX Dump 값 확인\nselect to_hex(v1) from test_table;\nto_hex(v1)                                                                      \n------------------------------------------------------------------------------------\n000102030405060708090A0B0C0D0E0F101112131415161718191A1B1C1D1E1F2021222324252627\n28292A2B2C2D2E2F303132333435363738393A3B3C3D3E3F404142434445464748494A4B4C4D4E4F\n505152535455565758595A5B5C5D5E5F606162636465666768696A6B6C6D6E6F7071727374757677\n78797A7B7C7D7E7F808182838485868788898A8B8C8D8E8F909192939495969798999A9B9C9D9E9F\nA0A1A2A3A4A5A6A7A8A9AAABACADAEAFB0B1B2B3B4B5B6B7B8B9BABBBCBDBEBFC0C1C2C3C4C5C6C7\nC8C9CACBCCCDCECFD0D1D2D3D4D5D6D7D8D9DADBDCDDDEDFE0E1E2E3E4E5E6E7E8E9EAEBECEDEEEF\nF0F1F2F3F4F5F6F7F8F9FAFBFCFDFEFF                                                \n[1] row(s) selected.\n\n\nHTTP_AUTH Property 사용\n\nRequest Header에 ‘Authorization: Basic Base64String’ 문자열을 포함하여 정상 유저임을 인증하도록 설정하는 옵션이다.\n\nBase64 문자열은 ID@Host:Password 구조로 작성한다. (단, Host명은 정확하지 않아도 된다. ID, Password는 Machbase의 유저 정보를 입력해야 한다.)\n\n인증을 위한 Basic Base64String 생성 방법\n## ID: sys, Password: manager 일 경우의 생성 예\necho -n \"sys@localhost:manager\" | base64\n  \n## 생성된 Base64String\nc3lzQGxvY2FsaG9zdDptYW5hZ2Vy\n\n\nBase64String 사용 Sample (HTTP_AUTH = 1 인 경우)\n## ID: sys, Password: manager 일 경우의 생성 예\necho -n \"sys@localhost:manager\" | base64\n  \n## 생성된 Base64String\nc3lzQGxvY2FsaG9zdDptYW5hZ2Vy\n\n\nBase64String 사용 Sample (HTTP_AUTH = 1 인 경우)\n## Authorization을 넣지 않은 요청의 예\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from v$stmt'\n  \n## 에러 발생\n{\"error_code\":3118, \"error_message\" :\"There is No Authorization Header.\", \"data\":[]}\n  \n## Request Header에 'Authorization:Base64String' 추가한 요청의 예\ncurl -H \"Authorization: Basic c3lzQGxvY2FsaG9zdDptYW5hZ2Vy\"  -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from v$stmt'\n  \n## 정상 응답\n{\"error_code\":0, \"error_message\": \"No Error\", \"columns\" : [{\"name\":\"ID\", \"type\":8, \"length\":4},{\"name\":\"SESS_ID\", \"type\":8, \"length\":4},{\"name\":\"STATE\", \"type\":5, \"length\":64},{\"name\":\"RECORD_SIZE\", \"type\":8, \"length\":4},{\"name\":\"QUERY\", \"type\":5, \"length\":32767}],\"data\" :[{\"ID\":0, \"SESS_ID\":52, \"STATE\":\"Fetch prepared\", \"RECORD_SIZE\":0, \"QUERY\":\"select * from v$stmt\"}]}\n\n\n출력 소수점 Scale 지정 (s 옵션)\n\n응답 데이터의 소수점을 몇자리까지 출력할 지 지정\n\n0 ~ 9 값으로 설정 (범위 값이 아닐 경우 3으로 동작)\n\n소수점 5자리까지 출력 Sample (s=5)\n## 소수점 5자리까지 출력\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from test_table' --data-urlencode 's=5';\n  \n## 정상 응답\n{\"error_code\" :0, \"error_message\": \"\", \"columns\" : [{\"name\":\"C1\", \"type\":16, \"length\":4},{\"name\":\"C2\", \"type\":20, \"length\":8}],\"data\" :[{\"C1\":12345.00000, \"C2\":1234.01235}]}\n\n\n데이터 Fetch 모드 변경 (m 옵션)\n\n응답 data에 column 이름을 항상 붙여서 표시할 지 결정 (0 : 표시, 1 : 표시 안함)\n\nDefault Fetch mode Sample (m=0)\n\n## fetch mode (m=0) 요청\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from tag limit 2' --data-urlencode 'm=0';\n  \n## 정상 응답 (Column Name을 포함한 결과 출력)\n{\"error_code\" :0, \"error_message\": \"\", \"columns\" : [{\"name\":\"NAME\", \"type\":5, \"length\":20},{\"name\":\"TIME\", \"type\":6, \"length\":8},{\"name\":\"VALUE\", \"type\":20, \"length\":8}],\n\"data\" :[{\"NAME\":\"tag1\", \"TIME\":\"2001-09-09 10:46:40 000:000:000\", \"VALUE\":1000000000.000}, {\"NAME\":\"tag1\", \"TIME\":\"2001-09-09 10:46:41 000:000:000\", \"VALUE\":1000000001.000}]}\n\n\nAdvanced Fetch mode Sample (m=1)\n\n## fetch mode (m=1) 요청\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from tag limit 2' --data-urlencode 'm=1';\n  \n## 정상 응답 (Column Name이 생략된 결과 출력)\n{\"error_code\" :0, \"error_message\": \"\", \"columns\" : [{\"name\":\"NAME\", \"type\":5, \"length\":20},{\"name\":\"TIME\", \"type\":6, \"length\":8},{\"name\":\"VALUE\", \"type\":20, \"length\":8}],\n\"data\" :[[\"tag1\", \"2001-09-09 10:46:40 000:000:000\", 1000000000.000], [\"tag1\", \"2001-09-09 10:46:41 000:000:000\", 1000000001.000]]}\n\nNULL 값의 처리\n\nDML의 처리 중 Insert나 Append 시 NULL 값은 그대로 null로 입력하면 된다.\n\nAppend 시 NULL 값을 포함한 JSON Sample\n\n[[\"data1\", \"data2\", \"data3\"],[\"data11\", \"data12\", \"data13\"],[\"data21\", \"data22\", \"data23\"],[null,null,null]]\n\n\nSelect 시의 NULL 값 포함 결과 Sample\n[{\"C1\":null, \"C2\":null, \"C3\":null, \"C4\":null, \"C5\":null, \"C6\":null, \"C7\":null, \"C8\":null, \"C9\":null, \"C10\":null, \"C11\":null, \"C12\":null}]\n\n\nRestAPI for Tag Table 사용\n\nTag table에 접근할 수 있는 Historian like한 RestAPI를 제공한다.\n\n기본 요청 형식으로 http://ipaddr:port/machiot/ 또는 http://ipaddr:port/machiot-rest-api/ 를 사용한다.\n\n또한 URL에 아래와 같은 parameter를 넘길 수 있다.\n\n\n  \n    \n      Parameter\n      설명\n      Sample\n    \n  \n  \n    \n      f 또는 DateFormat\n      date format 지정\n      XXX?f=YYYY/MM/DD XXX?DateFormat=YYYY/MM/DD\n    \n    \n      s 또는 Scale\n      Scale 지정\n      XXX?s=12 XXX?Scale=12\n    \n    \n      m 또는 FetchMode\n      fetch 모드 지정\n      XXX?m=1 XXX?FetchMode=1\n    \n  \n\n\nRaw 데이터 처리 함수\n\nRaw Value 입력 API\n\n이 API는 주어진 테이블에 데이터를 대량으로 입력하는 함수이다.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/raw/{Table}\n\nhttp://ipaddr:port/machiot/v1/datapoints/raw/{Table}\n\n\n  HTTP method : POST\n  Table : 입력할 대상 태그 테이블\n\n\n사용법\n\n요청\ncurl  -X POST -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot-rest-api/datapoints/raw/tag\"\n-d '{\"date_format\":\"YYYY-MM-DD HH24:MI:SS\",\n     \"values\":\n       [\n          [\"tag1\", \"1999-01-01 00:00:00\", 0],\n          [\"tag1\", \"1999-01-01 00:00:01\", 1],\n          [\"tag1\", \"1999-01-01 00:00:02\", 2]\n        ]\n     }';\n응답\n{\n   \"error_code\":0,\n   \"error_message\" :\"No Error\",\n   \"timezone\":\"+0900\",\n   \"data\":[],\n   \"append_success\":3,\n   \"append_failure\":0\n}\n\n\nRaw Value 추출 API\n\n이 API는 주어진 테이블의 데이터를 얻는 함수이다.\n\n직접 URL을 모두 지정하는 방법을 기본으로 지원하고, GET method의 인자로 넘기는 방법도 지원한다.\n\n아래의 URL에서 각각의 디렉토리명을 인자로 지정할 수도 있다.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/raw/{Table}/{TagNames}/{Start}/{End}/{Direction}/{Count}/{Offset}\n\nhttp://ipaddr:port/machiot/v1/datapoints/raw/{Table}/{TagNames}/{Start}/{End}/{Direction}/{Count}/{Offset}\n\n\n  HTTP method : GET\n  Table : 데이터를 가져올 대상 테이블명\n  TagNames : 데이터를 가져올 대상 태그명\n    \n      이 태그명은 ,(콤마)로 구분해 복수의 Tag 결과를 하나의 Series로 얻을 수 있음.\n    \n  \n  Start : 데이터를 추출할 시작 시간값을 나타냄.\n  End :  데이터를 추출할 마지막 시간값을 나타냄\n    \n      시간 포맷 아래와 같이 스페이스가 없는 형태와 있는 형태 둘다 지원한다. (curl로 테스트할 경우 부가형태를 활용할 수 있다)\n        \n          기본 형태 (스페이스 지원, 나노초까지 지원)\n            \n              연-월-일 시:분:초,밀리초\n              연-월-일 시:분:초 밀리:마이크로:나노\n            \n          \n          부가 형태 (스페이스 없음, 스페이스 대신 대문자 T를 사용하며, 밀리초까지 지원)\n            \n              연-월-일T시:분:초,밀리초\n            \n          \n        \n      \n      사용예)\n        \n          “2020-12-12”\n          “2020-12-12 03:22:22”\n          “2020-12-12 03:22:22 222:333:444”\n          “2020-12-12T03:22:22”\n          “2020-12-12T03:22:22,234”\n        \n      \n    \n  \n  Direction (생략 가능)\n    \n      0 (디폴트): 디폴트 값으로서 입력된 순서대로 출력\n      1 : 시간이 감소하는 방향으로 값을 출력\n      2 : 시간이 증가하는 방향으로 값을 출력\n    \n  \n  \n    Count (생략 가능)\n\n    \n      0 (디폴트): 전체 데이터를 모두 출력\n      기타 값 : 주어진 갯수 만큼 레코드를 출력\n    \n  \n  Offset (생략 가능)\n    \n      0 (디폴트) : 건너뛰지 않는다.\n      기타 값 : 주어진 값 만큼 값을 건너 뜀\n    \n  \n\n\n사용법\n\n요청 (URL을 모두 지정하는 방법)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw/tag/tag-1/2001-09-09T00:00:00,000/2001-09-09T01:20:00,000/0/5/0\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:00:01 000:000:000\",\n      \"VALUE\": 8001\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:01:41 000:000:000\",\n      \"VALUE\": 8101\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:03:21 000:000:000\",\n      \"VALUE\": 8201\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:05:01 000:000:000\",\n      \"VALUE\": 8301\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:06:41 000:000:000\",\n      \"VALUE\": 8401\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:08:21 000:000:000\",\n      \"VALUE\": 8501\n    }\n  ]\n}\n \n요청(인자를 넘기는 방법)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw?Table=tag&amp;amp;TagNames=tag-1&amp;amp;Start=2001-09-09T00:00:00,000&amp;amp;End=2001-09-09T01:20:00,000&amp;amp;Direction=0&amp;amp;Count=5&amp;amp;Offset=0\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:00:01 000:000:000\",\n      \"VALUE\": 8001\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:01:41 000:000:000\",\n      \"VALUE\": 8101\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:03:21 000:000:000\",\n      \"VALUE\": 8201\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:05:01 000:000:000\",\n      \"VALUE\": 8301\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:06:41 000:000:000\",\n      \"VALUE\": 8401\n    }\n  ]\n}\n  \n요청 (다수의 태그(tag-1, tag-2, tag-3)를 지정하는 경우)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw/tag/tag-1,tag-2,tag-3/2001-09-09T00:00:00,000/2001-09-09T01:20:00,000/0/0/0\";\n응답\n[\n  {\n    \"NAME\": \"tag-1\",\n    \"TIME\": \"2001-09-09 01:00:01 000:000:000\",\n    \"VALUE\": 8001\n  },\n  {\n    \"NAME\": \"tag-1\",\n    \"TIME\": \"2001-09-09 01:01:41 000:000:000\",\n    \"VALUE\": 8101\n  },\n  {\n    \"NAME\": \"tag-2\",\n    \"TIME\": \"2001-09-09 01:00:02 000:000:000\",\n    \"VALUE\": 8002\n  },\n  {\n    \"NAME\": \"tag-2\",\n    \"TIME\": \"2001-09-09 01:01:42 000:000:000\",\n    \"VALUE\": 8102\n  },\n  {\n    \"NAME\": \"tag-3\",\n    \"TIME\": \"2001-09-09 01:00:03 000:000:000\",\n    \"VALUE\": 8003\n  },\n  {\n    \"NAME\": \"tag-3\",\n    \"TIME\": \"2001-09-09 01:01:43 000:000:000\",\n    \"VALUE\": 8103\n  },\n  {\n    \"NAME\": \"tag-3\",\n    \"TIME\": \"2001-09-09 01:03:23 000:000:000\",\n    \"VALUE\": 8203\n  }\n]\n\n\n전체 Tag 기준 Raw Value 삭제 API\n\n이 API는 입력된 모든 태그에 대해 특정 시간 이전의 데이터를 모두 삭제한다.\n\n이 함수는 디스크의 용량이 부족하거나, 백업이 완료된 후 더 이상 필요하지 않은 데이터를 제거하는 데 유용하게 사용할 수 있다.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/raw/{Table}/{BeforeTime}\n\nhttp://ipaddr:port/machiot/v1/datapoints/raw/{Table}/{BeforeTime}\n\n\n  HTTP method : DELETE\n  Table : 삭제할 데이터가 저장된 테이블명\n  BeforeTime : 삭제할 이전 시간의 데이터 범위\n\n\n사용법\n요청\ncurl -X DELETE  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw/tag/2001-09-09T01:20:00,000\";\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"No Error\",\n  \"timezone\": \"+0900\",\n  \"effect_rows\": \"1201\",\n  \"data\": []\n}\n\n특정 Tag 기준 Raw Value 삭제 API\n\n이 API는 특정 태그에 대해 특정 시간 이전의 데이터를 모두 삭제한다.\n\n이 함수는 디스크의 용량이 부족하거나, 백업이 완료된 후 더 이상 필요하지 않은 데이터를 제거하는 데 유용하게 사용할 수 있다.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/raw/{Table}/{TagNames}/{BeforeTime}\n\nhttp://ipaddr:port/machiot/v1/datapoints/raw/{Table}/{TagNames}/{BeforeTime}\n\n\n  HTTP method : DELETE\n  Table : 삭제할 데이터가 저장된 테이블명\n  TagNames : 삭제할 대상 태그명들. ,(콤마)로 구분된 다수의 태그를 지정할 수 있음\n  BeforeTime : 삭제할 이전 시간의 데이터 범위\n\n\n사용법\n\n요청\ncurl -X DELETE  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw/tag/tag-2,tag-3/2001-09-09T01:20:00,000\";\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"No Error\",\n  \"timezone\": \"+0900\",\n  \"effect_rows\": \"32\",\n  \"data\": []\n}\n\n통계 데이터 처리 함수\n\n통계 추출 API\n\n이 API는 저장된 데이터에 대한 통계 결과를 빠르게 얻는 함수이다.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/calculated/{Table}/{TagNames}/{Start}/{End}/{CalculationMode}/{Count}/{IntervalType}/{IntervalValue}\n\nhttp://ipaddr:port/machiot/v1/datapoints/calculated/{Table}/{TagNames}/{Start}/{End}/{CalculationMode}/{Count}/{IntervalType}/{IntervalValue}\n\n\n  HTTP method : GET\n  Table : 데이터를 추출할 대상 테이블명\n  TagNames : 대상 태그명\n    \n      만일 다수의 태그를 지정할 경우에는 그 태그들에 대한 총 연산 결과가 출력됨. (각각의 태그에 대한 통계 결과를 얻고 싶을 경우 반복 호출해야 함)\n    \n  \n  Start, End : 데이터를 얻고자 하는 시간 범위 지정 (Raw 데이터 추출 API 참조)\n  Count : 데이터의 출력 갯수\n  CalculationMode : 얻고자 하는 통계 함수를 지정하며, ,(콤마)를 통해 다수의 통계 함수를 지정할 수 있다. (지정되는 함수명은 아래와 동일해야 한다)\n    \n      min : 최소값\n      max : 최대값\n      sum : 값의 총합\n      count : 값의 갯수\n      avg : 평균 값\n    \n  \n  IntervalType : 얻고자 하는 시간 종류 (시, 분, 초)\n    \n      sec : 초 단위\n      min : 분 단위\n      hour : 시간 단위\n    \n  \n  IntervalValue : 얻고자 하는 시간 유닛 단위\n    \n      0보다 큰 값으로서 60의 약수로 지정한다.\n      주로 5, 10, 15, 30 등이 지정된다.\n    \n  \n\n\n사용법\n\n요청 (단일 통계함수)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/calculated/tag/tag-1/2001-09-09T00:00:00,000/2001-09-09T01:20:00,000/sum/5/min/5\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"time\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"sum\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"time\": \"2001-09-09 01:00:00 000:000:000\",\n      \"sum\": 24303\n    },\n    {\n      \"time\": \"2001-09-09 01:05:00 000:000:000\",\n      \"sum\": 25203\n    },\n    {\n      \"time\": \"2001-09-09 01:10:00 000:000:000\",\n      \"sum\": 26103\n    },\n    {\n      \"time\": \"2001-09-09 01:15:00 000:000:000\",\n      \"sum\": 27003\n    },\n    {\n      \"time\": \"2001-09-09 01:20:00 000:000:000\",\n      \"sum\": 9201\n    }\n  ]\n}\n요청(다중 통계함수)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/calculated/tag/tag-1/2001-09-09T00:00:00,000/2001-09-09T01:20:00,000/sum,min,max/5/min/5\"\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"time\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"sum\",\n      \"type\": 20,\n      \"length\": 17\n    },\n    {\n      \"name\": \"min\",\n      \"type\": 20,\n      \"length\": 17\n    },\n    {\n      \"name\": \"max\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"time\": \"2001-09-09 01:00:00 000:000:000\",\n      \"sum\": 24303,\n      \"min\": 8001,\n      \"max\": 8201\n    },\n    {\n      \"time\": \"2001-09-09 01:05:00 000:000:000\",\n      \"sum\": 25203,\n      \"min\": 8301,\n      \"max\": 8501\n    },\n    {\n      \"time\": \"2001-09-09 01:10:00 000:000:000\",\n      \"sum\": 26103,\n      \"min\": 8601,\n      \"max\": 8801\n    },\n    {\n      \"time\": \"2001-09-09 01:15:00 000:000:000\",\n      \"sum\": 27003,\n      \"min\": 8901,\n      \"max\": 9101\n    },\n    {\n      \"time\": \"2001-09-09 01:20:00 000:000:000\",\n      \"sum\": 9201,\n      \"min\": 9201,\n      \"max\": 9201\n    }\n  ]\n}\n\n\n태그 메타 데이터 처리 함수\n\n이 섹션에서 사용되는 테이블의 구조는 아래와 같이 생성되었다.\n\ncurl -X GET \"http://127.0.0.1:${ITF_HTTP_PORT}/machbase\" --data-urlencode 'q=create tagdata table TAG (name_multi varchar(20) primary key, time_multi datetime basetime, value_multi double summarized, value2_multi short, value3_multi varchar(10)) metadata (myshortmeta short, baseip ipv4);';\n\n\n태그 정보 INSERT API\n\n이 API는 사용할 태그를 등록할 때 사용한다.  Tag ID와 함께 테이블 생성 시 추가했던 metadata 컬럼의 갯수만큼 데이터를 입력한다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/list/{TableName}\n\nhttp://ipaddr:port/machiot/v1/tags/list/{TableName}\n\n\n  HTTP method : POST\n  TableName : 입력할 대상 태그 테이블명을 지정한다.\n\n\n사용법\n\n요청\ncurl -X POST -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag\" -d\n       '{\n        \"values\":[\n            [\"tag3\", 0, \"127.0.0.0\"],\n            [\"tag4\", 1, \"127.0.0.1\"],\n            [\"tag4\", 1, \"127.0.0.1\"],\n            [\"tag5\", 2, \"127.0.0.2\"]\n         ]\n        }';\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"No Error\",\n  \"timezone\": \"+0900\",\n  \"data\": [],\n  \"append_success\": 3,\n  \"append_failure\": 1\n}\n#tag4의 중복 입력으로 에러 1건, 나머지 3건 성공\n\n\n태그 정보 SELECT API\n\nURL\n\nhttp://ipaddr:port/machiot/tags/list/{Table}/{TagName}\n\nhttp://ipaddr:port/machiot/v1/tags/list/{Table}/{TagName}\n\n\n  HTTP method : GET\n  Table : 추출 대상 태그 테이블\n    \n      만일 테이블명만 지정될 경우 모든 태그 이름의 리스트를 출력\n    \n  \n  TagName : 추출 대상 태그명\n    \n      해당 태그의 상세 정보 출력\n    \n  \n\n\n사용법\n\n요청(전체 태그명 얻기)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name_multi\",\n      \"type\": 5,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name_multi\": \"tag0\"\n    },\n    {\n      \"name_multi\": \"tag1\"\n    },\n    {\n      \"name_multi\": \"tag3\"\n    },\n    {\n      \"name_multi\": \"tag4\"\n    },\n    {\n      \"name_multi\": \"tag5\"\n    }\n  ]\n}\n\n\n태그 정보 UPDATE API\n\n이 API는 부가 태그 정보에 대한 수정을 지원한다.\n\nPUT 혹은 PATCH 모두 지원되며, 입력시 사용되는 JSON 포맷의 값은 해당 테이블의 컬럼명과 동일해야 한다.\n\n또한, 다수의 컬럼명을 지원하기 때문에 한번에 두개 이상의 컬럼의 값을 변경할 수 있다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/list/{Table}/{TagName}\n\nhttp://ipaddr:port/machiot/v1/tags/list/{Table}/{TagName}\n\n\n  HTTP method : PUT / PATCH\n  Table : 수정 대상 태그 테이블명\n  TagName : 수정 대상 태그명\n\n\n사용법\n\n요청(PUT 사용시)\ncurl -X PUT -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag/tag0\" -d  '{\"baseip\":\"192.168.0.1\"}';\n응답\n{\n   \"error_code\":0,\n   \"error_message\" :\"No Error\",\n   \"timezone\":\"+0900\",\n   \"effect_rows\":\"1\",\n   \"data\":[]\n}\n  \n요청 (PATCH 사용시)\ncurl -X PATCH -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag/tag3\" -d  '{\"baseip\":\"255.255.255.0\", \"myshortmeta\":9999 }';\n응답\n{\n   \"error_code\":0,\n   \"error_message\" :\"No Error\",\n   \"timezone\":\"+0900\",\n   \"effect_rows\":\"1\",\n   \"data\":[]\n}\n\n\n태그 정보 삭제 API\n\n이 API는 지정된 태그를 삭제한다. 하지만 해당 태그에 raw 데이터가 존재할 경우 삭제에 실패한다.\n\n데이터가 존재하는 태그를 삭제하기 위해서는 해당 태그의 raw 데이터에 대한 삭제를 먼저 수행하고 난 뒤에 이 함수를 호출해야 한다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/list/{Table}/{TagNames}\n\nhttp://ipaddr:port/machiot/v1/tags/list/{Table}/{TagNames}\n\n\n  HTTP method : DELETE\n  Table: 삭제 대상 태그 테이블명\n  TagName: 삭제할 대상 태그명\n\n\n사용법\n\n요청 (데이터가 있는 태그의 경우)\ncurl -X DELETE -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag/tag0\";\n응답 (에러)\n{\n   \"error_code\":2324,\n   \"error_message\" :\"Cannot delete tagmeta. there exist data with deleted_tag key.\",\n   \"timezone\":\"+0900\",\n   \"data\":[]\n}\n \n \n요청 (데이터가 모두 삭제된 태그의 경우)\ncurl -X DELETE -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag/tag4\";\n응답 (성공)\n{\n   \"error_code\":0,\n   \"error_message\" :\"No Error\",\n   \"timezone\":\"+0900\",\n   \"effect_rows\":\"1\",\n   \"data\":[]\n}\n\n\n기타 함수\n\n시간 범위 얻기 API\n\n이 API는 지정된 테이블 및 태그의 데이터에 대한 전체의 시간 범위(최소, 최대)를 얻는다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/range/{Table}/{TagName}\n\nhttp://ipaddr:port/machiot/v1/tags/range/{Table}/{TagName}\n\n\n  HTTP method : GET\n  Table : 추출한 대상 테이블명\n  TagName : 추출할 태그명\n    \n      지정하지 않았을 경우 전체 시간 범위 반환(이때 태그명은 ALL로 되돌아온다)\n    \n  \n\n\n사용법\n\n요청 (전체 테이블 범위)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/range/tag\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 3\n    },\n    {\n      \"name\": \"min\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"max\",\n      \"type\": 6,\n      \"length\": 31\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"ALL\",\n      \"min\": \"2001-09-09 01:00:00 000:000:000\",\n      \"max\": \"2032-09-09 10:46:49 000:000:000\"\n    }\n  ]\n}\n  \n요청(특정 태그 tag-1, tag-2에 대한 시간 범위)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/range/tag/tag-1,tag-2\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"min\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"max\",\n      \"type\": 6,\n      \"length\": 31\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"min\": \"2001-09-09 01:00:01 000:000:000\",\n      \"max\": \"2001-09-21 12:31:41 000:000:000\"\n    },\n    {\n      \"name\": \"tag-2\",\n      \"min\": \"2001-09-09 01:00:02 000:000:000\",\n      \"max\": \"2001-09-21 12:31:42 000:000:000\"\n    }\n  ]\n}\n\n\n최소 value 얻기 API\n\n이 API는 지정된 테이블 혹은 태그에 존재하는 최소 Value를 얻는다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/min/{Table}/{TagName}\n\nhttp://ipaddr:port/machiot/v1/tags/min/{Table}/{TagName}\n\n\n  HTTP method : GET\n  Table : 추출한 대상 테이블명\n  TagName : 추출할 태그명\n    \n      지정하지 않았을 경우 해당 테이블의 최소 value만을 출력\n    \n  \n\n\n사용법\n\n요청 (전체 테이블)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/min/tag\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"min\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"min\": 0.0\n    }\n  ]\n}\n \n \n요청 (특정 태그 tag-1, tag-2)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/min/tag/tag-1,tag-2\";\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 100\n    },\n    {\n      \"name\": \"time\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"min\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"time\": \"2001-09-09 10:46:42 000:000:000\",\n      \"min\": 10001.0\n    },\n    {\n      \"name\": \"tag-2\",\n      \"time\": \"2001-09-09 10:46:43 000:000:000\",\n      \"min\": 10002.0\n    }\n  ]\n}\n\n\n최대 value 얻기 API\n\n이 API는 지정된 테이블 혹은 태그에 존재하는 최대 Value를 얻는다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/max/{Table}/{TagName}\n\nhttp://ipaddr:port/machiot/v1/tags/max/{Table}/{TagName}\n\n\n  HTTP method : GET\n  Table : 추출한 대상 테이블명\n  TagName : 추출할 태그명\n    \n      지정하지 않았을 경우 해당 테이블의 최대 value만을 출력\n    \n  \n\n\n사용법\n\n요청 (전체 테이블)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/max/tag\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"max\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"max\": 10000000000.0\n    }\n  ]\n}\n \n \n요청 (특정 태그 tag-1, tag-2)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/max/tag/tag-1,tag-2\";\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 100\n    },\n    {\n      \"name\": \"time\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"max\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"time\": \"2001-09-09 13:12:12 000:000:000\",\n      \"max\": 9999999991.0\n    },\n    {\n      \"name\": \"tag-2\",\n      \"time\": \"2001-09-09 13:12:13 000:000:000\",\n      \"max\": 9999999992.0\n    }\n  ]\n}\n\n\n최초 row 얻기 API\n\n이 API는 지정된 테이블 혹은 태그에 존재하는 가장 작은 time값의 row를 얻는다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/first/{Table}/{TagName}\n\nhttp://ipaddr:port/machiot/v1/tags/first/{Table}/{TagName}\n\n\n  HTTP method : GET\n  Table : 추출한 대상 테이블명\n  TagName : 추출할 태그명\n    \n      지정하지 않았을 경우 해당 테이블의 최초 row를 출력\n    \n  \n\n\n사용법\n\n요청 (전체 테이블)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/first/tag\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-0\",\n      \"TIME\": \"2001-09-09 01:00:00 000:000:000\",\n      \"VALUE\": 8000.0\n    }\n  ]\n}\n \n \n요청 (특정 태그 tag-1, tag-2)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/first/tag/tag-1,tag-2\";\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:00:01 000:000:000\",\n      \"VALUE\": 8001.0\n    },\n    {\n      \"NAME\": \"tag-2\",\n      \"TIME\": \"2001-09-09 01:00:02 000:000:000\",\n      \"VALUE\": 8002.0\n    }\n  ]\n}\n\n최후 row 얻기 API\n\n이 API는 지정된 테이블 혹은 태그에 존재하는 가장 큰 time값의 row를 얻는다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/last/{Table}/{TagName}\n\nhttp://ipaddr:port/machiot/v1/tags/last/{Table}/{TagName}\n\n\n  HTTP method : GET\n  Table : 추출한 대상 테이블명\n  TagName : 추출할 태그명\n    \n      지정하지 않았을 경우 해당 테이블의 최후 row를 출력\n    \n  \n\n\n사용법\n요청 (전체 테이블)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/last/tag\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"dummy\",\n      \"TIME\": \"2032-09-09 10:46:49 000:000:000\",\n      \"VALUE\": 1000000009.0\n    }\n  ]\n}\n \n \n요청 (특정 태그 tag-1, tag-2)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/last/tag/tag-1,tag-2\";\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-21 12:31:41 000:000:000\",\n      \"VALUE\": 999901.0\n    },\n    {\n      \"NAME\": \"tag-2\",\n      \"TIME\": \"2001-09-21 12:31:42 000:000:000\",\n      \"VALUE\": 999902.0\n    }\n  ]\n}\n\n\n레코드 갯수 얻기 API\n\n이 API는 지정된 테이블 혹은 태그에 존재하는 레코드의 갯수를 얻는다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/count/{Table}/{TagNames}\n\n혹은\n\nhttp://ipaddr:port/machiot/tags/cnt/{Table}/{TagNames}\n\nhttp://ipaddr:port/machiot/v1/tags/count/{Table}/{TagNames}\n\n혹은\n\nhttp://ipaddr:port/machiot/v1/tags/cnt/{Table}/{TagNames}\n\n\n  HTTP method : GET\n  Table : 추출한 대상 테이블명\n  TagName : 추출할 태그명\n    \n      지정하지 않았을 경우 해당 테이블의 전체 레코드 갯수 출력\n    \n  \n\n\n사용법\n\n요청 (전체 테이블)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/count/tag\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"count\",\n      \"type\": 12,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"count\": 1000001\n    }\n  ]\n}\n \n \n요청 (특정 태그 tag-1, tag-2)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/count/tag/tag-1,tag-2\";\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 100\n    },\n    {\n      \"name\": \"count\",\n      \"type\": 12,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"count\": 10000\n    },\n    {\n      \"name\": \"tag-2\",\n      \"count\": 10000\n    }\n  ]\n}\n\n\n디스크 사용량 얻기 API\n\n이 API는 지정된 테이블 혹은 태그가 사용중인 디스크 사용량의 근사치를 얻는다.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/disksize/{Table}/{TagNames}\n\nhttp://ipaddr:port/machiot/v1/tags/disksize/{Table}/{TagNames}\n\n\n  HTTP method : GET\n  Table : 추출한 대상 테이블명\n  TagName : 추출할 태그명\n    \n      지정하지 않았을 경우 해당 테이블의 전체 디스크 사용량 출력\n    \n  \n\n\n사용법\n\n요청 (전체 테이블)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/disksize/tag/\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"disksize\",\n      \"type\": 12,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"disksize\": 276904448\n    }\n  ]\n}\n \n요청 (특정 태그 tag-1, tag-2)\ncurl -X  GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/disksize/tag/tag-1,tag-2\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 100\n    },\n    {\n      \"name\": \"disksize\",\n      \"type\": 12,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"disksize\": 240000\n    },\n    {\n      \"name\": \"tag-2\",\n      \"disksize\": 240000\n    }\n  ]\n}\n\n\n롤업 요청  API\n\n이 API는 특정 롤업 테이블에 대한 강제적인 수행을 요청한다. 이를 통해 아직 계산되지 않은 통계 값을 계산하도록 강제한다.\n\n이 API를 호출하면 상황에 따라 몇초에서 몇분까지 대기할 수 있으므로 신중하게 사용해야 한다.\n\nURL\n\nhttp://ipaddr:port/machiot/rollup/{Table}\n\nhttp://ipaddr:port/machiot/v1/rollup/{Table}\n\n\n  HTTP method : GET\n  Table : 강제로 롤업을 수행할 태그 테이블명\n\n\n사용법\n\n요청\ncurl -X HTTP GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/rollup/tag\"\n응답\n{\n  \"error_code\": 0,\n  \"error_message\": \"No Error\",\n  \"timezone\": \"+0900\",\n  \"data\": []\n}"
					}
					
				
		
				
					,
					
					"feature-table-retention-html": {
						"id": "feature-table-retention-html",
						"title": "데이터 자동 삭제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/retention.html",
						"content": "데이터 보존 기간을 지정하여, 해당 기간이 지나면 데이터가 자동으로 삭제되는 기능이다.\n\n보존 기간 및 삭제 주기를 지정한 Retention Policy를 생성하고, ALTER 구문을 통해 테이블에 적용/해제할 수 있다.\n\nRetention Policy 생성\n\n보존 기간 및 삭제 주기를 지정하여 RETENTION POLICY를 생성한다.\n\n보존 기간은 월(MONTH), 일(DAY) 단위로 지정할 수 있으며, 삭제 주기는 일(DAY), 시간(HOUR) 단위로 지정할 수 있다.\n\nPOLICY 정보는 M$RETENTION 테이블 조회하여 확인할 수 있다.\n\nSyntax:\nCREATE RETENTION policy_name DURATION duration {MONTH|DAY} INTERVAL interval {DAY|HOUR}\n\n\n\n  policy_name : 생성할 policy 이름\n  duration : 삭제할 데이터의 보존 기간(시스템 시간 기준)\n  interval : 보존 기간 확인 주기\n\n\nExample:\n-- 1일 이상 지난 데이터를 삭제하고, 갱신 주기를 1시간으로 한다.\nMach&gt; CREATE RETENTION policy_1d_1h DURATION 1 DAY INTERVAL 1 HOUR;\nExecuted successfully.\n\n-- 1달 이상 지난 데이터를 삭제하고, 갱신 주기를 3일로 한다.\nMach&gt; CREATE RETENTION policy_1m_3d DURATION 1 MONTH INTERVAL 3 DAY;\nExecuted successfully.\n\nMach&gt; SELECT * FROM M$RETENTION;\nUSER_ID     POLICY_NAME                               DURATION             INTERVAL             \n-----------------------------------------------------------------------------------------------------\n1           POLICY_1D_1H                              86400                3600                 \n1           POLICY_1M_3D                              2592000              259200               \n[2] row(s) selected.\n\n\nRetention Policy 적용\n\n사전에 생성된 RETENTION POLICY를 테이블에 적용한다.\n\n적용 이후에는 삭제 주기마다 보존 기간을 확인하여 삭제한다.\n\nRETENTION POLICY가 적용된 테이블 정보는 V$RETENTION_JOB 테이블을 조회하여 확인할 수 있다.\n\nSyntax:\nALTER TABLE table_name ADD RETENTION policy_name\n\n\n\n  table_name : 적용할 table 이름\n  policy_name : 적용할 policy 이름\n\n\nExample:\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nExecuted successfully.\n\nMach&gt; ALTER TABLE tag ADD RETENTION policy_1d_1h;\nAltered successfully.\n\nMach&gt; SELECT * FROM V$RETENTION_JOB;\nUSER_NAME                                                                         TABLE_NAME                                                                        \n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nPOLICY_NAME                                                                       STATE                                                                             LAST_DELETED_TIME               \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSYS                                                                               TAG                                                                               \nPOLICY_1D_1H                                                                      WAITING                                                                           NULL                            \n[1] row(s) selected.\n\n\n\nRetention Policy 해제\n\n테이블에 적용된 RETENTION POLICY를 해제한다.\n\n해제 이후에는 데이터가 삭제되지 않고 영구 보존된다.\n\nSyntax:\nALTER TABLE table_name DROP RETENTION;\n\n\n\n  table_name : 해제할 table 이름\n\n\nExample:\nMach&gt; ALTER TABLE tag DROP RETENTION;\nAltered successfully.\n\n\nRetention Policy 삭제\n\n해당 RETENTION POLICY가 적용 중인 테이블이 존재하면 삭제할 수 없다.\n\n적용 중인 테이블의 RETENTION을 해제하고 삭제해야 한다.\n\nSyntax:\nDROP RETENTION policy_name\n\n\n\n  policy_name : 삭제할 policy 이름\n\n\nExample:\nMach&gt; ALTER TABLE tag ADD RETENTION policy_1d_1h;\nAltered successfully.\n\n-- ERROR\nMach&gt; DROP RETENTION policy_1d_1h;\n[ERR-02702: Policy (POLICY_1D_1H) is in use.]\n\nMach&gt; ALTER TABLE tag DROP RETENTION;\nAltered successfully.\n\n-- SUCCESS\nMach&gt; DROP RETENTION policy_1d_1h;\nDropped successfully."
					}
					
				
		
				
					,
					
					"feature-table-tag-rollup-html": {
						"id": "feature-table-tag-rollup-html",
						"title": "롤업 테이블의 생성 및 조회",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/rollup.html",
						"content": "목차\n\n\n  ROLLUP 테이블 생성\n  ROLLUP 테이블 시작/중지\n  ROLLUP 테이블 즉시 수집\n  ROLLUP 테이블 삭제\n  데이터 샘플\n  ROLLUP 평균값 얻기\n  ROLLUP 최소/최대값 얻기\n  ROLLUP 합계/개수 얻기\n  ROLLUP 제곱합 얻기\n  다양한 시간 간격으로 그룹화\n  JSON 타입 대상의 ROLLUP 활용\n\n\nROLLUP 테이블 생성\n\nTag Table 생성시 Rollup이 기본으로 생성되지 않고, 사용자가 직접 생성하는 방식으로 변경되었으며 문법은 아래와 같다.\n\n\n\n\n  rollup name : 생성될 rollup table의 이름 (40자 이내의 문자열로 자유롭게 생성 가능)\n  source table name : 생성될 rollup이 데이터를 집계할 source table 이름\n  src_table_column : rollup 대상 데이터 칼럼 이름\n    \n      숫자형 타입의 칼럼만 가능\n      source table이 rollup table인 경우 생략하며, source table의 rollup 대상 칼럼으로 자동 지정\n    \n  \n  \n    number sec/min/hour : 집계할 시간 숫자와 시간 단위 \n  ex) 1초 단위 집계 : 1 sec \n  ex) 30초 단위 집계 : 30 sec \n  ex) 1분 단위 집계 : 1 min \n  ex) 1시간 단위 집계 : 1 hour \n  \n  제약조건\n    \n      집계할 source table은 tag table 또는 rollup table만 지정 가능하다.\n      집계할 source table이 rollup table일 경우 생성될 rollup table의 시간은 source table의 시간보다 크며, 배수어야 한다.\n    \n  \n\n\n롤업 테이블 생성 예시\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE, strvalue VARCHAR(20));\nExecuted successfully.\n \n-- tag table의 value 칼럼 대상 1초 rollup 생성\nMach&gt; CREATE ROLLUP _tag_rollup_sec ON tag(value) INTERVAL 1 SEC;\n  \n-- tag table의 value 칼럼 대상 1분 rollup 생성\nMach&gt; CREATE ROLLUP _tag_rollup_min ON tag(value) INTERVAL 1 MIN;\n  \n-- tag table 대상 1시간 rollup 생성\nMach&gt; CREATE ROLLUP _tag_rollup_hour ON tag(value) INTERVAL 1 HOUR;\n  \n-- tag table 대상 30초 rollup 생성\nMach&gt; CREATE ROLLUP _tag_rollup_30sec ON tag(value) INTERVAL 30 SEC;\n  \n-- rollup table(위 30초 rollup) 대상 10분 rollup 생성\nMach&gt; CREATE ROLLUP _tag_rollup_10min ON _tag_rollup_30sec INTERVAL 10 MIN;\n \n-- 숫자형 타입이 아닌 칼럼에 대해 rollup 생성 시 에러\nMach&gt; CREATE ROLLUP _tag_rollup_sec ON tag(strvalue) INTERVAL 1 SEC;\n[ERR-02671: Invalid type for ROLLUP column (STRVALUE).]\n\n\nROLLUP 테이블 시작/중지\n\nrollup을 생성해도 rollup thread가 자동으로 시작되지 않아 사용자가 직접 rollup 을 시작해야한다.\n\n\n-- 특정 rollup 시작\nEXEC ROLLUP_START(rollup_name)\n \n-- 특정 rollup 중지\nEXEC ROLLUP_STOP(rollup_name)\n\n\nROLLUP 테이블 즉시 수집\n\nrollup은 기본적으로 설정된 시간 단위마다 데이터 집계를 시작한다.\n\n\n  ex) 1시간 단위 rollup이라면 1시간 마다 한번씩 데이터 집계를 하고, 나머지 시간은 대기한다.\n\n\n사용자가 수동으로 대기 시간을 무시하고 강제로 데이터 집계를 실행할 수 있다.\n\n-- 특정 rollup 즉시 수집\nEXEC ROLLUP_FORCE(rollup_name)\n\n\nROLLUP 테이블 삭제\n\nRollup을 삭제한다.\n\nDROP ROLLUP rollup_name\n\n\n\n  rollup_name : 삭제할 rollup 이름\n  제약조건: 삭제할 rollup table을 source table로 참조하고 있는 rollup이 존재할 경우 삭제 할 수 없으며 rollup 간의 의존성이 있는 경우 rollup 을 생성한 역순으로 삭제해야 한다.\n\n\nmach&gt; create tag table tag (name varchar(20) primary key, time datetime basetime, value double summarized);\nmach&gt; create rollup _tag_rollup_1 on tag(value) interval 1 sec;\nmach&gt; create rollup _tag_rollup_2 on _tag_rollup_1 interval 1 min;\nmach&gt; create rollup _tag_rollup_3 on _tag_rollup_2 interval 1 hour;\n  \n위와 같이 생성했을 경우 참조 순서는 아래와 같다.\n  \ntag -&gt; _tag_rollup_1 -&gt; _tag_rollup_2 -&gt; _tag_rollup_3\n  \n이 때 tag table이나, 중간에 있는 rollup을 삭제하려고 하면 에러가 발생한다.\n  \nmach&gt; drop rollup tag\n&gt; [ERR-02651: Dependent ROLLUP table exists.]\nmach&gt; drop rollup _tag_rollup_1\n&gt; [ERR-02651: Dependent ROLLUP table exists.]\n  \n아래 순서대로 삭제해야 정상적으로 삭제할 수 있다.\n  \nmach&gt; drop rollup _tag_rollup_3;\nmach&gt; drop rollup _tag_rollup_2;\nmach&gt; drop rollup _tag_rollup_1;\nmach&gt; drop table tag;\n\n\n조회 문법\n\nSELECT TIME ROLLUP 3 SECOND, AVG(VALUE) FROM TAG WHERE ...;\n\n\n위와 같이 BASETIME 속성으로 지정된 Datetime 형 컬럼 뒤에 ROLLUP 절을 붙여 지정하면 롤업 테이블 조회가 된다.\n\n[BASETIME_COLUMN] ROLLUP [PERIOD] [TIME_UNIT]\n\n\n\n  BASETIME_COLUMN : BASETIME 속성으로 지정된 TAG 테이블의 Datetime 형 컬럼\n  PERIOD : DATE_TRUNC() 함수에서 사용 가능한 시간 단위별 범위를 지정할 수 있다. (아래 참고)\n  TIME_UNIT : DATE_TRUNC() 함수에서 사용 가능한 모든 시간 단위를 사용할 수 있다. (아래 참고)\n\n\nTIME_UNIT 의 선택에 따라, 조회되는 롤업 테이블이 달라진다.\n\n\n  \n    \n      시간 단위(축약어)\n      시간 범위\n      조회 대상 롤업 테이블\n    \n  \n  \n    \n      nanosecond (nsec)\n      1000000000 (1초)\n      SECOND\n    \n    \n      microsecond (usec)\n      60000000 (60초)\n      SECOND\n    \n    \n      milisecond (msec)\n      60000 (60초)\n      SECOND\n    \n    \n      second (sec)\n      86400 (1일)\n      SECOND\n    \n    \n      minute (min)\n      1440 (1일)\n      MINUTE\n    \n    \n      hour\n      24 (1일)\n      HOUR\n    \n    \n      day\n      1\n      HOUR\n    \n    \n      month\n      1\n      HOUR\n    \n    \n      year\n      1\n      HOUR\n    \n  \n\n\nROLLUP 절을 사용하는 것은 롤업 테이블 조회를 직접 하는 것이기 때문에, 집계 함수를 사용하려면 다음의 특징이 있다.\n\n\n  숫자형 타입의 컬럼에 집계 함수를 호출해야 한다. 단, 롤업 테이블에서 지원하는 여섯 가지 집계 함수 (SUM, COUNT, MIN, MAX, AVG, SUMSQ) 만 지원한다.\n  ROLLUP 하는 BASETIME 컬럼으로 GROUP BY 를 직접 해야 한다.\n    \n      같은 의미의 ROLLUP 절을 그대로 사용해도 된다.\n      또는, ROLLUP 절에 별명 (alias) 를 붙이고, 별명으로 GROUP BY 에 작성해도 된다.\n    \n  \n\n\nSELECT   time rollup 3 sec mtime, avg(value)\nFROM     TAG\nGROUP BY time rollup 3 sec mtime;\n \n-- 또는\nSELECT   time rollup 3 sec mtime, avg(value)\nFROM     TAG\nGROUP BY mtime;\n\n\n데이터 샘플\n\n아래는 롤업 테스트를 위한 샘플 데이터이다.\n\ncreate tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\n \ninsert into tag metadata values ('TAG_0001');\n \ninsert into tag values('TAG_0001', '2018-01-01 01:00:01 000:000:000', 1);\ninsert into tag values('TAG_0001', '2018-01-01 01:00:02 000:000:000', 2);\ninsert into tag values('TAG_0001', '2018-01-01 01:01:01 000:000:000', 3);\ninsert into tag values('TAG_0001', '2018-01-01 01:01:02 000:000:000', 4);\ninsert into tag values('TAG_0001', '2018-01-01 01:02:01 000:000:000', 5);\ninsert into tag values('TAG_0001', '2018-01-01 01:02:02 000:000:000', 6);\n \ninsert into tag values('TAG_0001', '2018-01-01 02:00:01 000:000:000', 1);\ninsert into tag values('TAG_0001', '2018-01-01 02:00:02 000:000:000', 2);\ninsert into tag values('TAG_0001', '2018-01-01 02:01:01 000:000:000', 3);\ninsert into tag values('TAG_0001', '2018-01-01 02:01:02 000:000:000', 4);\ninsert into tag values('TAG_0001', '2018-01-01 02:02:01 000:000:000', 5);\ninsert into tag values('TAG_0001', '2018-01-01 02:02:02 000:000:000', 6);\n \ninsert into tag values('TAG_0001', '2018-01-01 03:00:01 000:000:000', 1);\ninsert into tag values('TAG_0001', '2018-01-01 03:00:02 000:000:000', 2);\ninsert into tag values('TAG_0001', '2018-01-01 03:01:01 000:000:000', 3);\ninsert into tag values('TAG_0001', '2018-01-01 03:01:02 000:000:000', 4);\ninsert into tag values('TAG_0001', '2018-01-01 03:02:01 000:000:000', 5);\ninsert into tag values('TAG_0001', '2018-01-01 03:02:02 000:000:000', 6);\n\n\n태그 하나에 대해서 3시간 동안 초단위의 각기 다른 값을 입력해 놓았다.\n\nROLLUP 평균값 얻기\n\n아래는 해당 태그에 대해 초, 분, 시 단위의 평균값을 얻는 예제이다.\n\nMach&gt; SELECT time rollup 1 sec mtime, avg(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           avg(value)\n---------------------------------------------------------------\n2018-01-01 01:00:01 000:000:000 1\n2018-01-01 01:00:02 000:000:000 2\n2018-01-01 01:01:01 000:000:000 3\n2018-01-01 01:01:02 000:000:000 4\n2018-01-01 01:02:01 000:000:000 5\n2018-01-01 01:02:02 000:000:000 6\n2018-01-01 02:00:01 000:000:000 1\n2018-01-01 02:00:02 000:000:000 2\n2018-01-01 02:01:01 000:000:000 3\n2018-01-01 02:01:02 000:000:000 4\n2018-01-01 02:02:01 000:000:000 5\n2018-01-01 02:02:02 000:000:000 6\n2018-01-01 03:00:01 000:000:000 1\n2018-01-01 03:00:02 000:000:000 2\n2018-01-01 03:01:01 000:000:000 3\n2018-01-01 03:01:02 000:000:000 4\n2018-01-01 03:02:01 000:000:000 5\n2018-01-01 03:02:02 000:000:000 6\n[18] row(s) selected.\n \nMach&gt; SELECT time rollup 1 min mtime, avg(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           avg(value)\n---------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 1.5\n2018-01-01 01:01:00 000:000:000 3.5\n2018-01-01 01:02:00 000:000:000 5.5\n2018-01-01 02:00:00 000:000:000 1.5\n2018-01-01 02:01:00 000:000:000 3.5\n2018-01-01 02:02:00 000:000:000 5.5\n2018-01-01 03:00:00 000:000:000 1.5\n2018-01-01 03:01:00 000:000:000 3.5\n2018-01-01 03:02:00 000:000:000 5.5\n[9] row(s) selected.\n \nMach&gt; SELECT time rollup 1 hour mtime, avg(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           avg(value)\n---------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 3.5\n2018-01-01 02:00:00 000:000:000 3.5\n2018-01-01 03:00:00 000:000:000 3.5\n[3] row(s) selected.\n\n\nROLLUP 최소/최대값 얻기\n\n아래는 해당 태그의 시간 범위에 따른 최소/최대값을 얻는 예제를 나타낸다. 이전 예제와 다른 점은, 쿼리 한 번에 최대값과 최소값을 동시에 얻을 수 있다는 것이다.\n\nMach&gt; SELECT time rollup 1 hour mtime, min(value), max(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           min(value)                  max(value)\n--------------------------------------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 1                           6\n2018-01-01 02:00:00 000:000:000 1                           6\n2018-01-01 03:00:00 000:000:000 1                           6\n[3] row(s) selected.\n \nMach&gt; SELECT time rollup 1 min mtime, min(value), max(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           min(value)                  max(value)\n--------------------------------------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 1                           2\n2018-01-01 01:01:00 000:000:000 3                           4\n2018-01-01 01:02:00 000:000:000 5                           6\n2018-01-01 02:00:00 000:000:000 1                           2\n2018-01-01 02:01:00 000:000:000 3                           4\n2018-01-01 02:02:00 000:000:000 5                           6\n2018-01-01 03:00:00 000:000:000 1                           2\n2018-01-01 03:01:00 000:000:000 3                           4\n2018-01-01 03:02:00 000:000:000 5                           6\n[9] row(s) selected.\n\n\nROLLUP 합계/개수 얻기\n\n아래는 합계 및 데이터 개수 값을 얻는 예제이다. 역시 하나의 쿼리에 합계와 개수를 얻을 수 있다.\n\nMach&gt; SELECT time rollup 1 min  mtime, sum(value), count(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           sum(value)                  count(value)\n-------------------------------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 3                           2\n2018-01-01 01:01:00 000:000:000 7                           2\n2018-01-01 01:02:00 000:000:000 11                          2\n2018-01-01 02:00:00 000:000:000 3                           2\n2018-01-01 02:01:00 000:000:000 7                           2\n2018-01-01 02:02:00 000:000:000 11                          2\n2018-01-01 03:00:00 000:000:000 3                           2\n2018-01-01 03:01:00 000:000:000 7                           2\n2018-01-01 03:02:00 000:000:000 11                          2\n[9] row(s) selected.\n\n\nROLLUP 제곱합 얻기\n\n아래는 제곱합 값을 얻는 예제이다.\n\nMach&gt; SELECT time ROLLUP 1 SEC mtime, SUMSQ(value) FROM tag GROUP BY mtime ORDER BY mtime;\nmtime                           SUMSQ(value)               \n---------------------------------------------------------------\n2018-01-01 01:00:01 000:000:000 1                          \n2018-01-01 01:00:02 000:000:000 4                          \n2018-01-01 01:01:01 000:000:000 9                          \n2018-01-01 01:01:02 000:000:000 16                         \n2018-01-01 01:02:01 000:000:000 25                         \n2018-01-01 01:02:02 000:000:000 36                         \n2018-01-01 02:00:01 000:000:000 1                          \n2018-01-01 02:00:02 000:000:000 4                          \n2018-01-01 02:01:01 000:000:000 9                          \n2018-01-01 02:01:02 000:000:000 16                         \n2018-01-01 02:02:01 000:000:000 25                         \n2018-01-01 02:02:02 000:000:000 36                         \n2018-01-01 03:00:01 000:000:000 1                          \n2018-01-01 03:00:02 000:000:000 4                          \n2018-01-01 03:01:01 000:000:000 9                          \n2018-01-01 03:01:02 000:000:000 16                         \n2018-01-01 03:02:01 000:000:000 25                         \n2018-01-01 03:02:02 000:000:000 36                         \n[18] row(s) selected.\n \nMach&gt; SELECT time ROLLUP 1 MIN mtime, SUMSQ(value) FROM tag GROUP BY mtime ORDER BY mtime;\nmtime                           SUMSQ(value)               \n---------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 5                          \n2018-01-01 01:01:00 000:000:000 25                         \n2018-01-01 01:02:00 000:000:000 61                         \n2018-01-01 02:00:00 000:000:000 5                          \n2018-01-01 02:01:00 000:000:000 25                         \n2018-01-01 02:02:00 000:000:000 61                         \n2018-01-01 03:00:00 000:000:000 5                          \n2018-01-01 03:01:00 000:000:000 25                         \n2018-01-01 03:02:00 000:000:000 61                         \n[9] row(s) selected.\n\n\n다양한 시간 간격으로 그룹화\n\nROLLUP 절의 장점은, DATE_TRUNC() 를 의도적으로 사용해서 시간 간격을 다변화할 필요가 없다는 것이다.\n\n3초 간격의 합계와 데이터 개수를 얻으려면 아래와 같이 하면 된다.\n예제 시간 범위가 0초, 1초, 2초 뿐이라 전부 0초로 수렴된 것을 확인할 수 있다. 결과적으로는 ‘분 단위 롤업’ 조회 결과와 일치한다.\n\nMach&gt; SELECT time rollup 3 sec  mtime, sum(value), count(value) FROM TAG WHERE name = 'TAG_0001' GROUP BY mtime ORDER BY mtime;\nmtime                           sum(value)                  count(value)\n-------------------------------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 3                           2\n2018-01-01 01:01:00 000:000:000 7                           2\n2018-01-01 01:02:00 000:000:000 11                          2\n2018-01-01 02:00:00 000:000:000 3                           2\n2018-01-01 02:01:00 000:000:000 7                           2\n2018-01-01 02:02:00 000:000:000 11                          2\n2018-01-01 03:00:00 000:000:000 3                           2\n2018-01-01 03:01:00 000:000:000 7                           2\n2018-01-01 03:02:00 000:000:000 11                          2\n\n\nJSON 타입 대상의 ROLLUP 활용\n\n7.5 버전부터 JSON 타입을 대상으로 ROLLUP을 사용할 수 있다.\n\n생성 구문에 JSON PATH를 OPERATOR와 연결하면 된다.\n\nJSON 타입 특성상, 하나의 JSON 칼럼에 PATH 별로 ROLLUP을 생성할 수 있다.\n\n-- create tag table\nCREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, jval JSON);\n \n-- insert data\ninsert into tag values ('tag-01', '2022-09-01 01:01:01', \"{ \\\"x\\\": 1, \\\"y\\\": 1.1}\");\ninsert into tag values ('tag-01', '2022-09-01 01:01:02', \"{ \\\"x\\\": 2, \\\"y\\\": 1.2}\");\ninsert into tag values ('tag-01', '2022-09-01 01:01:03', \"{ \\\"x\\\": 3, \\\"y\\\": 1.3}\");\ninsert into tag values ('tag-01', '2022-09-01 01:01:04', \"{ \\\"x\\\": 4, \\\"y\\\": 1.4}\");\ninsert into tag values ('tag-01', '2022-09-01 01:01:05', \"{ \\\"x\\\": 5, \\\"y\\\": 1.5}\");\ninsert into tag values ('tag-01', '2022-09-01 01:02:00', \"{ \\\"x\\\": 6, \\\"y\\\": 1.6}\");\ninsert into tag values ('tag-01', '2022-09-01 01:03:00', \"{ \\\"x\\\": 7, \\\"y\\\": 1.7}\");\ninsert into tag values ('tag-01', '2022-09-01 01:04:00', \"{ \\\"x\\\": 8, \\\"y\\\": 1.8}\");\ninsert into tag values ('tag-01', '2022-09-01 01:05:00', \"{ \\\"x\\\": 9, \\\"y\\\": 1.9}\");\ninsert into tag values ('tag-01', '2022-09-01 01:06:00', \"{ \\\"x\\\": 10, \\\"y\\\": 2.0}\");\n \n-- create rollup\nCREATE ROLLUP _tag_rollup_jval_x_sec ON tag(jval-&gt;'$.x') INTERVAL 1 SEC;\nCREATE ROLLUP _tag_rollup_jval_y_sec ON tag(jval-&gt;'$.y') INTERVAL 1 SEC;\n\n\nROLLUP 조회도 동일하게 사용하면 된다.\n\nMach&gt; SELECT time ROLLUP 2 SEC mtime, MIN(jval-&gt;'$.x'), MAX(jval-&gt;'$.x'), SUM(jval-&gt;'$.x'), COUNT(jval-&gt;'$.x'), SUMSQ(jval-&gt;'$.x') FROM tag GROUP BY mtime ORDER BY mtime;\nmtime                           min(jval-&gt;'$.x')            max(jval-&gt;'$.x')            sum(jval-&gt;'$.x')            count(jval-&gt;'$.x')   sumsq(jval-&gt;'$.x')         \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n2022-09-01 01:01:00 000:000:000 1                           1                           1                           1                    1                          \n2022-09-01 01:01:02 000:000:000 2                           3                           5                           2                    13                         \n2022-09-01 01:01:04 000:000:000 4                           5                           9                           2                    41                         \n2022-09-01 01:02:00 000:000:000 6                           6                           6                           1                    36                         \n2022-09-01 01:03:00 000:000:000 7                           7                           7                           1                    49                         \n2022-09-01 01:04:00 000:000:000 8                           8                           8                           1                    64                         \n2022-09-01 01:05:00 000:000:000 9                           9                           9                           1                    81                         \n2022-09-01 01:06:00 000:000:000 10                          10                          10                          1                    100                        \n[8] row(s) selected.\n \nMach&gt; SELECT time ROLLUP 2 SEC mtime, MIN(jval-&gt;'$.y'), MAX(jval-&gt;'$.y'), SUM(jval-&gt;'$.y'), COUNT(jval-&gt;'$.y'), SUMSQ(jval-&gt;'$.y') FROM tag GROUP BY mtime ORDER BY mtime\nmtime                           min(jval-&gt;'$.y')            max(jval-&gt;'$.y')            sum(jval-&gt;'$.y')            count(jval-&gt;'$.y')   sumsq(jval-&gt;'$.y')         \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n2022-09-01 01:01:00 000:000:000 1.1                         1.1                         1.1                         1                    1.21                       \n2022-09-01 01:01:02 000:000:000 1.2                         1.3                         2.5                         2                    3.13                       \n2022-09-01 01:01:04 000:000:000 1.4                         1.5                         2.9                         2                    4.21                       \n2022-09-01 01:02:00 000:000:000 1.6                         1.6                         1.6                         1                    2.56                       \n2022-09-01 01:03:00 000:000:000 1.7                         1.7                         1.7                         1                    2.89                       \n2022-09-01 01:04:00 000:000:000 1.8                         1.8                         1.8                         1                    3.24                       \n2022-09-01 01:05:00 000:000:000 1.9                         1.9                         1.9                         1                    3.61                       \n2022-09-01 01:06:00 000:000:000 2                           2                           2                           1                    4                          \n[8] row(s) selected."
					}
					
				
		
				
					,
					
					"sdk-sdk-html": {
						"id": "sdk-sdk-html",
						"title": "SDK",
						"version": "all",
						"categories": "",
						"url": " /sdk/sdk.html",
						"content": "MACHBASE SDK (Software Development Kit) 에 대한 문서이다.\n\n\n  CLI/ODBC\n  CLI/ODBC 예제\n  JDBC\n  Python\n  RESTful API\n  .NET Connector\n  Timezone"
					}
					
				
		
				
		
				
		
				
					,
					
					"feature-table-log-select-select-data-html": {
						"id": "feature-table-log-select-select-data-html",
						"title": "데이터 조회",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/select/select-data.html",
						"content": "ANSI 표준 SQL로 데이터를 검색할 수 있다.\n\n아래 예제 모두는 인덱스를 생성하지 않고 검색한 예이다. 즉, 마지막으로 입력된 데이터가 제일 먼저 출력된다.\n\n보다 자세한 내용은 SQL 레퍼런스의 [SELECT](/kr/sql-ref/select.html) 부분을 참조하면 된다.\n\n# 목차\n* [기본 조회](#기본-조회)\n* [조건 절 조회](#조건-절-조회)\n* [힌트(hint)를 이용한 검색 방향 지정하기](#힌트hint를-이용한-검색-방향-지정하기)\n  * [역방향 검색](#역방향-검색)\n  * [정방향 검색](#정방향-검색)\n  * [기본 스캔 방향 프로퍼티로 설정](#기본-스캔-방향-프로퍼티로-설정)\n\n## 기본 조회\n\n```sql\nSELECT * FROM table_name;\n```\n```sql\nMach> SELECT * FROM mach_log;\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.1     NULL                            NULL       \nNULL                                                                             \n192.168.0.2     2014-06-15 19:50:03 484:382:010 82         \nerror code = 20, critical warning                                                \n192.168.0.2     2014-06-15 19:50:03 484:382:008 57         \nerror code = 20                                                                  \n192.168.0.1     2014-06-15 19:50:03 484:382:006 99         \nerror code = 10, critical bug                                                    \n192.168.0.1     2014-06-15 19:50:03 484:382:004 55         \nerror code = 10                                                                  \n192.168.0.2     2014-06-15 19:50:03 484:382:002 31       \nnormal state                                                                     \n192.168.0.1     2014-06-15 19:50:03 484:382:000 32         \nnormal state                                                                     \n[7] row(s) selected.\nMach>\n```\n\n## 조건 절 조회\n\n```sql\nSELECT column_name,column_name\nFROM table_name\nWHERE column_name operator value;\n```\n```sql\nMach> SELECT * FROM mach_log WHERE device = '192.168.0.1';\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.1     NULL                            NULL       \nNULL                                                                             \n192.168.0.1     2014-06-15 19:50:36 488:663:006 99         \nerror code = 10, critical bug                                                    \n192.168.0.1     2014-06-15 19:50:36 488:663:004 55         \nerror code = 10                                                                  \n192.168.0.1     2014-06-15 19:50:36 488:663:000 32         \nnormal state                                                                     \n[4] row(s) selected.\n \nMach> SELECT * FROM mach_log WHERE device = '192.168.0.1' AND temp > 30 AND temp  SELECT * FROM mach_log where device > '192.168.0.1';\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.2     2014-06-15 19:50:36 488:663:010 82         \nerror code = 20, critical warning                                                \n192.168.0.2     2014-06-15 19:50:36 488:663:008 57         \nerror code = 20                                                                  \n192.168.0.2     2014-06-15 19:50:36 488:663:002 31         \nnormal state                                                                     \n[3] row(s) selected.\n \nMach> SELECT * FROM mach_log WHERE msg LIKE '%error%';\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.2     2014-06-15 19:50:36 488:663:010 82         \nerror code = 20, critical warning                                                \n192.168.0.2     2014-06-15 19:50:36 488:663:008 57         \nerror code = 20                                                                  \n192.168.0.1     2014-06-15 19:50:36 488:663:006 99         \nerror code = 10, critical bug                                                    \n192.168.0.1     2014-06-15 19:50:36 488:663:004 55         \nerror code = 10                                                                  \n[4] row(s) selected.\n```\n\n## 힌트(hint)를 이용한 검색 방향 지정하기\n\n로그 테이블은 일반적으로 최근에 입력한 레코드부터 조회가 가능하다. 가장 먼저 입력한 레코드부터 조회하고 싶을 때에는 힌트를 이용해 조회 방향을 제어할 수 있다.\n\n### 역방향 검색\n\n기본값이며, /*+ SCAN_BACKWARD(table_name) */ 힌트를 추가하여 조회가 가능하다.\n\n```sql\nMach> SELECT * FROM LOG;\nTIME                           \n----------------------------------\n2021-01-04 00:00:00 000:000:000\n2021-01-03 00:00:00 000:000:000\n2021-01-02 00:00:00 000:000:000\n2021-01-01 00:00:00 000:000:000\n[4] row(s) selected.\nElapsed time: 0.001\n \nMach> SELECT /*+ SCAN_BACKWARD(LOG) */ * FROM LOG;\nTIME                           \n----------------------------------\n2021-01-04 00:00:00 000:000:000\n2021-01-03 00:00:00 000:000:000\n2021-01-02 00:00:00 000:000:000\n2021-01-01 00:00:00 000:000:000\n[4] row(s) selected.\nElapsed time: 0.001\n```\n### 정방향 검색\n\n/*+ SCAN_FORWARD(table_name) */ 힌트를 추가하여 조회가 가능하다.\n\n```sql\nMach> SELECT /*+ SCAN_FORWARD(LOG) */ * FROM LOG;\nTIME                           \n----------------------------------\n2021-01-01 00:00:00 000:000:000\n2021-01-02 00:00:00 000:000:000\n2021-01-03 00:00:00 000:000:000\n2021-01-04 00:00:00 000:000:000\n[4] row(s) selected.\nElapsed time: 0.001\n```\n\n### 기본 스캔 방향 프로퍼티로 설정\n\n[TABLE_SCAN_DIRECTION](/kr/setting-monitoring/property.md) 프로퍼티로 SELECT 문에 힌트가 없을 때 로그 테이블의 스캔 방향을 설정할 수 있다."
					}
					
				
		
				
					,
					
					"sql-ref-select-hint-html": {
						"id": "sql-ref-select-hint-html",
						"title": "SELECT Hint",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/select-hint.html",
						"content": "# 목차\n* [개요](#개요)\n* [PARALLEL](#parallel)\n* [NOPARALLEL](#noparallel)\n* [FULL](#full)\n* [NO_INDEX](#no_index)\n* [RID_RANGE](#rid_range)\n* [SCAN_FORWARD, SCAN_BACKWARD](#scan_forward-scan_backward)\n\n# 개요\n\nSELECT 쿼리에 함께 사용할 수 있는 Hint를 설명한다.\n* Cluster / Standard Edition에 따라서 지원되는 Hint는, 지원/미지원 여부가 표기 되어 있다\n* 별도의 표기가 없는 경우에는, 모든 Edition에서 지원한다\n\n# PARALLEL\n\nParallel query 수행을 위한 parallel factor를 지정한다.\n\n- Cluster : 지원함\n- Standard : 미지원\n\n```sql\nSELECT /*+ PARALLEL(table_name, parallel_factor) */ ...\n```\n\n```sql\nMach> CREATE TABLE log_parallel_test (sensor VARCHAR(32), frequency DOUBLE, value DOUBLE, ts DATETIME);\nMach> CREATE INDEX idx_ts ON log_parallel_test (ts);\n \nMach> EXPLAIN SELECT /*+ PARALLEL(log_parallel_test, 8) */ sensor, frequency, avg(value)\n      FROM log_parallel_test\n      WHERE ts >= TO_DATE('2007-07-01', 'YYYY-MM-DD') and ts = TO_DATE('2007-07-01', 'YYYY-MM-DD')                                 \n     * ts  CREATE TABLE log_parallel_test (sensor VARCHAR(32), frequency DOUBLE, value DOUBLE, ts DATETIME);\nMach> CREATE INDEX idx_ts ON log_parallel_test (ts);\n \nMach> EXPLAIN SELECT /*+ NOPARALLEL(log_parallel_test) */ sensor, frequency, avg(value)\n      FROM log_parallel_test\n      WHERE ts >= TO_DATE('2007-07-01', 'YYYY-MM-DD') and ts = TO_DATE('2007-07-01', 'YYYY-MM-DD')                                 \n     * ts  CREATE TABLE log_full_test (sensor VARCHAR(32), I1 INTEGER);\nMach> CREATE INDEX idx_I1 ON log_full_test (I1);\n \nMach> EXPLAIN SELECT * FROM log_full_test WHERE I1 = 1;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   *BITMAP RANGE (table id:14, column id:2, index id:15)                         \n   [KEY RANGE]                                                                   \n    * I1 = 1                                                                     \n[5] row(s) selected.\n \nMach> EXPLAIN SELECT /*+ FULL(log_full_test) */ * FROM log_full_test WHERE I1 = 1;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  FULL SCAN                                                                      \n[2] row(s) selected.\n```\n\n# NO_INDEX\n\n쿼리에서 index_name의 INDEX를 사용하지 않는다.\n\n```sql\nSELECT /*+ NO_INDEX(table_name,index_name) */ ...\n```\n```sql\nMach> CREATE TABLE log_no_index_test (sensor VARCHAR(32), I1 INTEGER, I2 INTEGER);\nMach> CREATE INDEX idx_I1 ON log_no_index_test (I1);\nMach> CREATE INDEX idx_I2 ON log_no_index_test (I2);\n \nMach> EXPLAIN SELECT * FROM TEST WHERE I1 = 1;\nPLAN\n------------------------------------------------------------------------------------\n PROJECT\n  INDEX SCAN\n   *BITMAP RANGE (t:7, c:1, i:8) with BLOOMFILTER\n   [KEY RANGE]                                                                   \n    * I1 = 1                                                                     \n[5] row(s) selected.\n \nMach> EXPLAIN SELECT /*+ NO_INDEX(TEST,TEST_IDX) */ * FROM TEST WHERE I1 = 1;\nPLAN\n------------------------------------------------------------------------------------\n PROJECT\n  FULL SCAN\n[2] row(s) selected.\n \nMach> EXPLAIN SELECT * FROM log_no_index_test WHERE I1 = 1 or I2 = 2;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   INDEX (OR)                                                                    \n    *BITMAP RANGE (table id:21, column id:2, index id:22)                        \n    *BITMAP RANGE (table id:21, column id:3, index id:23)                        \n   [KEY RANGE]                                                                   \n    * I1 = 1 or I2 = 2                                                           \n[7] row(s) selected.\n \nMach> EXPLAIN SELECT /*+ NO_INDEX(log_no_index_test, idx_I1) */ * FROM log_no_index_test WHERE I1 = 1 or I2 = 2;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  FULL SCAN (LOG_NO_INDEX_TEST)                                                                      \n[2] row(s) selected.\n \nMach> EXPLAIN SELECT * FROM log_no_index_test WHERE I1 = 1 and I2 = 2;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   *BITMAP RANGE (table id:21, column id:2, index id:22)                         \n   *BITMAP RANGE (table id:21, column id:3, index id:23)                         \n   [KEY RANGE]                                                                   \n    * I1 = 1                                                                     \n    * I2 = 2                                                                     \n[7] row(s) selected.\n \nMach> EXPLAIN SELECT /*+ NO_INDEX(log_no_index_test, idx_I1) */ * FROM log_no_index_test WHERE I1 = 1 and I2 = 2;\n \nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   *BITMAP RANGE (table id:21, column id:3, index id:23)                         \n   [KEY RANGE]                                                                   \n    * I2 = 2                                                                     \n   [FILTER]                                                                      \n    * I1 = 1                                                                     \n[7] row(s) selected.\nElapsed time: 0.001\nMach>\nMach>\nMach> EXPLAIN SELECT /*+ NO_INDEX(log_no_index_test, idx_I2) */ * FROM log_no_index_test WHERE I1 = 1 and I2 = 2;\n \nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   *BITMAP RANGE (table id:21, column id:2, index id:22)                         \n   [KEY RANGE]                                                                   \n    * I1 = 1                                                                     \n   [FILTER]                                                                      \n    * I2 = 2                                                                     \n[7] row(s) selected.\n```\n\n# RID_RANGE\n\nRID 범위 내에서 수행한다.\n\n```sql\nSELECT /*+ RID_RANGE(table_name,number,number) */ ...\n```\n\n```\nMach> SELECT /*+ RID_RANGE(TEST,45,50) */ _RID, * FROM TEST;\n_RID                 I1\n------------------------------------\n49                   1\n48                   1\n47                   1\n46                   1\n45                   1\n[5] row(s) selected.\n```\n\n# SCAN_FORWARD, SCAN_BACKWARD\n\n테이블의 스캔 방향을 지정한다. SCAN_FORWARD를 힌트로 사용하면 가장 먼저 입력한 레코드 우선으로, SCAN_BACKWARD를 힌트로 사용하면 가장 나중에 입력한 레코드 우선으로 조회한다.\n\n* Standard edition의 LOG 테이블에 대해서만 지원된다.\n\n```sql\nSELECT /*+ SCAN_FORWARD(table_name) */ ...\nSELECT /*+ SCAN_BACKWARD(table_name) */ ...\n```\n\n```sql\nMach> SELECT /*+ SCAN_FORWARD(mytbl) */  _ARRIVAL_TIME, VALUE FROM mytbl LIMIT 10;\n_ARRIVAL_TIME                   VALUE                   \n----------------------------------------------------------------\n2017-01-01 00:00:49 500:000:000 0                         \n2017-01-01 00:01:39 500:000:000 1                         \n2017-01-01 00:02:29 500:000:000 2                         \n2017-01-01 00:03:19 500:000:000 3                         \n2017-01-01 00:04:09 500:000:000 4                         \n2017-01-01 00:04:59 500:000:000 5                         \n2017-01-01 00:05:49 500:000:000 6                         \n2017-01-01 00:06:39 500:000:000 7                         \n2017-01-01 00:07:29 500:000:000 8                         \n2017-01-01 00:08:19 500:000:000 9                         \n[10] row(s) selected.\n \nMach> SELECT /*+ SCAN_BACKWARD(mytbl) */ _ARRIVAL_TIME, VALUE FROM mytbl LIMIT 10;\n_ARRIVAL_TIME                   VALUE                   \n----------------------------------------------------------------\n2017-02-27 20:53:19 500:000:000 9                         \n2017-02-27 20:52:29 500:000:000 8                         \n2017-02-27 20:51:39 500:000:000 7                         \n2017-02-27 20:50:49 500:000:000 6                         \n2017-02-27 20:49:59 500:000:000 5                         \n2017-02-27 20:49:09 500:000:000 4                         \n2017-02-27 20:48:19 500:000:000 3                         \n2017-02-27 20:47:29 500:000:000 2                         \n2017-02-27 20:46:39 500:000:000 1                         \n2017-02-27 20:45:49 500:000:000 0                         \n[10] row(s) selected.\n \nMach>\n```"
					}
					
				
		
				
					,
					
					"feature-table-log-select-select-time-data-html": {
						"id": "feature-table-log-select-select-time-data-html",
						"title": "시계열 데이터 조회",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/select/select-time-data.html",
						"content": "SELECT문의 DURATION절은 검색 대상 시간 조건절을 정의한다. DURATION절을 이용하는 가장 큰 이유는 검색 대상을 줄여서 대량의 데이터를 검색하더라도 성능을 향상시키기 위함이다.\n\n마크베이스는 입력 시간을 기준으로 데이터를 파티션하여 저장하므로, 시간 조건으로 데이터를 쉽게 검색하도록 하였다. 입력 시간은 사용자가 정의한 칼럼이 아니라 '_ARRIVAL_TIME'이라는 자동 생성 칼럼에 저장된다. 그러므로 마크베이스를 가장 효율적으로 사용하기 위해서는 추가로 시간 칼럼을 지정하지 않고 내장된 '_ARRIVAL_TIME' 칼럼을 이용하는 것이 좋다.\n\n마크베이스는 입력된 순서의 역순으로 데이터를 출력한다. 즉, 최근 데이터가 먼저, 오래전 데이터가 나중에 출력되는 것이다. 일반적으로 시계열 데이터를 검색할 때, 최근 데이터가 더 중요하고 먼저 얻어야 하는 경우가 많으므로 이와 같은 순서로 출력한다. 또한 모든 DURATION조건절에 의한 데이터 출력은 최근에서 과거 순으로 출력된다. 과거에서 최근 순으로 출력하려면 AFTER 절을 이용하여야 한다. 문법은 다음과 같다.\n\n# 목차\n* [문법](#문법)\n* [DURATION...BEFORE](#durationbefore)\n  * [절대 시간 값 기준 검색](#절대-시간-값-기준-검색)\n  * [상대 시간 값 기준 검색](#상대-시간-값-기준-검색)\n* [DURATION...AFTER](#durationafter)\n* [DURATION...FROM/TO](#durationfromto)\n\n## 문법\n\n```sql\nDURATION    time_expression [BEFORE time_expression | TO_DATE(time) ];\nDURATION    time_expression [AFTER TO_DATE(time)]; \ntime_expression\n -  ALL\n -  n   year\n -  n   month\n -  n   week\n -  n   day\n -  n   hour   \n -  n   minute \n -  n   second\n ```\n\n## DURATION...BEFORE\n\n앞서 말한 것 처럼, BEFORE를 명시적 이용하거나 정의되지 않은 경우(자동으로 BEFORE를 적용)에는 최근에서 과거 순으로 데이터를 출력한다.\n\n절대 시간 값 또는 상대 시간 값을 기준으로 데이터를 조회할 수 있다.\n\n### 절대 시간 값 기준 검색\n\n```sql\nMach> CREATE TABLE time_table (id INTEGER);\nCreated successfully.\n \nMach> INSERT INTO time_table(_arrival_time, id) VALUES(TO_DATE('2014-6-12 10:00:00', 'YYYY-MM-DD HH24:MI:SS'), 1);\n1 row(s) inserted.\n \nMach> INSERT INTO time_table(_arrival_time, id) VALUES(TO_DATE('2014-6-12 11:00:00', 'YYYY-MM-DD HH24:MI:SS'), 2);\n1 row(s) inserted.\n \nMach> INSERT INTO time_table(_arrival_time, id) VALUES(TO_DATE('2014-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS'), 3);\n1 row(s) inserted.\n \nMach> INSERT INTO time_table(_arrival_time, id) VALUES(TO_DATE('2014-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS'), 4);\n1 row(s) inserted.\n \nMach> INSERT INTO time_table VALUES(5);\n1 row(s) inserted.\n \nMach> SELECT _arrival_time, * FROM time_table DURATION 1 MINUTE;\n_arrival_time                   ID\n-----------------------------------------------\n2017-02-16 12:17:01 880:937:028 5\n[1] row(s) selected.\n \nMach> SELECT _arrival_time, * FROM time_table DURATION 1 DAY BEFORE TO_DATE('2014-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2014-06-12 12:00:00 000:000:000 3\n2014-06-12 11:00:00 000:000:000 2\n2014-06-12 10:00:00 000:000:000 1\n[3] row(s) selected.\n```\n\n### 상대 시간 값 기준 검색\n\n상대 시간 값을 기준으로 한 검색은, 바로 현재를 기준으로 한 검색으로 볼 수 있다.\n\n```sql\nMach> CREATE TABLE relative_table(id INTEGER);\nCreated successfully.\n \nMach> INSERT INTO relative_table values(1);\n1 row(s) inserted.\n \n------ WAIT for 30 SECONDS before the second value ------\n \nMach> INSERT INTO relative_table values(2);\n1 row(s) inserted.\n \nMach> SELECT _arrival_time, * FROM relative_table;\n_arrival_time                   ID\n-----------------------------------------------\n2017-02-16 12:35:34 476:055:014 2\n2017-02-16 12:35:04 430:802:356 1\n[2] row(s) selected.\n \nMach> SELECT id FROM relative_table DURATION 30 second ;\nid\n--------------\n2\n[1] row(s) selected.\n \nMach> SELECT id FROM relative_table DURATION 60 second ;\nid\n--------------\n2\n1\n[2] row(s) selected.\n \nMach> SELECT id FROM relative_table DURATION 30 second BEFORE 30 second;\nid\n--------------\n1\n[1] row(s) selected.\n```\n\n### DURATION...AFTER\n\nAFTER를 적용한 경우, 데이터는 과거에서 최근순으로 출력된다.\n\nBEFORE명령은, 최근에서 과거로 출력하는것에 비교하면 데이터가 입력 시간을 기준으로 자동으로 역순으로 출력된다.\n\n```sql\nMach> CREATE TABLE after_table (id INTEGER);\nCreated successfully.\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 10:00:00', 'YYYY-MM-DD HH24:MI:SS'), 1);\n1 row(s) inserted.\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 11:00:00', 'YYYY-MM-DD HH24:MI:SS'), 2);\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS'), 3);\n1 row(s) inserted.\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS'), 4);\n1 row(s) inserted.\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 14:00:00', 'YYYY-MM-DD HH24:MI:SS'), 5);\n1 row(s) inserted.\n \nMach> select _arrival_time, * from after_table duration ALL after TO_DATE('2016-6-12 11:00:00', 'YYYY-MM-DD HH24:MI:SS');\n \n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 11:00:00 000:000:000 2\n2016-06-12 12:00:00 000:000:000 3\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 14:00:00 000:000:000 5\n[4] row(s) selected.\n \nMach> select _arrival_time, * from after_table duration ALL before TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 12:00:00 000:000:000 3\n2016-06-12 11:00:00 000:000:000 2\n2016-06-12 10:00:00 000:000:000 1\n[4] row(s) selected.\n```\n\n### DURATION...FROM/TO\n사용자가 두개의 절대 시간을 기준으로 데이터를 검색하려고 할 때, \"DURATION FROM A TO B\" 형태의 조건절을 이용한다.\n\nA와 B는 절대적 시간이며 TO_DATE함수를 이용하여 표현된다. A와 B는 사용자의 의도에 따라 다르게 설정될 수 있다. 예를 들어,\n\n* A가 B보다 이후의 시간일 경우 BEFORE를 사용한 것과 같이, 검색 방향은 최근에서 과거 순으로 데이터를 출력한다.\n* B가 A보다 과거인 경우 AFTER를 사용한 것과 같이, 검색 방향은 과거에서 최근 순으로 데이터를 출력한다.\n아래의 예제를 보면 데이터의 출력 방식을 쉽게 이해할 수 있다.\n\n```sql\nMach> CREATE TABLE from_table (id INTEGER);\nCreated successfully.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 10:00:00', 'YYYY-MM-DD HH24:MI:SS'), 1);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 11:00:00', 'YYYY-MM-DD HH24:MI:SS'), 2);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS'), 3);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS'), 4);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 14:00:00', 'YYYY-MM-DD HH24:MI:SS'), 5);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 15:00:00', 'YYYY-MM-DD HH24:MI:SS'), 6);\n1 row(s) inserted.\n \nMach> SELECT _arrival_time, * FROM from_table DURATION FROM TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 14:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 12:00:00 000:000:000 3\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 14:00:00 000:000:000 5\n[3] row(s) selected.\n \nMach> SELECT _arrival_time, * FROM from_table limit 2 DURATION FROM TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 15:00:00',\n'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 12:00:00 000:000:000 3\n2016-06-12 13:00:00 000:000:000 4\n[2] row(s) selected.\n \nMach> SELECT _arrival_time, * FROM from_table DURATION FROM TO_DATE('2016-6-12 15:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 15:00:00 000:000:000 6\n2016-06-12 14:00:00 000:000:000 5\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 12:00:00 000:000:000 3\n[4] row(s) selected.\n \nMach> SELECT _arrival_time, * FROM from_table LIMIT 2 duration FROM TO_DATE('2016-6-12 15:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 12:00:00',\n'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 15:00:00 000:000:000 6\n2016-06-12 14:00:00 000:000:000 5\n[2] row(s) selected.\n \nMach> SELECT _arrival_time, * from from_table duration FROM TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 13:00:00 000:000:000 4\n[1] row(s) selected.\n \nMach> SELECT _arrival_time, * from from_table duration FROM TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 20:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 14:00:00 000:000:000 5\n2016-06-12 15:00:00 000:000:000 6\n[3] row(s) selected.\n \nMach> SELECT _arrival_time, * from from_table duration FROM TO_DATE('2016-6-12 20:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 15:00:00 000:000:000 6\n2016-06-12 14:00:00 000:000:000 5\n2016-06-12 13:00:00 000:000:000 4\n[3] row(s) selected.\n```"
					}
					
				
		
				
					,
					
					"sql-ref-select-html": {
						"id": "sql-ref-select-html",
						"title": "SELECT",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/select.html",
						"content": "SELECT는 마크베이스에서 각종 테이블로부터 데이터를 찾거나 필터링 및 조작하는 데 사용되는 구문이다.\n\n# 목차\n\n* [SELECT Syntax](#select-syntax)\n* [SET OPERATOR](#set-operator)\n* [TARGET LIST](#target-list)\n  * [CASE 구문](#case-구문)\n* [FROM](#from)\n  * [SUBQUERY(INLINE VIEW) 사용](#subqueryinline-view-사용)\n  * [INNER JOIN 및 OUTER JOIN](#inner-join-및-outer-join)\n  * [PIVOT](#pivot)\n* [WHERE](#where)\n  * [SUBQUERY의 사용](#subquery의-사용)\n  * [SEARCH 구문](#search-구문)\n  * [ESEARCH 구문](#esearch-구문)\n  * [NOT SEARCH 구문](#not-search-구문)\n  * [REGEXP 구문](#regexp-구문)\n  * [IN 구문](#in-구문)\n  * [IN 구문과 SUBQUERY의 사용](#in-구문과-subquery의-사용)\n  * [BETWEEN 구문](#between-구문)\n  * [RANGE 구문](#range-구문)\n* [GROUP BY / HAVING](#group-by--having)\n* [ORDER BY](#order-by)\n* [SERIES BY](#series-by)\n* [LIMIT](#limit)\n* [DURATION](#duration)\n* [SAVE DATA](#save-data)\n\n## SELECT Syntax\n\n```sql\nselect_stmt UNION ALL select_stmt\n```\n```sql\nSELECT target_list FROM TableList WHERE Condition GROUP BY Expr ORDER BY Expr [Desc] HAVING Expr SERIES BY Expr LIMIT N[,N] DURATION TimeExpr;\n```\n\n## SET OPERATOR\n\n여러 개의 Select문의 결과를 하나의 질의 결과로 전달받을 경우에 사용한다. 마크베이스는 UNION ALL 집합 연산자만을 지원한다. 집합 연산자는 좌우에 기술된 Select문이 (1) 같거나 호환가능한 타입이며 (2) 질의 결과값의 개수가 동일한 경우에만 실행이 가능하며 두 조건 중 하나라도 일치하지 않은 경우에는 오류로 처리된다.\n\n다음의 기준으로 데이터 타입 변환이나 호환성 검증을 수행한다.\n* 부호 있는 정수형 타입과 부호 없는 정수형 타입은 호환이 되지 않는다.\n* 정수형 타입은 실수형 타입과 호환이 되며 질의 결과는 실수형 타입으로 변환되어 반환된다.\n* 문자형 타입은 길이가 달라도 호환이 된다.\n* IPv6 타입과 IPv4 타입은 호환이 되지 않는다.\n* 두 개의 SELECT 문 중 항상 왼쪽 질의의 컬럼명이 사용된다.\n\n실행 예제\n```sql\nSELECT i1, i2 FROM table_1\nUNION ALL\nSELECT c1, c2 FROM table_2\n```\n\n## TARGET LIST\n\nSelect 문이 대상으로 하는 컬럼 또는 Subquery 의 리스트이다.\n\nTarget list에 사용된 Subquery는 WHERE 조건절에서 사용되는 Subquery와 같이 두 개 이상의 값을 갖거나 두 개 이상의 결과 컬럼을 갖는 경우에는 오류로 처리된다.\n\n```sql\nSELECT i1, i2 ...\nSELECT i1 (Select avg(c1) FROM t1), i2 ...\n```\n\n### CASE 구문\n\n```sql\nCASE  [else_clause] END\n \nsimple_case_expression ::=\n    expr WHEN comparison_expr THEN return_expr\n        [WHEN comparison_expr THEN return_expr ...]\n \nsearched_case_expression ::=\n    WHEN condtion_expr THEN return EXPR [WHEN condtion_expr THEN return EXPR ...]\n \nelse_clause ::=\n    ELSE else_value_expr\n```\n\n일반적인 프로그램 언어의 IF... THEN... ELSE블록을 지원하는 표현식이다. simple_case_expression은 하나의 컬럼이나 표현식이 when 뒤에 오는 comparison_expr 값과 같은 경우 return_expr을 반환하는 형태로 수행되며 이 when ... then 절은 원하는 만큼 반복하여 기술할 수 있다.\n\nsearched_case_expression은 CASE 이후에 표현식을 지정하지 않고 when절에 비교연산자를 포함한 조건절을 기술한다. 각 비교연산의 결과가 참이면 then 절의 값을 반환한다. else 절은 when 절들의 값이 만족하지 않을 경우(expression 결과가 NULL인 경우에도) else_value를 반환한다.\n\n```sql\nselect * from t1;\nI1          I2         \n---------------------------\n2           2          \n1           1          \n[2] row(s) selected.\n \nselect case i1 when 1 then 100 end from t1;\ncase i1 when 1 then 100 end\n------------------------------\nNULL       \n100        \n[2] row(s) selected.\n```\n\nsimple_case_expression의 예제에서 i1 컬럼의 값이 2인 경우에 해당하는 값이 없으면 NULL을 반환한다.\n\n```sql\nselect case when i1 > 0 then 100 when i1 > 1 then 200 end from t1;\ncase when i1 > 0 then 100 when i1 > 1 then 200 end\n------------------------------------------\n100        \n100        \n[2] row(s) selected.\n```\n\nsearched_case_expression에서 만족하는 첫 번째 조건절을 반환하므로 첫 번째 조건절의 반환값인 100이 반환되며 두 번째 조건절은 실행이 되지 않는다.\n\n## FROM\n\nFROM 절에는 테이블 이름이나 Inline view를 지정할 수 있다. 테이블 간의 Join을 수행하려면 테이블 혹은 Inline view를 쉼표(,)로 구분해서 나열한다.\n\n```sql\nFROM table_name\n```\n\ntable_name로 지정한 테이블 내의 데이터를 검색한다.\n\n#### SUBQUERY(INLINE VIEW) 사용\n\n```sql\nFROM (Select statement)\n```\n\n괄호로 둘려쳐진 subquery의 내용에 대하여 데이터를 검색한다.\n\n* 마크베이스 서버는 correlated subquery를 지원하지 않으므로 outer query에서 subquery 내의 column을 참조할 수 없다.\n\n#### JOIN(INNER JOIN)\n\n```sql\nFROM TABLE_1, TABLE_2\n```\n\n두 개의 테이블 table_1 과 table_2를 JOIN한다. INNER JOIN은 테이블이 3개 이상 나열될 때에도 사용이 가능하며 WHERE 절에 검색 조건절과 JOIN 조건절을 모두 기술하여 사용한다.\n\n```sql\nSELECT t1.i1, t2.i1 FROM t1, t2 WHERE t1.i1 = t2.i1 AND t1.i1 > 1 AND t2.i2 = 3;\n```\n\n#### INNER JOIN 및 OUTER JOIN\n\nANSI 스타일의 INNER JOIN, LEFT OUTER JOIN, RIGHT OUTER JOIN을 지원한다. FULL OUTER JOIN은 지원하지 않는다.\n\n```sql\nFROM TABLE_1 [INNER|LEFT OUTER|RIGHT OUTER] JOIN TABLE_2 ON expression\n```\n\nANSI 스타일 JOIN절의 ON절에는 JOIN에서 수행하는 조건절을 사용한다. OUTER JOIN 질의에서 where절에 Inner table(ON 절의 조건을 만족하지 않으면 NULL이 채워지는 테이블)에 대한 조건절이 있는 경우, 해당 질의는 INNER JOIN으로 변환된다.\n\n```sql\nSELECT t1.i1 t2.i1 FROM t1 LEFT OUTER JOIN t2 ON (t1.i1 = t2.i1) WHERE t2.i2 = 1;\n```\n\n위 질의는 WHERE 절의 조건 t2.i2 = 1에 의하여 INNER JOIN으로 변환된다.\n\n#### PIVOT\n\n* PIVOT 구문은 마크베이스 5.5.6 버전부터 지원한다.\n\n**pivot_clause:**\n\n![pivot](/kr/sql-ref/select_image/pivot_clause.png)\n\n\nPIVOT 구문은 ROW로 출력되는 GROUP BY에 대한 집계 결과를 컬럼으로 재배열하여 보여준다.\n\nInline view와 함께 사용되며 다음과 같이 수행된다.\n\n* Inline view의 결과 컬럼 중 PIVOT 절에 사용되지 않은 컬럼으로 GROUP BY를 수행한 후 PIVOT IN 절에 나열된 값 별로 집계함수를 수행한다.\n* 결과로 나온 grouping 컬럼과 집계 결과를 회전하여 컬럼으로 보여준다.\n\n예) 여러 센서로부터 수집된 데이터에서 각 device 별로 value 값을 집계해서 출력하라.\n\nCASE 구문을 통해 수행해야하는 질의를 PIVOT 구문을 통해 간단히 표현할 수 있다.\n\n```sql\n-- w/o PIVOT\nSELECT * FROM (\n    SELECT\n             regtime,\n             SUM(CASE WHEN tagid = 'FRONT_AXIS_TORQUE' THEN dvalue ELSE 0 END)  AS front_axis_torque,\n             SUM(CASE WHEN tagid = 'REAR_AXIS_TORQUE' THEN dvalue ELSE 0 END)  AS rear_axis_torque,\n             SUM(CASE WHEN tagid = 'HOIST_AXIS_TORQUE' THEN dvalue ELSE 0 END)  AS hoist_axis_torque,\n             SUM(CASE WHEN tagid = 'SLIDE_AXIS_TORQUE' THEN dvalue ELSE 0 END)  AS slide_axis_torque\n    FROM     result_d\n    WHERE    regtime BETWEEN TO_DATE('2018-12-07 00:00:00') AND TO_DATE('2018-12-08 05:00:00')\n    GROUP BY regtime\n) WHERE front_axis_torque >= 40 AND rear_axis_torque >= 20;\n  \n-- w/ PIVOT\nSELECT * FROM (\n    SELECT regtime, tagid, dvalue FROM result_d\n    WHERE  regtime BETWEEN TO_DATE('2018-12-07 00:00:00') AND TO_DATE('2018-12-08 05:00:00')\n) PIVOT (SUM(dvalue) FOR tagid IN ('FRONT_AXIS_TORQUE', 'REAR_AXIS_TORQUE', 'HOIST_AXIS_TORQUE', 'SLIDE_AXIS_TORQUE'))\nWHERE front_axis_torque >= 40 AND rear_axis_torque >= 20;\n \n-- Result\nregtime                         'FRONT_AXIS_TORQUE'         'REAR_AXIS_TORQUE'          'HOIST_AXIS_TORQUE'         'SLIDE_AXIS_TORQUE'       \n------------------------------------------------------------------------------------------------------------------------------------------------------\n2018-12-07 16:42:29 840:000:000 12158                       7244                        NULL                        NULL                      \n2018-12-07 14:56:26 220:000:000 3308                        663                         NULL                        NULL                      \n2018-12-07 12:20:13 844:000:000 3804                        113                         NULL                        NULL                      \n2018-12-07 11:10:01 957:000:000 8729                        5384                        NULL                        NULL                      \n2018-12-07 17:46:57 812:000:000 7500                        4559                        NULL                        NULL                      \n2018-12-07 14:30:06 138:000:000 5080                        6817                        NULL                        -429                      \n2018-12-07 13:09:20 464:000:000 5233                        1869                        -7253                       NULL                      \n2018-12-07 15:43:03 539:000:000 7491                        4453                        NULL                        NULL\n...\n```\n\n## WHERE\n\n#### SUBQUERY의 사용\n\n조건절에 대해서 subquery의 사용이 가능하다. IN 구문을 제외한 조건절에서 subquery가 두 개 이상의 레코드를 리턴하거나, subquery의 결과 컬럼이 두 개 이상인 경우는 지원하지 않는다.\n\n```sql\nWHERE i1 = (SELECT MAX(c2) FROM T1)\n```\n\nsubquery를 조건연산자 오른쪽에 괄호를 둘러쳐서 사용한다.\n\n* 마크베이스 서버는 correlated subquery를 지원하지 않으므로 outer query에서 subquery 내의 column을 \n참조할 수 없다.\n\n#### SEARCH 구문\n\n일반 데이터베이스와의 문법이 동일하다. 단, 반드시 keyword index를 등록해야 하며, 텍스트 검색을 위한 연산자 키워드인 \"SEARCH\"를 추가하여, 부가적인 검색 연산이 가능하다.\n\n```sql\n-- drop table realdual;\ncreate table realdual (id1 integer, id2 varchar(20), id3 varchar(20));\n \ncreate keyword index idx1 on realdual (id2);\ncreate keyword index idx2 on realdual (id3);\n \ninsert into realdual values(1, 'time time2', 'series series2');\n \nselect * from realdual;\n \nselect * from realdual where id2 search 'time';\nselect * from realdual where id3 search 'series' ;\nselect * from realdual where id2 search 'time' and id3 search 'series';\n```\n\n수행 결과는 다음과 같다.\n\n```sql\nMach> create table realdual (id1 integer, id2 varchar(20), id3 varchar(20));\nCreated successfully.\n \nMach> create keyword index idx1 on realdual (id2);\nCreated successfully.\n \nMach> create keyword index idx2 on realdual (id3);\nCreated successfully.\n \nMach> insert into realdual values(1, 'time time2', 'series series2');\n1 row(s) inserted.\n \nMach> select * from realdual;\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time time2            series series2 \n[1] row(s) selected.\n \nMach> select * from realdual where id2 search 'time';\nID1         ID2                   ID3\n------------------------------------------------------------\n1           time time2            series series2\n[1] row(s) selected.\n \nMach> select * from realdual where id3 search 'series';\nID1         ID2                   ID3\n------------------------------------------------------------\n1           time time2            series series2\n[1] row(s) selected.\n \nMach> select * from realdual where id2 search 'time' and id3 search 'series';\nID1         ID2                   ID3\n------------------------------------------------------------\n1           time time2            series series2\n[1] row(s) selected.\n```\n\n#### ESEARCH 구문\n\nESEARCH 구문은 ASCII 문자 텍스트에 대한 확장 검색을 가능하게 해주는 검색 키워드이다. 이러한 확장을 위해 % 문자를 이용하여 원하는 패턴의 검색을 수행한다. 이 Like 연산에서 앞에 %가 오는 경우 모든 레코드를 검사해야 하지만, ESEARCH의 장점은 이 경우에도 빠르게 해당 단어를 찾을 수 있다는 데 있다. 이 기능은 영문 문자열(에러 문자열 혹은 코드)의 일부를 찾을 때 매우 유용하게 사용할 수 있다.\n\n```sql\n\n예제\n \nselect id2 from realdual where id2 esearch 'bbb%';\nid2\n--------------------------------------------\nbbb ccc1\naaa bbb1\n \n[2] row(s) selected.\n \n검색 pattern 'bbb%'에 의하여 bbb1도 검색 결과에 포함된다.\n \n \nselect id3 from realdual where id3 esearch '%cd%';\nid3\n--------------------------------------------\ncdf def1\nbcd/cdf1ad\nabc, bcd1\n[3] row(s) selected.\n \n% 문자는 검색 pattern의 처음, 끝 뿐만 아니라 가운데에 있어도 동작한다.\n \nselect id3 from realdual where id3 esearch '%cd%';\nid3\n--------------------------------------------\ncdf def1\nbcd/cdf1ad\nabc, bcd1\n[3] row(s) selected.\n```\n\n#### NOT SEARCH 구문\n\nNOT SEARCH는 SEARCH구문에서 검색되는 조건 이외의 레코드들에 대해서 참을 리턴하는 구문이다.\n\nNOT ESEARCH는 사용할 수 없다.\n\n```sql\ncreate table t1 (id integer, i2 varchar(10));\ncreate keyword index t1_i2 on t1(i2);\ninsert into t1 values (1, 'aaaa');\ninsert into t1 values (2, 'bbbb');\n \nselect id from t1 where i2 not search 'aaaa';\n \nid\n--------------------------------------------\n2\n[1] row(s) selected.\n```\n\n#### REGEXP 구문\n\nREGEXP 구문은 정규표현식을 사용하여 데이터에 대한 검색을 수행하는데 사용된다. 일반적으로 특정 컬럼의 패턴을 정규표현식을 사용하여 데이터를 필터링하게 된다.\n\n한가지 주의할 점은 REGEXP 구문을 사용할 경우 인덱스를 활용할 수 없기 때문에 전체 검색 범위를 줄이기 위해 반드시 다른 컬럼에 대한 인덱스 조건을 넣어서 전체적인 검색 비용을 낮춰야 한다.\n\n특정 패턴을 검사하고자 할 때에는 SEARCH 혹은 ESEARCH를 통해 인덱스를 활용하도록 하고, 이를 통해 전체적인 데이터 건수가 작아진 상태에서 다시 REGEXP를 이용하는 것이 시스템 전체 효율 향상에 도움이 된다\n\n```sql\nMach>\ncreate table realdual (id1 integer, id2 varchar(20), id3 varchar(20));\ncreate table dual (id integer);\ninsert into dual values(1);\ninsert into realdual values(1, 'time1', 'series1 series21');\ninsert into realdual values(1, 'time2', 'series2 series22');\ninsert into realdual values(1, 'time3', 'series3 series32');\n \n \nMach> select * from realdual where id2 REGEXP 'time' ;\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time3                 series3 series32\n1           time2                 series2 series22\n1           time1                 series1 series21\n[3] row(s) selected.\n \nMach> select * from realdual where id2 REGEXP 'time[12]' ;\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time2                 series2 series22\n1           time1                 series1 series21\n[2] row(s) selected.\n \nMach> select * from realdual where id2 REGEXP 'time[13]' ;\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time3                 series3 series32\n1           time1                 series1 series21\n[2] row(s) selected.\n \nMach> select * from realdual where id2 regexp 'time[13]' and id3 regexp 'series[12]';\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time1                 series1 series21 \n[1] row(s) selected.\n \nMach> select * from realdual where id2 NOT REGEXP 'time[12]';\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time3                 series3 series32\n[1] row(s) selected.\n \nMach> SELECT 'abcde' REGEXP 'a[bcd]{1,10}e' from dual;\n'abcde' REGEXP 'a[bcd]{1,10}e'\n---------------------------------\n1          \n[1] row(s) selected.\n```\n\n#### IN 구문\n\n```sql\ncolumn_name IN (value1, value2,...)\n```\n\nIN 구문은 뒤의 value 리스트에서 만족할 경우 TRUE를 리턴한다. OR로 연결된 구문과 동일하다.\n\n#### IN 구문과 SUBQUERY의 사용\n\n조건절의 IN 구문의 오른쪽에 subquery를 사용할 수 있다. 단, IN 조건절의 왼쪽에는 컬럼 두 개 이상의 컬럼을 지정하면 오류로 처리하고 오른쪽의 subquery에서 리턴되는 결과 집합이 왼쪽 컬럼값에 존재하는지를 검사한다.\n\n```sql\nWHERE i1 IN (Select c1 from ...)\n```\n\n* 마크베이스 서버는 correlated subquery를 지원하지 않으므로 outer query에서 subquery 내의 column을 참조할 수 없다.\n\n#### BETWEEN 구문\n\n```sql\ncolumn_name BETWEEN value1 AND value2\n```\n\nBETWEEN 구문은 column의 값이 value1과 value2 범위에 있을 경우, TRUE를 리턴한다.\n\n#### RANGE 구문\n\n```sql\ncolumn_name RANGE duration_spec;\n\n-- duration_spec : integer (YEAR | WEEK | HOUR | MINUTE | SECOND);\n```\n\n지정된 컬럼에 대해 시간 조건절을 쉽게 지정하는 Range 연산자를 제공한다. Range 연산자는 (BEFORE 키워드로 지정하는 것처럼) 특정 시점을 지정하는 게 아니라 현재 시점부터의 시간 범위를 연산의 대상 조건으로 지정한다. 이 연산자를 사용하면 손쉽게 원하는 시간 범위 내의 결과 레코드들을 검색할 수 있다.\n\n```sql\nselect * from test where id  ]\n \nselect id1, avg(id2) from exptab where id2 group by id1 order by id1;\nid1 컬럼을 기준으로 id2의 평균값을 구한다.\n```\n\n## ORDER BY\nORDER BY 절은 질의 결과를 오름차순 또는 내림차순으로 정렬하며, ASC 또는 DESC와 같은 정렬 옵션을 명시하지 않으면 디폴트로 오름차순으로 정렬한다. ORDER BY 절을 지정하지 않으면, 조회되는 레코드의 순서는 질의에 따라 다르다.\n\n```sql\nSELECT ...\nORDER BY {col_name | expr} [ASC | DESC]\n \nselect id1, avg(id2) from exptab where id2 group by id1 order by id1;\nid1 컬럼을 기준으로 id2의 평균값을 구한다.\n```\n\n## SERIES BY\n\nSERIES BY 절은 정렬된 결과집합을 SERIES BY 조건절을 만족하는 연속된 결과값들로 추출한다. 만약 ORDER BY 절이 지정되지 않은 경우에는 _ARRIVAL_TIME 컬럼값을 이용하여 정렬된 결과를 생성하므로, _ARRIVAL_TIME 컬럼이 없는 휘발성 테이블이나 참조 테이블에 대한 질의나, GROUP BY 절을 이용하는 경우에는 반드시 ORDER BY 절을 이용해야 한다.\n\n조건절을 만족하는 결과값들은 같은 SERIESNUM() 함수의 반환값을 갖게 된다.\n\n```sql\n예를 들어 다음의 데이터에 대해서\nCREATE TABLE T1 (C1 INTEGER, C2 INTEGER);\nINSERT INTO T1 VALUES (0, 1);\n \nINSERT INTO T1 VALUES (1, 2);\n \nINSERT INTO T1 VALUES (2, 3);\n \nINSERT INTO T1 VALUES (3, 2);\n \nINSERT INTO T1 VALUES (4, 1);\n \nINSERT INTO T1 VALUES (5, 2);\n \nINSERT INTO T1 VALUES (6, 3);\n \nINSERT INTO T1 VALUES (7, 1);\n \n \n아래의 질의는 다음의 결과를 출력한다.\nSELECT C1,C2 FROM T1 ORDER BY C1 SERIES BY C2>1;\nC1          C2         \n---------------------------\n1           2          \n2           3          \n3           2          \n5           2          \n6           3   \n \nC2 컬럼의 값이 1 보다 큰 C1의 RANGE값을 알고 싶은 경우, SERIESNUM 함수로 각 레코드가 어느 그룹에 포함되는지를 출력하여 RANGE를 결정할 수 있다.\n```\n\n## LIMIT\n\nLIMIT 절은 출력되는 레코드의 개수를 제한할 때 사용한다. 결과 집합의 특정 행부터 마지막 행까지 출력하기 위해 정수를 지정할 수 있다\n\n```sql\nLIMIT [offset,] row_count\n \nselect id1, avg(id2) from exptab where id2 group by id1 order by id1 LIMIT 10;\n```\n\nDURATION\nDURATION은 _arrival_time을 기준으로 데이터 검색 범위를 손쉽게 결정하도록 해 주는 키워드이다. BEFORE 구문과 함께 사용되어 특정 시점의 특정 데이터 범위를 설정하게 해 준다. 이 DURATION을 잘 활용하면 검색 성능을 현격하게 올림과 동시에 시스템 부하를 획기적으로 낮출 수 있다. 더 자세한 활용 용도는 다음을 참조한다.\n\n```sql\nDURATION Number TimeSpec [BEFORE/AFTER Number TimeSpec]\nTimeSpec : YEAR | MONTH | WEEK |  DAY | HOUR | MINUTE | SECOND\n```\n\n```sql\ncreate table t8(i1 integer);\ninsert into t8 values(1);\ninsert into t8 values(2);\n \nselect i1 from t8;\n \n# BEFORE 절 없이\nselect i1 from t8 duration 2 second;\nselect i1 from t8 duration 1 minute;\nselect i1 from t8 duration 1 hour;\nselect i1 from t8 duration 1 day;\nselect i1 from t8 duration 1 week;\nselect i1 from t8 duration 1 month;\nselect i1 from t8 duration 1 year;\n \n# DURATION 구문 전체를 써서\nselect i1 from t8 duration 1 second before 1 day;\nselect i1 from t8 duration 1 minute before 1 day;\nselect i1 from t8 duration 1 hour before 1 day;\nselect i1 from t8 duration 1 day before 1 day;\nselect i1 from t8 duration 1 week before 1 day;\nselect i1 from t8 duration 1 month before 1 day;\nselect i1 from t8 duration 1 year before 1 day;\n```\n\n수행 결과는 다음과 같다.\n\n```sql\nMach> create table t8(i1 integer);\nCreated successfully.\n \nMach> insert into t8 values(1);\n1 row(s) inserted.\n \nMach> insert into t8 values(2);\n1 row(s) inserted.\n \nMach> select i1 from t8;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \n# BEFORE 절 없이\nMach> select i1 from t8 duration 2 second;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 minute;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 hour;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 day;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 week;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 month;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 year;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \n# DURATION 구문 전체를 써서\nMach> select i1 from t8 duration 1 second before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 minute before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 hour before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 day before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 week before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 month before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 year before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n```\n\n## SAVE DATA\n\n질의의 결과를 CSV 데이터 파일로 바로 저장한다.\n\n```sql\nSAVE DATA INTO 'file_name.csv' [HEADER ON|OFF] [(FIELDS | COLUMNS) [TERMINATED BY 'char'] [ENCLOSED BY 'char']] [ENCODED BY coding_name] AS select query;\n```\n\n옵션의 설명은 다음과 같다.\n\n|옵션|설명|\n|--|--|\n|HEADER (ON\\|OFF)|생성할 csv 파일의 첫번째 라인에 컬럼명을 입력할지를 결정한다. 기본값은 OFF이다.|\n|(FIELDS\\|COLUMNS) TERMINATED BY 'term_char'ENCLOSED BY 'escape_char'|생성할 csv 파일의 컬럼 구분자와 이스케이프 구분자를 지정한다.|\n|ENCODED BY coding_namecoding_name = ( UTF8, MS949, KSC5601, EUCJP, SHIFTJIS, BIG5, GB231280 )|출력 데이터 파일의 인코딩 포맷을 지정한다. 기본값은 UTF8이다.|\n\n```sql\nSAVE DATA INTO '/tmp/aaa.csv' AS select * from t1;\n-- select 문을 실행하여 그 결과를 '/tmp/aaa.csv' 파일에 csv 포멧으로 기록한다.\n  \nSAVE DATA INTO '/tmp/ccc.csv' HEADER ON FIELDS TERMINATED BY ';' ENCLOSED BY '\\'' ENCODED BY MS949 AS select * from t1 where i1 > 100;\n-- select 문을 실행하여 그 결과를 /tmp/ccc.csv파일에 기록한다. 필드 구분자와 이스케이프 구분자를 각각 지정하고 저장되는 데이터의 인코딩은 MS949로 설정한다.\n```"
					}
					
				
		
				
					,
					
					"feature-table-volatile-select-html": {
						"id": "feature-table-volatile-select-html",
						"title": "휘발성 테이블의 추출",
						"version": "all",
						"categories": "",
						"url": " /feature-table/volatile/select.html",
						"content": "### 데이터 조회\n\n데이터 조회는 다른 테이블 유형과 마찬가지로, 아래와 같이 수행할 수 있다.\n\n```sql\nMach> create volatile table vtable (id integer primary key, name varchar(20));\nCreated successfully.\nMach> insert into vtable values(1, 'west device');\n1 row(s) inserted.\nMach> insert into vtable values(2, 'east device');\n1 row(s) inserted.\nMach> insert into vtable values(3, 'north device');\n1 row(s) inserted.\nMach> insert into vtable values(4, 'south device');\n1 row(s) inserted.\nMach> select * from vtable;\nID          NAME                 \n-------------------------------------\n1           west device          \n2           east device          \n3           north device         \n4           south device         \n[4] row(s) selected.\nMach> select * from vtable where id = 1;\nID          NAME                 \n-------------------------------------\n1           west device          \n[1] row(s) selected.\nMach> select * from vtable where name like 'west%';\nID          NAME                 \n-------------------------------------\n1           west device          \n[1] row(s) selected.\n```"
					}
					
				
		
				
					,
					
					"feature-table-lookup-select-html": {
						"id": "feature-table-lookup-select-html",
						"title": "참조 데이터의 추출",
						"version": "all",
						"categories": "",
						"url": " /feature-table/lookup/select.html",
						"content": "SQL문을 이용하여 데이터를 추출하며 사용 방법은 휘발성 테이블과 동일하다."
					}
					
				
		
				
					,
					
					"feature-table-log-select-html": {
						"id": "feature-table-log-select-html",
						"title": "로그 데이터의 추출",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/select.html",
						"content": "마크베이스는 표준 ANSI SQL 구문을 이용하여 데이터를 추출할 수 있고, 시계열 데이터를 편리하게 다룰 수 있는 확장 구문을 제공한다.\n\n* [데이터 조회](/kr/feature-table/log/select/select-data.html)\n* [시계열 데이터 조회](/kr/feature-table/log/select/select-time-data.html)\n* [텍스트 검색](/kr/feature-table/log/select/text-search.html)\n* [단순 Join](/kr/feature-table/log/select/simple-join.html)\n* [네트워크 데이터 타입 / 연산자](/kr/feature-table/log/select/network-type.html)"
					}
					
				
		
				
					,
					
					"feature-table-tag-manipulate-select-html": {
						"id": "feature-table-tag-manipulate-select-html",
						"title": "태그 데이터의 추출",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/manipulate/select.html",
						"content": "마크베이스는 고속의 태그 데이터 추출 성능을 제공하며, 특히 특정 태그의 시간 범위에 대한 탁월한 성능을 제공한다.\n\n# 목차\n\n* [샘플 스키마](#샘플-스키마)\n* [전체 TAG 데이터 추출](#전체-tag-데이터-추출)\n* [임의 TAG명에 대한 데이터 추출](#임의-tag명에-대한-데이터-추출)\n* [시간 범위에 대한 쿼리](#시간-범위에-대한-쿼리)\n* [다중 태그에 대한 시간 범위 검색](#다중-태그에-대한-시간-범위-검색)\n* [특정 값 이상의 태그만 출력하기](#특정-값-이상의-태그만-출력하기)\n* [특정 태그 아이디별 통계 정보 출력하기](#특정-태그-아이디별-통계-정보-출력하기)\n* [RESTful API를 통한 추출](#restful-api를-통한-추출)\n* [힌트(hint)를 이용한 검색 방향 지정하기](#힌트hint를-이용한-검색-방향-지정하기)\n\n\n# 샘플 스키마\n\n이후의 샘플은 아래와 같이  TAG 테이블이 생성되고, 두개의 태그를 생성하였다.\n\n각 태그에 대해 각각 2018년 1월 1일부터 2월 10일까지의 데이터를 입력하였다.\n\n```sql\ncreate tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\n \ninsert into tag metadata values ('TAG_0001');\ninsert into tag metadata values ('TAG_0002');\n \ninsert into tag values('TAG_0001', '2018-01-01 01:00:00 000:000:000', 1);\ninsert into tag values('TAG_0001', '2018-01-02 02:00:00 000:000:000', 2);\ninsert into tag values('TAG_0001', '2018-01-03 03:00:00 000:000:000', 3);\ninsert into tag values('TAG_0001', '2018-01-04 04:00:00 000:000:000', 4);\ninsert into tag values('TAG_0001', '2018-01-05 05:00:00 000:000:000', 5);\ninsert into tag values('TAG_0001', '2018-01-06 06:00:00 000:000:000', 6);\ninsert into tag values('TAG_0001', '2018-01-07 07:00:00 000:000:000', 7);\ninsert into tag values('TAG_0001', '2018-01-08 08:00:00 000:000:000', 8);\ninsert into tag values('TAG_0001', '2018-01-09 09:00:00 000:000:000', 9);\ninsert into tag values('TAG_0001', '2018-01-10 10:00:00 000:000:000', 10);\n \ninsert into tag values('TAG_0002', '2018-02-01 01:00:00 000:000:000', 11);\ninsert into tag values('TAG_0002', '2018-02-02 02:00:00 000:000:000', 12);\ninsert into tag values('TAG_0002', '2018-02-03 03:00:00 000:000:000', 13);\ninsert into tag values('TAG_0002', '2018-02-04 04:00:00 000:000:000', 14);\ninsert into tag values('TAG_0002', '2018-02-05 05:00:00 000:000:000', 15);\ninsert into tag values('TAG_0002', '2018-02-06 06:00:00 000:000:000', 16);\ninsert into tag values('TAG_0002', '2018-02-07 07:00:00 000:000:000', 17);\ninsert into tag values('TAG_0002', '2018-02-08 08:00:00 000:000:000', 18);\ninsert into tag values('TAG_0002', '2018-02-09 09:00:00 000:000:000', 19);\ninsert into tag values('TAG_0002', '2018-02-10 10:00:00 000:000:000', 20);\n```\n\n# 전체 TAG 데이터 추출\n\n```sql\nMach> select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\nTAG_0001 2018-01-01 01:00:00 000:000:000 1\nTAG_0001 2018-01-02 02:00:00 000:000:000 2\nTAG_0001 2018-01-03 03:00:00 000:000:000 3\nTAG_0001 2018-01-04 04:00:00 000:000:000 4\nTAG_0001 2018-01-05 05:00:00 000:000:000 5\nTAG_0001 2018-01-06 06:00:00 000:000:000 6\nTAG_0001 2018-01-07 07:00:00 000:000:000 7\nTAG_0001 2018-01-08 08:00:00 000:000:000 8\nTAG_0001 2018-01-09 09:00:00 000:000:000 9\nTAG_0001 2018-01-10 10:00:00 000:000:000 10\nTAG_0002 2018-02-01 01:00:00 000:000:000 11\nTAG_0002 2018-02-02 02:00:00 000:000:000 12\nTAG_0002 2018-02-03 03:00:00 000:000:000 13\nTAG_0002 2018-02-04 04:00:00 000:000:000 14\nTAG_0002 2018-02-05 05:00:00 000:000:000 15\nTAG_0002 2018-02-06 06:00:00 000:000:000 16\nTAG_0002 2018-02-07 07:00:00 000:000:000 17\nTAG_0002 2018-02-08 08:00:00 000:000:000 18\nTAG_0002 2018-02-09 09:00:00 000:000:000 19\nTAG_0002 2018-02-10 10:00:00 000:000:000 20\n[20] row(s) selected.\n```\n\n위와 같이 특별한 조건이 없으면, 각 시간 순으로 정렬된 태그별로 데이터를 추출할 수 있다.\n\n# 임의 TAG명에 대한 데이터 추출\n\n아래는 TAG 이름이 TAG_0002 인 데이터를 출력하는 예제이다. WHERE 절에 주어진 name에 대한 조건을 설정한다.\n\n```sql\nMach> select * from tag where name='TAG_0002';\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0002              2018-02-01 01:00:00 000:000:000 11                         \nTAG_0002              2018-02-02 02:00:00 000:000:000 12                         \nTAG_0002              2018-02-03 03:00:00 000:000:000 13                         \nTAG_0002              2018-02-04 04:00:00 000:000:000 14                         \nTAG_0002              2018-02-05 05:00:00 000:000:000 15                         \nTAG_0002              2018-02-06 06:00:00 000:000:000 16                         \nTAG_0002              2018-02-07 07:00:00 000:000:000 17                         \nTAG_0002              2018-02-08 08:00:00 000:000:000 18                         \nTAG_0002              2018-02-09 09:00:00 000:000:000 19                         \nTAG_0002              2018-02-10 10:00:00 000:000:000 20                         \n[10] row(s) selected.\n```\n\n# 시간 범위에 대한 쿼리\n\n아래는 TAG_0002에 대한 시간 범위를 주고, 데이터를 받아오는 쿼리이다.\n\n> between 절을 활용해서 시간 범위를 주는 것이 일반적인 방법이다. 물론, time을  기호로 시간 범위를 입력해도 같은 결과를 얻을 수 있다.\n\n```sql\nMach> select * from tag where name = 'TAG_0002' and time between to_date('2018-02-01') and to_date('2018-02-05');\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0002              2018-02-01 01:00:00 000:000:000 11                         \nTAG_0002              2018-02-02 02:00:00 000:000:000 12                         \nTAG_0002              2018-02-03 03:00:00 000:000:000 13                         \nTAG_0002              2018-02-04 04:00:00 000:000:000 14                         \n[4] row(s) selected.\n \nMach> select * from tag where name = 'TAG_0002' and time > to_date('2018-02-01') and time  select * from tag where name in ('TAG_0002', 'TAG_0001') and time between to_date('2018-01-05') and to_date('2018-02-05');\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0001              2018-01-05 05:00:00 000:000:000 5                          \nTAG_0001              2018-01-06 06:00:00 000:000:000 6                          \nTAG_0001              2018-01-07 07:00:00 000:000:000 7                          \nTAG_0001              2018-01-08 08:00:00 000:000:000 8                          \nTAG_0001              2018-01-09 09:00:00 000:000:000 9                          \nTAG_0001              2018-01-10 10:00:00 000:000:000 10                         \nTAG_0002              2018-02-01 01:00:00 000:000:000 11                         \nTAG_0002              2018-02-02 02:00:00 000:000:000 12                         \nTAG_0002              2018-02-03 03:00:00 000:000:000 13                         \nTAG_0002              2018-02-04 04:00:00 000:000:000 14                         \n[10] row(s) selected.\n```\n\n# 특정 값 이상의 태그만 출력하기\n\n간단한 예제이긴 하지만, 태그 값에 대한 조건도 함께 아래와 같이 줄 수 있다.\n\nTAG_0002의 값 중에 12보다 크고, 15보다 작을 것들에 대해 필터링을 수행했다.\n\n```sql\nMach> select * from tag where name = 'TAG_0002' and value > 12 and value  CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nExecuted successfully.\n \nMach> DESC v$tag_stat;\n[ COLUMN ]                             \n----------------------------------------------------------------------------------------------------\nNAME                                                        NULL?    TYPE                LENGTH       \n----------------------------------------------------------------------------------------------------\nNAME                                                                 varchar             100                \nROW_COUNT                                                            ulong               20                 \nMIN_TIME                                                             datetime            31             \nMAX_TIME                                                             datetime            31             \nMIN_VALUE                                                            double              17                 \nMIN_VALUE_TIME                                                       datetime            31             \nMAX_VALUE                                                            double              17                 \nMAX_VALUE_TIME                                                       datetime            31             \nRECENT_ROW_TIME                                                      datetime            31\n```\n\n수집하는 통계 정보는 아래와 같다.\n\n| 컬럼이름|정보|\n|--|--|\nNAME|태그 아이디의 이름|\nROW_COUNT|Row 개수|\nMIN_TIME|해당 태그 아이디 Row 중 가장 작은 Basetime 컬럼 값|\nMAX_TIME|해당 태그 아이디 Row 중 가장 큰 Basetime 컬럼 값|\nMIN_VALUE|해당 태그 아이디 Row 중 가장 작은 Summarized 컬럼 값|\nMIN_VALUE_TIME|MIN_VALUE 값과 같이 입력된 Basetime 컬럼 값|\nMAX_VALUE|해당 태그 아이디 Row 중 가장 큰 Summarized 컬럼 값|\nMAX_VALUE_TIME|MAX_VALUE 값과 같이 입력된 Basetime 컬럼 값|\nRECENT_ROW_TIME|해당 태그 아이디 Row 중 가장 최근에 입력된 Basetime 컬럼 값|\n\n3번째 칼럼에 SUMMARIZED 키워드가 없으면, VALUE 관련 정보(MIN_VALUE, MAX_VALUE, MIN_VALUE_TIME, MAX_VALUE_TIME)는 저장하지 않는다.\n\n조회 예시는 아래와 같다.\n\n```sql\n1. SUMMARIZED 칼럼이 존재하는 경우\nMach> CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nExecuted successfully.\n \nMach> INSERT INTO tag VALUES('tag-0', TO_DATE('2021-08-12'), 10);\nMach> INSERT INTO tag VALUES('tag-0', TO_DATE('2021-08-13'), 10);\nMach> INSERT INTO tag VALUES('tag-0', TO_DATE('2021-08-14'), 20);\nMach> INSERT INTO tag VALUES('tag-0', TO_DATE('2021-08-11'), 5);\nMach> INSERT INTO tag VALUES('tag-1', TO_DATE('2022-08-12'), 100);\nMach> INSERT INTO tag VALUES('tag-1', TO_DATE('2022-08-11'), 200);\nMach> INSERT INTO tag VALUES('tag-1', TO_DATE('2022-08-10'), 50);\n \nMach> SELECT * FROM v$tag_stat;\nNAME                                                                              ROW_COUNT            MIN_TIME                        MAX_TIME                        MIN_VALUE                  \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMIN_VALUE_TIME                  MAX_VALUE                   MAX_VALUE_TIME                  RECENT_ROW_TIME                \n---------------------------------------------------------------------------------------------------------------------------------\ntag-0                                                                             4                    2021-08-11 00:00:00 000:000:000 2021-08-14 00:00:00 000:000:000 5                          \n2021-08-11 00:00:00 000:000:000 20                          2021-08-14 00:00:00 000:000:000 2021-08-11 00:00:00 000:000:000\ntag-1                                                                             3                    2022-08-10 00:00:00 000:000:000 2022-08-12 00:00:00 000:000:000 50                         \n2022-08-10 00:00:00 000:000:000 200                         2022-08-11 00:00:00 000:000:000 2022-08-10 00:00:00 000:000:000\n[2] row(s) selected.\n \n2. SUMMARIZED 칼럼이 존재하지 않는 경우\nMach> CREATE TAG TABLE other_tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE);\nExecuted successfully.\n \nMach> INSERT INTO other_tag VALUES('tag-0', TO_DATE('2021-08-12'), 10);\nMach> INSERT INTO other_tag VALUES('tag-0', TO_DATE('2021-08-13'), 10);\nMach> INSERT INTO other_tag VALUES('tag-0', TO_DATE('2021-08-14'), 20);\nMach> INSERT INTO other_tag VALUES('tag-0', TO_DATE('2021-08-11'), 5);\nMach> INSERT INTO other_tag VALUES('tag-1', TO_DATE('2022-08-12'), 100);\nMach> INSERT INTO other_tag VALUES('tag-1', TO_DATE('2022-08-11'), 200);\nMach> INSERT INTO other_tag VALUES('tag-1', TO_DATE('2022-08-10'), 50);\n \nMach> SELECT * FROM v$other_tag_stat;\nNAME                                                                              ROW_COUNT            MIN_TIME                        MAX_TIME                        MIN_VALUE                  \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMIN_VALUE_TIME                  MAX_VALUE                   MAX_VALUE_TIME                  RECENT_ROW_TIME                \n---------------------------------------------------------------------------------------------------------------------------------\ntag-0                                                                             4                    2021-08-11 00:00:00 000:000:000 2021-08-14 00:00:00 000:000:000 NULL                       \nNULL                            NULL                        NULL                            2021-08-11 00:00:00 000:000:000\ntag-1                                                                             3                    2022-08-10 00:00:00 000:000:000 2022-08-12 00:00:00 000:000:000 NULL                       \nNULL                            NULL                        NULL                            2022-08-10 00:00:00 000:000:000\n[2] row(s) selected.\n```\n\n# RESTful API를 통한 추출\n\n## RESTful API를 위한 준비 사항\n\n아래 프로퍼티의 값을 지정하고 서버를 시작한다.\n\nmachbase.conf\n\n```\nHTTP_ENABLE = 1\nHTTP_PORT_NO = 5678\n```\n\n\n## RESTful API 호출 규약 \n\n```bash\n{MWA URL}/machiot-rest-api/datapoints/raw/{TagName}/{Start}/{End}/{Direction}/{Count}/{Offset}/ \n \nTagName    : Tag Name. 복수의 Tag 지원(,로 구분하여 사용)\nStart, End : 기간, YYYY-MM-DD HH24:MI:SS 또는 YYYY-MM-DD 또는 YYYY-MM-DD HH24:MI:SS,mmm (mmm: millisecond, 생략시 start는 000, End는 999이며, 마이크로와 나노도 모두 999임)\n실제 스트링으로 지정할 때는 날짜와 시간 사이에 T를 넣어서 빈공간을 없애준다.\nDirection  : 0(ascending), 추후 지원 (시간이 증가하는 방향)\nCount      : LIMIT, 0이면 전체\nOffset     : offset (기본값 = 0)\n```\n\n## CURL을 통한 단일 태그 데이터 가져오기 샘플 \n\n아래와 같이 192.168.0.148에 설치된 마크베이스에 대한 호출을 수행하면, 해당 데이터를 웹으로 부터 가져올 수 있다.\n\n```bash\n$ curl -G \"http://192.168.0.148:5001/machiot-rest-api/v1/datapoints/raw/TAG_0001/2018-01-01T00:00:00/2018-01-06T00:00:00\"\n \n{\"ErrorCode\": 0,\n \"ErrorMessage\": \"\",\n \"Data\": [{\"DataType\": \"DOUBLE\",\n \"ErrorCode\": 0,\n \"TagName\": \"TAG_0001\",\n \"CalculationMode\": \"raw\",\n \"Samples\": [{\"TimeStamp\": \"2018-01-01 01:00:00 000:000:000\", \"Value\": 1.0, \"Quality\": 1},\n             {\"TimeStamp\": \"2018-01-02 02:00:00 000:000:000\", \"Value\": 2.0, \"Quality\": 1},\n             {\"TimeStamp\": \"2018-01-03 03:00:00 000:000:000\", \"Value\": 3.0, \"Quality\": 1},\n             {\"TimeStamp\": \"2018-01-04 04:00:00 000:000:000\", \"Value\": 4.0, \"Quality\": 1},\n             {\"TimeStamp\": \"2018-01-05 05:00:00 000:000:000\", \"Value\": 5.0, \"Quality\": 1}]}]\n}\n```\n\n## CURL을 통한 다중 태그 데이터 가져오기\n\n아래는 두개의 태그에 대한 값을 가져오는 샘플 예제이다.\n\n```bash\n$ curl -G \"http://192.168.0.148:5001/machiot-rest-api/datapoints/raw/TAG_0001,TAG_0002/2018-01-05T00:00:00/2018-02-05T00:00:00\"\n{\"ErrorCode\": 0,\n \"ErrorMessage\": \"\",\n \"Data\": [{\"DataType\": \"DOUBLE\",\n           \"ErrorCode\": 0,\n           \"TagName\": \"TAG_0001,TAG_0002\",\n           \"CalculationMode\": \"raw\",\n           \"Samples\": [{\"TimeStamp\": \"2018-01-05 05:00:00 000:000:000\", \"Value\": 5.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-06 06:00:00 000:000:000\", \"Value\": 6.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-07 07:00:00 000:000:000\", \"Value\": 7.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-08 08:00:00 000:000:000\", \"Value\": 8.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-09 09:00:00 000:000:000\", \"Value\": 9.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-10 10:00:00 000:000:000\", \"Value\": 10.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-02-01 01:00:00 000:000:000\", \"Value\": 11.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-02-02 02:00:00 000:000:000\", \"Value\": 12.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-02-03 03:00:00 000:000:000\", \"Value\": 13.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-02-04 04:00:00 000:000:000\", \"Value\": 14.0, \"Quality\": 1}\n]}]}\n```\n\n# 힌트(hint)를 이용한 검색 방향 지정하기\n\n태그 테이블은 일반적으로 입력한 순서가 오래된 레코드부터 조회가 가능하다. 가장 최근에 입력한 레코드부터 조회하고 싶을 때에는 힌트를 이용해 조회 방향을 제어할 수 있다.\n\n## 정방향 검색\n\n기본값이며, `/*+ SCAN_FORWARD(table_name) */` 힌트를 추가하여 조회가 가능하다.\n\n```sql\nMach> SELECT * FROM tag WHERE t_name='TAG_99' LIMIT 10;\nT_NAME                T_TIME                          T_VALUE                    \n--------------------------------------------------------------------------------------\nTAG_99                2017-01-01 00:00:49 500:000:000 0                          \nTAG_99                2017-01-01 00:01:39 500:000:000 1                          \nTAG_99                2017-01-01 00:02:29 500:000:000 2                          \nTAG_99                2017-01-01 00:03:19 500:000:000 3                          \nTAG_99                2017-01-01 00:04:09 500:000:000 4                          \nTAG_99                2017-01-01 00:04:59 500:000:000 5                          \nTAG_99                2017-01-01 00:05:49 500:000:000 6                          \nTAG_99                2017-01-01 00:06:39 500:000:000 7                          \nTAG_99                2017-01-01 00:07:29 500:000:000 8                          \nTAG_99                2017-01-01 00:08:19 500:000:000 9                          \n[10] row(s) selected.\nElapsed time: 0.001\n \nMach> SELECT /*+ SCAN_FORWARD(tag) */  * FROM tag WHERE t_name='TAG_99' LIMIT 10;\nT_NAME                T_TIME                          T_VALUE                    \n--------------------------------------------------------------------------------------\nTAG_99                2017-01-01 00:00:49 500:000:000 0                          \nTAG_99                2017-01-01 00:01:39 500:000:000 1                          \nTAG_99                2017-01-01 00:02:29 500:000:000 2                          \nTAG_99                2017-01-01 00:03:19 500:000:000 3                          \nTAG_99                2017-01-01 00:04:09 500:000:000 4                          \nTAG_99                2017-01-01 00:04:59 500:000:000 5                          \nTAG_99                2017-01-01 00:05:49 500:000:000 6                          \nTAG_99                2017-01-01 00:06:39 500:000:000 7                          \nTAG_99                2017-01-01 00:07:29 500:000:000 8                          \nTAG_99                2017-01-01 00:08:19 500:000:000 9                          \n[10] row(s) selected.\nElapsed time: 0.001\nMach>\n```\n\n## 역방향 검색\n\n`/*+ SCAN_BACKWARD(table_name) */` 힌트를 추가하여 조회가 가능하다.\n\n```sql\nMach> SELECT /*+ SCAN_BACKWARD(tag) */ * FROM tag WHERE t_name='TAG_99' LIMIT 10;\nT_NAME                T_TIME                          T_VALUE                    \n--------------------------------------------------------------------------------------\nTAG_99                2017-02-27 20:53:19 500:000:000 9                          \nTAG_99                2017-02-27 20:52:29 500:000:000 8                          \nTAG_99                2017-02-27 20:51:39 500:000:000 7                          \nTAG_99                2017-02-27 20:50:49 500:000:000 6                          \nTAG_99                2017-02-27 20:49:59 500:000:000 5                          \nTAG_99                2017-02-27 20:49:09 500:000:000 4                          \nTAG_99                2017-02-27 20:48:19 500:000:000 3                          \nTAG_99                2017-02-27 20:47:29 500:000:000 2                          \nTAG_99                2017-02-27 20:46:39 500:000:000 1                          \nTAG_99                2017-02-27 20:45:49 500:000:000 0                          \n[10] row(s) selected.\nElapsed time: 0.001\nMach>\n```\n\n## 기본 스캔 방향 프로퍼티로 설정\n\n`TABLE_SCAN_DIRECTION` 프로퍼티로 SELECT 문에 힌트가 없을 때 태그 테이블의 스캔 방향을 설정할 수 있다."
					}
					
				
		
				
					,
					
					"config-monitor-setting-monitoring-html": {
						"id": "config-monitor-setting-monitoring-html",
						"title": "설정 / 모니터링",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/setting-monitoring.html",
						"content": "이 장은 마크베이스의 프로퍼티의 의미와 설정 방법, 메타 테이블, 버추얼 테이블에 대해서 기술한다.\n\n* [Meta Table](./setting-monitoring/meta-table.md)\n* [Virtual Table](./setting-monitoring/virtual-table.md)\n* [Property](./setting-monitoring/property.md)\n* [Property (Cluster)](./setting-monitoring/property-cl.md)"
					}
					
				
		
				
					,
					
					"feature-table-log-select-simple-join-html": {
						"id": "feature-table-log-select-simple-join-html",
						"title": "단순 Join",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/select/simple-join.html",
						"content": "로그 테이블, 휘발성 테이블, 참조 테이블과 메타 테이블은 Join 하여 검색할 수 있다.\n\n# 목차\n* [단순 Join](#단순-join)\n* [Alias 를 이용한 Join](#alias-를-이용한-join)\n* [GROUP BY/ORDER BY 사용](#group-byorder-by-사용)\n* [조건절 없는 Join](#조건절-없는-join)\n* [Inner Join / Outer Join](#inner-join--outer-join)\n\n## 단순 Join\n\n```sql\nMach> CREATE TABLE logtable (code INT,value INT);\nCreated successfully.\n \nMach> INSERT INTO logtable VALUES(1,20 );\n1 row(s) inserted.\n \nMach> INSERT INTO logtable VALUES(2,10 );\n1 row(s) inserted.\n \nMach> INSERT INTO logtable VALUES(3,15 );\n1 row(s) inserted.\n \nMach> INSERT INTO logtable VALUES(4,20 );\n1 row(s) inserted.\n \nMach> INSERT INTO logtable VALUES(5,10 );\n1 row(s) inserted.\n \nMach> CREATE VOLATILE table VTABLE (code INT,name VARCHAR(32));\nCreated successfully.\n \nMach> INSERT INTO vtable VALUES(1, 'Sam');\n1 row(s) inserted.\n \nMach> INSERT INTO vtable VALUES(3, 'Thomas');\n1 row(s) inserted.\n \nMach> INSERT INTO vtable VALUES(5, 'Micheal');\n1 row(s) inserted.\n \nMach> INSERT INTO vtable VALUES(7, 'Jessica');\n1 row(s) inserted.\n \nMach> SELECT name,value FROM logtable, vtable WHERE logtable.code=vtable.code;\nname                              value\n-------------------------------------------------\nMicheal                           10\nThomas                            15\nSam                               20\n[3] row(s) selected.\n```\n\n## Alias 를 이용한 Join\n\nJoin을 사용할 때, join 대상 테이블에 alias를 사용할 수 있다.\n\n```sql\nSELECT c.name FROM m$sys_tables t, m$sys_columns c WHERE t.id = c.table_id AND t.name = 'T1'\nAND c.id NOT IN(0, 65534) ORDER BY c.name;\n \nc.name                                  \n--------------------------------------------\nADDR\nISTYPE\nSRCIP                        \n[3] row(s) selected.\n```\n\n## GROUP BY/ORDER BY 사용\n\nGROUP BY, ORDER BY 와 집계 함수도 사용 가능하다.\n\n```sql\nMach> SELECT t.name, COUNT(c.name) FROM m$sys_columns c, m$sys_tables t WHERE t.id = c.table_id GROUP BY t.name ORDER BY t.name;\nt.name                                    count(c.name)\n------------------------------------------------------------------\nCOMMON_TABLE                              5\nDURATIONT                                 3\n[2] row(s) selected.\n```\n\n## 조건절 없는 Join\n\nJOIN 조건절이 없는 join 질의는 에러를 발생시킨다. 로그 테이블에 너무나 많은 데이터가 있기 때문에, join 조건절이 없는 질의의 속도는 예측할 수 없을 정도로 느리기 때문이다.\n\n또한, 두개의 로그 테이블 join은 매우 성능이 느릴 수 있다. 그래서 데이터베이스를 설계할 때, 역정규화(denormalization)를 고려하여 join이 발생하지 않도록 설계하는 것이 좋다.\n\n```sql\nMach> CREATE TABLE log_table1(i1 INTEGER);\nCreated successfully.\nMach> INSERT INTO log_table1 VALUES(1);\n1 row(s) inserted.\nMach> INSERT INTO log_table1 VALUES(20);\n1 row(s) inserted.\nMach> INSERT INTO log_table1 VALUES(30);\n1 row(s) inserted.\n \n \nMach>CREATE TABLE log_table2(i1 INTEGER);\nCreated successfully.\nMach> INSERT INTO log_table2 VALUES(1);\n1 row(s) inserted.\nMach> INSERT INTO log_table2 VALUES(30);\n1 row(s) inserted.\nMach> INSERT INTO log_table2 VALUES(50);\n1 row(s) inserted.\n \nMach> SELECT log_table1.i1 FROM log_table1, log_table2;\n[ERR-02101 : Error in joining tables. Cannot join without join predicate.]\n \nMach> SELECT log_table1.i1 FROM log_table1, log_table2 where log_table1.i1 = 1;\n[ERR-02101 : Error in joining tables. Cannot join without join predicate.]\n \nMach> SELECT log_table1.i1 from log_table1, log_table2 WHERE log_table1.i1 = log_table2.i1;\ni1\n--------------\n30\n1\n[2] row(s) selected.\n```\n\n## Inner Join / Outer Join\n\nANSI 타입의 INNER, LEFT OUTER, RIGHT OUTER join을 사용할 수 있으나 FULL OUTER JOIN은 사용할 수 없다.\n\n```sql\nFROM    TABLE_1 [INNER|LEFT OUTER|RIGHT OUTER]  JOIN    TABLE_2 ON  expression\n```\n```sql\nSELECT t1.i1 t2.i1 FROM t1 LEFT OUTER JOIN t2 ON (t1.i1 = t2.i1) WHERE t2.i2 = 1;\n```\n위 질의는 where절의 t2.i2 = 1 조건에 의해서 Inner Join 으로 변경되어 실행된다."
					}
					
				
		
				
					,
					
					"sitemap-xml": {
						"id": "sitemap-xml",
						"title": "",
						"version": "all",
						"categories": "",
						"url": " /sitemap.xml",
						"content": "/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% for section in site.data.toc %}\n     {{ site.baseurl }}{{ section.url }}/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% endfor %}"
					}
					
				
		
				
					,
					
					"sql-ref-sql-ref-html": {
						"id": "sql-ref-sql-ref-html",
						"title": "SQL 레퍼런스",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/sql-ref.html",
						"content": "* [자료형](/kr/sql-ref/data-types.html)\n* [DDL](/kr/sql-ref/ddl.html)\n* [DML](/kr/sql-ref/dml.html)\n* [SELECT](/kr/sql-ref/select.html)\n* [SELECT Hint](/kr/sql-ref/select-hint.html)\n* [사용자 관리](/kr/sql-ref/user.html)\n* [지원 함수](/kr/sql-ref/func.html)\n* [시스템/세션 관리](./sys.md)"
					}
					
				
		
				
					,
					
					"intro-edition-standard-html": {
						"id": "intro-edition-standard-html",
						"title": "Standard Edition",
						"version": "all",
						"categories": "",
						"url": " /intro/edition/standard.html",
						"content": "# 필요성\n\nStandard Edition은 Edge에서 전송되는 대량의 데이터를 저장하고, 분석하기 위해 사용되는 제품이다.\n\n이 Standard Edition은 단일 하드웨어 장비에서 최대한의 성능을 낼 수 있도록 설계 되었기 때문에 복잡한 설치와 구성 필요 없이 매우 쉽고 빠르게 설치, 관리, 운용할 수 있다.\n\n특히, 단일 Appliance에 내장되어 공장 및 건물의 장비로 설치 및 활용되며,  고객의 데이터 수집을 1차적으로 담당하는 역할을 주로 수행한다.\n\n\n# 지원 하드웨어 및 운영체제\n\nStandard Edition은 인텔 CPU 기반의 64-bit Linux 및 Windows 2000 이상의 운영체제를 지원한다."
					}
					
				
		
				
					,
					
					"feature-table-stream-stream-create-drop-html": {
						"id": "feature-table-stream-stream-create-drop-html",
						"title": "스트림 생성 및 삭제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/stream/stream-create-drop.html",
						"content": "### 스트림 생성\n\n스트림 질의는 Insert... Select 의 형태로만 생성 가능하며, 스트림을 생성할 때, 질의문을 검사하여 정상 실행이 가능한 질의인지를 확인한다.\n\n스트림을 생성하기 위해서 아래의 저장 프로시저를 이용한다.\n\n```sql\nEXEC STREAM_CREATE(stream_name, stream_query_string);\n```\n\n스트림을 성공적으로 생성하더라도 바로 실행이 시작되지 않는다. 관련 사항은 스트림 시작 및 종료를 참조하라.\n\n#### 스트림 질의\n\n스트림 질의는 Insert... Select문의 형태를 하고 있다. 기본적인 스트림 질의는 매 입력 데이터에 대해서 실행되는 것이 기본으로, 이 경우 SUM, AVG등의 통계 질의를 사용할 수 없다.\n\n```sql\nEXEC STREAM_CREATE(normal_query, 'INSERT INTO CEP_LOG_TABLE SELECT * FROM EVENT WHERE C1 = 0');\n```\n\n하지만 Insert select문의 마지막에 스트림 질의가 수행될 주기를 설정하면 일정 주기 마다 입력 데이터에 대한 통계 질의문을 이용할 수 있다.\n\n```sql\nEXEC STREAM_CREATE(aggr_1_sec, 'insert into aggr select sum(i1), i2 from base group by i2 BY 1 SECOND');\n```\n\n위 stream 질의는 매 1초마다 수행하여 group by 질의를 마지막 수행 이후에 입력된 최신 데이터에 대해 실행하고, 그 결과를 aggr 로그 테이블에 입력하게 된다.\n\n만약 스트림 질의의 수행 시점을 사용자가 정의하여 수행하고 싶다면 실행 주기 설정 절에 아래와 같이 지정하면, 스트림 질의는 사용자의 명시적인 호출 이전에는 수행되지 않는다.\n\n```sql\nEXEC STREAM_CREATE(base_trig, 'insert into aggr select sum(i1), i2 from base group by i2 BY USER');\n```\n\n스트림 질의 실행조건절을 BY USER로 하면 STREAM_EXECUTE 프로시저를 이용하여 그 스트림 질의를 명시적으로 호출될 때 까지 실행되지 않는다. STREAM_EXECUTE로 호출된 STREAM은 이전에 읽어들인 부분을 제외하고 실행 기간 동안 추가된 증분 데이터에 대해서만 스트림 질의를 수행한다.\n\n#### 스트림 삭제\n생성된 스트림의 목록은 V$STREAMS 메타 테이블을 이용하여 조회할 수 있다. 스트림을 삭제하려면, 스트림을 생성하였을 때 결정한 스트림의 이름을 매개변수로 다음의 저장 프로시저를 이용한다.\n\n```sql\nEXEC STREAM_DROP(stream_name);\n```\n\n실행중인 스트림은 삭제가 되지 않으며, 스트림을 삭제하기 전에 먼저 스트림의 실행을 종료시켜야 한다. 관련 사항은 스트림 시작 및 종료를 참조하라.\n\n### 스트림 메타 테이블 V$STREAMS\nDB서버에 등록된 스트림들의 현재 상태를 조회하기 위한 메타 테이블이다. 자세한 설명은 메뉴얼의 virtual table에 기술되어 있다."
					}
					
				
		
				
					,
					
					"feature-table-stream-stream-ex-html": {
						"id": "feature-table-stream-stream-ex-html",
						"title": "스트림 활용을 위한 샘플 예제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/stream/stream-ex.html",
						"content": "### 샘플 데이터 다운로드\n\n아래 가이드를 따라 샘플 데이터를 다운로드한다.\n\n```bash\n## 1. MACHBASE git repository에서 샘플 데이터를 clone한다.\n$ git clone https://www.github.com/MACHBASE/TagTutorial.git MyTutorial\n \n## 2. 필요한 데이터를 압축 해제한다.\n$ cd MyTutorial/\n$ gunzip edu_3_plc_stream/*.gz\n \n## 3. 샘플 데이터가 있는 이렉토리로 이동한다.\n$ cd edu_3_plc_stream/\n```\n\n### TAG, LOG 테이블의 생성\n\nSTREAM 기능을 사용하기 위해 아래 명령어들을 환경에 맞게 수정 후 실행해 TAG, LOG 테이블을 생성한다.\n\n```bash\n$ pwd\n~/MyTutorial/edu_3_plc_stream\n \n## 1-1. TAG 테이블 생성\n$ machsql --server=127.0.0.1 --port=${MACHBASE_PORT_NO} --user=SYS --password=MANAGER --script=1_create_tag.sql\n## 1-2. TAG Meta 로드\n$ sh 2_load_meta.sh\n \n## 2. LOG 테이블 생성\n$ machsql --server=127.0.0.1 --port=${MACHBASE_PORT_NO} --user=SYS --password=MANAGER --script=3_create_plc_tag_table.sql\n```\n\n### STREAM 생성, 구동\n\n생성된 TAG, LOG 테이블에 맞게 작성된 샘플 파일을 수행해 STREAM을 시작한다.\n\n```bash\n$ machsql --server=127.0.0.1 --port=${MACHBASE_PORT_NO} --user=SYS --password=MANAGER --script=4_plc_stream_tag.sql\n```\n\n해당 샘플 파일에 포함된 쿼리에는 두 종류가 있는데 각각 STREAM을 생성하고 생성된 STREAM을 구동하는 역할을 한다.\n\n```sql\n## STREAM 생성 Query 예\n## event_v0라는 이름의 MTAG_V00를 name으로 가지고 plc_tag_table에 입력되는 데이터 중 tm과 v0 column 데이터를 time, value로 가지는 row를 tag 테이블에 입력하는 STREAM을 생성\nEXEC STREAM_CREATE(event_v0, 'insert into tag select ''MTAG_V00'', tm, v0 from plc_tag_table;');\n## STREAM 구동 Query 예\nEXEC STREAM_START(event_v0);\n```\n\nSTREAM을 정상적으로 구동시켰다면 이제부터 plc_tag_table에 데이터가 입력되는 순간 각 STREAM이 동작해 해당되는 데이터를 TAG 테이블에 입력한다.\n\n### STREAM 상태 확인\n\nMachbase에서 지원하는 가상 테이블인 v$streams를 통해 수행중인 STREAM의 갯수, 사용 쿼리, 상태, 에러 메시지 등을 확인할 수 있다.\n\n```sql\nMach> desc v$streams;\n[ COLUMN ]\n----------------------------------------------------------------------------------------------------\nNAME                                                        NULL?    TYPE                LENGTH\n----------------------------------------------------------------------------------------------------\nNAME                                                                 varchar             100\nLAST_EX_TIME                                                         datetime            31\nTABLE_NAME                                                           varchar             100\nEND_RID                                                              long                20\nSTATE                                                                varchar             10\nQUERY_TXT                                                            varchar             2048\nERROR_MSG                                                            varchar             2048\nFREQUENCY                                                            ulong               20\n```\n\n다음과 같이 모든 STREAM의 상태를 확인할 수 있다.\n\n```sql\nMach> select state, name, table_name, query_txt from v$streams;\nSTATE   NAME     TABLE_NAME    QUERY_TXT\n------------------------------------------------------------------------------------------------\nRUNNING EVENT_V0 PLC_TAG_TABLE insert into tag select 'MTAG_V00', tm, v0 from plc_tag_table;\nRUNNING EVENT_V1 PLC_TAG_TABLE insert into tag select 'MTAG_V00', tm, v1 from plc_tag_table;\nRUNNING EVENT_C0 PLC_TAG_TABLE insert into tag select 'MTAG_C00', tm, c0 from plc_tag_table;\nRUNNING EVENT_C1 PLC_TAG_TABLE insert into tag select 'MTAG_C01', tm, c1 from plc_tag_table;\nRUNNING EVENT_C2 PLC_TAG_TABLE insert into tag select 'MTAG_C02', tm, c2 from plc_tag_table;\nRUNNING EVENT_C3 PLC_TAG_TABLE insert into tag select 'MTAG_C03', tm, c3 from plc_tag_table;\nRUNNING EVENT_C4 PLC_TAG_TABLE insert into tag select 'MTAG_C04', tm, c4 from plc_tag_table;\nRUNNING EVENT_C5 PLC_TAG_TABLE insert into tag select 'MTAG_C05', tm, c5 from plc_tag_table;\nRUNNING EVENT_C6 PLC_TAG_TABLE insert into tag select 'MTAG_C06', tm, c6 from plc_tag_table;\nRUNNING EVENT_C7 PLC_TAG_TABLE insert into tag select 'MTAG_C07', tm, c7 from plc_tag_table;\nRUNNING EVENT_C8 PLC_TAG_TABLE insert into tag select 'MTAG_C08', tm, c8 from plc_tag_table;\nRUNNING EVENT_C9 PLC_TAG_TABLE insert into tag select 'MTAG_C09', tm, c9 from plc_tag_table;\nRUNNING EVENT_C10 PLC_TAG_TABLE insert into tag select 'MTAG_C10', tm, c10 from plc_tag_table;\nRUNNING EVENT_C11 PLC_TAG_TABLE insert into tag select 'MTAG_C11', tm, c11 from plc_tag_table;\nRUNNING EVENT_C12 PLC_TAG_TABLE insert into tag select 'MTAG_C12', tm, c12 from plc_tag_table;\nRUNNING EVENT_C13 PLC_TAG_TABLE insert into tag select 'MTAG_C13', tm, c13 from plc_tag_table;\nRUNNING EVENT_C14 PLC_TAG_TABLE insert into tag select 'MTAG_C14', tm, c14 from plc_tag_table;\nRUNNING EVENT_C15 PLC_TAG_TABLE insert into tag select 'MTAG_C15', tm, c15 from plc_tag_table;\n```\n\n### 데이터 로드\n\nSTREAM이 모두 구동중임을 확인했으니 Machloader를 사용해 데이터를 입력, 작동을 확인한다.\n\nSTREAM은 입력 방식에 관계 없이 작동하므로 CLI, JDBC, Collector 등 어떤 방식의 입력이 발생하더라도 TAG 테이블로 자동 입력된다.\n\n```bash\n$ cat 5_plc_tag_load.sh\nmachloader  -t plc_tag_table -i -d 5_plc_tag.csv -F \"tm YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn\"\n \n$ sh 5_plc_tag_load.sh\n-----------------------------------------------------------------\n     Machbase Data Import/Export Utility.\n     Release Version 6.5.1.official\n     Copyright 2014, MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n-----------------------------------------------------------------\nNLS            : US7ASCII            EXECUTE MODE   : IMPORT\nTARGET TABLE   : plc_tag_table       DATA FILE      : 5_plc_tag.csv\nIMPORT MODE    : APPEND              FIELD TERM     : ,\nROW TERM       : \\n                  ENCLOSURE      : \"\nESCAPE         : \\                   ARRIVAL_TIME   : FALSE\nENCODING       : NONE                HEADER         : FALSE\nCREATE TABLE   : FALSE\n \n Progress bar                       Imported records        Error records\n                                               80000                    0\n```\n\n데이터 로드 중 TAG 테이블 데이터를 확인해보면 실시간으로 데이터가 입력됨을 확인할 수 있다.\n\n```sql\nMach> select count(*) from TAG;\ncount(*)\n-----------------------\n16775979\n[1] row(s) selected.\nMach> select count(*) from TAG;\ncount(*)\n-----------------------\n17609187\n[1] row(s) selected.\nMach> select count(*) from TAG;\ncount(*)\n-----------------------\n18238357\n[1] row(s) selected.\nElapsed time: 0.000\n```\n\n### STREAM 동작의 결과 확인\n\n아래와 같이 STREAM이 소스 테이블(plc_tag_table)의 데이터를 어디까지 읽었는지를 확인할 수 있다.\n\n```sql\nMach> select name, state, end_rid from v$streams;\nname      state   end_rid\n---------------------------------------------------------\nEVENT_V0  RUNNING 909912\nEVENT_V1  RUNNING 1584671\nEVENT_C0  RUNNING 1312416\nEVENT_C1  RUNNING 1268520\nEVENT_C2  RUNNING 1636800\nEVENT_C3  RUNNING 1197840\nEVENT_C4  RUNNING 622728\nEVENT_C5  RUNNING 972780\nEVENT_C6  RUNNING 1021512\nEVENT_C7  RUNNING 1287474\nEVENT_C8  RUNNING 826956\nEVENT_C9  RUNNING 1639032\nEVENT_C10 RUNNING 725954\nEVENT_C11 RUNNING 1511436\nEVENT_C12 RUNNING 531079\nEVENT_C13 RUNNING 1004400\nEVENT_C14 RUNNING 741768\nEVENT_C15 RUNNING 746604\n[18] row(s) selected.\n```\n\nend_rid column의 값이 소스 테이블의 레코드 갯수와 동일하면 소스 테이블에서 더 이상 읽을 것이 없다는 뜻이다.\n\n```sql\nMach> select name, state, end_rid from v$streams;\nname      state   end_rid\n---------------------------------------------------------\nEVENT_V0  RUNNING 2000000\nEVENT_V1  RUNNING 2000000\nEVENT_C0  RUNNING 2000000\nEVENT_C1  RUNNING 2000000\nEVENT_C2  RUNNING 2000000\nEVENT_C3  RUNNING 2000000\nEVENT_C4  RUNNING 2000000\nEVENT_C5  RUNNING 2000000\nEVENT_C6  RUNNING 2000000\nEVENT_C7  RUNNING 2000000\nEVENT_C8  RUNNING 2000000\nEVENT_C9  RUNNING 2000000\nEVENT_C10 RUNNING 2000000\nEVENT_C11 RUNNING 2000000\nEVENT_C12 RUNNING 2000000\nEVENT_C13 RUNNING 2000000\nEVENT_C14 RUNNING 2000000\nEVENT_C15 RUNNING 2000000\n[18] row(s) selected.\n```\n\nTAG 테이블의 데이터 갯수가 소스 테이블의 갯수 * STREAM의 갯수와 같으므로 STREAM이 정상적으로 모든 데이터를 읽었음을 확인할 수 있다.\n\n```sql\nMach> select count(*) from TAG;\ncount(*)\n-----------------------\n36000000\n[1] row(s) selected.\n```\n\n입력된 데이터의 시간 범위도 다음과 같이 확인할 수 있다.\n\n```sql\nMach> select min(time), max(time) from TAG;\nmin(time)                       max(time)\n-------------------------------------------------------------------\n2009-01-28 07:03:34 000:000:000 2009-01-28 12:36:58 020:000:000\n[1] row(s) selected.\n```\n\n### 데이터 추가\n\nSTREAM이 실제로 각 데이터 입력마다 반응하는지 확인하기 위해 insert 구문을 통해 확인해볼 수 있다.\n\n```sql\nMach> insert into plc_tag_table values(TO_DATE('2009-01-28 12:37:00 000:000:000'), 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000);\n1 row(s) inserted.\n```\n\nPLC_TAG_TABLE에 레코드 하나를 더 추가한 순간 아래와 같이 각 스트림의 end_rid가 1건 늘어 2000001 건이 된 것을 확인할 수 있다.\n\n```sql\nMach> select name, state, end_rid from v$streams;\nname      state   end_rid\n---------------------------------------------------------------\nEVENT_V0  RUNNING 2000001\nEVENT_V1  RUNNING 2000001\nEVENT_C0  RUNNING 2000001\nEVENT_C1  RUNNING 2000001\nEVENT_C2  RUNNING 2000001\nEVENT_C3  RUNNING 2000001\nEVENT_C4  RUNNING 2000001\nEVENT_C5  RUNNING 2000001\nEVENT_C6  RUNNING 2000001\nEVENT_C7  RUNNING 2000001\nEVENT_C8  RUNNING 2000001\nEVENT_C9  RUNNING 2000001\nEVENT_C10 RUNNING 2000001\nEVENT_C11 RUNNING 2000001\nEVENT_C12 RUNNING 2000001\nEVENT_C13 RUNNING 2000001\nEVENT_C14 RUNNING 2000001\nEVENT_C15 RUNNING 2000001\n[18] row(s) selected.\n```\n\n### TAG Analyzer 그래프\n\nSTREAM으로 입력된 데이터들의 그래프를 Tag Analyzer로 확인하면 아래와 같다.\n\n마지막으로 입력한 데이터의 값이 다른 데이터의 값에 비해 크기 때문에 부각되어 보이는 것을 확인할 수 있다."
					}
					
				
		
				
					,
					
					"feature-table-stream-stream-execute-exit-html": {
						"id": "feature-table-stream-stream-execute-exit-html",
						"title": "스트림 실행 및 종료",
						"version": "all",
						"categories": "",
						"url": " /feature-table/stream/stream-execute-exit.html",
						"content": "### 스트림 실행\n\n저장 프로시저를 이용하여 등록된 스트림을 실행한다. 한번 실행한 스트림은 지속적으로 실행되며 서버를 재시작하더라도 마지막으로 실행한 시점 이후에 입력된 데이터에 대해서 계속 스트림 질의를 실행한다.\n\n```sql\nEXEC STREAM_START(stream_name);\n```\n\n### 스트림 종료\n\n실행중인 스트림을 종료시키기 위해서 아래의 저장 프로시저를 이용한다.\n\n```sql\nEXEC STREAM_STOP(stream_name);\n```\n\n### 스트림의 직접 실행\n\n스트림 실행 조건을 BY USER로 설정한 경우, 사용자에 의한 명시적 호출 없이는 해당 질의가 수행되지 않는다. 이 스트림 질의를 실행하기 위해 다음의 저장프로시저를 이용한다.\n\n```sql\nEXEC STREAM_EXECUTE(stream_name);\n```\n\n호출할 스트림 질의를 생성할 때, BY USER조건으로 생성하지 않았거나, STREAM_START로 실행 상태로 전환하지 않은 경우, 오류가 발생한다."
					}
					
				
		
				
					,
					
					"feature-table-stream-html": {
						"id": "feature-table-stream-html",
						"title": "스트림 (STREAM)",
						"version": "all",
						"categories": "",
						"url": " /feature-table/stream.html",
						"content": "# 개념\nSTREAM이란  마크베이스 5부터 새롭게 지원되는 CQL (Continuous Query Language) 기반의 실시간 데이터 처리 기능이다.\n\n즉, 로그 테이블로 입력되는 증분 데이터를 대상으로, 이 중 조건을 만족하는 데이터를 추출해 다른 테이블로 실시간 입력할 수 있다.\n\n만일 스트림 기능을 이용하지 않고 전체 데이터를 대상으로 조건 검색을 하게 되면 누적된 데이터의 조회가 느릴 뿐만 아니라 시스템에 큰 부담이 생길 수 있다.\n\n스트림을 이용하면 실시간으로 입력되는 특정 로그 데이터에 대해 조건을 검색하여 이벤트에 빠르게 대응할 수 있다.\n\n# 제약 사항\n* 현재 마크베이스는 Edge Edition과 Fog Edition에서만 STREAM을 지원하고 있다.\n  * Cluster Edition은 차후 버전에서 지원할 예정이다. \n* 데이터 입력 소스는 로그 테이블만 가능하다. \n  * 태그 테이블을 소스로 쓸 수 없다.\n* 데이터 출력 대상은 로그 및 태그 테이블이 가능하다.\n\n# 목차\n* [스트림 생성 및 삭제](/kr/feature-table/stream/stream-create-drop.html)\n* [스트림 실행 및 종료](/kr/feature-table/stream/stream-execute-exit.html)\n* [스트림 활용은 위한 샘플 예제](/kr/feature-table/stream/stream-ex.html)"
					}
					
				
		
				
					,
					
					"sql-ref-sys-session-html": {
						"id": "sql-ref-sys-session-html",
						"title": "시스템/세션 관리",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/sys-session.html",
						"content": "# 목차\n\n* [ALTER SYSTEM](#alter-system)\n  * [KILL SESSION](#kill-session)\n  * [CANCEL SESSION](#cancel-session)\n  * [CHECK DISK_USAGE](#check-disk_usage)\n  * [INSTALL LICENSE](#install-license)\n  * [INSTALL LICENSE (PATH)](#install-license-path)\n  * [SET](#set)\n* [ALTER SESSION](#alter-session)\n  * [SET SQL_LOGGING](#set-sql_logging)\n  * [SET DEFAULT_DATE_FORMAT](#set-default_date_format)\n  * [SET SHOW_HIDDEN_COLS](#set-show_hidden_cols)\n  * [SET FEEDBACK_APPEND_ERROR](#set-feedback_append_error)\n  * [SET MAX_QPX_MEM](#set-max_qpx_mem)\n  * [SET SESSION_IDLE_TIMEOUT_SEC](#set-session_idle_timeout_sec)\n  * [SET QUERY_TIMEOUT](#set-query_timeout)\n\n\n## ALTER SYSTEM\n\n시스템 단위의 자원을 관리하거나 설정을 변경하는 구문이다.\n\n#### KILL SESSION\n\n**alter_system_kill_session_stmt:**\n\n![alter_system_kill_session_stmt](/kr/sql-ref/sys_image/alter_system_kill_session_stmt.png)\n\n```sql\nalter_system_kill_session_stmt: 'ALTER SYSTEM KILL SESSION' number\n```\n\nSessionID를 가진 특정 세션을 종료시킨다.\n\n단, SYS 유저만이 구문을 수행할 수 있으며 자기 자신의 세션에 대해서는 KILL할 수 없다.\n\n#### CANCEL SESSION\n\n**alter_system_cancel_session_stmt:**\n\n![alter_system_cancel_session_stmt](/kr/sql-ref/sys_image/alter_system_cancel_session_stmt.png)\n\n```sql\nalter_system_cancel_session_stmt ::= 'ALTER SYSTEM CANCEL SESSION' number\n```\n\nSessionID를 가진 특정 세션을 취소시킨다.\n\n접속이 끊어지는 대신 수행중인 동작을 취소하고, 사용자에게 해당 수행이 취소되었다는 에러 코드를 되돌린다. 단, KILL과 마찬가지로 자기 자신이 연결된 세션에 대해서는 취소를 할 수 없다.\n\n#### CHECK DISK_USAGE\n\n**alter_system_check_disk_stmt:**\n\n![alter_system_check_disk_stmt](/kr/sql-ref/sys_image/alter_system_check_disk_stmt.png)\n\n```sql\nalter_system_check_disk_stmt ::= 'ALTER SYSTEM CHECK DISK_USAGE'\n```\n\nV$STORAGE에서 Log Table의 디스크 사용량을 나타내는 __DC_TABLE_FILE_SIZE__ 의 값을 보정한다.\n\nProcess Failure나 Power Failure 발생시 디스크 사용량이 부정확할 수 있다. 이 명령어를 통해서 파일 시스템으로부터 정확한 값을 읽어온다. 하지만 파일 시스템에 상당한 부하를 줄 수 있기 때문에 지양해야 한다.\n\n#### INSTALL LICENSE\n\n**alter_system_install_license_stmt:**\n\n![alter_system_install_license_stmt](/kr/sql-ref/sys_image/alter_system_install_license_stmt.png)\n\n```sql\nalter_system_install_license_stmt ::= 'ALTER SYSTEM INSTALL LICENSE'\n```\n\n라이선스 파일의 기본위치($MACHBASE_HOME/conf/license.dat)에 라이선스 파일을 설치한다.\n\n해당 라이선스가 설치에 적합한지 판별 후 설치된다.\n\n#### INSTALL LICENSE (PATH)\n\n**alter_system_install_license_path_stmt:**\n\n![alter_system_install_license_path_stmt](/kr/sql-ref/sys_image/alter_system_install_license_path_stmt.png)\n\n```sql\nalter_system_install_license_path_stmt: ::= 'ALTER SYSTEM INSTALL LICENSE' '=' \"'\" path \"'\"\n```\n\n특정 위치에 있는 라이선스 파일을 설치한다.\n\n해당 위치에 존재하지 않거나 올바르지 않은 라이선스 파일을 입력했을 시에는 에러가 발생한다. 경로는 반드시 절대경로로 입력해야 한다. 해당 라이선스가 설치에 적합한지 판별 후 설치된다.\n\n## SET\n\n* 5.6 이후 버전 부터 지원되는 기능입니다.\n\n**alter_system_set_stmt:**\n\n![alter_system_set_stmt](/kr/sql-ref/sys_image/alter_system_set_stmt.png)\n\n```sql\nalter_system_set_stmt ::= 'ALTER SYSTEM SET' prop_name '=' value\n```\n\nSystem 의 Property 를 수정할 수 있다. 수정 가능한 Property 는 다음과 같다.\n* QUERY_PARALLEL_FACTOR\n* DEFAULT_DATE_FORMAT\n* TRACE_LOG_LEVEL\n* PAGE_CACHE_MAX_SIZE\n* MAX_SESSION_COUNT\n* SESSION_IDLE_TIMEOUT_SEC\n* PROCESS_MAX_SIZE\n* TAG_CACHE_MAX_MEMORY_SIZE\n\n\n## ALTER SESSION\n\n세션 단위의 자원을 관리하거나 설정을 변경하는 구문이다.\n\n#### SET SQL_LOGGING\n\n**alter_session_sql_logging_stmt:**\n\n![alter_session_sql_logging_stmt](/kr/sql-ref/sys_image/alter_session_sql_logging_stmt.png)\n\n```sql\nalter_session_sql_logging_stmt ::= 'ALTER SESSION SET SQL_LOGGING' '=' flag\n```\n\n해당 세션의 Trace Log에 메시지를 남길지 여부를 결정한다.\n\n이 메시지를 Bit Flag 로서 다음의 값을 사용하면 된다.\n\n* 0x1 : Parsing, Validation, Optimization 단계에서 발생하는 에러를 남긴다.\n* 0x2 : DDL을 수행한 결과를 남긴다.\n\n즉, 해당 플래그의 값이 2일 경우에는 DDL만 로깅하고, 3일 경우에는 에러 및 DDL을 함께 로깅하는 것이다.\n\n아래는 해당 세션의 로깅 플래그를 변경하고, 에러 로깅을 남기는 예제이다.\n\n```sql\nMach> alter session set SQL_LOGGING=1;\nAltered successfully.\nMach> exit\n```\n\n#### SET DEFAULT_DATE_FORMAT\n\n**alter_session_set_defalut_dateformat_stmt:**\n\n![alter_session_set_defalut_dateformat_stmt](/kr/sql-ref/sys_image/alter_session_set_defalut_dateformat_stmt.png)\n\n```sql\nalter_session_set_defalut_dateformat_stmt ::= 'ALTER SESSION SET DEFAULT_DATE_FORMAT' '=' date_format\n```\n\n해당 세션의 Datetime 자료형의 기본 포맷을 설정한다.\n\n서버가 구동되면, Property 인 __DEFAULT_DATE_FORMAT__ 의 값이 세션 속성으로 설정이 된다.\nProperty 의 속성이 바뀌지 않았다면, 세션의 값 또한 \"YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn\"이 될 것이다.\n시스템과 무관하게, 특정 사용자에 한해 Datetime 자료형의 기본 포맷을 수정할 경우에 이 명령어를 사용한다.\n\nv$session 에 해당 세션마다 설정된 Default Date Format 이 있고 확인도 할 수 있다. 아래는 해당 세션의 값을 확인 및 변경하는 예제이다.\n\n```sql\nMach> CREATE TABLE time_table (time datetime);\nCreated successfully.\n \nMach> SELECT DEFAULT_DATE_FORMAT from v$session;\ndefault_date_format                                                              \n-----------------------------------------------\nYYYY-MM-DD HH24:MI:SS mmm:uuu:nnn                                                \n[1] row(s) selected.\n \nMach> INSERT INTO time_table VALUES(TO_DATE('2016-11-11'));\n[ERR-00300 : Invalid date format or input string.([2016-11-11]:[%Y-%m-%d %H:%M:%S %0:%1:%2])]\n \nMach> ALTER SESSION SET DEFAULT_DATE_FORMAT='YYYY-MM-DD';\nAltered successfully.\n \nMach> SELECT DEFAULT_DATE_FORMAT from v$session;\n \ndefault_date_format                                                              \n----------------------------------------------\nYYYY-MM-DD                                                                       \n[1] row(s) selected.\n \nMach> INSERT INTO time_table VALUES(TO_DATE('2016-11-11'));\n1 row(s) inserted.\n \nMach> SELECT * FROM time_table;\n \nTIME                              \n----------------------------------\n2016-11-11\n \n[1] row(s) selected.\n```\n\n#### SET SHOW_HIDDEN_COLS\n\n**alter_session_set_hidden_column_stmt:**\n\n![alter_session_set_hidden_column_stmt](/kr/sql-ref/sys_image/alter_session_set_hidden_column_stmt.png)\n\n```sql\nalter_session_set_hidden_column_stmt ::= 'ALTER SESSION SET SHOW_HIDDEN_COLS' '=' ( '0' | '1' )\n```\n\n해당 세션의 select 수행시 *로 표현된 컬럼에서 숨은 컬럼 (_arrival_time)을 출력할 것인지를 결정한다.\n\n서버가 구동되면, 전역 프로퍼티인 SHOW_HIDDEN_COLS의 값이 세션 속성으로 0이 설정된다.\n만일 사용자가 자기 세션의 기본 동작을 변경하고자 할 경우에는 이 값을 1로 설정하면 된다.\n\nv$session에 해당 세션마다 설정된 SHOW_HIDDEN_COLS 값이 있으며, 확인할 수 있다.\n\n```sql\nMach> SELECT * FROM  v$session;\nID                   CLOSED      USER_ID     LOGIN_TIME                      SQL_LOGGING SHOW_HIDDEN_COLS\n-----------------------------------------------------------------------------------------------------------------\nDEFAULT_DATE_FORMAT                                                               HASH_BUCKET_SIZE\n------------------------------------------------------------------------------------------------------\n1                    0           1           2015-04-29 17:23:56 248:263:000 3           0\nYYYY-MM-DD HH24:MI:SS mmm:uuu:nnn                                                 20011\n[1] row(s) selected.                            \nMach> ALTER SESSION SET SHOW_HIDDEN_COLS=1;\nAltered successfully.\nMach> SELECT * FROM v$session;\n_ARRIVAL_TIME                   ID                   CLOSED      USER_ID     LOGIN_TIME                      SQL_LOGGING\n--------------------------------------------------------------------------------------------------------------------------------\nSHOW_HIDDEN_COLS DEFAULT_DATE_FORMAT                                                               HASH_BUCKET_SIZE\n------------------------------------------------------------------------------------------------------------------------\n1970-01-01 09:00:00 000:000:000 1                    0           1           2015-04-29 17:23:56 248:263:000 3\n1           YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn                                                 20011\n[1] row(s) selected.\n```\n\n#### SET FEEDBACK_APPEND_ERROR\n\n**alter_session_set_feedback_append_err_stmt:**\n\n![alter_session_set_feedback_append_err_stmt](/kr/sql-ref/sys_image/alter_session_set_feedback_append_err_stmt.png)\n\n```sql\nalter_session_set_feedback_append_err_stmt ::= 'ALTER SESSION SET FEEDBACK_APPEND_ERROR' '=' ( '0' | '1' )\n```\n\n해당 세션의 Append 에러 메시지를 Client program으로 보낼 것인지를 설정한다.\n\n에러 메시지는 다음의 값을 사용하면 된다.\n\n* 0 = 에러 메시지를 보내지 않는다.\n* 1 = 에러 메시지를 보낸다.\n\n아래는 사용 예제이다.\n\n```sql\nmach> ALTER SESSION SET FEEDBACK_APPEND_ERROR=0;\nAltered successfully.\n```\n\n#### SET MAX_QPX_MEM\n\n**alter_session_set_max_qpx_mem_stmt:**\n\n![alter_session_set_max_qpx_mem_stmt](/kr/sql-ref/sys_image/alter_session_set_max_qpx_mem_stmt.png)\n\n```sql\nalter_session_set_max_qpx_mem_stmt ::= 'ALTER SESSION SET MAX_QPX_MEM' '=' value\n```\n\n해당 세션의 하나의 SQL Statement가 GROUP BY, DISTINCT, ORDER BY 연산을 수행할때 사용하는 최대 메모리의 크기를 지정한다.\n\n만약 최대 메모리 이상의 메모리 할당을 시도하면, 시스템은 그 SQL문의 수행을 취소하고 오류로 처리한다.\n오류 발생 시 machbase.trc에 해당 질의문을 포함한 에러 코드 및 에러 메시지를 기록한다.\n\n```sql\nMach> ALTER SESSION SET MAX_QPX_MEM=1073741824;\nAltered successfully.\n \nMach> SELECT * FROM v$session;\nID                   CLOSED      USER_ID     LOGIN_TIME                      CLIENT_TYPE                                                                      \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUSER_NAME                                                                         USER_IP                                                                           SQL_LOGGING SHOW_HIDDEN_COLS\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nFEEDBACK_APPEND_ERROR DEFAULT_DATE_FORMAT                                                               HASH_BUCKET_SIZE MAX_QPX_MEM          RS_CACHE_ENABLE      RS_CACHE_TIME_BOUND_MSEC\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nRS_CACHE_MAX_MEMORY_PER_QUERY RS_CACHE_MAX_RECORD_PER_QUERY RS_CACHE_APPROXIMATE_RESULT_ENABLE IDLE_TIMEOUT         QUERY_TIMEOUT       \n-----------------------------------------------------------------------------------------------------------------------------------------------\n14                   0           1           2021-03-08 16:33:01 503:181:809 CLI                                                                              \nNULL                                                                              192.168.0.194                                                                     11          0               \n1                     YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn                                                 20011            1073741824           1                    1000                    \n16777216                      50000                         0                                  0                    0                   \n[1] row(s) selected.\nElapsed time: 0.001\n```\n\n- 최대 메모리 크기 이상을 SQL문에서 사용했을 때, trc 에러\n\n```sql\n[2021-03-08 16:36:32 P-69000 T-140515328653056][INFO] DML FAILURE (2E10000084:Memory allocation error (alloc'd: 1048595, max: 1048576).)\n```\n\n- 최대 메모리 크기 이상을 SQL문에서 사용했을 때, machsql 에러 메세지\n\n```sql\nMach> select * from tag order by value DESC, time ASC;\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\n[ERR-00132: Memory allocation error (alloc'd: 1048595, max: 1048576).]\n[0] row(s) selected.\nElapsed time: 0.447\n```\n\n#### SET SESSION_IDLE_TIMEOUT_SEC\n\n**alter_session_set_session_idle_timeout_sec_stmt:**\n\n![alter_session_set_session_idle_timeout_sec_stmt](/kr/sql-ref/sys_image/alter_session_set_session_idle_timeout_sec_stmt.png)\n\n```sql\nalter_session_set_session_idle_timeout_sec_stmt ::= 'ALTER SESSION SET SESSION_IDLE_TIMEOUT_SEC' '=' value\n```\n\n해당 세션이 유휴 상태일 때의 연결 유지 시간을 지정한다.\n\n초단위로 지정하며 유휴 상태로 설정된 시간이 지나게 되면 세션이 종료된다.\n\nv$session 에서 세션에 설정된 idle timeout 시간을 조회할 수 있다.\n\n\n```sql\nMach> ALTER SESSION SET SESSION_IDLE_TIMEOUT_SEC=200;\nAltered successfully.\n \n \nMach> SELECT IDLE_TIMEOUT FROM V$SESSION;\nIDLE_TIMEOUT        \n-----------------------\n200                                     \n[1] row(s) selected.\n```\n\n## SET QUERY_TIMEOUT\n\n**alter_session_set_query_timeout_stmt:**\n\n![alter_session_set_query_timeout_stmt](/kr/sql-ref/sys_image/alter_session_set_query_timeout_stmt.png)\n\n```sql\nalter_session_set_query_timeout_stmt ::= 'ALTER SESSION SET QUERY_TIMEOUT' '=' value\n```\n\n세션에서 Query를 수행 시 서버의 응답을 대기하는 시간이다.\n\n초단위로 지정하며 Query 수행 후 서버에서의 응답이 지정된 시간을 초과하면 Query가 종료된다.\n\nv$session에서 세션에 설정된 Query timeout 시간을 조회할 수 있다.\n\n```sql\nMach> ALTER SESSION SET QUERY_TIMEOUT=200;\nAltered successfully.\n \nMach> SELECT QUERY_TIMEOUT FROM V$SESSION;\nQUERY_TIMEOUT        \n-----------------------\n200                                     \n[1] row(s) selected.\n```"
					}
					
				
		
				
					,
					
					"feature-table-tag-tag-index-html": {
						"id": "feature-table-tag-tag-index-html",
						"title": "태그 테이블 인덱스 생성 및 관리",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/tag-index.html",
						"content": "마크베이스의 태그테이블에는 추가 칼럼에 대한 TAG 인덱스 타입을 생성할 수 있다.\n\n자세한 내용은 SQL 레퍼런스의 DDL 페이지의 CREATE INDEX 문단을 참조하면 된다.\n\nTAG 인덱스: 기본칼럼(time, name)등을 제외한 추가 칼럼에 대한 인덱스\n\n# 목차\n\n* [인덱스 생성](#인덱스-생성)\n* [인덱스 삭제](#인덱스-삭제)\n\n\n# 인덱스 생성\n\nCREATE INDEX 구문을 이용하여 특정 컬럼에 대해서 인덱스를 생성한다.\n\n```sql\nCREATE INDEX index_name ON table_name (column_name) [index_type]\n    index_type ::= INDEX_TYPE { TAG }\n```\n\n```sql\nMach> CREATE INDEX id_index ON tag (id) INDEX_TYPE TAG;\nCreated successfully.\n```\n\n7.5 버전부터 tag table에 한해 json 타입의 칼럼에 대해서 json path 별로 인덱스를 생성할 수 있다.\n\n기존의 인덱스 생성 구문에 json path를 operator와 연결하면 된다.\n\njson operator의 return 타입이 varchar이므로, varchar 비교 시 인덱스를 사용할 수 있다.\n\n```sql\nMach> CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, jval JSON);\nExecuted successfully.\n \nMach> CREATE INDEX idx_jval_value1 ON tag (jval->'$.value1');\nCreated successfully.\n \nMach> CREATE INDEX idx_jval_value2 ON tag (jval->'$.value2');\nCreated successfully.\n \nMach> EXPLAIN SELECT * FROM tag WHERE jval->'$.value1' = '10';\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  TAG READ (RAW)                                                                 \n   KEYVALUE INDEX SCAN (_TAG_DATA_0)                                             \n    [KEY RANGE]                                                                  \n     * jval->'$.value1' = '10'                                                   \n   VOLATILE FULL SCAN (_TAG_META)                                                \n[6] row(s) selected.\n```\n\n# 인덱스 삭제\n\nDROP INDEX 구문을 이용하여 지정된 인덱스를 삭제한다. 단, 해당 테이블을 검색 중인 다른 세션이 존재할 경우에는 에러를 내면서 실패한다.\n\n```sql\nDROP INDEX index_name;\n```\n\n```sql\nMach> DROP INDEX id_index;\nDropped successfully.\n```"
					}
					
				
		
				
					,
					
					"feature-table-tag-html": {
						"id": "feature-table-tag-html",
						"title": "태그 테이블 (Tag Table)",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag.html",
						"content": "# 개념\n\n![tag](/kr/feature-table/tag.png)\n\n태그 테이블은 센서 데이터 처리를 위한 데이터 저장소 및 관련 부가 정보 관리를 담당한다.\n\nTAG 테이블에서는 아래의 세가지 개념적인 데이터 처리 공간을 제공하며, 세부적인 설명은 다음과 같다.\n\n## Sensor Partition\n\n이는 TAG 테이블이 생성될 때 사용자가 정의한 스키마를 기준으로 저장되는 내부 센서 데이터 테이블이다.\n\n이 데이터는 TAG 테이블에 대한 SELECT 질의를 통해서 추출이 가능하다.\n\n고속으로 초당 수만건에서 수십 만건의 센서 데이터를 로딩할 수 있다.\n\n고속으로 초당 수만건의 센서 데이터를 시간 범위의 조건으로 검색할 수 있다.\n\n실시간 압축을 통해 오랜 기간 동안의 센서 데이터를 저장할 수 있다.\n\n시간 순으로 오래된 센서 데이터에 대한 순차적인 삭제가 가능하다.\n\n저장되는 사용자의 센서 데이터는 기본적으로 시계열 데이터로서 해당 태그의 이름과 시간 그리고 64비트 실수 값을 갖는 특정한 데이터형이다.\n\n|태그이름(사용자 지정 길이 스트링))|시간(64비트)|실수 값(64비트)|(사용자 확장 컬럼들..)|\n|--|--|--|--|\n\n## ROLLUP Partition\n\n이것은 Sensor storage에 저장된 센서 데이터를 바탕으로 자동으로 통계 데이터를 생성하는 내부 테이블이다.\n\n이는 수일 혹은 수년의 긴 시간동안의 통계 데이터를 수초내의 실시간으로 얻을 목적으로 개발되었다.\n\n하나의 Sensor storage 당  Hour, Minute, Second 단위로 3개의 내부 ROLLUP 테이블이 별도로 생성된다.\n\n이 테이블에서는 MIN, MAX, AVG, SUM, COUNT  다섯개의 통계 데이터를 지원한다.\n\n이 ROLLUP 결과값을 얻기 위해서는 TAG 테이블에 대한 SELECT 질의의 Hint 지정을 통해서 가능하다.\n\n## META 테이블\n\n이것은 Sensor Storage에 저장될 태그의 이름 및 부가 메타 정보를 저장하는 별도의 테이블이다.\n\n사용자는 이 테이블에 대해 명시적으로 INSERT를 통해 태그의 메타 정보를 등록할 수 있다.\n\n또한, 사용자는 이 테이블에 대한 수정과 삭제도 가능하다.\n\n## 중복제거기능\n중복제거 설정 시 태그 테이블에 입력된 데이터 중 중복 데이터가 있으면 자동으로 삭제해 주는 기능이다.\n새로운 데이터가 입력되었을 때 기존 입력된 데이터 중 설정 기간(최대 30일) 이내에 TAG NAME과 TIME이 일치하는 데이터가 있을 경우 새로 들어온 데이터를 자동으로 삭제 처리한다.\n\n# 작업 방법\n\n* [태그 테이블 생성 및 삭제](/kr/feature-table/tag/create-drop.html)\n* [태그 메타(태그 이름) 관리](/kr/feature-table/tag/tagmeta.html)\n* [태그 데이터 조작](/kr/feature-table/tag/manipulate.html)\n* [롤업 테이블 생성 및 조회](/kr/feature-table/tag/rollup.html)\n* [태그 테이블 확용 샘플 예제](/kr/feature-table/tag/ex.html)\n* [태그 테이블 인덱스 생성 및 관리](/kr/feature-table/tag/tag-index.html)\n* [태그 테이블 중복제거 설정](/kr/feature-table/tag/duplication-removal.html)"
					}
					
				
		
				
					,
					
					"feature-table-tag-tagmeta-html": {
						"id": "feature-table-tag-tagmeta-html",
						"title": "태그 메타(태그 이름) 관리",
						"version": "all",
						"categories": "",
						"url": " /feature-table/tag/tagmeta.html",
						"content": "# 목차\n\n* [태그 메타의 개념](#태그-메타의-개념)\n* [이름만으로 이뤄진 태그 메타](#이름만으로-이뤄진-태그-메타)\n* [추가 정보를 갖는 태그 메타](#추가-정보를-갖는-태그-메타)\n* [RESTful API를 통한 태그 메타 조회](#restful-api를-통한-태그-메타-조회)\n\n# 태그 메타의 개념\n\n태그 메타는 마크베이스에서 저장될 임의의 태그가 가질 이름과 부가 정보를 나타낸다.\n\n즉, 특정 장비에 존재하는 태그가 3개라고 한다면, 이 태그를 나타내는 임의의 이름과 관련 부가 정보가 필요한데, 이것을 모두 태그의 메타 정보라고 한다.\n\n이 태그 메타는 최소한 이름이 존재할 수 있으며,  부가적으로 필요하다면 해당 장비에 맞는 다양한 종류의 데이터 타입을 지정할 수 있도록 되어 있다.\n\n# 이름만으로 이뤄진 태그 메타\n\n## 태그 메타의 생성\n\n아래는 가장 기본적인 태그 메타가 생성되는 TAG 테이블의 생성 명령어이다.\n\n```sql\ncreate tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\nMach> desc tag;\n[ COLUMN ]                             \n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nNAME                          varchar             20                 \nTIME                          datetime            31             \nVALUE                         double              17\n```\n\n위는 기본적인 TAG 테이블을 생성한 것이며, 태그 메타에 대한 별도의 정보는 보이지 않는다.\n\n이 경우 태그 메타는 VARCHAR(20)의 기본적인 이름만을 가진다.\n\n## 태그 메타의 입력\n\n이제 TAG1 이라는 이름을 갖는 하나의 태그 정보를 입력해 보자.\n\n```sql\nMach> insert into tag metadata values ('TAG_0001');\n1 row(s) inserted.\n```\n\n위의 질의를 통해서 TAG_0001 이라는 이름을 갖는 하나의 태그를 생성하였다.\n\n## 태그 메타의 출력\n\n마크베이스에서는 입력된 태그 메타의 정보를 확인하기 위한 특별한 테이블인 \\_tag\\_meta 를 제공한다.\n\n따라서, 사용자는 다음과 같은 질의를 통해서 마크베이스에 입력된 모든 태그의 정보를 확인할 수 있다.\n\n```sql\nMach> select * from _tag_meta;\nID                   NAME                 \n----------------------------------------------\n1                    TAG_0001             \n[1] row(s) selected.\n```\n\n위의 질의를 통해서 TAG_0001 이라는 NAME을 갖는 하나의 태그를 생성하였다.\n\nID는 내부적으로 관리되는 값으로서 자동으로 부여된다.\n\n## 태그 메타의 수정\n\n마크베이스는 입력된 태그 메타 정보를 수정할 수 있도록 해 주는데, 다음과 같이 이름이 수정 가능하다.\n\n```sql\nMach> update tag metadata set name = 'NEW_0001' where NAME = 'TAG_0001';\n1 row(s) updated.\n \nMach> select * from _tag_meta;\nID                   NAME                 \n----------------------------------------------\n1                    NEW_0001             \n[1] row(s) selected.\n```\n\n위와 같이 이름이 TAG_0001에서 NEW_0001로 수정된 것을 확인할 수 있다.\n\n## 태그 메타의 삭제\n\n아래와 같이 실제 태그 메타의 정보를 삭제할 수 있다.\n\n```sql\nMach> delete from tag metadata where name = 'NEW_0001';\n1 row(s) deleted.\n \nMach> select * from _tag_meta;\nID                   NAME                 \n----------------------------------------------\n[0] row(s) selected.\n```\n\n주의할 점은 태그 테이블에 실제 데이터가 해당 태그 메타를 참조하지 않을 때 태그 메타 삭제가 가능하다.\n\n# 추가 정보를 갖는 태그 메타\n\n## 태그 메타의 생성\n\n아래는 태그 메타의 정보에 16비트 정수와 시간 그리고, IPv4 의 정보를 부가적으로 더 추가해서 만들어 본다.\n\n주의할 점은 일단 생성된 태그 메타에 대해 값은 수정할 수 있지만, 그 구조는 수정할 수 없다는 것이다.\n\n```sql\ncreate tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized)\nmetadata (type short, create_date datetime, srcip ipv4) ;\n \nMach> desc tag;\n[ COLUMN ]                             \n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nNAME                          varchar             20                 \nTIME                          datetime            31             \nVALUE                         double              17                 \n[ META-COLUMN ]                             \n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nTYPE                          short               6              \nCREATE_DATE                   datetime            31             \nSRCIP                         ipv4                15   \n```\n\n## 태그 메타의 입력\n\n이름 뿐만 아니라 부가 정보가 있는 상태에서 아래와 같이 입력해서 정보를 확인할 수 있다.\n\n```sql\nMach> insert into tag metadata(name) values ('TAG_0001');\n1 row(s) inserted.\n \nMach> select * from _tag_meta;\nID                   NAME                  TYPE        CREATE_DATE                     SRCIP          \n-------------------------------------------------------------------------------------------------------------\n1                    TAG_0001              NULL        NULL                            NULL           \n[1] row(s) selected.\n```\n\n위와 같이 NAME 외 다른 컬럼에는 NULL이 입력된 것을 알 수 있다.\n\n이제 부가 정보를 아래와  같이 더 넣어 보자.\n\n```sql\nMach> insert into tag metadata values ('TAG_0002', 99, '2010-01-01', '1.1.1.1');\n1 row(s) inserted.\n \nMach> select * from _tag_meta;\nID                   NAME                  TYPE        CREATE_DATE                     SRCIP          \n-------------------------------------------------------------------------------------------------------------\n1                    TAG_0001              NULL        NULL                            NULL           \n2                    TAG_0002              99          2010-01-01 00:00:00 000:000:000 1.1.1.1        \n[2] row(s) selected.\n```\n\n부가 정보를 위와 같이 넣었고, 각 태그 메타가 주어진 풍부한 정보를 가질 수 있게 되었다.\n\n## 태그 메타의 수정\n\n이제 TAG_0001의 타입을 NULL에서 11로 수정해 보자.\n\n```sql\nMach> update tag metadata set type = 11 where name = 'TAG_0001';\n1 row(s) updated.\n \nMach> select * from _tag_meta;\nID                   NAME                  TYPE        CREATE_DATE                     SRCIP          \n-------------------------------------------------------------------------------------------------------------\n2                    TAG_0002              99          2010-01-01 00:00:00 000:000:000 1.1.1.1        \n1                    TAG_0001              11          NULL                            NULL           \n[2] row(s) selected.\n```\n\n위와 같이 수정되었다.\n\n즉,  UPDATE 구문을 통해 모든 필드의 값을 수정할 수 있다.\n\n단, 반드시 WHERE 절에 NAME이 지정되어야 하는 것은 공통적인 제약 사항이다.\n\n# RESTful API를 통한 태그 메타 조회\n\n## 모든 태그 리스트 얻기\n\n아래는 마크베이스 포함된 모든 태그의 리스트를 얻는 예제이다.\n\n```bash\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/list\"\n{\"ErrorCode\": 0,\n \"ErrorMessage\": \"\",\n \"Data\": [{\"NAME\": \"TAG_0001\"},\n          {\"NAME\": \"TAG_0002\"}]}\nHost:~$\n```\n\n## 특정 태그의 시간 범위 얻기\n\n아래는 원하는 태그가 가지고 있는 데이터의 최소 및 최대 시간 범위를 얻는 예제이다.\n\n이기능은 특정 태그의 차트를 그릴 때 매우 유용하다.\n\n### 문법\n\n```\n{MWA URL}/machiot-rest-api/tags/range/  # Time Range of whole DB\n{MWA URL}/machiot-rest-api/tags/range/{TagName}  # Time Range of a specific Tag\n```\n\n### 전체 시간 범위\n\n```\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/range/\"\n{\"ErrorCode\": 0,\n \"ErrorMessage\": \"\",\n \"Data\": [{\"MAX\": \"2018-02-10 10:00:00 000:000:000\", \"MIN\": \"2018-01-01 01:00:00 000:000:000\"}]}\n```\n\n### 특정 태그의 시간 범위\n\n```\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/range/TAG_0001\"\n{\"ErrorCode\": 0, \"ErrorMessage\": \"\", \"Data\": [{\"MAX\": \"2018-01-10 10:00:00 000:000:000\", \"MIN\": \"2018-01-01 01:00:00 000:000:000\"}]}\nHost:~$\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/range/TAG_0002\"\n{\"ErrorCode\": 0, \"ErrorMessage\": \"\", \"Data\": [{\"MAX\": \"2018-02-10 10:00:00 000:000:000\", \"MIN\": \"2018-02-01 01:00:00 000:000:000\"}]}\n```"
					}
					
				
		
				
					,
					
					"pages-tags-html": {
						"id": "pages-tags-html",
						"title": "Tags Index",
						"version": "all",
						"categories": "",
						"url": " /pages/tags.html",
						"content": "Tags Index\n{% capture site_tags %}{% for tag in site.tags %}{% if tag %}{{ tag | first }}{% unless forloop.last %},{% endunless %}{% endif %}{% endfor %}{% endcapture %}{% assign docs_tags = \"\" %}{% for doc in site.docs %}{% assign ttags = doc.tags | join:',' | append:',' %}{% assign docs_tags = docs_tags | append:ttags %}{% endfor %}\n{% assign all_tags = docs_tags | append:site_tags %}{% assign tags_list = all_tags | split:',' | uniq | sort %}\n\n{% for tag in tags_list %}{% if tag %}{{ tag }}\n\n    {% for post in site.tags[tag] %}\n    {{- post.title -}}\n     {{- post.date | date: \"%B %d, %Y\" -}}\n{% endfor %}\n{% for doc in site.docs %}{% if doc.tags contains tag %}\n\n    {{ doc.title }}\n         {{- doc.date | date: \"%B %d, %Y\" -}}\n    {% endif %}{% endfor %}\n{% endif %}{% endfor %}"
					}
					
				
		
				
					,
					
					"feature-table-log-select-text-search-html": {
						"id": "feature-table-log-select-text-search-html",
						"title": "텍스트 검색",
						"version": "all",
						"categories": "",
						"url": " /feature-table/log/select/text-search.html",
						"content": "이 문서는 키워드 인덱스를 이용한 텍스트 검색을 다룬다.\n\n텍스트 검색은 \"reverse index\"라는 특수한 종류의 인덱스를 탐색하여 원하는 문자열 패턴을 검색하기 때문에, 일반적인 DBMS의 LIKE검색과 비교할 수 없을 정도로 빠른 성능을 낸다.\n\n키워드 인덱스는 가변길이 문자형 칼럼인 varchar 타입과 text 타입 칼럼에 대해서만 생성할 수 있다. 단, 검색 대상 문자열이 반드시 정확히 일치해야 한다.\n\n마크베이스는 특수문자를 기반한 키워드나, 형태소 분석등을 수행하지는 않는다.\n\n\n# 목차\n* [SEARCH](#search)\n* [다중 언어 검색](#다중-언어-검색)\n* [ESEARCH](#esearch)\n* [REGEXP](#regexp)\n* [LIKE](#like)\n\n## SEARCH\n\n```sql\nSELECT  column_name(s)\nFROM    table_name\nWHERE   column_name\nSEARCH  pattern;\n```\n```sql\nMach> CREATE TABLE search_table (id INTEGER, name VARCHAR(20));\nCreated successfully.\n \nMach> CREATE INDEX idx_SEARCH ON SEARCH_table (name) INDEX_TYPE KEYWORD;\nCreated successfully.\n \nMach> INSERT INTO search_table VALUES(1, 'time flys');\n1 row(s) inserted.\n \nMach> INSERT INTO search_table VALUES(1, 'time runs');\n1 row(s) inserted.\n \nMach> SELECT * FROM search_table WHERE name SEARCH 'time' OR name SEARCH 'runs2' ;\nID          NAME\n-------------------------------------\n1           time runs\n1           time flys\n[2] row(s) selected.\n \nMach> SELECT * FROM search_table WHERE name SEARCH 'time' AND name SEARCH 'runs2' ;\nID          NAME\n-------------------------------------\n[0] row(s) selected.\n \nMach> SELECT * FROM search_table WHERE name SEARCH 'flys' OR name SEARCH 'runs2' ;\nID          NAME\n-------------------------------------\n1           time flys\n[1] row(s) selected.\n```\n\n## 다중 언어 검색\n\n마크베이스는 ASCII와 UTF-8로 저장된 여러 가지 종류의 언어의 가변길이 문자열에 대한 검색이 가능하다. 한국어나 일본어와 같은 언어의 문장에서 일부분만을 검색하기 위해서, 2-gram 기법을 이용한다.\n\n```sql\nSELECT  column_name(s)\nFROM    table_name\nWHERE   column_name\nSEARCH  pattern;\n```\n```sql\nMach> CREATE TABLE multi_table (message varchar(100));\nCreated successfully.\n \nMach> CREATE INDEX idx_multi ON multi_table(message)INDEX_TYPE KEYWORD;\nCreated successfully.\n \nMach> INSERT INTO multi_table VALUES(\"Machbase is the combination of ideal solutions\");\n1 row(s) inserted.\n \nMach> INSERT INTO multi_table VALUES(\"Machbase is a columnar DBMS\");\n1 row(s) inserted.\n \nMach> INSERT INTO multi_table VALUES(\"Machbaseは理想的なソリューションの組み合わせです\");\n1 row(s) inserted.\n \nMach> INSERT INTO multi_table VALUES(\"Machbaseは円柱状のDBMSです\");\n1 row(s) inserted.\n \nMach>  SELECT * from multi_table WHERE message SEARCH 'Machbase DBMS';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbaseは円柱状のDBMSです\nMachbase is a columnar DBMS\n[2] row(s) selected.\n \nMach> SELECT * from multi_table WHERE message SEARCH 'DBMS is';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbase is a columnar DBMS\n[1] row(s) selected.\n \nMach> SELECT * from multi_table WHERE message SEARCH 'DBMS' OR message SEARCH 'ideal';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbaseは円柱状のDBMSです\nMachbase is a columnar DBMS\nMachbase is the combination of ideal solutions\n[3] row(s) selected.\n \nMach> SELECT * from multi_table WHERE message SEARCH '組み合わせ';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbaseは理想的なソリューションの組み合わせです\n[1] row(s) selected.\nElapsed time: 0.001\nMach> SELECT * from multi_table WHERE message SEARCH '円柱';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbaseは円柱状のDBMSです\n[1] row(s) selected.\n```\n\n입력된 데이터가 \"대한민국\" 인 경우,  \"대한\", \"한민\", \"민국\"의 세 단어가 인덱스에 기록된다.  그러므로 \"대한\" 또는 \"민국\" 키워드로도 \"대한민국\"을 검색할 수 있다.\n\n기본적으로 search문에서 입력받은 키워드들은 AND조건으로 검색되므로, 세 단어만 입력하더라도 결과는 매우 정확하게 표시된다. 예를 들어, 검색 대상 키워드가 \"computer utilization guide\"인 경우, 세 단어 \"computer\", \"utilization\", \"guide\"가 AND 조건으로 설정되므로 세 단어가 한 데이터에서 모두 사용된 칼럼값만 표시된다.\n\n## ESEARCH\n\nESEARCH 연산자는 검색 대상 키워드를 확장하여 검색하기 위해 사용된다. 검색 대상 키워드는 반드시 ASCII여야 한다. 검색 키워드를 %문자를 이용하여 설정할 수 있다.\n\nLIKE조건절처럼 %문자로 시작되는 키워드를 이용하면, 모든 레코드를 검색해야 하지만 키워드 색인 내의 단어들에 대해서 이 조건을 검색하기 때문에, LIKE보다 빠르게 검색할 수 있다. \n\n이 기능은 알파벳 문자열(에러 문장 또는 코드등)을 빠르게 검색할 경우에 유용하다.\n\n```sql\nSELECT  column_name(s)\nFROM    table_name\nWHERE   column_name\nESEARCH pattern;\n```\n```sql\nMach> CREATE TABLE esearch_table(id INTEGER, name VARCHAR(20), data VARCHAR(40));\nCreated successfully.\n \nMach> CREATE INDEX idx1 ON esearch_table(name) INDEX_TYPE KEYWORD;\nCreated successfully.\n \nMach> CREATE INDEX idx2 ON esearch_table(data) INDEX_TYPE KEYWORD;\nCreated successfully.\n \nMach> INSERT INTO esearch_table VALUES(1, 'machbase', 'Real-time search technology');\n1 row(s) inserted.\n \nMach> INSERT INTO esearch_table VALUES(2, 'mach2flux', 'Real-time data compression');\n1 row(s) inserted.\n \nMach> INSERT INTO esearch_table VALUES(3, 'DB MS', 'Memory cache technology');\n1 row(s) inserted.\n \nMach> INSERT INTO esearch_table VALUES(4, 'ファ ッションアドバイザー、', 'errors');\n1 row(s) inserted.\n \nMach> INSERT INTO esearch_table VALUES(5, '인피 니 플럭스', 'socket232');\n1 row(s) inserted.\n \nMach> SELECT * FROM esearch_table WHERE name ESEARCH '%mach%';\nID          NAME                  DATA                                     \n--------------------------------------------------------------------------------\n2           mach2flux             Real-time data compression               \n1           machbase              Real-time search technology      \n \nMach> SELECT * FROM esearch_table where data ESEARCH '%echn%';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n3           DB MS                 Memory cache technology\n1           machbase            Real-time search technology\n[2] row(s) selected.\n \nMach> SELECT * FROM esearch_table where name ESEARCH '%피니%럭스';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n[0] row(s) selected.\n \nMach> SELECT * FROM esearch_table where data ESEARCH '%232';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n5           인피 니 플럭스  socket232\n[1] row(s) selected.\n```\n\n## REGEXP\n\nREGEXP 연산자는 정규표현식을 통하여 데이터에 대한 텍스트 검색을 수행하기 위해서 사용된다. REGEXP 연산자는 대상 칼럼에 정규표현식을 수행하여 실행되며, 색인을 사용할 수 없기 때문에, 검색 성능이 저하될 수 있다. \n\n따라서 검색 속도를 향상시키기 위해 색인을 사용할 수 있는 다른 검색 조건을 AND 연산자로 추가하여 사용하는 것이 좋다.\n\n특정한 정규표현식 패턴으로 검색하기 전에 인덱스를 사용할 수 있는 SEARCH 또는 ESEARCH 연산자를 먼저 적용하고, 결과집합을 축소시킨 다음 REGEXP를 사용하는 것이 검색 성능을 향상시킬 수 있는 방법이다.\n\n```sql\nMach> CREATE TABLE regexp_table(id INTEGER, name VARCHAR(20), data VARCHAR(40));\nCreated successfully.\n \nMach> INSERT INTO regexp_table VALUES(1, 'machbase', 'Real-time search technology');\n1 row(s) inserted.\n \nMach> INSERT INTO regexp_table VALUES(2, 'mach2base', 'Real-time data compression');\n1 row(s) inserted.\n \nMach> INSERT INTO regexp_table VALUES(3, 'DBMS', 'Memory cache technology');\n1 row(s) inserted.\n \nMach> INSERT INTO regexp_table VALUES(4, 'ファ ッショ', 'errors');\n1 row(s) inserted.\n \nMach> INSERT INTO regexp_table VALUES(5, '인피니플럭스', 'socket232');\n1 row(s) inserted.\n \nMach> SELECT * FROM regexp_table WHERE name REGEXP 'mach';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n2           mach2base           Real-time data compression\n1           machbase            Real-time search technology\n[2] row(s) selected.\n \nMach> SELECT * FROM regexp_table WHERE data REGEXP 'mach[1]';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n[0] row(s) selected.\n \nMach> SELECT * FROM regexp_table WHERE data REGEXP '[A-Za-z]';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n5           인피니플럭스  socket232\n4           ファ ッショ      errors\n3           DBMS                  Memory cache technology\n2           mach2base           Real-time data compression\n1           machbase            Real-time search technology\n[5] row(s) selected.\n```\n\n## LIKE\n\n마크베이스는 SQL표준의 LIKE연산자도 지원한다. \n\nLIKE연산자에 한국어, 일본어, 중국어도 사용 가능하다.\n\n```sql\nSELECT  column_name(s)\nFROM    table_name\nWHERE   column_name\nLIKE    pattern;\n```\n\nExample:\n```sql\nMach> CREATE TABLE like_table (id INTEGER, name VARCHAR(20), data VARCHAR(40));\nCreated successfully.\n \nMach> INSERT INTO like_table VALUES(1, 'machbase', 'Real-time search technology');\n1 row(s) inserted.\n \nMach> INSERT INTO like_table VALUES(2, 'mach2base', 'Real-time data compression');\n1 row(s) inserted.\n \nMach> INSERT INTO like_table VALUES(3, 'DBMS', 'Memory cache technology');\n1 row(s) inserted.\n \nMach> INSERT INTO like_table VALUES(4, 'ファ ッションアドバイザー、', 'errors');\n1 row(s) inserted.\n \nMach> INSERT INTO like_table VALUES(5, '인피 니 플럭스', 'socket232');\n1 row(s) inserted.\n \nMach> SELECT * FROM like_table WHERE name LIKE 'mach%';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n2           mach2base           Real-time data compression\n1           machbase            Real-time search technology\n[2] row(s) selected.\n \nMach> SELECT * FROM like_table WHERE name LIKE '%니%';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n5           인피 니 플럭스  socket232\n[1] row(s) selected.\n \nMach> SELECT * FROM like_table WHERE data LIKE '%technology';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n3           DBMS                  Memory cache technology\n1           machbase            Real-time search technology\n[2] row(s) selected.\n```"
					}
					
				
		
				
					,
					
					"install-linux-tgz-install-html": {
						"id": "install-linux-tgz-install-html",
						"title": "Tarball 설치",
						"version": "all",
						"categories": "",
						"url": " /install/linux/tgz-install.html",
						"content": "* [사용자 생성](#사용자-생성)\n* [패키지 설치](#패키지-설치)\n* [환경변수 설정](#환경변수-설정)\n* [마크베이스 프로퍼티 설정](#마크베이스-프로퍼티-설정)\n* [마크베이스 간단 사용](#마크베이스-간단-사용)\n\n# 사용자 생성\n\n마크베이스 설치 및 사용을 위한 리눅스 사용자 **'machbase'** 를 생성한다.\n\n```bash\nsudo useradd machbase\n```\n\n패스워드를 설정한 다음, machbase 계정으로 접속한다.\n\n\n# 패키지 설치\n\n**machbase_home** 디렉터리를 생성하고, 마크베이스 다운로드 사이트에서 패키지를 다운르도 받아서 설치한다.\n\n```bash\n[machbase@localhost ~]$ wget http://www.machbase.com/dist/machbase-fog-x.x.x.official-LINUX-X86-64-release.tgz\n[machbase@localhost ~]$ mkdir machbase_home\n[machbase@localhost ~]$ mv machbase-fog-x.x.x.official-LINUX-X86-64-release.tgz machbase_home/\n[machbase@localhost ~]$ cd machbase_home/\n[machbase@localhost machbase_home]$ tar zxf machbase-fog-x.x.x.official-LINUX-X86-64-release.tgz\n \n[machbase@loclahost machbase_home]$ ls -l\ndrwxrwxr-x  5 machbase machbase        64 Oct 30 16:10 3rd-party\ndrwxrwxr-x  2 machbase machbase      4096 Oct 30 16:10 bin\ndrwxrwxr-x  2 machbase machbase       306 Jan  2 11:36 conf\ndrwxrwxr-x  2 machbase machbase       136 Jan  2 11:37 dbs\ndrwxrwxr-x  3 machbase machbase        22 Oct 30 16:10 doc\ndrwxrwxr-x  2 machbase machbase        96 Oct 30 16:10 include\ndrwxrwxr-x  2 machbase machbase        29 Oct 30 16:10 install\ndrwxrwxr-x  2 machbase machbase       283 Oct 30 16:10 lib\n-rw-rw-r--  1 machbase machbase 139888377 Dec 20 11:33 machbase-fog-x.x.x.official-LINUX-X86-64-release.tgz\ndrwxrwxr-x  2 machbase machbase        22 Dec 21 15:43 msg\n \ndrwxrwxr-x  2 machbase machbase         6 Oct 30 16:10 package\ndrwxrwxr-x 12 machbase machbase       140 Oct 30 16:10 sample\ndrwxrwxr-x  2 machbase machbase      4096 Jan  2 09:37 trc\ndrwxrwxr-x 10 machbase machbase       160 Oct 30 16:10 tutorials\ndrwxrwxr-x  3 machbase machbase        19 Oct 30 16:10 webadmin\n \n[machbase@loclahost machbase_home]$\n```\n\n설치된 디렉터리 설명은 다음과 같다.\n\n|디렉터리|설명|\n|--|--|\n|bin| 실행 파일들|\n|conf| 설정 파일들|\n|dbs|데이터 저장 공간|\n|doc|라이선스 파일들|\n|include|CLI 프로그램을 위한 각종 헤더 파일들|\n|install|Makefile을 위한 mk 파일|\n|lib|각종 라이브러리|\n|msg|Machbase 서버 에러 메시지|\n|package|추가된 패키지를 저장할 경로 (Cluster Edition)|\n|sample|각종 예제 파일들|\n|trc|Machbase 서버 로그 및 추적 내용들|\n|3rd-party| grafana 연동 파일들|\n\n\n# 환경변수 설정\n\n.bashrc 파일에 마크베이스 관련 환경 변수를 추가한다.\n\n```bash\nexport MACHBASE_HOME=/home/machbase/machbase_home\nexport PATH=$MACHBASE_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$MACHBASE_HOME/lib:$LD_LIBRARY_PATH\n \n# 변경사항을 아래 명령어로 적용한다.\nsource .bashrc\n```\n\n\n# 마크베이스 프로퍼티 설정\n\n$MACHBASE\\_HOME/conf 디렉터리에 machbase.conf.sample  파일이 있다.\n\n```bash\n\n[machbase@localhost ~]$ cd $MACHBASE_HOME/conf\n[machbase@localhost conf]$ ls -l\n-rw-rw-r-- 1 machbase machbase   106 Oct 30 16:10 machtag.sql.sample\n-rw-rw-r-- 1 machbase machbase 17556 Oct 30 16:10 machbase.conf.sample\n \n[machbase@localhost conf]$\n```\n\n또한 리눅스 환경 변수를 이용하여 마크베이스 접속 포트번호를 변경할 수도 있다. 아래는 디폴트값(5656)이 아닌 다른 포트번호(7878)로 변경하는 예이다.\n\n```bash\nexport MACHBASE_PORT_NO=7878\n```\n\n\n# 마크베이스 간단 사용\n\n## 데이터베이스 생성\n\n데이터베이스 생성은 **machadmin** 유틸리티를 이용한다. --help 옵션으로 명령어를 볼 수 있다.\n\n```bash\n[machbase@localhost machbase_home]$ machadmin --help\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\n>\n  -u, --startup                         Startup Machbase server.\n      --recovery[=simple,complex,reset] Recovery mode. (default: simple)\n  -s, --shutdown                        Shutdown Machbase server.\n  -c, --createdb                        Create Machbase database.\n  -d, --destroydb                       Destroy Machbase database.\n  -k, --kill                            Terminate Machbase server.\n  -i, --silence                         Produce less output.\n  -r, --restore                         Restore Machbase database.\n  -x, --extract                         Extract BackupFile to BackupDirectory.\n  -w, --viewimage                       Display information of BackupImageFile.\n  -e, --check                           Check whether Machbase Server is running.\n  -t, --licinstall                      Install the license file.\n  -f, --licinfo                         Display information of installed license file.\n \n[machbase@localhost machbase_home]$\n```\n\n-c 옵션으로 데이터베이스를 생성한다.\n\n```bash\n\n[machbase@localhost machbase_home]$ machadmin -c\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDatabase created successfully.\n[machbase@localhost machbase_home]$\n```\n\n\n## 마크베이스 서버 실행\n\n-u 옵션으로 마크베이스 서버를 실행한다.\n\n```bash\n\n[machbase@localhost machbase_home]$ machadmin -u\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for Machbase server start.\nMachbase server started successfully.\n[machbase@localhost machbase_home]$\n```\n\nps 명령을 통해 아래와 같이 서버 데몬인 machbased 가 구동된 것을 확인할 수 있다.\n\n```bash\n[machbase@localhost machbase_home]$  ps -ef |grep machbased\nmachbase 11178     1  2 11:25 ?        00:00:01 /home/machbase/machbase_home/bin/machbased -s --recovery=simple\nmachbase 11276  9867  0 11:26 pts/1    00:00:00 grep --color=auto machbased\n[machbase@localhost machbase_home]$\n```\n\n\n## 마크베이스 서버 접속\n\n**machsql** 이라는 접속 유틸리티를 이용하여 마크베이스 서버에 접속한다. \n\n관리자 계정인 **SYS** 가 준비되어 있으며, 패스워드는 **MANAGER** 로 설정되어 있다.\n\n```bash\n[machbase@localhost machbase_home]$  machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version x.x.x.official\n     Copyright 2014 MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase server address (Default:127.0.0.1) :\nMachbase user ID  (Default:SYS)\nMachbase User Password :\nMACHBASE_CONNECT_MODE=INET, PORT=5656\nType 'help' to display a list of available commands.\nMach>\n```\n\n간단한 테이블 생성 및 데이터를 입력, 출력 해보자.\n\n```sql\ncreate table hello( id integer );\ninsert into hello values( 1 );\ninsert into hello values( 2 );\nselect * from hello;\nselect _arrival_time, * from hello;\n```\n\n```sql\nMach> create table hello( id integer );\nCreated successfully.\nElapsed time: 0.054\nMach> insert into hello values( 1 );\n1 row(s) inserted.\nElapsed time: 0.000\nMach> insert into hello values( 2 );\n1 row(s) inserted.\nElapsed time: 0.000\nMach> select * from hello;\nID\n--------------\n2\n1\n[2] row(s) selected.\nElapsed time: 0.000\nMach> select _arrival_time, * from hello;\n_arrival_time                   ID\n-----------------------------------------------\n2019-01-02 11:33:00 122:806:804 2\n2019-01-02 11:32:57 383:848:361 1\n[2] row(s) selected.\nElapsed time: 0.000\nMach>\n```\n\n위의 SELECT 결과를 보면 최근에 입력된 데이터가 가장 먼저 표시되는 것을 확인할 수 있다.\n또한 \\_arrival\\_time 칼럼을 통해 해당 레코드가 입력된 시간이 나노초 단위까지 설정된 것을 알 수 있다.\n\n## 마크베이스 서버 중단\n\n-s 옵션으로 마크베이스 서버를 종료한다.\n\n```bash\n[machbase@localhost machbase_home]$ machadmin -s\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for Machbase server shut down...\nMachbase server shut down successfully.\n[machbase@localhost machbase_home]$\n```\n\n## 데이터베이스 삭제\n\n-d 옵션으로 데이터베이스를 삭제한다.\n\n**모든 데이터가 삭제되므로 매우 주의해서 사용해야 한다.**\n\n\n```bash\n[machbase@localhost machbase_home]$ machadmin -d\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDestroy Machbase database. Are you sure?(y/N) y\nDatabase destoryed successfully.\n[machbase@localhost machbase_home]$\n```"
					}
					
				
		
				
					,
					
					"sdk-timezone-html": {
						"id": "sdk-timezone-html",
						"title": "Timezone",
						"version": "all",
						"categories": "",
						"url": " /sdk/timezone.html",
						"content": "# 마크베이스의 Timezone\n마크베이스는 각 클라이언트의 타임존을 세션 단위에서 유효한 것으로 가정한다.\n\n일반적으로는 특정한 시간을 나타내는 스트링으로 타임존을 지정하는 방식을 사용한다.\n```\n\"YYYY-MM-DD HH24:MI:SS ZZZ(타임존스트링)\"\n\n예) \n\"12:06:56.568+01:00\"  \n\"2006.07.10 at 15:08:56 -05:00\"\n\"09  AM, GMT+09:00\"\n```\n그러나 위의 방식은 특정 시간을 매번 타임존을 기준으로 지정해야 하는 불편함이  있을 뿐만 아니라, 대량의 데이터에 대해서 모두 타임존의 값이 포함된 경우 데이터 전송량이 선형적으로 늘어나는 문제가  있다.\n\n따라서 마크베이스에서는 클라이언트와 서버가 연결된 세션에 대해 타임존 속성을 지정하는 방식을 지원한다.\n\n다음은, 마크베이스에서 제공하는 타임존이 동작에 대한 단계적 설명이다.\n\n- 서버는 그 서버가 설치된 운영체제에서 제공하는 기본 타임존을 기준으로 동작한다.\n즉, 아무런 설정을 하지  않았을 경우 마크베이스는 해당 운영체제가 동작하는 타임존을 읽어서 활용한다.\n\n- 클라이언트 프로그램에서 타임존 설정없이 서버로 접속하면 해당 클라이언트의 타임존의 서버의 타임존으로 설정된다.\n즉, 서버에서 설정된 TIMEZONE이 KST이라면, 클라이언트 역시 KST로 동작한다는 뜻이다.\n\n- 클라이언트 프로그램에서 타임존을 명시적으로 설정한 경우에는 해당 서버의 해당 세션은 클라이언트에서 지정한 타임존으로 동작한다.\n즉, 서버에서  설정된 TIMEZONE이 KST이라 하더라도, 만일 클라이언트가 접속시 EDT로 타임존을 설정한 경우에는 해당 세션은 EDT로 동작한다.\n\n# 마크베이스의 Timezone 지원 형식\n마크베이스는 사용상의 편의성 증진과 복잡성을 제거하기  위해 5자리의 문자로 구성된 단 한가지 포맷만을 제공한다.\n\n즉, 첫번째 문자는 + 혹은 -의 기호로 시간의 부호를 나타내고, 이어지는 두개의 문자는 00에서 23 사이의 값을 가진다. 그리고, 마지막 두개의 문자는 00에서 59까지의 시간을 가지는 것으로 한다.\n\n아래는 마크베이스에서 지원하는 TIMEZONE의 형식을 나타낸  것이다.\n```\nex)\nTIMEZONE=+0900\nTIMEZONE=-0900\n```\n\n# machsql\nmachsql은 구동시 아래와 같은 옵션을 통해 동작할 타임존을 설정할 수 있다.\n```\n-z, --timezone=+-HHMM\n```\nSHOW TIMEZONE 명령을 통해 현재 자신이 설정된 타임존을 확인할 수 있다.\n```\nSHOW TIMEZONE;\n\nMach> show timezone;\nTimezone : +0900\n```\n# machloader\nmachloader는  구동시 아래와 같은 옵션을 통해 동작할 타임존을 설정할 수 있다.\n```\n-z, --timezone=+-HHMM\n```\n지정된 타임존으로 접속하고, 시간 연산도 해당 타임존을 기준으로 동작한다.\n\n# SDK\n연결 스트링에 TIMEZONE이 추가되었으며, 해당 세션에 대한 타임존을 지정할 수 있다.\n\n만일 연결 스트링에 TIMEZONE을 지정하지 않을 경우에는 서버의 타임존을 기준으로 동작한다.\n\n이는 CLI, ODBC, JDBC, DOTNET 모두 동일한다.\n\n`연결 string 예제`\n```\nSERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;NLS_USE=UTF8;PORT_NO=5656;TIMEZONE=+0300\n```\n# Rest API\nRest API는 동작 요청시 HTTP 프로토콜의 HEADER에서 지정된 타임존을 기준으로 동작한다.\n\n그 헤더의 이름은 `The-Timezone-Machbase으로 명명되었으며,  사용법은 아래와 같다.`\n```\nAuthorization: Basic XXXXXXXXXXXXXXXXXXX\n...................\nThe-Timezone-Machbase: +0900\n...............\n```\n앞에서 기술한 바와 같이 원하는 타임존 스트링을 지정하면 된다.\n\n타임존을 지정하지  않았을 경우에는 서버의 타임존으로 동작한다.\n\n요청 예제 : UTC로 설정\n```\ncurl -H \"The-Timezone-Machbase: +0000\" -G \"http://127.0.0.1:${ITF_HTTP_PORT}/machbase\" --data-urlencode 'q=select * from test_table order by c4 asc';\n \n \n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"C1\",\n      \"type\": 4,\n      \"length\": 6\n    },\n    {\n      \"name\": \"C2\",\n      \"type\": 8,\n      \"length\": 11\n    },\n    {\n      \"name\": \"C3\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"C4\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"C5\",\n      \"type\": 32,\n      \"length\": 15\n    }\n  ],\n  \"timezone\": \"+0000\",\n  \"data\": [\n    {\n      \"C1\": 1,\n      \"C2\": 2,\n      \"C3\": \"test1\",\n      \"C4\": \"1999-09-09 00:09:09 000:000:000\",\n      \"C5\": \"127.0.0.1\"\n    },\n  ]\n}\n```\n결과 JSON에 \"timezone\" 항목에 설정된 타임존 값이 되돌아온다."
					}
					
				
		
				
					,
					
					"tools-tools-html": {
						"id": "tools-tools-html",
						"title": "도구",
						"version": "all",
						"categories": "",
						"url": " /tools/tools.html",
						"content": "마크베이스는 다양한 커맨드 라인 도구와 웹 기반 관리 도구, 데이터 수집 도구를 제공한다.\n\n* [Utilities](./utilities/utilities.md)"
					}
					
				
		
				
					,
					
					"upgrade-upgrade-html": {
						"id": "upgrade-upgrade-html",
						"title": "Cluster Edition 업그레이드 및 복구",
						"version": "all",
						"categories": "",
						"url": " /upgrade/upgrade.html",
						"content": "# 목차\n\n* [Coordinator 업그레이드](#coordinator-업그레이드)\n    * [주의사항](#주의사항)\n    * [Coordinator 종료](#coordinator-종료)\n    * [Coordinator 백업 (Optional)](#coordinator-백업-optional)\n    * [Coordinator 업그레이드](#coordinator-업그레이드)\n    * [Coordinator 시작](#coordinator-시작)\n* [Deployer 업그레이드](#deployer-업그레이드)\n    * [주의사항](#주의사항)\n    * [Deployer 종료](#deployer-종료)\n    * [Deployer 백업 (Optional)](#deployer-백업-optional)\n    * [Deployer 업그레이드](#deployer-업그레이드)\n    * [Deployer 시작](#deployer-시작)\n* [Package 등록](#package-등록)\n* [Broker/Warehouse 업그레이드](#brokerwarehouse-업그레이드)\n    * [Node 종료](#node-종료)\n    * [Node 업그레이드](#node-업그레이드)\n    * [Node 구동](#node-구동)\n* [Snapshot Failover](#snapshot-failover)\n    * [Snapshot 기본 개념](#snapshot-기본-개념)\n    * [Snapshot Failover 동작 방식](#snapshot-failover-동작-방식)\n    * [Snapshot 자동 수행](#snapshot-자동-수행)\n    * [Snapshot 수동 수행](#snapshot-수동-수행)\n    * [Scrapped node 를 Snapshot 기반으로 복구](#scrapped-node-를-snapshot-기반으로-복구)\n    * [Scrapped node의 Snapshot 기반 복구 과정](#scrapped-node의-snapshot-기반-복구-과정)\n    * [Snapshot 관련 Property](#snapshot-관련-property)\n\n\n# Coordinator 업그레이드\n\nCoordinator / Deployer 는 부득이하게 수동으로 업그레이드 해야 한다.\n\n## 주의사항\n\n* DDL 또는 DELETE 수행 중이 아니어야 한다. (INSERT, APPEND, SELECT 는 상관없다.)\n* 업그레이드 중 Node 추가/구동/종료/삭제 등의 명령을 내릴 수 없다.\n\n## Coordinator Shutdown\n\nCoordinator / Deployer 는 종료되어도 Broker / Warehouse 의 INSERT, APPEND, SELECT 에 영향을 주지 않는다.\n다만, 종료된 동안에는 Broker / Warehouse 가 도중에 죽는 것을 감지하지 못한다. (재시작 후에는 정상 감지된다.)\n\n```bash\nmachcoordinatoradmin --shutdown\n```\n\n## Coordinator 백업 (Optional)\n\n$MACH_COORDINATOR_HOME 에 있는 dbs/, conf/ 디렉토리를 백업한다.\n\n## Coordinator 업그레이드\n\n**lightweight package 가 아닌 full package 로 진행한다.**\n\nPackage 를 $MACH_COORDINATOR_HOME 에 압축을 풀어 덮어쓴다.\n\n```bash\ntar -zxvf machbase-ent-new.official-LINUX-X86-64-release.tgz -C $MACHBASE_COORDINATOR_HOME\n```\n\n## Coordinator 시작\n\n```bash\nmachcoordinatoradmin --startup\n```\n\n\n# Deployer 업그레이드\n\nCoordinator 와 동일하다.\n\n## 주의사항\n \n업그레이드 중 Node 추가/구동/종료/삭제 등의 명령을 내릴 수 없다.\n\n## Deployer 종료\n\n```bash\nmachdeployeradmin --shutdown\n```\n\n## Deployer 백업 (Optional)\n\n$MACH_DEPLOYER_HOME 에 있는 dbs/, conf/ 디렉토리를 백업한다.\n\n## Deployer 업그레이드\n\nDeployer 가 설치된 Host 에서 MWA 를 수행하거나 Collector 를 수행하지 않는다면, lightweight package 로 진행해도 무방하다.\n\nPackage 를 $MACH_DEPLOYER_HOME 에 압축을 풀어 덮어쓴다.\n\n```bash\ntar -zxvf machbase-ent-new.official-LINUX-X86-64-release.tgz -C $MACH_DEPLOYER_HOME\n```\n\n## Deployer 시작\n\n```bash\nmachdeployeradmin --startup\n```\n\n\n# Package 등록\n\nBroker/Warehouse 업그레이드를 위한 작업으로, Package 를 Coordinator 에 등록해서 업그레이드를 진행한다.\n\n> lightweight package 로 등록하는 것이 좋다.\n\n먼저, Package 를 $MACH_COORDINATOR_HOME 이 위치한 host 에 옮긴다.\n\n그 다음, 아래 명령으로 패키지를 추가한다. \n\n```bash\nmachcoordinatoradmin --add-package=new_package --file-name=./machbase-ent-new.official-LINUX-X86-64-release-lightweight.tgz\n```\n\n|옵션|설명|\n|--|--|\n|--add-package|추가할 패키지의 이름을 지정한다.|\n|--file-name|추가할 패키지 파일의 경로를 지정한다.**이미 같은 파일 이름의 package 가 추가되어 있다면 에러가 발생하므로, 파일 이름을 확인해야 한다.**|\n\n\n## Broker/Warehouse 업그레이드\n\nCoordinator 에서 다음 명령을 수행한다.\n\n# Node 종료 \n\n```bash\nmachcoordinatoradmin --shutdown-node=localhost:5656\n```\n\n# Node 업그레이드 \n\n```bash\nmachcoordinatoradmin --upgrade-node=localhost:5656 --package-name=new_package\n```\n\n|옵션|설명|\n|--|--|\n|--upgrade-node|업그레이드 대상 Node 이름을 입력한다.|\n|--package-name|업그레이드할 Package 이름을 입력한다.|\n\nNode 종료 없이 Node 업그레이드를 수행하면, 자동으로 Node 를 종료시키고 Node 업그레이드를 수행한다.\n하지만, 안정성을 위해서 Node 종료를 명시적으로 수행하도록 한다.\n\n# Node 구동\n\n```bash\nmachcoordinatoradmin --startup-node=localhost:5656\n```\n\n\n# Snapshot Failover\n\nMachbase 6.5 Cluster Edition에 Snapshot Failover 기능이 추가되었다.\n\nSnapshot Failover는 DBMS가 정상적인 상황일 때 Snapshot 을 기록해두고 특정 Warehouse의 장애 발생 시 정상인 Snapshot은 제외하고 문제가 발생한 부분에 대해서만 Failover를 수행함으로써 빠른 복구를 제공하는 기능이다.\n\n## Snapshot 기본 개념\n\nCluster Edition의 각 Group별로 Group 내 존재하는 Warehouse들 사이의 정상 데이터의 위치를 기록하는 개념이다.\n\nGroup 내의 Warehouse 에 생성된 Snapshot 이전의 데이터는 모두 정상 상태의 Data임을 보장하며 각 Group 별로 각각의 Snapshot을 기록한다.\n\n## Snapshot Failover 동작 방식\n\n특정 Warehouse에 문제가 발생했을 경우 이 Warehouse는 Scrapped 상태로 데이터 복구가 필요한 상황이 된다.\n\nSnapshot Recovery를 수행하게 되면 문제가 발생한 Warehouse에서의 정상 Snapshot을 기준으로 Snapshot 이후의 Data는 Clear하고 같은 Group 내 정상 상태의 Warehouse의 기준 Snapshot 이후 Data를 문제가 발생한 Warehouse로 Replication해서 복구를 완료한다.\n\n## Snapshot 자동 수행\n\nDefault로 Snapshot 자동 수행이 Enable 되어 있으며 Snapshot을 수행하는 주기는 60초로 설정 되어 있으며 Clustser에 Warehouse Group이 여러 개일 경우 Snapshot 주기마다 하나의 Group만 순서대로 Snapshot을 수행한다.\n\n수행 주기를 0으로 설정하면 Snapshot 자동 수행이 Disable된다.\n\nSnapshot 주기 설정은 명령어를 실행하면 즉시 반영된다.\n\n```bash\n# Snapshot 주기 설정\nmachcoordinatoradmin --snapshot-interval=[sec]\n  \n# 현재 Snapshot 주기 확인\nmachcoordinatoradmin --configuration\n```\n\n## Snapshot 수동 수행\n\nmachcoordinatoradmin tool을 이용해 group_name을 지정하고 수동으로 Snapshot을  수행한다.\n\ngroup_name은 group1, group2와 같이 미리 설정되어 있다.\n\nCluster에 Group이 여러 개일 경우 전체 Snapshot을 찍기 위해서는 모든 각각의 Group에 Snapshot을 수행해줘야 한다.\n\n```bash\n# group_name에 대한 Snapshot 수동 수행\nmachcoordinatoradmin --exec-snapshot --group='group_name'\n```\n\n## Scrapped node 를 Snapshot 기반으로 복구\n\nScrapped node 가 발생한 경우 아래와 같이 복구한다.\n\n```bash\n# 해당 group 을 readonly 로 변경\n# 이후의 단계에서 group이 normal로 변경되는 것을 방지\nmachcoordinatoradmin --set-group-state=readonly --group=[groupname]\n  \n# snapshot 기반으로 복구\nmachcoordinatoradmin --snapshot-recover=[nodename]\n  \n# replication을 통해 snapshot 이후의 최신 data를 복제\n# replication이 끝나면 warehouse 의 상태는 normal로 자동 변경\nmachcoordinatoradmin --exec-sync=[nodename]\n  \n# group 상태를 normal 로 변경\nmachcoordinatoradmin --set-group-state=normal --group=[groupname]\n```\n\n## Scrapped node의 Snapshot 기반 복구 과정\n\nSnapshot으로 Scrapped node를 복구시 아래와 같은 과정이 수행된다.\n\n```bash\n/*최초 cluster 상태*/\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30420 | group1          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n/*group1의 warehouse 0이 죽었을 때*/\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly        | scrapped      | **unknown**   | ----------- |\n| warehouse   | localhost:30420 | group1          | readonly        | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# 해당 group 을 readonly 로 변경\nmachcoordinatoradmin --set-group-state=readonly --group=[groupname]\n  \nkellen@kellen-ku:~$ machcoordinatoradmin --set-group-state=readonly --group=group1\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - 321a012d05.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nGroup Name: group1\nFlag      : 1\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly         | scrapped      | **unknown**   | ----------- |\n| warehouse   | localhost:30420 | group1          | readonly         | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n#죽은 Warehouse를 다시 startup 수행한다\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly         | scrapped      | scrapped      | ----------- |\n| warehouse   | localhost:30420 | group1          | readonly         | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# snapshot 기반으로 복구\nmachcoordinatoradmin --snapshot-recover=[nodename]\n  \nkellen@kellen-ku:~$ machcoordinatoradmin --snapshot-recover=localhost:30410\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - 321a012d05.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nNode-Name: localhost:30410\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly         | scrapped      | scrapped      | ----------- |\n| warehouse   | localhost:30420 | group1          | readonly         | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# replication을 통해 snapshot 이후의 최신 data를 복제\nmachcoordinatoradmin --exec-sync=[nodename]\n  \nkellen@kellen-ku:~$ machcoordinatoradmin --exec-sync=localhost:30410\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - 321a012d05.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nNode-Name: localhost:30410\nSource:\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly         | scrapped      | scrapped      | stopped     |\n| warehouse   | localhost:30420 | group1          | readonly         | normal        | normal        | stopped     |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly         | sync-standby  | sync-standby  | running     |\n| warehouse   | localhost:30420 | group1          | readonly         | sync-active   | sync-active   | running     |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly         | normal        | normal        | stopped     |\n| warehouse   | localhost:30420 | group1          | readonly         | normal        | normal        | stopped     |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# group 상태를 normal 로 변경\nmachcoordinatoradmin --set-group-state=normal --group=[groupname]\n  \nkellen@kellen-ku:~$ machcoordinatoradmin --set-group-state=normal --group=group1\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - 321a012d05.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nGroup Name: group1\nFlag      : 0\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | normal          | normal        | normal        | stopped     |\n| warehouse   | localhost:30420 | group1          | normal          | normal        | normal        | stopped     |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n```\n\n## Snapshot 관련 Property\n\n|Property|설명|설정위치|\n|--|--|--|\n|GROUP_SNAPSHOT_TIMEOUT_SEC|Snapshot 실행 시의 timeout 시간을 결정Default: 60 (초)최소 값 : 0 (무한 대기)최대 값 : uint32_max (초)|Coordinator, Broker, Warehouse 각각의 machbase.conf 파일 내 작성\n|"
					}
					
				
		
				
					,
					
					"sql-ref-user-html": {
						"id": "sql-ref-user-html",
						"title": "사용자 관리",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/user.html",
						"content": "# 목차\n* [CREATE USER](#create-user)\n* [DROP USER](#drop-user)\n* [ALTER USER](#alter-user)\n* [CONNECT](#connect)\n* [GRANT/REVOKE](#grantrevoke)\n* [사용자 관리 예제](#사용자-관리-예제)\n\n## CREATE USER\n\n**create_user_stmt:**\n\n![create_user_stmt](/kr/sql-ref/user_image/create_user_stmt.png)\n\n```sql\ncreate_user_stmt ::= 'CREATE USER' user_name 'IDENTIFIED BY' password\n```\n\n사용자를 생성하는 구문은 다음과 같다.\n\n```sql\n--예제\nCREATE USER new_user IDENTIFIED BY password\n```\n\n## DROP USER\n\n**drop_user_stmt:**\n\n![drop_user_stmt](/kr/sql-ref/user_image/drop_user_stmt.png)\n\n```sql\ndrop_user_stmt ::= 'DROP USER' user_name\n```\n\n사용자를 삭제하는 구문은 다음과 같다. SYS 사용자는 삭제할 수 없으며, 삭제 대상 사용자가 이미 생성한 테이블이 있을 경우에는 에러를 나타낸다.\n\n```sql\n--예제\nDROP USER old_user\n```\n\n## ALTER USER\n\n**alter_user_pwd_stmt:**\n\n![alter_user_pwd_stmt](/kr/sql-ref/user_image/alter_user_pwd_stmt.png)\n\n```sql\nalter_user_pwd_stmt ::= 'ALTER USER' user_name 'IDENTIFIED BY' password\n```\n\n사용자는 아래의 구문을 통해 비밀번호를 변경할 수 있다.\n\n```sql\n--예제\nALTER USER user1 IDENTIFIED BY password\n```\n\n## CONNECT\n\n**user_connect_stmt:**\n\n![user_connect_stmt](/kr/sql-ref/user_image/user_connect_stmt.png)\n\n```sql\nuser_connect_stmt: 'CONNECT' user_name '/' password\n```\n\n사용자는 응용 프로그램을 종료하지 않고, 다음의 구문을 통해 다른 사용자로 재접속할 수 있다.\n\n```sql\n--예제\nCONNECT user1/password;\n```\n\n\n## GRANT/REVOKE\n\n![grant_stmt](/kr/sql-ref/user_image/grant_stmt.png)\n\n![revoke_stmt](/kr/sql-ref/user_image/revoke_stmt.png)\n\n![priv_value](/kr/sql-ref/user_image/priv_value.png)\n\nGRANT 구문을 통하여 사용자에게 테이블에 대한 권한을 부여한다.\n\n\n```sql\n-- user1 에게 mytable 에 대한 SELECT 권한 부여\nGRANT SELECT ON mytable TO user1;\n \n-- user1 에게 mytable 에 대한 모든 권한 부여\nGRANT ALL ON mytable TO user1;\n```\n\nREVOKE 구문을 통하여 사용자에게 부여된 권한을 회수한다.\n\n```sql\n-- user1 에게 부여된 mytable 에 대한 UPDATE 권한 회수\nREVOKE UPDATE ON mytable FROM user1;\n \n-- user1 에게 부여된 mytable 에 대한 모든 권한 회수\nREVOKE ALL ON mytable FROM user1;\n```\n\n\n## 사용자 관리 예제\n\n위의 쿼리를 수행한 예제와 그 결과를 나타냈다.\n\n```sql\n############################################\n# Connect SYS : SYS 계정으로 접속함.\n############################################\nMach> create user demo identified by 'demo';\nCreated successfully.\n \nMach> drop user demo;\nDropped successfully.\n \nMach> create user demo1 identified by 'demo1';\nCreated successfully.\n \nMach> create user demo2 identified by 'demo2';\nCreated successfully.\n \nMach> alter user demo2 identified by 'demo22';\nAltered successfully.\n \nMach> create table demo1_table (id integer);\nCreated successfully.\n \nMach> create bitmap index demo1_table_index1 on demo1_table(id);\nCreated successfully.\n \nMach> insert into demo1_table values(99991);\n1 row(s) inserted.\n \nMach> insert into demo1_table values(99992);\n1 row(s) inserted.\n \nMach> insert into demo1_table values(99993);\n1 row(s) inserted.\n \nMach> select * from demo1_table;\nID\n--------------\n99993\n99992\n99991\n[3] row(s) selected.\n \n#Error: 자기 자신을 Drop 할 수 없음.\nMach> drop user SYS;\n[ERR-02083 : Drop user error. You cannot drop yourself(SYS).]\n \n############################################\n# Connect DEMO1\n############################################\nMach> connect demo1/demo1;\nConnected successfully.\n \n#Error: 일반 유저는 다른 사용자의 비밀번호를 바꿀 수 없다.\nMach> alter user demo2 identified by 'demo22';\n[ERR-02085 : ALTER user error. The user(DEMO2) does not have ALTER privileges.]\n \nMach> alter user demo1 identified by demo11;\nAltered successfully.\n \nMach> connect demo2/demo22;\nConnected successfully.\n \n#Error: wrong password\nMach> connect demo1/demo11234;\n [ERR-02081 : User authentication error. Invalid password (DEMO11234).]\n \n# Correct password\nMach> connect demo1/demo11;\nConnected successfully.\n \nMach> create table demo1_table (id integer);\nCreated successfully.\n \nMach> create bitmap index demo1_table_index1 on demo1_table(id);\nCreated successfully.\n \nMach> insert into demo1_table values(1);\n1 row(s) inserted.\n \nMach> insert into demo1_table values(2);\n1 row(s) inserted.\n \nMach> insert into demo1_table values(3);\n1 row(s) inserted.\n \nMach> select * from demo1_table;\nID\n--------------\n3\n2\n1\n[3] row(s) selected.\n \nMach> select * from demo1.demo1_table;\nID\n--------------\n3\n2\n1\n[3] row(s) selected.\n \n############################################\n# Connect SYS again\n############################################\nMach> connect SYS/MANAGER;\nConnected successfully.\n \nMach> select * from demo1_table;\nID\n--------------\n99993\n99992\n99991\n[3] row(s) selected.\n \nMach> select * from demo1.demo1_table;\nID\n--------------\n3\n2\n1\n[3] row(s) selected.\n \n#Error: demo1 유저에 속한 테이블이 존재함.\nMach> drop user demo1;\n[ERR-02084 : DROP user error. The user's tables still exist. Drop those tables first.]\n \nMach> connect demo1/demo11;\nConnected successfully.\n \nMach> drop table demo1_table;\nDropped successfully.\n \nMach> connect SYS/MANAGER;\nConnected successfully.\n \nMach> drop user demo1;\nDropped successfully.\n```"
					}
					
				
		
				
					,
					
					"tools-utilities-html": {
						"id": "tools-utilities-html",
						"title": "유틸리티 모음",
						"version": "all",
						"categories": "",
						"url": " /tools/utilities.html",
						"content": "마크베이스 서버의 시작, 종료, 데이터 입력 및 출력을 위해서 사용되는 유틸리티들이다.\n\n* [machadmin](/kr/tools/utilities/machadmin.html)\n* [machsql](/kr/tools/utilities/machsql.html)\n* [machloader](/kr/tools/utilities/machloader.html)\n* [csvimport/csvexport](/kr/tools/utilities/csv.html)"
					}
					
				
		
				
					,
					
					"config-monitor-virtual-table-html": {
						"id": "config-monitor-virtual-table-html",
						"title": "Virtual Table",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/virtual-table.html",
						"content": "Virtual Table은 마크베이스 서버의 다양한 운영 정보들을 테이블 형태로 표현하는 가상 테이블이다. 이 테이블들의 이름은 V$문자로 시작한다.\n\n마크베이스 서버가 어떤 상태로 동작하고 있는지를 알기 위해서 이 데이터를 읽어서 저장해 두고 이용할 수 있다. \n추가로, 이 Virtual Table 을 다른 테이블들과 JOIN 연산을 통해서 다양한 정보를 얻을 수 있다.\n\nVirtual Table 은 읽기 전용으로 사용자가 추가/삭제/갱신할 수 없다.\n\n# Session/System\n## V$PROPERTY\n\n서버에 설정된 프로퍼티 정보를 표시한다.\n\n| 컬럼 이름 | 설명           |\n| ----- | ------------ |\n| NAME  | 프로퍼티명        |\n| VALUE | 프로퍼티 값       |\n| TYPE  | 데이터 타입       |\n| DEFLT | 기본 값         |\n| MIN   | 설정할 수 있는 최소값 |\n| MAX   | 설정할 수 있는 최대값 |\n\n## V$SESSION\n\nMACHBASE 서버에 접속된 세션 정보를 표시한다.\n\n| 컬럼 이름                              | 설명                                                                                                                               |\n| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n| HOSTNAME (Cluster Only)            | 세션 연결된 HOST 이름                                                                                                                   |\n| ID                                 | 세션 식별자                                                                                                                           |\n| CLOSED                             | 연결이 닫혀있는지 여부                                                                                                                     |\n| USER_ID                            | 사용자 식별자                                                                                                                          |\n| LOGIN_TIME                         | 접속 시각                                                                                                                            |\n| CLIENT_TYPE                        | 접속 Client 타입                                                                                                                     |\n| USER_NAME                          | 사용자 이름                                                                                                                           |\n| USER_IP                            | 사용자 IP                                                                                                                           |\n| SQL_LOGGING                        | 해당 세션의 Trace Log 에 메시지를 남길지 여부Parsing, Validation, Optimization 단계에서 발생하는 에러를 남긴다.DDL을 수행한 결과를 남긴다.(위의 두 케이스 모두 남긴다) |\n| SHOW_HIDDEN_COLS                   | SELECT 시, 숨겨진 컬럼을 나타낼 것인지 여부                                                                                                     |\n| FEEDBACK_APPEND_ERROR              | APPEND 시 에러를 찾으면 곧바로 실패할 것인지 여부                                                                                                  |\n| DEFAULT_DATE_FORMAT                | Datetime 입력 시 기본 입력 포맷                                                                                                           |\n| HASH_BUCKET_SIZE                   | 쿼리 수행 시 생성할, Temp Hashtable 의 Bucket 개수                                                                                          |\n| MAX_QPX_MEM                        | 쿼리 수행 시 가용할 최대 메모리 크기                                                                                                            |\n| RS_CACHE_ENABLE                    | Result Cache 사용 여부                                                                                                               |\n| RS_CACHE_TIME_BOUND_MSEC           | Result Cache 사용 시, 결과를 저장하기 위한 최대 경과 시간                                                                                          |\n| RS_CACHE_MAX_MEMORY_PER_QUERY      | Result Cache 사용 시, 쿼리 마다 사용할 최대 메모리 크기                                                                                           |\n| RS_CACHE_MAX_RECORD_PER_QUERY      | Result Cache 사용 시, 쿼리 마다 사용할 최대 결과 개수                                                                                            |\n| RS_CACHE_APPROXIMATE_RESULT_ENABLE | Result Cache 사용 시, 정확하지 않은 쿼리의 결과를 캐싱해 갈 것인지 여부                                                                                  |\n| IDLE_TIMEOUT                       | 세션 연결 후 해당 시간 동안 Client 가 아무일도 하지 않을 시 세션 종료                                                                                     |\n| QUERY_TIMEOUT                      | 쿼리 수행 시 응답 대기 시간                                                                                                                 |\n\n## V$SESMEM\n\n세션 메모리 정보를 표시한다.\n\n| 컬럼 이름 | 설명          |\n| ----- | ----------- |\n| SID   | 세션 식별자      |\n| ID    | 메모리 매니저 식별자 |\n| USAGE | 사용 크기       |\n\n## V$SESSTAT\n\n세션의 통계 정보를 표시한다.\n\n| 컬럼 이름 | 설명        |\n| ----- | --------- |\n| SID   | 세션 식별자    |\n| ID    | 통계 정보 식별자 |\n| VALUE | 통계 정보 값   |\n\n## V$SESTIME\n\n세션의 시간 정보를 표시한다.\n\n| 컬럼 이름      | 설명             |\n| ---------- | -------------- |\n| SID        | 세션 식별자         |\n| ID         | 수행 단위 식별자      |\n| ACCUM_TICK | 누적 시간          |\n| MAX_TICK   | (각 수행 중) 최대 시간 |\n\n## V$SYSMEM\n\n시스템의 메모리 정보를 표시한다.\n\n| 컬럼 이름     | 설명           |\n| --------- | ------------ |\n| ID        | 메모리 매니저 식별자  |\n| NAME      | 메모리 매니저 이름   |\n| USAGE     | 현재 사용량       |\n| MAX_USAGE | (기록된) 최대 사용량 |\n\n## V$SYSSTAT\n\n시스템의 통계 정보를 표시한다.\n\n| 컬럼 이름 | 설명        |\n| ----- | --------- |\n| ID    | 통계 정보 식별자 |\n| NAME  | 통계 정보 이름  |\n| VALUE | 통계 정보 값   |\n\n## V$SYSTIME\n\n시스템의 시간 정보를 표시한다.\n\n| 컬럼 이름      | 설명             |\n| ---------- | -------------- |\n| ID         | 수행 단위 식별자      |\n| NAME       | 수행 단위 이름       |\n| ACCUM_TICK | 누적 시간          |\n| AVG_TICK   | (각 수행 중) 평균 시간 |\n| MIN_TICK   | (각 수행 중) 최소 시간 |\n| MAX_TICK   | (각 수행 중) 최대 시간 |\n| COUNT      | 수행 횟수          |\n\n## V$STMT\n\n사용자가 현재 실행중인 쿼리문에 대한 정보를 표시한다.\n\n| 컬럼 이름       | 설명                            |\n| ----------- | ----------------------------- |\n| ID          | 쿼리 식별자                        |\n| SESS_ID     | 쿼리를 수행한 세션 식별자                |\n| STATE       | 쿼리 상태                         |\n| RECORD_SIZE | SELECT 구문 수행 중인 경우, 결과 레코드 크기 |\n| QUERY       | 쿼리 구문                         |\n\n## V$VERSION\n\nMACHBASE 의 버전에 대한 정보를 표시한다.\n\n| 컬럼 이름                     | 설명                                       |\n| ------------------------- | ---------------------------------------- |\n| BINARY_DB_MAJOR_VERSION   | DB 메이저 버전                                |\n| BINARY_DB_MINOR_VERSION   | DB 마이너 버전                                |\n| BINARY_META_MAJOR_VERSION | META 메이저 버전                              |\n| BINARY_META_MINOR_VERSION | META 마이너 버전                              |\n| BINARY_CM_MAJOR_VERSION   | Client (Communication Level) 메이저 버전      |\n| BINARY_CM_MINOR_VERSION   | Client (Communication Level) 마이너 버전      |\n| BINARY_SIGNATURE          | DB서버 파일의 버전 명                            |\n| FILE_DB_MAJOR_VERSION     | File DB 메이저 버전                           |\n| FILE_DB_MINOR_VERSION     | File DB 메이저 버전                           |\n| FILE_META_MAJOR_VERSION   | File META 메이저 버전                         |\n| FILE_META_MINOR_VERSION   | File META 마이너 버전                         |\n| FILE_CM_MAJOR_VERSION     | File Client (Communication Level) 메이저 버전 |\n| FILE_CM_MINOR_VERSION     | File Client (Communication Level) 마이너 버전 |\n| FILE_CREATE_TIME          | 파일 생성 시각                                 |\n| EDITION                   | MACHBASE 유형                              |\n\n# Result Cache\n## V$RS_CACHE_LIST\n\n결과 캐시 목록을 표시한다.\n\n| 컬럼 이름           | 설명                          |\n| --------------- | --------------------------- |\n| AGGR_HIT_COUNT  | 집계 결과인 경우, 캐시 히트 횟수         |\n| AGGR_TOUCH_TIME | 집계 결과인 경우, 캐시를 사용하거나 생성한 시각 |\n| HIT_COUNT       | 캐시 히트 횟수                    |\n| QUERY           | 캐시를 만든 쿼리문                  |\n| RECORD_COUNT    | 결과 레코드 개수                   |\n| REFERENCE_COUNT | 현재 참조중인 세션의 개수              |\n| TABLE_COUNT     | 쿼리문과 연관된 테이블 개수             |\n| TIME_SPENT      | 결과를 생성하기까지 경과 시간            |\n| TOUCH_TIME      | 캐시를 사용하거나 생성한 시각            |\n| USER_ID         | 캐시를 생성한 사용자 식별자             |\n\n## V$RS_CACHE_STAT\n\n하나의 세션에서의 결과 캐시의 통계 정보를 표시한다.\n\n| 컬럼 이름              | 설명                |\n| ------------------ | ----------------- |\n| CACHE_COUNT        | 결과 캐시의 개수         |\n| CACHE_HIT          | 총 캐시 히트 횟수        |\n| AGGR_HIT           | 집계 결과의 총 캐시 히트 횟수 |\n| CACHE_REPLACED     | 캐시 교체 횟수          |\n| CACHE_MEMORY_USAGE | 캐시로 사용된 메모리 크기    |\n\n# Storage\n## V$STORAGE\n\n저장 시스템의 내부 정보를 표시한다.\n\n| 컬럼 이름                     | 설명                                   |\n| ------------------------- | ------------------------------------ |\n| DC_TABLE_FILE_SIZE        | 디스크 컬럼 데이터의 총 용량                     |\n| DC_INDEX_FILE_SIZE        | 인덱스 파일 데이터의 총 용량                     |\n| DC_TABLESPACE_DWFILE_SIZE | 모든 컬럼데이터를 위한 DWFILE의 총 용량            |\n| DC_KV_TABLE_FILE_SIZE     | TAGDATA 테이블의 파티션 테이블이 가지는 데이터파일 총 용량 |\n\n## V$STORAGE_MOUNT_DATABASES\n\n마운트 기능을 이용하여 마운트한 백업 데이터베이스의 정보를 표시한다.\n\n| 컬럼 이름             | 설명                     |\n| ----------------- | ---------------------- |\n| NAME              | 마운트된 데이터베이스의 이름        |\n| PATH              | 백업 파일의 위치              |\n| BACKUP_TBSID      | 백업 데이터베이스의 테이블스페이스 식별자 |\n| BACKUP_SCN        | 백업 데이터베이스의 식별자         |\n| MOUNTDB           | 백업 시간                  |\n| DB_BEGIN_TIME     | 백업 데이터베이스의 최초입력 시간     |\n| DB_END_TIME       | 백업 데이터베이스의 최종 입력 시간    |\n| BACKUP_BEGIN_TIME | 백업 실행시 시작 시간           |\n| BACKUP_END_TIME   | 백업 실행시 종료 시간           |\n| FLAG              | 프로퍼티 플래그               |\n\n## V$CACHE\n\nStorage Manager 에서 읽은 결과를 캐싱한, 캐시 객체에 대한 종합 정보를 표시한다.\n\n| 컬럼 이름     | 설명               |\n| --------- | ---------------- |\n| OBJ_COUNT | 결과집합 캐시 객체의 현재 수 |\n\n## V$CACHE_OBJECTS\n\n저장 시스템에서 읽은 결과를 캐싱한, 각 캐시 객체에 대한 정보를 표시한다.\n\n| 컬럼 이름     | 설명             |\n| --------- | -------------- |\n| OID       | 객체식별자          |\n| REF_COUNT | 참조 카운트         |\n| FLAG      | (서버 내부 사용 플래그) |\n\n## V$STORAGE_DC_TABLESPACES\n\n저장 시스템의 테이블스페이스 정보를 표시한다.\n\n| 컬럼 이름      | 설명                           |\n| ---------- | ---------------------------- |\n| NAME       | 테이블스페이스 이름                   |\n| ID         | 테이블스페이스 식별자                  |\n| FLAG       | 테이블스페이스 Property 를 나타내는 Flag |\n| REF_COUNT  | 테이블스페이스 참조 횟수                |\n| DISK_COUNT | 테이블스페이스에 속한 디스크 개수           |\n\n## V$STORAGE_DC_TABLESPACE_DISKS\n\n저장 시스템의 테이블스페이스 정보를 표시한다.\n\n| 컬럼 이름              | 설명                  |\n| ------------------ | ------------------- |\n| NAME               | 디스크 이름              |\n| ID                 | 디스크 식별자             |\n| TABLESPACE_ID      | 디스크가 속한 테이블스페이스 식별자 |\n| PATH               | 디스크의 경로             |\n| IO_THREAD_COUNT    | I/O Thread 개수       |\n| IO_JOB_COUNT       | I/O Job 개수          |\n| VIRTUAL_DISK_COUNT | 가상 디스크 개수           |\n\n## V$STORAGE_DC_DWFILES\n\n저장 시스템에서 운용하는 Double-write 파일 (DW File) 의 정보를 표시한다.\n\n| 컬럼 이름                | 설명                      |\n| -------------------- | ----------------------- |\n| TBS_ID               | 테이블스페이스 식별자             |\n| DISK_ID              | 디스크 식별자                 |\n| FILE                 | 파일의 경로                  |\n| TABLE_ID             | 테이블 식별자                 |\n| COLUMN_ID            | 컬럼 식별자                  |\n| PARTITION_ID         | 파티션 식별자                 |\n| PAGE_ID              | 페이지 식별자                 |\n| DISK_OFFSET          | 디스크 오프셋                 |\n| DISK_IMAGE_SIZE      | 디스크 이미지 크기              |\n| HEAD_CRC32CODE_IMAGE | CRC32 Code 의 Head Image |\n| TAIL_CRC32CODE_IMAGE | CRC32 Code 의 Tail Image |\n| CRC32CODE_PAGE       | CRC32 Code 의 Page       |\n| HEAD_TIMESTAMP_PAGE  | Timestamp 의 Head Page   |\n| TAIL_TIMESTAMP_PAGE  | Timestamp 의 TailPage    |\n\n## V$STORAGE_DC_PAGECACHE\n\n저장 시스템에서 운용하는 Page Cache 에 대한 정보를 표시한다.\n\n| 컬럼 이름        | 설명                     |\n| ------------ | ---------------------- |\n| MAX_MEM_SIZE | Page Cache 의 최대 메모리 크기 |\n| CUR_MEM_SIZE | Page Cache 의 현재 메모리 크기 |\n| PAGE_CNT     | 캐싱된 페이지 개수             |\n| CHECK_TIME   | 검사 시간                  |\n\n## V$STORAGE_DC_PAGECACHE_LRU_LST\n\n저장 시스템에서 운용하는 Page Cache 의 LRU List 에 대한 정보를 표시한다.\n\n| 컬럼 이름        | 설명                  |\n| ------------ | ------------------- |\n| SIZE         | 페이지 크기              |\n| REF_CNT      | 참조 횟수               |\n| PARTITION_ID | 파티션 식별자             |\n| OFFSET       | Page Cache 의 Offset |\n| OBJECT_ID    | 객체 식별자              |\n| LEVEL        | 파티션 레벨              |\n\n## V$STORAGE_USAGE\n\n저장 시스템에서 사용 중인 스토리지의 사용량을 표시한다.\n\n| 컬럼 이름       | 설명                                                     |\n| ----------- | ------------------------------------------------------ |\n| TOTAL_SPACE | $MACHBASE_HOME/dbs 디렉터리가 위치한 스토리지의 총 용량                |\n| USED_SPACE  | $MACHBASE_HOME/dbs 디렉터리가 위치한 스토리지의 사용량                 |\n| USED_RATIO  | 사용량 비율(%)                                              |\n| RATIO_CAP   | 스토리지 사용량 한계. USED_RATIO이 이 한계에 도달하면 데이터 입력/인덱스 구축이 멈춤. |\n\n## V$STORAGE_TABLES\n\n테이블의 상세 정보를 표시한다.\n\n| 컬럼 이름         | 설명                                                                                                                                                                                                         |\n| ------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| ID            | 테이블의 ID                                                                                                                                                                                                    |\n| TYPE          | 테이블 형태Persistent: LOG 테이블과 TAG 테이블Volatile: 휘발성(Volatile) 테이블Key-Value: TAG 테이블의 부속 테이블                                                                                                        |\n| STATUS        | 현재 상태Creating...: CREATE TABLE로 테이블 생성 진행중Normal: 정상Predrop: DROP TABLE 명령 접수 상태Dropping...: DROP TABLE 명령 수행 상태Dropped: DROP TABLE 명령 완료 상태Mounted: 백업된 데이터베이스를 mount 명령으로 불러온 상태 |\n| STORAGE_USAGE | 해당 테이블이 스토리지에서 점유한 용량                                                                                                                                                                                      |\n\n# Log Table\n## V$STORAGE_DC_TABLES\n\nLog Table 에 대한 내부 정보를 표시한다.\n\n| 컬럼 이름                | 설명                                         |\n| -------------------- | ------------------------------------------ |\n| ID                   | 테이블의 식별자                                   |\n| DATABASE_ID          | 데이터베이스 식별자                                 |\n| CREATE_SCN           | 생성 당시의 시스템 변경 번호 (System Change Number)    |\n| UPDATE_SCN           | 최근 변경 당시의 시스템 변경 번호 (System Change Number) |\n| DDL_REF_COUNT        | DDL 구문 수행으로, 해당 테이블을 참조하고 있는 세션의 개수.       |\n| BEGIN_RID            | 테이블의 최소 RID                                |\n| END_RID              | 테이블의 마지막 Row ID + 1                        |\n| BEGIN_META_RID       | 메타 정보를 기록하기 시작한 시점의 ID                     |\n| END_META_RID         | 메타 정보의 기록이 종료한 시점의 ID                      |\n| END_SYNC_RID         | 디스크에 기록된 마지막 Row ID + 1                    |\n| FLAG                 | Table Property 를 나타내는 Flag                 |\n| COLUMN_COUNT         | 테이블의 컬럼 수                                  |\n| INDEX_COUNT          | 테이블의 인덱스 수                                 |\n| INDEX_MIN_END_RID    | 인덱스에 기록된 마지막 RID + 1                       |\n| LAST_ARRIVAL_TIME    | 마지막으로 기록된 \\_arrival_time 값                 |\n| LAST_CHECKPOINT_TIME | 마지막으로 Checkpoint 를 지난 시점                   |\n| TYPE                 | 테이블 유형                                     |\n| REMAINING_ROW_COUNT  | 자동 삭제 기능 사용시, 삭제되지 않는 레코드의 수               |\n| KEPT_DURATION        | 자동 삭제 기능 사용시, 데이터를 유지할 기간                  |\n\n## V$STORAGE_DC_TABLES_STAT\n\nLog Table 에 대한 내부 정보를 표시한다.\n\n| 컬럼 이름         | 설명          |\n| ------------- | ----------- |\n| TABLESPACE_ID | 테이블스페이스 식별자 |\n| TABLE_ID      | 테이블 식별자     |\n| COUNT         | 레코드 개수      |\n| COLUMN_ID     | 컬럼 식별자      |\n\n## V$STORAGE_DC_TABLE_COLUMNS\n\nLog Table 의 컬럼에 대한 정보를 표시한다.\n\n| 컬럼 이름                     | 설명                              |\n| ------------------------- | ------------------------------- |\n| TABLE_ID                  | 테이블 식별자                         |\n| DATABASE_ID               | 데이터베이스 식별자                      |\n| ID                        | 컬럼 식별자                          |\n| FLAG                      | 프로퍼티 플래그                        |\n| SIZE                      | 컬럼의 데이터 크기                      |\n| PARTITION_VALUE_COUNT     | 파티션에 저장되는 최대 데이터 수              |\n| PAGE_VALUE_COUNT          | 페이지에 저장되는 최대 데이터 수              |\n| CACHE_VALUE_COUNT         | 캐시 값의 최대 수                      |\n| MINMAX_CACHE_SIZE         | 컬럼 파티션에 대한 MIN/MAX 캐시의 최대 크기    |\n| CUR_APPEND_PARTITION_ID   | 현재 입력을 진행중인 파티션의 식별자            |\n| CUR_CACHE_PARTITION_COUNT | 현재 캐시에 데이터를 읽어들인 파티션의 수         |\n| CUR_MINMAX_CACHE_SIZE     | 현재 MIN/MAX캐시의 크기                |\n| END_RID_FOR_DEFAULT_VALUE | 이 값보다 작은 RID를 갖는 컬럼은 디폴트값으로 지정됨 |\n| DISK_FILE_SIZE            | 해당 컬럼에 대한 컬럼 파티션 데이터 파일의 전체 크기  |\n| MEMORY_TOTAL_SIZE         | 테이블이 사용 중인 메모리 크기               |\n| MEMORY_ALLOC_SIZE         | 테이블이 할당받은 메모리 크기                |\n\n## V$STORAGE_DC_TABLE_COLUMN_PARTS\n\nLog Table 의 컬럼 파티션 정보를 표시한다.\n\n| 컬럼 이름                         | 설명                                                                                 |\n| ----------------------------- | ---------------------------------------------------------------------------------- |\n| TABLE_ID                      | 테이블 식별자                                                                            |\n| DATABASE_ID                   | 데이터베이스 식별자                                                                         |\n| COLUMN_ID                     | 컬럼 식별자                                                                             |\n| ID                            | 파티션 식별자                                                                            |\n| FLAG                          | 컬럼 Property 를 나타내는 Flag                                                            |\n| BEGIN_RID                     | 파티션에 저장된 최소 RID                                                                    |\n| END_RID                       | 파티션에 저장된 최종 RID                                                                    |\n| END_SYNC_RID                  | SYNC가 끝난 최종 RID.시작 RID 보다 크고 마지막 SYNC RID 보다 작은 RID 를 갖는 데이터는 파티션 파일에 기록되어 있다. |\n| MIN_TIME                      | 컬럼 파티션에 최초로 데이터를 입력한 시간                                                            |\n| MAX_TIME                      | 컬럼 파티션에 마지막으로 데이터를 입력한 시간                                                          |\n| MAX_VALUE_COUNT_PER_PARTITION | 파티션의 최대 데이터 수                                                                      |\n| MAX_VALUE_COUNT_PER_PAGE      | 페이지당 최대 데이터 수                                                                      |\n| MAX_PAGE_COUNT                | 파티션당 최대 페이지의 수                                                                     |\n| PAGE_SIZE                     | 컬럼 파티션에 저장된 페이지의 크기                                                                |\n| PAGE_COUNT                    | 현재 컬럼 파티션에 생성된 페이지의 수                                                              |\n| COMPRESS_RATIO                | 컬럼 파티션의 압축률. 0이면 아직 데이터 압축이 실행되지 않은 경우이다.                                          |\n| DISK_FILENAME                 | 파티션 파일의 이름                                                                         |\n| EXTERNAL_PART_SIZE            | 데이터의 양이 큰 값은 외부 파티션 파일에 기록하는데, 그 파일의 크기를 표시                                        |\n| MIN_VALUE                     | 컬럼 파티션의 최소값                                                                        |\n| MAX_VALUE                     | 컬럼 파티션의 최대값                                                                        |\n\n## V$STORAGE_DC_TABLE_INDEXES\n\nLog Table 에 생성된 인덱스 정보를 표시한다.\n\n| 컬럼 이름                | 설명                           |\n| -------------------- | ---------------------------- |\n| TABLE_ID             | 테이블 식별자                      |\n| DATABASE_ID          | 데이터베이스 식별자                   |\n| ID                   | 인덱스 식별자                      |\n| FLAG                 | 인덱스 Property 를 나타내는 Flag     |\n| TABLE_BEGIN_RID      | 테이블의 입력된 최소 RID              |\n| TABLE_END_RID        | 테이블의 마지막 RID                 |\n| BEGIN_RID            | 인덱스의 최소 RID                  |\n| END_RID              | 인덱스의 최대 RID                  |\n| END_SYNC_RID         | 파일에 기록된 최대 RID+1             |\n| COLUMN_COUNT         | 인덱스 컬럼 수                     |\n| BEGIN_PART_ID        | 인덱스의 최초 파티션 식별자              |\n| END_PART_ID          | 인덱스의 최종 파티션 식별자              |\n| FLUSH_REQUEST_COUNT  | 디스크에 반영요청된 인덱스 파티션의 수        |\n| MAX_KEY_SIZE         | 최대 키 크기                      |\n| INDEX_TYPE           | 인덱스 유형                       |\n| DISK_FILE_SIZE       | 해당 인덱스에 대한 인덱스 파티션 파일의 전체 크기 |\n| LAST_CHECKPOINT_TIME | 마지막으로 Checkpoint 를 지난 시점     |\n\n# LSM Index\n## V$STORAGE_DC_LSMINDEX_LEVEL_PARTS\n\nLSM Index 파티션에 대한 정보를 표시한다.\n\n| 컬럼 이름                      | 설명                                      |\n| -------------------------- | --------------------------------------- |\n| TABLE ID                   | 인덱스가 생성된 테이블의 식별자                       |\n| TABLESPACE_ID              | 테이블스페이스 식별자                             |\n| INDEX_ID                   | 인덱스 식별자                                 |\n| LEVEL                      | 인데스 파티션의 LSM 레벨                         |\n| PARTITION_ID               | 파티션 식별자                                 |\n| BEGIN_RID                  | 파티션에 입력된 최소 RID                         |\n| END_RID                    | 파티션에 입력된 최대 RID+1                       |\n| KEY_VALUE_COUNT            | 파티션에 입력된 키값의 수                          |\n| KEY_VALUE_TABLE_SIZE       | 키값을 저장하는 페이지 크기                         |\n| KEY_VALUE_TABLE_PAGE_COUNT | 키값을 저장하는 페이지의 수                         |\n| MIN_KEY_VALUE              | 최소 키 값                                  |\n| MAX_KEY_VALUE              | 최대 키 값                                  |\n| BITMAP_TABLE_SIZE          | 비트맵 값을 저장하는 페이지의 합계                     |\n| BITMAP_TABLE_PAGE_COUNT    | 비트맵 값을 저장하는 페이지의 수                      |\n| META_SIZE                  | 메타 정보를 저장하는 페이지의 합계                     |\n| META_PAGE_COUNT            | 메타 정보를 저장하는 페이지의 수                      |\n| TOTAL_BUILD_MSEC           | 해당 파티션을 완성하기 까지의 총 시간                   |\n| KEYVAL_BUILD_MSEC          | KeyValue Mode 에서, 해당 파티션을 완성하기 까지의 총 시간 |\n| BITMAP_BUILD_MSEC          | Bitmap Mode 에서, 해당 파티션을 완성하기 까지의 총 시간   |\n\n## V$STORAGE_DC_LSMINDEX_LEVEL_PARTS_CACHE\n\nLSM Index 파티션 캐시에 대한 정보를 표시한다.\n\n| 컬럼 이름                      | 설명                          |\n| -------------------------- | --------------------------- |\n| BEGIN_RID                  | 파티션에 입력된 최소 RID             |\n| BITMAP_TABLE_PAGE_COUNT    | 비트맵 값을 저장하는 페이지의 수          |\n| BITMAP_TABLE_SIZE          | 비트맵 값을 저장하는 페이지의 합계         |\n| END_RID                    | 파티션에 입력된 최대 RID+1           |\n| INDEX_ID                   | 인덱스 식별자                     |\n| KEY_VALUE_COUNT            | 파티션에 입력된 키값의 수              |\n| KEY_VALUE_TABLE_PAGE_COUNT | 키값을 저장하는 페이지의 수             |\n| KEY_VALUE_TABLE_SIZE       | 키값을 저장하는 페이지의 크기            |\n| LEVEL                      | 인데스 파티션의 LSM 레벨             |\n| MEMORY_SIZE                | 메모리 사용량                     |\n| MEMORY_SIZE_RBTREE         | Redblack Tree 가 사용한 메모리 사용량 |\n| META_PAGE_COUNT            | 메타 정보를 저장하는 페이지의 수          |\n| META_SIZE                  | 메타 정보를 저장하는 페이지의 합계         |\n| PARTITION_ID               | 파티션 식별자                     |\n| TABLE_ID                   | 인덱스가 생성된 테이블의 식별자           |\n| TABLESPACE_ID              | 테이블스페이스 식별자                 |\n\n## V$STORAGE_DC_LSMINDEX_LEVELS\n\nLSM 인덱스의 레벨에 관한 정보를 표시한다.\n\n| 컬럼 이름          | 설명                     |\n| -------------- | ---------------------- |\n| TABLE ID       | 테이블 식별자                |\n| DATABASE_ID    | 데이터베이스 식별자             |\n| INDEX_ID       | 인덱스 식별자                |\n| LEVEL          | 레벨                     |\n| BEGIN_RID      | 파티션의 첫번째 RID           |\n| END_RID        | 파티션의 마지막 RID+1         |\n| META_BEGIN_RID | 메타정보를 기록하기 시작한 시점의 RID |\n| META_END_RID   | 메타정보의 기록이 끝난 시점의 RID   |\n| DELETE_END_RID | 삭제된 RID 최대값 +1         |\n\n## V$STORAGE_DC_LSMINDEX_FILES\n\nLSM Index 를 구성하는 파일에 대한 정보를 표시한다.\n\n| 컬럼 이름        | 설명              |\n| ------------ | --------------- |\n| TABLE_ID     | 테이블 식별자         |\n| DATABASE_ID  | 데이터베이스 식별자      |\n| INDEX_ID     | 인덱스 식별자         |\n| LEVEL        | 인데스 파티션의 LSM 레벨 |\n| PARTITION_ID | 파티션 식별자         |\n| BEGIN_RID    | 파티션의 첫번째 RID    |\n| END_RID      | 파티션의 마지막 RID+1  |\n| PATH         | 인덱스 파일의 위치      |\n\n## V$STORAGE_DC_LSMINDEX_AGER_JOBS\n\nLSM Index 의 삭제를 담당하는 Ager 의 작업 상태를 표시한다.\n\n| 컬럼 이름     | 설명                 |\n| --------- | ------------------ |\n| TABLE_ID  | 테이블 식별자            |\n| INDEX_ID  | 인덱스 식별자            |\n| LEVEL     | 인데스 파티션의 LSM 레벨    |\n| BEGIN_RID | 파티션의 첫번째 RID       |\n| END_RID   | 파티션의 마지막 RID+1     |\n| STATE     | Index Ager 의 작업 상태 |\n\n# Volatile Table\n## V$STORAGE_DC_VOLATILE_TABLE\n\nVolatile Table 에 대한 정보를 표시한다.\n\n| 컬럼 이름        | 설명                          |\n| ------------ | --------------------------- |\n| MAX_MEM_SIZE | Volatile Tablespace 의 최대 크기 |\n| CUR_MEM_SIZE | Volatile Tablespace 의 현재 크기 |\n\n# Tag Table\n## V$STORAGE_TAG_TABLES\n\nTagdata Table 의 파티션 테이블에 대한 정보를 표시한다.\n\n| 컬럼 이름                | 설명                                                                                                                                                                                           |\n| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| ID                   | 테이블 식별자                                                                                                                                                                                      |\n| TABLE_BEGIN_RID      | 테이블 시작 RID                                                                                                                                                                                   |\n| TABLE_END_RID        | 테이블 끝 RID                                                                                                                                                                                    |\n| WRITE_END_RID        | 데이터 파일에 기록된 마지막 RID                                                                                                                                                                          |\n| EXT_ROW_COUNT        | VARCHAR 레코드 중 외부 파티션에 입력된 개수                                                                                                                                                                 |\n| EXT_WRITE_COUNT      | VARCHAR 레코드 중 데이터파일에 기록된 개수                                                                                                                                                                  |\n| DISK_INDEX_END_RID   | 스토리지에 저장된 인덱스의 끝 RID                                                                                                                                                                         |\n| MEMORY_INDEX_END_RID | 메모리 인덱스에 상주한 테이블 끝 RID                                                                                                                                                                       |\n| DELETE_MIN_DATE      | DELETE ... BETWEEN ... 수행시 삭제 대상의 최소 시각                                                                                                                                                      |\n| DELETE_MAX_DATE      | DELETE ... BETWEEN ..., 혹은 DELETE ... BEFORE ... 수행시 삭제 대상의 최대 시각                                                                                                                            |\n| INDEX_STATE          | 현재 인덱스 구축 상태IDLE: 구축 완료, 대기중.PROGRESS: 구축 진행중IOWAIT: 스토리지에 입출력 연산 대기.PENDING: 테이블에 읽기 잠금 대기중SHUTDOWN: 정지됨. DELETE 연산, 혹은 DROP 연산 진행중.ABNORMAL: 비정상 종료                |\n| DELETE_STATE         | 현재 DELETE 연산의 상태. DELETE 명령이 들어올 때에만 수행되므로 IDLE이 없다.PROGRESS: 삭제 진행중IOWAIT: 스토리지에 입출력 연산 대기.PENDING: 테이블에 읽기/쓰기 잠금 대기중SHUTDOWN: 정지됨. DELETE 연산이 진행되지 않음.ABNORMAL: 비정상 종료 |\n| SAVE_STATE           | 현재 테이블 저장 연산의 상태.IDLE: 저장 완료, 대기중.PROGRESS: 저장 진행중IOWAIT: 스토리지에 입출력 연산 대기.PENDING: 테이블에 읽기 잠금 대기중SHUTDOWN: 정지됨. DELETE 연산, 혹은 DROP 연산 진행중.ABNORMAL: 비정상 종료           |\n\n## V$STORAGE_TAG_CACHE\n\nTagdata Table 의 파티션 테이블에서 사용하는 캐시 정보를 표시한다.\n\n| 컬럼 이름       | 설명                       |\n| ----------- | ------------------------ |\n| CATEGORY    | 캐쉬되고 있는 객체 분류            |\n| USED_MEMORY | 사용중인 메모리 크기              |\n| BLOCK_COUNT | 데이터 캐시 개수                |\n| CACHE_HIT   | 데이터 캐시 히트 횟수             |\n| CACHE_MISS  | 데이터 캐시 미스 횟수             |\n| FLUSHOUT    | 데이터 캐시 충돌로 페이지를 비운 횟수    |\n| COLDREAD    | 스토리지에서 직접 읽어온 데이터 페이지 개수 |\n| MEMORY_WAIT | 데이터 메모리가 캐시 충돌로 대기한 횟수   |\n| IO_WAIT     | 데이터 읽기 연산 대기 횟수          |\n\n## V$STORAGE_TAG_CACHE_OBJECTS\n\nTagdata Table의 파티션 테이블에서 사용하는 각각의 캐시 블럭에 대한 상세정보를 표시한다.\n\n| 컬럼 이름      | 설명                                                                                                                   |\n| ---------- | -------------------------------------------------------------------------------------------------------------------- |\n| CATEGORY   | 캐쉬되고 있는 객체 분류                                                                                                        |\n| LATEST_HIT | 마지막 접근 시각                                                                                                            |\n| STATUS     | 캐시 상태None: 메모리 할당을 마친 상태Resides: 캐시에 보존된 상태Loading: 스토리지에서 테이블 데이터를 불러 오는 중ERROR!: 데이터를 불러오는 중 오류 발생 |\n| WAIT_COUNT | Loading 상태에서 해당 캐시를 읽지 못해 대기한 회수                                                                                     |\n| REF_COUNT  | 현재 캐시 블럭을 참조 중인 세션 수                                                                                                 |\n| HIT_COUNT  | 캐시 블럭을 참조한 회수                                                                                                        |\n| TABLE_ID   | 테이블 식별자                                                                                                              |\n| FILE_ID    | 파일 식별자                                                                                                               |\n| PART_ID    | 데이터파일 내부의 파티션 식별자                                                                                                    |\n| SAVE_SCN   | 테이블 저장 SCN                                                                                                           |\n| VSAVE_SCN  | 테이블 저장 SCN                                                                                                           |\n| DELETE_SCN | DELETE 연산 SCN                                                                                                        |\n| OFFSET     | 데이터파일 오프셋                                                                                                            |\n| DATA_SIZE  | 압축 이전 데이터 크기, 혹은 0                                                                                                   |\n\n## V$STORAGE_TAG_TABLE_FILES\n\nTagdata Table 의 파티션 테이블의 파일 정보를 표시한다.\n\n| 컬럼 이름     | 설명                                                                                                                                 |\n| --------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n| TABLE_ID  | 테이블 식별자                                                                                                                            |\n| FILE_ID   | 파일 식별자                                                                                                                             |\n| STATE     | 인덱싱 상태COMPLETE: 데이터 저장, 인덱싱 완료INDEXING: 인덱스 구축 중.FILLED: 데이터가 꽉 찬 상태, 인덱싱 대기 중PARTIAL: 아직 데이터가 꽉 차지 않았음. 인덱싱 대기 중. |\n| REF_COUNT | 현재 파일을 참조 중인 세션 수                                                                                                                  |\n| ROW_COUNT | 삭제됐던 레코드를 포함하여 파일에 저장된 레코드 개수                                                                                                      |\n| DEL_COUNT | 파일에서 삭제된 레코드 개수                                                                                                                    |\n| MIN_DATE  | 해당 파일에 기록된 데이터의 최소 일자                                                                                                              |\n| MAX_DATE  | 해당 파일에 기록된 데이터의 최대 일자                                                                                                              |\n\n## V$STORAGE_TAG_INDEX\n\nTagdata Table 에 생성된 인덱스 정보를 표시한다.\n\n| 컬럼 이름                | 설명                                                                                                    |\n| -------------------- | ----------------------------------------------------------------------------------------------------- |\n| TABLE_ID             | 테이블 식별자                                                                                               |\n| INDEX_ID             | 인덱스 식별자(INDEX_ID가 4294967295인 경우 tag테이블 생성시 자동으로 생성되는 기본 인덱스를 의미함)                                    |\n| INDEX_STATE          | 인덱싱 상태IDLE: 인덱싱이 완료되어 대기중인 상태INDEXING: 인덱싱이 진행중인 상태STORAGE FULL: Disk full상태로 인덱싱이 중단된 상태 |\n| DISK_INDEX_END_RID   | 마지막으로 disk에 반영된 인덱스의 EndRID                                                                           |\n| MEMORY_INDEX_END_RID | 마지막으로 memory에 반영된 인덱스의 EndRID                                                                         |\n| TABLE_END_RID        | 테이블에 마지막으로 반영된 데이터의 EndRID                                                                            |\n\n# Tag Rollup\n## V$ROLLUP\n\nTagdata 테이블의 Rollup 정보를 표시한다.\n\n| 컬럼 이름             | 설명                                  |\n| ----------------- | ----------------------------------- |\n| ID                | Rollup 작업 ID                        |\n| ROLLUP_TABLE      | Rollup 이 저장할 테이블 이름                 |\n| SOURCE_TABLE      | Rollup 이 조회할 테이블 이름                 |\n| USER_ID           | Rollup Table과 Source Table의 User ID |\n| ROOT_TABLE        | 최상위 Source Table 이름                 |\n| INTERVAL          | Rollup의 실행 주기 (sec)                 |\n| ENABLED           | Rollup 진행 여부를 나타냄                   |\n| END_RID           | Source Table의 마지막 RID               |\n| LAST_ELAPSED_MSEC | 최근에 진행했던 Rollup의 경과 시간              |\n\n# Stream\n## V$STREAMS\n\nStream 정보를 표시한다.\n\n| 컬럼 이름        | 설명                                                                          |\n| ------------ | --------------------------------------------------------------------------- |\n| NAME         | 서버에 등록된 stream질의의 이름. 서버내에서 유일해야 함.                                         |\n| LAST_EX_TIME | 해당 STREAM질의가 마지막으로 수행된 시간                                                   |\n| TABLE_NAME   | STREAM질의의 검색 대상 테이블 이름                                                      |\n| END_RID      | STREAM 질의가 마지막으로 읽어 들인 RID                                                  |\n| STATE        | STREAM질의의 현재 상태                                                             |\n| QUERY_TXT    | 사용자가 입력한 STREAM질의의 원본                                                       |\n| ERROR_MSG    | 마지막으로 실행했을 때의 에러 메시지                                                        |\n| FREQUENCY    | 질의 수행의 최소 대기 시간. 0이면 매 레코드마다 실행되며 0이 아니면 해당 시간이 지날 때 마다 수행된다.단위는 나노초이다. |\n\n# License\n## V$LICENSE_INFO\n\n라이선스 정보를 표시한다.\n\n| 컬럼 이름            | 설명                     |\n| ---------------- | ---------------------- |\n| INSTALL_DATE     | 설치일                    |\n| TYPE             | 라이선스 유형                |\n| POLICY           | 라이선스 정책 유형             |\n| CUSTOMER         | 고객사 이름                 |\n| ISSUE_DATE       | 발행일                    |\n| ID               | 호스트 ID                 |\n| EXPIRY_DATE      | 만료일                    |\n| SIZE_LIMIT       | 일 입력제한량                |\n| ADDENDUM         | 추가 데이터 비율              |\n| VIOLATION_ACTION | 위반 시 행동                |\n| VIOLATION_LIMIT  | 서비스가 중단될 위반 횟수 (매월 갱신) |\n| STOP_ACTION      | 중단 행동                  |\n| RESET_FLAG       | (서버 내부 사용)             |\n\n## V$LICENSE_STATUS\n\n라이선스 상태를 표시한다.\n\n| 컬럼 이름               | 설명                   |\n| ------------------- | -------------------- |\n| USER_DATA_PER_DAY   | 하루에 입력할 수 있는 데이터 제한량 |\n| PREVIOUS_CHECK_DATE | 직전 라이선스 검사일          |\n| VIOLATION_COUNT     | 라이선스 위반 횟수           |\n| STOP_ENABLED        | (제거됨)                |\n\n# Mutex\n## V$MUTEX\n\n현재 뮤텍스 상태를 보여준다.\n\n| 필드명            | 설명                         | 비고                                                                                                         |\n| -------------- | -------------------------- | ---------------------------------------------------------------------------------------------------------- |\n| OBJECT         | 뮤텍스 객체의 주소                 |                                                                                                            |\n| NAME           | 뮤텍스 생성시 부여한 이름             |                                                                                                            |\n| TYPE           | 뮤텍스 타입                     | Mutex: pmuMutexRW Mutex: pmuRWMutex                                                                    |\n| OWNER          | 뮤텍스를 획득한 스레드의 ID           | Mutex: 뮤텍스를 획득한 스레드가 없으면 0.RW Mutex w/ Read-Lock: 0RW Mutex w/ Write-Lock: Write Lock을 획득한 스레드의 ID |\n| LOCK_COUNT     | 뮤텍스를 획득한 스레드 개수            | RW Mutex는 2 이상이 될 수 있음.                                                                                    |\n| PEND_COUNT     | 뮤텍스를 획득하려고 대기 중인 스레드 개수    | TRACE_MUTEX_WAIT_STATUS=1일 때에만 수집                                                                          |\n| TRY_COUNT      | 뮤텍스를 획득하려고 시도한 회수          | TRACE_MUTEX_WAIT_STATUS=1일 때에만 수집                                                                          |\n| CONFLICT_COUNT | 뮤텍스 획득에 실패한 회수             | TRACE_MUTEX_WAIT_STATUS=1일 때에만 수집                                                                          |\n| WAIT_TICK      | 뮤텍스 획득 대기 시간의 총합           | TRACE_MUTEX_WAIT_STATUS=1일 때에만 수집RW Mutex에는 기록하지 않음                                                    |\n| WAIT_TICK_AVG  | 뮤텍스 획득 시도 후 성공까지의 평균 시간    | TRACE_MUTEX_WAIT_STATUS=1일 때에만 수집RW Mutex에는 기록하지 않음                                                    |\n| HELD_TICK      | 뮤텍스를 획득한 이후 해제할 때까지의 시간 총합 | TRACE_MUTEX_WAIT_STATUS=1일 때에만 수집RW Mutex에는 기록하지 않음                                                    |\n| HELD_TICK_AVG  | 뮤텍스 획득 이후 해제까지의 시간 평균      | TRACE_MUTEX_WAIT_STATUS=1일 때에만 수집RW Mutex에는 기록하지 않음                                                    |\n\n## V$MUTEX_WAIT_STAT\n\n현재 대기중인 뮤텍스의 콜스택을 보여준다.\n\n| 필드        | 설명                  | 비고                               |\n| --------- | ------------------- | -------------------------------- |\n| THREAD_ID | 뮤텍스 획득 대기 중인 스레드 ID |                                  |\n| OBJECT    | 획득 시도 중인 뮤텍스의 주소    | V$MUTEX의 OBJECT와 동일              |\n| DEPTH     | 호출 깊이               | TRACE_MUTEX_WAIT_STACK=1일 때에만 수집 |\n| SYMBOL    | 뮤텍스 획득을 호출한 함수의 심볼  | TRACE_MUTEX_WAIT_STACK=1일 때에만 수집 |\n\n# Cluster\n\n## V$NODE_STATUS\n\nCluster 각 Node 의 상태를 표시한다. 1건만 표시된다.\n\n| 컬럼 이름    | 설명                                                            |\n| -------- | ------------------------------------------------------------- |\n| NODETYPE | Node 의 유형. 쿼리로 조회 가능한 Type 은 두 가지 뿐이다.BrokerWarehouse |\n| STATE    | Node 의 상태                                                     |\n\n## V$DDL_INFO\n\nCluster 에서 수행한 DDL 정보를 표시한다.\n\n| 컬럼 이름          | 설명                      |\n| -------------- | ----------------------- |\n| SEQUENCENUMBER | DDL 순서 번호               |\n| TIME           | DDL 수행 시간               |\n| VALUE          | DDL 쿼리 결과 값 (서버 내부 사용)  |\n| CLIENT         | 클라이언트 이름                |\n| BROKER         | Leader Broker 의 Node 이름 |\n| USER           | 사용자 이름                  |\n| SQL            | DDL 쿼리 값                |\n\n## V$REPLICATION\n\nReplication 작동에 대한 정보를 표시한다.\n\n| 컬럼 이름            | 설명                                 |\n| ---------------- | ---------------------------------- |\n| HOSTNAME         | Replication 이 작동되는 Node 의 Hostname |\n| MODE             | (서버 내부 사용)                         |\n| STATE            | Node 의 상태                          |\n| ADDR             | Replication Manager 의 주소           |\n| PORT_NO          | Replication Manager 의 포트번호         |\n| MAX_SENDER_COUNT | 생성 가능한 Sender 최대 개수                |\n| RUN_SENDER_COUNT | 작동중인 Sender 최대 개수                  |\n\n## V$REPL_SENDER\n\nReplication 작동 시, Sender 의 정보를 표시한다.\n\n| 컬럼 이름              | 설명                                 |\n| ------------------ | ---------------------------------- |\n| HOSTNAME           | Replication 이 작동되는 Node 의 Hostname |\n| ID                 | Sender 식별자                         |\n| STATUS             | Sender Thread 의 작동상태               |\n| PAYLOAD_RECV_COUNT | Sender 로부터 받은 페이로드 개수              |\n| PAYLOAD_RECV_BYTES | Sender 로부터 받은 페이로드 크기 총합           |\n| QUEUE_REMAIN_COUNT | Receive Queue 에 남은 버퍼의 개수          |\n| NET_SEND_COUNT     | 전체 전송 횟수                           |\n| NET_SEND_SIZE      | 전체 전송 크기 총합                        |\n| NET_RECV_COUNT     | 전체 수신 횟수                           |\n| NET_RECV_SIZE      | 전체 수신 크기 총합                        |\n\n## V$REPL_SENDER_META\n\nReplication 작동 시, Sender 의 메타데이터를 표시한다.\n\n| 컬럼 이름      | 설명                                 |\n| ---------- | ---------------------------------- |\n| HOSTNAME   | Replication 이 작동되는 Node 의 Hostname |\n| SENDER_ID  | Sender 식별자                         |\n| TABLE_ID   | 대상 테이블 식별자                         |\n| TABLE_TYPE | 대상 테이블 유형                          |\n| BEGIN_RID  | 대상 레코드의 시작 RID                     |\n| END_RID    | 대상 레코드의 끝 RID                      |\n\n## V$REPL_RECEIVER\n\nReplication 작동 시, Receiver 의 정보를 표시한다.\n\n| 컬럼 이름              | 설명                                 |\n| ------------------ | ---------------------------------- |\n| HOSTNAME           | Replication 이 작동되는 Node 의 Hostname |\n| STATUS             | Receiver Thread 의 작동상태             |\n| PAYLOAD_RECV_COUNT | Sender 로부터 받은 페이로드 개수              |\n| PAYLOAD_RECV_BYTES | Sender 로부터 받은 페이로드 크기 총합           |\n| QUEUE_REMAIN_COUNT | Receive Queue 에 남은 버퍼의 개수          |\n| NET_SEND_COUNT     | 전체 전송 횟수                           |\n| NET_SEND_SIZE      | 전체 전송 크기 총합                        |\n| NET_RECV_COUNT     | 전체 수신 횟수                           |\n| NET_RECV_SIZE      | 전체 수신 크기 총합                        |\n\n## V$REPL_RECEIVER_META\n\nReplication 작동 시, Receiver 의 메타데이터를 표시한다.\n\n| 컬럼 이름      | 설명                                 |\n| ---------- | ---------------------------------- |\n| HOSTNAME   | Replication 이 작동되는 Node 의 Hostname |\n| TABLE_ID   | 대상 테이블 식별자                         |\n| TABLE_TYPE | 대상 테이블 유형                          |\n| BEGIN_RID  | 대상 레코드의 시작 RID                     |\n| END_RID    | 대상 레코드의 끝 RID                      |\n\n## V$REPL_READER\n\nReplication 작동 시, Reader 의 정보를 표시한다.\n\n| 컬럼 이름       | 설명                                 |\n| ----------- | ---------------------------------- |\n| HOSTNAME    | Replication 이 작동되는 Node 의 Hostname |\n| SENDER_ID   | Sender 식별자                         |\n| ID          | Reader 식별자                         |\n| STATUS      | Reader Thread의 작동상태                |\n| FETCH_COUNT | FETCH 수행 횟수                        |\n\n## V$REPL_READER_META\n\nReplication 작동 시, Reader 의 메타데이터를 표시한다.\n\n| 컬럼 이름      | 설명                                 |\n| ---------- | ---------------------------------- |\n| HOSTNAME   | Replication 이 작동되는 Node 의 Hostname |\n| SENDER_ID  | Sender 식별자                         |\n| ID         | Reader 식별자                         |\n| TABLE_ID   | 대상 테이블 식별자                         |\n| TABLE_TYPE | 대상 테이블 유형                          |\n| BEGIN_RID  | 대상 레코드의 시작 RID                     |\n| END_RID    | 대상 레코드의 끝 RID                      |\n\n## V$REPL_WRITER\n\nReplication 작동 시, Writer 의 정보를 표시한다.\n\n| 컬럼 이름        | 설명                                 |\n| ------------ | ---------------------------------- |\n| HOSTNAME     | Replication 이 작동되는 Node 의 Hostname |\n| ID           | Writer 식별자                         |\n| STATUS       | Writer Thread 의 작동상태               |\n| APPEND_COUNT | APPEND 수행 횟수                       |\n\n## V$REPL_WRITER_META\n\nReplication 작동 시, Writer 의 메타데이터를 표시한다.\n\n| 컬럼 이름      | 설명                                 |\n| ---------- | ---------------------------------- |\n| HOSTNAME   | Replication 이 작동되는 Node 의 Hostname |\n| ID         | Writer 식별자                         |\n| TABLE_ID   | 대상 테이블 식별자                         |\n| TABLE_TYPE | 대상 테이블 유형                          |\n| BEGIN_RID  | 대상 레코드의 시작 RID                     |\n| END_RID    | 대상 레코드의 끝 RID                      |\n\n# Others\n## V$TABLES\n\nV$로 시작하는 모든 Virtual Table 을 표시한다.\n\n| 컬럼 이름       | 설명           |\n| ----------- | ------------ |\n| NAME        | 테이블 이름       |\n| TYPE        | 테이블 유형       |\n| DATABASE_ID | 데이터베이스 식별자   |\n| ID          | 테이블 식별자      |\n| USER ID     | 테이블을 생성한 사용자 |\n| COLCOUNT    | 컬럼의 갯수       |\n\n## V$COLUMNS\n\nVirtual Table 의 컬럼 정보를 표시한다.\n\n| 컬럼 이름                | 설명         |\n| -------------------- | ---------- |\n| NAME                 | 컬럼명        |\n| TYPE                 | 컬럼의 데이터 타입 |\n| DATABASE_ID          | 데이터베이스 식별자 |\n| ID                   | 컬럼의 식별자    |\n| LENGTH               | 컬럼의 크기     |\n| TABLE_ID             | 테이블 식별자    |\n| FLAG                 | 비공개 데이터    |\n| PART_PAGE_COUNT      | (사용되지 않음)  |\n| PAGE_VALUE_COUNT     | (사용되지 않음)  |\n| MINMAX_CACHE_SIZE    | (사용되지 않음)  |\n| MAX_CACHE_PART_COUNT | (사용되지 않음)  |\n\n## V$RETENTION_JOB\n\nRETENTION POLICY가 적용된 테이블 정보를 표시한다.\n\n| 컬럼 이름         | 설명                                     |\n|-------------------|------------------------------------------|\n| USER_NAME         | 사용자 이름                              |\n| TABLE_NAME        | 대상 TAG TABLE 이름                      |\n| POLICY_NAME       | 적용되어 있는 POLICY 이름                |\n| STATE             | RETENTION 상태 (RUNNING/WAITING/STOPPED) |\n| LAST_DELETED_TIME | 마지막으로 삭제된 시간                   |"
					}
					
				
		
				
					,
					
					"feature-table-volatile-volatile-ex-html": {
						"id": "feature-table-volatile-volatile-ex-html",
						"title": "휘발성 테이블 활용 샘플 예제",
						"version": "all",
						"categories": "",
						"url": " /feature-table/volatile/volatile-ex.html",
						"content": "### 센서 데이터의 현재 값을 저장\n\n휘발성 테이블의 데이터는 메모리 상에만 존재하여 기본 키에 의한 갱신 연산이 매우 빠른 특징이 있다. 이 특징을 이용하여, 매우 빠르게 변화하는 센서의 현재 값을 저장하는 테이블을 만든다. 테이블 생성 스크립트의 예는 아래와 같다.\n\n```sql\ncreate volatile table sensor_current (sensor_id varchar(40) primary key, value double);\n```\n\n### 휘발성 데이터의 입력 및 갱신\n\n테이블을 생성하였으므로, 데이터 입력과 갱신 연산을 통하여 센서의 현재 값을 반영할 수 있다. 입력되는 센서값은 기본 키 sensor_id 컬럼을 기준으로 입력 혹은 갱신을 수행할 것인지가 결정된다. 입력 혹은 갱신은 다음의 질의문으로 수행할 수 있다.\n\n```sql\ninsert into sensor_current values('SENSOR_001',100.0) on duplicate key update set value=100.0;\n```\n\n위 질의문에서 입력되는 데이터는 기본 키에 해당하는 컬럼인 sensor_id 값이 'SENSOR_001'인 데이터가 있으면 그 레코드의 value 컬럼 값을 100.0으로 갱신하고, 없으면 Insert문의 문법 대로 새로운 레코드를 삽입한다.\n\n### 휘발성 데이터의 검색\n\n특정 센서 데이터의 현재 값을 알려면 다음의 질의를 이용하여 검색한다. 일반적인 SQL질의문과 동일한 문법을 사용하여 검색을 수행할 수 있다.\n\n```sql\nSELECT value FROM sensor_current WHERE sensor_id = 'SENSOR_001'\n```"
					}
					
				
		
				
					,
					
					"feature-table-volatile-volatile-index-html": {
						"id": "feature-table-volatile-volatile-index-html",
						"title": "휘발성 인덱스 생성 및 관리",
						"version": "all",
						"categories": "",
						"url": " /feature-table/volatile/volatile-index.html",
						"content": "### 인덱스 생성 및 활용\n\n휘발성 테이블은 실시간 검색에 최적화된 레드블랙(RED-BLACK) 트리를 기본으로 제공하고 있으며, 모든 데이터 타입에 대해서 인덱스를 설치할 수 있다. 단, 하나의 컬럼에 대해 하나의 인덱스가 생성 가능하며, 복합(composite) 인덱스는 제공하지 않는다.\n\n```sql\nMach> create volatile table vtable (id integer, name varchar(10));\nCreated successfully.\nMach> create index idx_vrb on vtable (name) index_type redblack;\nCreated successfuly.\nMach> desc vtable;\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nID                            integer             11             \nNAME                          varchar             10                 \n \n[ INDEX ]                             \n----------------------------------------------------------------\nNAME                          TYPE                COLUMN\n----------------------------------------------------------------\nIDX_VRB                       REDBLACK            NAME               \niFlux>\n```\n\n### 기본 키 인덱스\n\n또한 휘발성 테이블의 특정 컬럼에 기본 키를 부여하게 되면, 여기에 레드블랙 트리 인덱스를 자동으로 생성하게 된다. 이 때는 유일성(Uniqueness) 속성을 지닌 특별한 인덱스가 생성되며 중복된 값을 허용하지 않는다.\n\n```sql\nMach> create volatile table vtable (id integer primary key, name varchar(20));\nCreated successfully.\nMach> desc vtable;\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nID                            integer             11             \nNAME                          varchar             20                 \n \n[ INDEX ]                             \n----------------------------------------------------------------\nNAME                          TYPE                COLUMN\n----------------------------------------------------------------\n__PK_IDX_VTABLE               REDBLACK            ID  \n \niFlux>\n```\n \n### 다른 인덱스 유형\n\n로그 테이블에서 사용하던 비트맵 혹은 키워드 인덱스는 휘발성 테이블에서 사용할 수 없다.\n\n```sql\nMach> create bitmap   index idx_1237 on vtable(id1);\n[ERR-02069 : Error in index for invalid table. BITMAP Index can only be created for LOG Table.]\nMach> create keyword  index idx_1238 on vtable(name);\n[ERR-02069 : Error in index for invalid table. KEYWORD Index can only be created for LOG Table.]\n```"
					}
					
				
		
				
					,
					
					"feature-table-volatile-html": {
						"id": "feature-table-volatile-html",
						"title": "휘발성 테이블 (Volatile Table)",
						"version": "all",
						"categories": "",
						"url": " /feature-table/volatile.html",
						"content": "## 개념\n\n휘발성(Volatile) 테이블은 모든 데이터가 임시 메모리 공간에 상주하고, 로그 테이블과의 Join을 통해 데이터 결과를 풍부하게 만드는 임시 테이블이다.\n\n휘발성 테이블은 로그 테이블에 간략한 기호나 숫자로 표현된 특정 디바이스나 장비의 다양한 정보를 담고 있는 부가 정보 테이블이라고 할 수 있다. 고속으로 입력 및 갱신이 가능하여 데이터의 현재 상태 (시계열 데이터와는 맞지 않음) 를 실시간으로 유지해야 하는 경우에 많이 사용한다.\n\n이 테이블의 특성은 다음과 같다\n\n### 스키마 보존\n\n휘발성 테이블의 구조(스키마) 정보는 Machbase 서버가 종료되었다가 다시 구동이 되더라도 유지된다. 해당 테이블을 삭제하기 위해서는 명시적으로 DROP table을 수행하면 된다.\n\n### 데이터 휘발성\n\n휘발성 테이블에 담긴 데이터는 서버를 종료하는 즉시 모두 사라진다. 따라서 서버의 구동 시에 휘발성 테이블의 내용을 다시 INSERT 해야 한다.\n\n### 인덱스 제공 및 Join 기능\n\n휘발성 테이블의 빠른 데이터 접근을 위해 실시간에 최적화된 인덱스인 RED-BLACK 인덱스를 제공한다. 따라서 로그 테이블과의 Join 이나 검색 과정에서 효율적으로 활용될 수 있다.\n* 테이블 컬럼에서 기본 키(Primary Key) 를 지정할 수 있다.\n* 중복되는 기본 키 값을 가진 데이터를 삽입하는 경우, 기존 데이터의 값을 갱신(UPDATE)할 수 있다.\n* 조건 절 (WHERE 절)을 이용해, 기본 키 값 조건과 일치하는 데이터를 삭제할 수 있다.\n* _ARRIVAL_TIME 컬럼이 존재하지 않는다.\n\n### 기본 키(Primary Key)\n\n기본 키는 테이블 컬럼 값의 유일성(Uniqueness) 제약 조건을 형성하고 테이블 데이터를 구별할 키 컬럼을 지정하는 목적으로 생성할 수 있다.\n\n기본 키가 지정된 휘발성 테이블에 데이터를 삽입할 때, 삽입 데이터의 기본 키 컬럼 값은 테이블 안의 다른 기본 키 컬럼 값들과 반드시 달라야 한다. 이 제약사항을 유일성 조건이라고 할 수 있다.\n\n기본 키의 생성 제약사항은 다음과 같다.\n* 기본 키는 휘발성 테이블에서만 생성할 수 있다.\n* 기본 키 컬럼은 1개만 지정할 수 있으며, 2개 이상의 컬럼을 함께 기본 키로 지정할 수 없다.\n\n### 갱신(UPDATE) 기능\n\n다른 테이블 유형과는 다르게, 휘발성 테이블은 제한적이나마 갱신 기능을 제공한다.\n\n삽입하고자 하는 데이터의 기본 키 값이 이미 다른 데이터의 기본 키 값 중 하나와 중복되는 경우에는 '삽입' 이 아니라 '갱신' 모드로 전환되며, 중복된 키 값을 가진 기존 데이터의 다른 컬럼 값이 삽입하고자 하는 데이터의 컬럼 값으로 변경된다. 기본 키가 지정된 휘발성 테이블에서만 갱신 기능을 사용할 수 있으며, 삽입 과정에서 기본 키 값을 지정하지 않았을 때는 갱신 기능을 사용할 수 없다.\n\n### 삭제(DELETE) 기능\n\n휘발성 테이블은 기본 키 값을 이용해 특정 데이터를 삭제할 수 있는 기능을 제공한다.\n\nDELETE 구문에서 조건 절(WHERE 절)을 추가해 기본 키 값을 지정하면, 해당 기본 키 값에 해당하는 데이터가 존재하는 경우에 한하여 이를 삭제할 수 있다. 기본 키가 지정된 휘발성 테이블에서만 삭제 기능을 사용할 수 있으며, 이 때 조건 절에 들어갈 수 있는 조건은 (기본 키 컬럼) = (값) 으로 제한되어 있다.\n\n## 작업 방법\n* [휘발성 테이블 생성 및 관리](/kr/feature-table/volatile/create-drop.html)\n* [휘발성 데이터의 입력 및 갱신](/kr/feature-table/volatile/insert-update.html)\n* [휘발성 데이터의 추출](/kr/feature-table/volatile/select.html)\n* [휘발성 데이터의 삭제](/kr/feature-table/volatile/delete.html)\n* [휘발성 인덱스 생성 및 관리](/kr/feature-table/volatile/volatile-index.html)\n* [휘발성 테이블 활용 샘플 예제](/kr/feature-table/volatile/volatile-ex.html)"
					}
					
				
		
				
					,
					
					"install-windows-windows-env-html": {
						"id": "install-windows-windows-env-html",
						"title": "Windows 환경 설치 준비",
						"version": "all",
						"categories": "",
						"url": " /install/windows/windows-env.html",
						"content": "# 방화벽 포트 개방\n\n마크베이스를 윈도우에 설치하는 경우 윈도우 방화벽에서 마크베이스가 사용하는 포트를 열어주어야 한다.\n\n기본적으로 마크베이스는 **5656, 5001** 2개의 포트를 사용한다.\n\n1. 방화벽에 해당 포트를 등록하기 위해서는 제어판 – Windows 방화벽 또는 Windows Defender 방화벽 을 선택하여 실행한다.  \n   실행화면에서 \"고급 설정\" 메뉴를 클릭한다.\n   * Windows 7  \n   ![winenv1](/kr/install/windows/winenv1.jpeg)\n   * Windows 10  \n   ![winenv2](/kr/install/windows/winenv2.jpeg)\n\n3. 고급설정에서 **인바운드 규칙 - 새 규칙** 을 선택하여 클릭한다.\n   * Windows 7  \n   ![winenv3](/kr/install/windows/winenv3.jpeg)\n   * Windows 10  \n   ![winenv4](/kr/install/windows/winenv4.jpeg)\n\n5. 새 규칙 설정 마법사 창이 표시되면 아래와 같이 포트 옵션을 선택하고 **다음**을 클릭한다.  \n   ![winenv5](/kr/install/windows/winenv5.png)\n\n6. **TCP(T)** 옵션을 선택하고 특정 로컬 포트 입력란에 **5656,5001** 을 입력한 후 다음을 클릭한다.  \n   ![winenv6](/kr/install/windows/winenv6.png)\n\n7. **연결 허용** 옵션을 선택하고 다음을 클릭한다.  \n   ![winenv7](/kr/install/windows/winenv7.png)\n\n8. **도메인, 개인, 공용**을 체크하고 **다음**을 클릭한다.  \n   ![winenv8](/kr/install/windows/winenv8.png)\n\n9. **이름**과 **설명** 입력란에 내용을 입력한 후 마침을 클릭한다.\n   ![winenv9](/kr/install/windows/winenv9.png)"
					}
					
				
		
				
					,
					
					"install-windows-html": {
						"id": "install-windows-html",
						"title": "Windows 설치",
						"version": "all",
						"categories": "",
						"url": " /install/windows.html",
						"content": "* [MSI 설치](/kr/install/windows/msi-install.html)\n* [Windows 환경 설치 준비](/kr/install/windows/windows-env.html)"
					}
					
				
		
				
					,
					
					"assets-css-style-css": {
						"id": "assets-css-style-css",
						"title": "",
						"version": "all",
						"categories": "",
						"url": " /assets/css/style.css",
						"content": "@import \"jekyll-theme-primer\";"
					}
					
				
		
	};
</script>
<script src="/kr/assets/js/lunr.min.js"></script>
<script src="/kr/assets/js/search.js"></script>
<script
  src="/kr/assets/js/jquery-3.3.1/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8= sha256-T+aPohYXbm0fRYDpJLr+zJ9RmYTswGsahAoIsNiMld4="
  crossorigin="anonymous"></script>

<script>
$(document).ready(function() {

    var toc = $('#TOC');

    // Select each header
    sections = $('.td-content h1');
        $.each(sections, function(idx, v) {
            section = $(v);
            var div_id = $(section).attr('id');
            var div_text = section.text().split('¶')[0];
            var parent = $("#" + div_id)
            var content = '<li id="link_' + div_id + '" class="md-nav__item"><a class="md-nav__link" href="#' + div_id + '" title="' + div_text +'">' + div_text +'</a></li>';
            $(toc).append(content);

            // Add section code to subnavigation
            var children = $('<nav class="md-nav"><ul class="md-nav__list"></nav></ul>')
            var contenders = $("#" + div_id).nextUntil("h1");
            $.each(contenders, function(idx, contender){
               if($(contender).is('h2') || $(contender).is('h3')) {
                   var contender_id = $(contender).attr('id');
                   var contender_text = $(contender).text().split('¶')[0];
                   var content = '<li class="md-nav__item"><a class="md-nav__link" href="#' + contender_id + '" title="' + contender_text +'">' + contender_text +'</a></li>';
                   children.append(content);
                }
             })
             $("#link_" + div_id).append(children);
        });
    });
</script>

<script>
var headers = ["h1", "h2", "h3", "h4"]
var colors = ["red", "orange", "green", "blue"]

$.each(headers, function(i, header){
    var color = colors[i];
    $(header).each(function () {
        var href=$(this).attr("id");
        $(this).append('<a class="headerlink" style="color:' + color + '" href="#' + href + '" title="Permanent link">¶</a>')
    });
})
</script>



	
              
              <!-- <style>
  .feedback--answer {
    display: inline-block;
  }
  .feedback--answer-no {
    margin-left: 1em;
  }
  .feedback--response {
    display: none;
    margin-top: 1em;
  }
  .feedback--response__visible {
    display: block;
  }
</style>
<h5 class="feedback--title">Feedback</h5>
<p class="feedback--question">Was this page helpful?</p>
<button class="feedback--answer feedback--answer-yes">Yes</button>
<button class="feedback--answer feedback--answer-no">No</button>
<p class="feedback--response feedback--response-yes">
  Glad to hear it! Please <a href="https://github.com/machbase/dbms-manual/issues/new">tell us how we can improve</a>.
</p>
<p class="feedback--response feedback--response-no">
  Sorry to hear that. Please <a href="https://github.com/machbase/dbms-manual/issues/new">tell us how we can improve</a>.
</p>
<script>
  const yesButton = document.querySelector('.feedback--answer-yes');
  const noButton = document.querySelector('.feedback--answer-no');
  const yesResponse = document.querySelector('.feedback--response-yes');
  const noResponse = document.querySelector('.feedback--response-no');
  const disableButtons = () => {
    yesButton.disabled = true;
    noButton.disabled = true;
  };
  const sendFeedback = (value) => {
    if (typeof ga !== 'function') return;
    const args = {
      command: 'send',
      hitType: 'event',
      category: 'Helpful',
      action: 'click',
      label: window.location.pathname,
      value: value
    };
    ga(args.command, args.hitType, args.category, args.action, args.label, args.value);
  };
  yesButton.addEventListener('click', () => {
    yesResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(1);
  });
  noButton.addEventListener('click', () => {
    noResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(0);
  });
</script><br/>

 -->
           </div>
          </main>
        </div>
      </div>
      <footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        <ul class="list-inline mb-0">  
          
            <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="Twitter" data-original-title="Twitter">
              <a class="text-white" target="_blank" href="https://twitter.com/machbase">
                <i class="fab fa-twitter"></i>
              </a>
            </li>
          
          
        </ul>
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        <ul class="list-inline mb-0">  
          <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="GitHub" data-original-title="GitHub">
            <a class="text-white" target="_blank" href="https://github.com/machbase/dbms-manual">
              <i class="fab fa-github"></i>
            </a>
          </li>
        </ul>
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">© 2023  All Rights Reserved</small>
        
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<script src="/kr/assets/js/main.js"></script>

  </body>
</html>