<!DOCTYPE html>
<html lang="en" class="no-js">
  <!-- Copyright 2019-2021 Vanessa Sochat-->
  <head>
    <meta charset="utf-8">
    <!-- Copyright 2019-2021 Vanessa Sochat-->
    
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="generator" content="Hugo 0.55.6" />
    
    <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    
    <link rel="alternate" type="application/rss&#43;xml" href="/docs/index.xml">
    
    <link rel="shortcut icon" href="/en/assets/favicons/favicon.ico" >
    <!-- <link rel="apple-touch-icon" href="/en/assets/favicons/apple-touch-icon-180x180.png" sizes="180x180">
    <link rel="icon" type="image/png" href="/en/assets/favicons/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/en/assets/favicons/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/en/assets/favicons/android-icon-36x36.png" sizes="36x36">
    <link rel="icon" type="image/png" href="/en/assets/favicons/android-icon-48x48.png" sizes="48x48">
    <link rel="icon" type="image/png" href="/en/assets/favicons/android-icon-72x72.png" sizes="72x72">
    <link rel="icon" type="image/png" href="/en/assets/favicons/android-icon-96x196.png" sizes="96x196">
    <link rel="icon" type="image/png" href="/en/assets/favicons/android-icon-144x144.png" sizes="144x144">
    <link rel="icon" type="image/png" href="/en/assets/favicons/android-icon-192x192.png"sizes="192x192"> -->
    
    <title>Search</title>
    <meta property="og:title" content="Search" />
    <meta property="og:description" content="machbase-manual documents.
" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="" />
    <meta property="og:site_name" content="" />
    
    <meta itemprop="name" content="Search">
    <meta itemprop="description" content="machbase-manual documents.
">
    
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Search"/>
    <meta name="twitter:description" content="machbase-manual documents.
"/>
    
    <link rel="stylesheet" href="/en/assets/css/main.css">
    <link rel="stylesheet" href="/en/assets/css/palette.css">
    <script
      src="/en/assets/js/jquery-3.3.1/jquery-3.3.1.min.js"
      integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8= sha256-T+aPohYXbm0fRYDpJLr+zJ9RmYTswGsahAoIsNiMld4="
      crossorigin="anonymous"></script>
    </head>
  

  
  <body class="td-section">
    <header>
    <nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
    <a class="navbar-brand" href="/en/">
        <span class="navbar-logo"></span><span class="text-uppercase font-weight-bold"><img src="/en/assets/img/logo.png" alt="Machbase manual"></span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">
    <ul class="navbar-nav mt-2 mt-lg-0">
        <li class="nav-item mr-4 mb-2 mb-lg-0">
                        <!-- <a class="nav-link" href="https://github.com/machbase/dbms-manual" target="_blank"><span>GitHub</span></a> -->
        </li>
        <li class="nav-item mr-4 mb-2 mb-lg-0">
                        <!-- <a class="nav-link" href="/en/about" ><span>About</span></a> -->
        </li>
        <li class="nav-item mr-4 mb-2 mb-lg-0">
                        <!-- <a class="nav-link" href="/en/docs" ><span>Documentation</span></a> -->
        </li>
  <!--  -->
  <!-- <li class="nav-item mr-4 mb-2 mb-lg-0"> -->
    <!-- 
      <span class="nav-link ">Release:</span>
    
    <li class="nav-item dropdown d-none d-lg-block">
      <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Current</a>
      <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
        
        
          
            <a class="dropdown-item" href="/en/search/">Current</a>
          
        
        
          
            
            <a class="dropdown-item" href="/en/search/">Previous</a>
          
         -->
      </div>
    </li>
  <!-- </li> -->
  
    </ul>
</div>
<div class="navbar-nav d-none d-lg-block">
<input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="on">
    </div>

<!-- <div class="navbar-nav d-none d-lg-block">
      <a class="gh-source" data-gh-source="github" href="https://github.com/machbase/dbms-manual" title="Go to repository" data-md-state="done">
      <div class="gh-source__repository">
        <i class="fab fa fa-github fa-2x" style='padding-right:20px; float:left; margin-top:5px'></i>
        machbase/dbms-manual
      <ul class="gh-source__facts"><li class="gh-source__fact" id='stars'></li><li id="forks" class="gh-source__fact"></li></ul></div></a>
    </div> -->
  </div>


</nav>
</header>

<script>
$(document).ready(function() {
var url = "https://api.github.com/search/repositories?q=machbase/dbms-manual";
fetch(url, { 
  headers: {"Accept":"application/vnd.github.preview"}
}).then(function(e) {
return e.json()
}).then(function(r) {
 console.log(r.items[0])
 stars = r.items[0]['stargazers_count']
 forks = r.items[0]['forks_count']
 $('#stars').text(stars + " Stars")
 $('#forks').text(forks + " Forks")
});
});
</script>

    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          <div id="td-sidebar-menu" class="td-sidebar__inner">
  <form class="td-sidebar__search d-flex align-items-center">
    <input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation"></button>
  </form>  
  
  <nav class="collapse td-sidebar-nav pt-2 pl-4" id="td-section-nav">
    
      <ul class="td-sidebar-nav__section pr-md-3">
        
        
      
        <li class="td-sidebar-nav__section-title">
          <a href="" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">Machbase Manual</a>
        </li>
        
        
          <ul>
            <li class="collapse show" id="machbase-manual">
              <ul class="td-sidebar-nav__section pr-md-3">
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">Introduction</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="introduction">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-introduction-introduction-of-machbase" href="/en//intro/introduction">Introduction of Machbase</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-introduction-machbase-features" href="/en//intro/features">Machbase Features</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-introduction-introduction-of-machbase-products" href="/en//intro/products">Introduction of Machbase Products</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">Installation</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="installation">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-installation-package-overview" href="/en//install/package">Package Overview</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-installation-linux-installation" href="/en//install/linux">Linux Installation</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-installation-windows-installation" href="/en//install/windows">Windows Installation</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-installation-license-installation" href="/en//install/license">License Installation</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-installation-cluster-edition-installation" href="/en//install/cluster">Cluster Edition Installation</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-installation-upgrade" href="">Upgrade</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">Feature and Tables</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="feature-and-tables">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-feature-and-tables-tag-table" href="/en//feature-tables/tag">Tag Table</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-feature-and-tables-log-table" href="/en//feature-tables/log">Log Table</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-feature-and-tables-volatile-table" href="/en//feature-tables/volatile">Volatile Table</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-feature-and-tables-lookup-table" href="/en//feature-tables/lookup">Lookup Table</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-feature-and-tables-stream" href="/en//feature-tables/stream">STREAM</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-feature-and-tables-backup-and-mount" href="/en//feature-tables/backup-mount">Backup and Mount</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">Config/Monitoring</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="config-monitoring">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-config-monitoring-meta-table" href="/en//config-monitor/metatable">Meta Table</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-config-monitoring-virtual-table" href="/en//config-monitor/virtualtable">Virtual Table</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-config-monitoring-property" href="/en//config-monitor/property">Property</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-config-monitoring-property-cluster" href="/en//config-monitor/property-cluster">Property(Cluster)</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">SQL Reference</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="sql-reference">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sql-reference-datatypes" href="/en//sql-ref/datatypes">Datatypes</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sql-reference-ddl" href="/en//sql-ref/ddl">DDL</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sql-reference-dml" href="/en//sql-ref/dml">DML</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sql-reference-select" href="/en//sql-ref/select">SELECT</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sql-reference-select-hint" href="/en//sql-ref/select-hint">SELECT Hint</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sql-reference-user-management" href="/en//sql-ref/user-manage">User Management</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sql-reference-functions" href="/en//sql-ref/func">Functions</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sql-reference-system-session-management" href="/en//sql-ref/sys-session-manage">System/Session Management</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">SDK</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="sdk">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sdk-cli-odbc" href="/en//sdk/cli-odbc">CLI/ODBC</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sdk-cli-odbc-example" href="/en//sdk/cli-odbc-example">CLI/ODBC Example</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sdk-jdbc" href="/en//sdk/jdbc">JDBC</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sdk-python" href="/en//sdk/python">Python</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sdk-net-connector" href="/en//sdk/dotnet">.Net Connector</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sdk-restfull-api" href="/en//sdk/restful">RESTfull API</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-sdk-timezone" href="/en//sdk/timezone">Timezone</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">Tools</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="tools">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-tools-utilities" href="/en//tools/utils">Utilities</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-tools-machcoordinatoradmin" href="/en//tools/machcoordinatoradmin">machcoordinatoradmin</a>
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-tools-machdeployeradmin" href="/en//tools/machdeployeradmin">machdeployeradmin</a>
                        
                      </li>
                    </ul>
                  
                  
                
                  
                  
                  <li class="td-sidebar-nav__section-title">
                    <a href="" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section ">FAQ</a>
                  </li>
                  
                  
                    <ul>
                      <li class="collapse show" id="faq">
                        
                          <a class="td-sidebar-link td-sidebar-link__page " id="m-machbase-manual-faq-how-to-fix-properties-when-an-insufficient-memory-error-occurs" href="/en//faq/memory-error">How to fix properties when an insufficient memory error occurs</a>
                        
                      </li>
                    </ul>
                  
                  
                
              </ul>
            </li>
          </ul>
        
        
      </ul>
    
  </nav>
</div>

          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
              <div class="td-page-meta ml-2 pb-1 pt-2 mb-0">
              </div>
              <nav id="TableOfContents"><ul>
              <li><ul id="TOC">
                <!-- Links will be appended here-->
              </ul></li>
              </ul></nav>
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            <nav aria-label="breadcrumb" class="d-none d-md-block d-print-none">
	      <ol class="breadcrumb spb-1">
              
              
                
                  
                    <li class="breadcrumb-item active" aria-current="page">
                      <a href="/en/search/">Search</a>
                    </li>
                  
                
              
	      </ol>
           </nav>
           <div class="td-content">        
            
              
            
            
	      <input class="form-control td-search-input" type="search" name="q" id="search-input" placeholder="&#xf002 Search this site…"  style="margin-top:5px" autofocus>
<i style="color:white; margin-right:8px; margin-left:5px" class="fa fa-search"></i>

<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>

<ul id="search-results"></ul>


<script>
	window.data = {
		
		
		
				
					
					
					"404-html": {
						"id": "404-html",
						"title": "Page Not Found",
						"version": "all",
						"categories": "",
						"url": " /404.html",
						"content": "Page Not Found\n\nUnfortunately we were unable to find the page you requested. It could be the page doesn’t exist or only exists for a specific version of these documents so please use the search feature to see if you are able to locate the information you were after."
					}
					
				
		
				
					,
					
					"install-cluster-command-coordinator-deployer-install-html": {
						"id": "install-cluster-command-coordinator-deployer-install-html",
						"title": "Coordinator / Deployer Installation, Add Package",
						"version": "all",
						"categories": "",
						"url": " /install/cluster/command/Coordinator-Deployer-Install.html",
						"content": "Index\n\n\n  Installing Coordinator\n    \n      Configuration\n      Create and Unzip Directory\n      Port Configuration and Service Activation\n      Node Registration and Verification\n    \n  \n  Delete Coordinator\n  Secondary Coordinator Installation\n    \n      Create and Unzip Directory\n      Port Settings\n      Node Registration and Verification\n      Run Service\n    \n  \n  Delete Secondary Coordinator\n  Deployer Installation\n    \n      Configuration\n      Create and Unzip Directory\n      Port Configuration and Service Activation\n      Node Registration and Verification\n    \n  \n  Add Package\n  Delete Package\n\n\nInstalling Coordinator\n\nConfiguration\n\nLog in with machbase account and execute with machbase privilege.\n\nConfigure installation directory and path information.\n\n# Edit .bashrc.                                  \nexport MACHBASE_COORDINATOR_HOME=~/coordinator\nexport MACHBASE_DEPLOYER_HOME=~/deployer\nexport MACHBASE_HOME=~/coordinator\nexport PATH=$MACHBASE_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$MACHBASE_HOME/lib:$LD_LIBRARY_PATH\n \n# Reflect changes.\nsource .bashrc\n\n\nCreate and Unzip Directory\n\nCreate a dedicated directory and unzip the package archive into that directory.\n\n# Create directory.                                     \nmkdir $MACHBASE_COORDINATOR_HOME\n  \n# Unzip.\ntar zxvf machbase-ent-x.y.z.official-LINUX-X86-64-release.tgz -C $MACHBASE_COORDINATOR_HOME\n\n\nPort Configuration and Service Activation\n\nModify the machbase.conf file to set the port and start the service.\n\n# Set port from machbase.conf file.\ncd $MACHBASE_COORDINATOR_HOME/conf\nvi machbase.conf                                      \nCLUSTER_LINK_HOST       = 192.168.0.83 (Node ip to be added)\nCLUSTER_LINK_PORT_NO    = 5101\nHTTP_ADMIN_PORT         = 5102\n  \n# Create meta information and run service.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -c\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -u\n\n\nNode Registration and Verification\n\nAdd and confirm the Coordinator node.\n\n# Register node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.83:5101\" --node-type=coordinator                                      \n  \n# Check node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\n\n  \n    \n      Optianl\n      Description\n      Example\n    \n  \n  \n    \n      –add-node\n      Specifies the node name to be added as “IP: PORT”.The PORT value is the CLUSTER_LINK_PORT_NO value.\n      192.168.0.83:5101\n    \n    \n      –node-type\n      Specifies the node type.There are four types: coordinator / deployer / broker / warehouse.\n      coordinator\n    \n  \n\n\nDelete Coordinator\n\nConnect to the server where the Coordinator is installed, terminate the Coordinator process properly, and delete the Coordinator directory.\n\n# Terminate coordinator and delete directory.\nprocess$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -s\nrm -rf $MACHBASE_COORDINATOR_HOME\n\n\nSecondary Coordinator Installation\n\nIf you install an additional Coordinator in addition to the Primary Coordinator, note the following:\n\n\n  Before the Startup of the Secondary Coordinator, you must go to the Primary Coordinator and Add-Node the Secondary Coordinator.\n  When you start the Secondary Coordinator, you must specify the Primary Coordinator as the –primary option.\n  Do not add-node the Primary Coordinator to the Secondary Coordinator.\n\n\nIf this is not followed, the Secondary Coordinator will behave like a Primary Coordinator.\n\nCreate and Unzip Directory\n\nCreate a dedicated directory and unzip the package archive into that directory.\n\n\n# Create directory.                                     \nmkdir $MACHBASE_COORDINATOR_HOME\n  \n# Unzip.\ntar zxvf machbase-ent-x.y.z.official-LINUX-X86-64-release.tgz -C $MACHBASE_COORDINATOR_HOME\n\n\nPort Settings\n\nModify the machbase.conf file to set the port only. When the service starts, it will work like the Primary Coordinator.\n\n# Set port from machbase.conf file.\ncd $MACHBASE_COORDINATOR_HOME/conf\nvi machbase.conf                                      \nCLUSTER_LINK_HOST       = 192.168.0.83 (ip address to be added)\nCLUSTER_LINK_PORT_NO    = 5111\nHTTP_ADMIN_PORT         = 5112\n\n\nNode Registration and Verification\n\nIn the Primary Coordinator, add and confirm the Secondary Coordinator node.\n\n# Register node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.83:5111\" --node-type=coordinator                                      \n  \n# Check node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\nRun Service\n\nNow run the Secondary Coordinator. During startup, the Primary Coordinator must be specified as a –primary option.\n\n# Create meta information and run service.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -c\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -u --primary=\"192.168.0.83:5101\"\n\n\nDelete Secondary Coordinator\n\nAfter removing the Secondary Coordinator registered in the Primary Coordinator, the Secondary Coordinator must be terminated properly.\n\n# Delete node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.83:5101\"\n  \n# Terminate secondary coordinator and delete directory.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin -s\nrm -rf $MACHBASE_COORDINATOR_HOME\n  \n# Check node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\n\n  \n    \n      Optional\n      Description\n      Example\n    \n  \n  \n    \n      –remove-node\n      Specifies the name of the node to be deleted as “IP: PORT”.The PORT value is the CLUSTER_LINK_PORT_NO value.\n      192.168.0.84:5201\n    \n  \n\n\nDeployer Installation\n\n\n  Information\n  The Deployer must be installed in advance on all hosts (= servers) where the broker and warehouse are installed\n\n\nConfiguration\n\nConfigure installation directory and path information.\n\n# Edit .bashrc.                                     \nexport MACHBASE_DEPLOYER_HOME=~/deployer\nexport MACHBASE_HOME=~/deployer\nexport PATH=$MACHBASE_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$MACHBASE_HOME/lib:$LD_LIBRARY_PATH\n  \n# Reflect changes.\nsource .bashrc\n\n\nCreate and Unzip Directory\n\nCreate a dedicated directory and unzip the package archive into that directory.\n\n# Create directory.                                     \nmkdir $MACHBASE_DEPLOYER_HOME\n \n# Unzip.\ntar zxvf machbase-ent-x.y.z.official-LINUX-X86-64-release.tgz -C $MACHBASE_DEPLOYER_HOME\n\n\nPort Configuration and Service Activation\n\nModify the machbase.conf file to set the port and start the service.\n\n# Set port from machbase.conf file.\ncd $MACHBASE_DEPLOYER_HOME/conf\nvi machbase.conf\nCLUSTER_LINK_HOST       = 192.168.0.84\nCLUSTER_LINK_PORT_NO    = 5201\nHTTP_ADMIN_PORT         = 5202\n  \n# Create meta information and run service.\n$MACHBASE_DEPLOYER_HOME/bin/machdeployeradmin -c\n$MACHBASE_DEPLOYER_HOME/bin/machdeployeradmin -u\n\n\nNode Registration and Verification\n\n\n  Caution\n  This should be done at the coordinator node.\n\n\nAdd and verify the Deployer node.\n\n# Register node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5201\" --node-type=deployer                                         \n \n# Check node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\n\n  \n    \n      Optional\n      Description\n      Example\n    \n  \n  \n    \n      –add-node\n      Specifies the node name to be added as “IP: PORT”.The PORT value is the CLUSTER_LINK_PORT_NO value.\n      192.168.0.84:5201\n    \n    \n      –node-type\n      Specifies the node type.There are four types: coordinator / deployer / broker / warehouse.\n      deployer\n    \n  \n\n\nDelete Deployer\n\nYou must delete the Deployer node from the Coordinator node and properly terminate the Deployer process on the server where the Deployer is located.\n\n# Delete node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.84:5201\"\n \n# Terminate deployer and delete directory.\n$MACHBASE_DEPLOYER_HOME/bin/machdeployeradmin -d\nrm -rf $MACHBASE_DEPLOYER_HOME\n  \n# Check node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --cluster-status\n\n\nAdd Package\n\nAdd a package to be installed as broker and warehouse to Coordinator. At this time, the registered package registers the lightweight version excluding MWA.\n\n# Register installation package.                             \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-package=machbase \\\n    --file-name=\"/home/machbase/machbase-ent-x.y.z.official-LINUX-X86-64-release-lightweight.tgz\"\n\n\n\n  \n    \n      Optional\n      Description\n      Example\n    \n  \n  \n    \n      –add-package\n      Specifies the package name to be added.\n      machbase\n    \n    \n      –file-name\n      Specifies the full path and file name of the package file.Specifies a lightweight package that excludes MWA files, since this package is for broker and warehouse installations only.\n      /home/machbase/machbase-ent-5.0.0.official-LINUX-X86-64-release-lightweight.tgz\n    \n  \n\n\nDelete Package\n\nDelete the package registered in Coordinator.\n\n# Delete registered package.                             \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-package=machbase"
					}
					
				
		
				
					,
					
					"install-cluster-command-lookup-broker-warehouse-install-html": {
						"id": "install-cluster-command-lookup-broker-warehouse-install-html",
						"title": "Lookup / Broker / Warehouse Installation",
						"version": "all",
						"categories": "",
						"url": " /install/cluster/command/Lookup-Broker-Warehouse-Install.html",
						"content": "Index\n\n\n  Lookup Installation\n    \n      Installation Conditions\n    \n  \n  Delete Lookup\n  Shut Down/Stop Lookup\n  Change Lookup Master\n  Broker Installation\n  Delete Broker\n  Shut Down/Stop Broker\n  Warehouse Installation\n    \n      Group 1 Installation\n      Add Node to Group 1\n      Group 2 Installation\n      Add Node to Group 2\n    \n  \n  Delete Warehouse\n  Shut Down/Stop Warehouse\n\n\nLookup Installation\n\nAdd a lookup node in the Coordinator node. Multiple lookup nodes can be registered.\n\nThe deployer node must be pre-installed on the server.\n\nWhen the deployer node is installed, all operations are performed on the coordinator node, and there is nothing to set up by connecting to the server.\n\n# Add lookup master node\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5301\"  \\\n        --node-type=lookup --lookup-type=master --deployer=\"192.168.0.84:5201\"      \\\n        --home-path=\"/home/machbase/lookup1\"\n \n \n# Add lookup monitor node\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5302\"  \\\n        --node-type=lookup --lookup-type=monitor --deployer=\"192.168.0.84:5201\"         \\\n        --home-path=\"/home/machbase/lookupm1\"\n \n \n# Add lookup slave node\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5303\"  \\\n        --node-type=lookup --lookup-type=slave --deployer=\"192.168.0.84:5201\"       \\\n        --home-path=\"/home/machbase/lookup3\"\n \n  \n# Run lookup node\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5301\"\n \n# You can run lookup nodes in batches\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-lookup\n\n\n\n  \n    \n      Option items\n      Description\n      Example\n    \n  \n  \n    \n      –add-node\n      Specifies the node name to be added as “IP: PORT”.\n      192.168.0.84:5301\n    \n    \n      –node-type\n      Specifies the node type.There are five types: coordinator, deployer, lookup, broker, and warehouse.\n      lookup\n    \n    \n      –deployer\n      Register the deployer node information of the server to be installed.\n      192.168.0.84:5201\n    \n    \n      –lookup-type\n      Specify the lookup typeThere are three types: master, slave, monitor.\n      master\n    \n    \n      –home-path\n      Specifies the path to install.Specifies /home/machbase/lookup in machbase account.\n      /home/machabse/lookup\n    \n  \n\n\nInstallation Conditions\n\nThere are 3 types of Lookup node, Master, Slave, and Monitor, and it must be installed according to the conditions below.\n\n1. Lookup Master Node\n    a. It is a Lookup node that must have only one.\n    b. It must be installed before Monitor and Slave nodes.\n2. Lookup Monitor Node\n    a. It is a Lookup node that must exist at least one.\n    b.For stable HA, there should be one in each server.\n3. Lookup Slave Node\n    a. It is recommended that there be more than one for HA. (If not, HA cannot be guaranteed)\n\n\nDelete Lookup\n\nRemove the broker node from the Coordinator node.\n\n# Delete lookup node\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.84:5301\"\n\n\nShut Down/Stop Lookup\n\nShut down / kill the lookup node on the Coordinator node.\n\n\n# Terminate lookup node\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --shutdown-node=\"192.168.0.84:5301\"\n \n# You can terminate lookup nodes in batches\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --shutdown-lookup\n\n\nChange Lookup Master\n\nYou can change the lookup master node in the coordinator node.\n\nOnly lookup slaves can be changed to lookup masters, and the existing lookup masters become lookup slaves.\n\n# Change lookup master\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --set-lookup-master=\"192.168.0.84:5301\"\n\n\nBroker Installation\n\nAdd a broker node to the coordinator node. Multi-broker node registration is possible.\n\nThe deployer node must be installed on the server in advance.\n\nOnce the deployer node is installed, there is no setting by connecting to the server as  all work can be done on the coordinator node.\n\nThe first registered node becomes the leader broker, and the additional registered node becomes the follower broker.\n\n# Add broker node.                                \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5301\"  \\\n        --node-type=broker --deployer=\"192.168.0.84:5201\" --port-no=\"5656\"          \\\n        --home-path=\"/home/machbase/broker\" --package-name=machbase\n  \n# Run broker node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5301\"\n\n\n\n  \n    \n      Option items\n      Description\n      Example\n    \n  \n  \n    \n      –add-node\n      Specifies the node name to be added as “IP: PORT”.The PORT value is set to the CLUSTER_LINK_PORT_NO value.\n      192.168.0.84:5301\n    \n    \n      –node-type\n      Specifies the node type.There are five types: coordinator, deployer, lookup, broker, and warehouse.\n      broker\n    \n    \n      –deployer\n      Register the deployer node information of the server to be installed.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      Specifies ‘machbased’ port.The Broker specifies a default value of 5656.This port is used when connecting to client and machsql.\n      5656\n    \n    \n      –home-path\n      Specifies the path to install.Specifies /home/machbase/broker in machbase account\n      /home/machbase/broker\n    \n    \n      –package-name\n      Sets the package name specified when package was added.\n      machbase\n    \n  \n\n\nDelete Broker\n\nRemove the broker node from the Coordinator node.\n\n# Delete broker node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.84:5301\"\n\n\nShut Down/Stop Broker\n\nThere is a way to shut down / kill the broker node on the Coordinator node.\n\n# Terminate broker node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --shutdown-node=\"192.168.0.84:5301\"\n  \n# Stop broker node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --kill-node=\"192.168.0.84:5301\"\n\n\nAlternatively, you can shut down / kill the process directly from the server where the broker is installed.\n\n# Terminate broker node.\n$MACHBASE_HOME/bin/machadmin -s\n  \n# Stop broker node.\n$MACHBASE_HOME/bin/machadmin -k\n\n\nWarehouse Installation\n\nInstall the active node and the standby node from the Coordinator node.\n\nThey will be installed through a pre-installed deployer.\n\nGroup 1 Installation\n\nInstall the first Warehouse Group1 node.\n\n# Install group1 warehouse.                         \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.83:5401\"  \\\n        --node-type=warehouse --deployer=\"192.168.0.83:5201\" --port-no=\"5400\"       \\\n        --home-path=\"/home/machbase/warehouse_g1\" --package-name=machbase           \\\n        --replication=\"192.168.0.83:5402\"  --group=\"group1\" --no-replicate\n  \n# Run installed node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5401\"\n\n\n\n  \n    \n      Option items\n      Description\n      Example\n    \n  \n  \n    \n      –add-node\n      Specifies the node name to be added as “IP: PORT”.The PORT value is set to the CLUSTER_LINK_PORT_NO value.\n      192.168.0.84:5401\n    \n    \n      –node-type\n      Specifies the node type.There are five types: coordinator, deployer, lookup, broker, and warehouse.\n      warehouse\n    \n    \n      –deployer\n      Registers the deployer node information of the server to be installed.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      Specifies the working port of ‘machbased’.Since the value was set to 5656 on the Broker, a different port must be specified if it is installed on the same server. The warehouse port number is set as 5400.This port is used when connecting to client and machsql.\n      5400\n    \n    \n      –home-path\n      Specifies the path to install. To distinguish the groups, set them in order of warehouse_g1, g2, g3.\n      /home/machbase/warehouse_g1\n    \n    \n      –package-name\n      Sets the package name specified when adding the package.\n      machbase\n    \n    \n      –replication\n      Specifies the node in charge of replication as “IP: PORT”.The port value is set to the warehouse port number 5402.\n      192.168.0.84:5402\n    \n    \n      –no-replicate\n      Specifies whether to replicate data when adding a node if there is warehouse data in the group.\n       \n    \n    \n      –set-group-state\n      Specifies the state of the group as normal and readonly.Normal is read, write / readonly is read only\n       \n    \n  \n\n\nAdd Node to Group 1\n\nAdd another node to Warehouse Group1.\n\n# Add warehouse node to group1.              \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5401\"  \\\n        --node-type=warehouse --deployer=\"192.168.0.84:5201\" --port-no=\"5400\"       \\\n        --home-path=\"/home/machbase/warehouse_g1\" --package-name=machbase           \\\n        --replication=\"192.168.0.84:5402\" --group=\"group1\" --no-replicate\n  \n# Run installed node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5401\"\n\n\n\n  \n    \n      Option items\n      Description\n      Example\n    \n  \n  \n    \n      –add-node\n      Specifies the node name to be added as “IP: PORT”.The PORT value is set to the CLUSTER_LINK_PORT_NO value.\n      192.168.0.84:5401\n    \n    \n      –node-type\n      Specifies the node type.There are five types: coordinator, deployer, lookup, broker, and warehouse.\n      warehouse\n    \n    \n      –deployer\n      Registers the deployer node information of the server to be installed.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      Specifies the working port  of ‘machbased’.Since the value was set to 5656 on the Broker, a different port must be specified if it is installed on the same server. The warehouse port number is set as 5400.This port is used when connecting to client and machsql.\n      5400\n    \n    \n      –home-path\n      Specifies the path to install. To distinguish the groups, set them in order of warehouse_g1, g2, g3.\n      /home/machabse/warehouse_g1\n    \n    \n      –package-name\n      Sets the package name specified when adding the package.\n      machbase\n    \n    \n      –replication\n      Specify the node in charge of replication as “IP: PORT”.The port value is set to the warehouse port number 5402.\n      192.168.0.84:5402\n    \n    \n      –group\n      Specifies the Group name.\n      group1\n    \n    \n      –no-replicate\n      Specifies whether to replicate data when adding a node if there is warehouse data in the group.\n       \n    \n    \n      –set-group-state\n      Specifies the state of the group as normal and readonly.Normal is read, write / readonly is read only\n       \n    \n  \n\n\nGroup 2 Installation\n\nInstall the second Warehouse Group2 node.\n\n# Install group1 warehouse.                         \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.84:5411\"  \\\n        --node-type=warehouse --deployer=\"192.168.0.84:5201\" --port-no=\"5410\"       \\\n        --home-path=\"/home/machbase/warehouse_g2\" --package-name=machbase           \\\n        --replication=\"192.168.0.84:5412\"  --group=\"group2\" --no-replicate\n  \n# Run installed node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.84:5411\"\n\n\n\n  \n    \n      Option items\n      Description\n      Example\n    \n  \n  \n    \n      –add-node\n      Specifies the node name to be added as “IP: PORT”.The PORT value is set to the CLUSTER_LINK_PORT_NO value.\n      192.168.0.84:5411\n    \n    \n      –node-type\n      Specifies the node type.There are five types: coordinator, deployer, lookup, broker, and warehouse.\n      warehouse\n    \n    \n      –deployer\n      Registers the deployer node information of the server to be installed.\n      192.168.0.84:5201\n    \n    \n      –port-no\n      Specifies the working port  of ‘machbased’.Since the value was set to 5656 on the Broker, a different port must be specified if it is installed on the same server. The warehouse port number is set as 5410.This port is used when connecting to client and machsql.\n      5410\n    \n    \n      –home-path\n      Specifies the path to install. To distinguish the groups, set them in order of warehouse_g1, g2, g3.\n      /home/machbase/warehouse_g2\n    \n    \n      –package-name\n      Sets the package name specified when adding the package.\n      machbase\n    \n    \n      –replication\n      Specifies the node in charge of replication as “IP: PORT”.The port value is set to the warehouse port number 5412.\n      192.168.0.84:5412\n    \n    \n      –group\n      Specifies the Group name.\n      group\n    \n    \n      –no-replicate\n      Specifies whether to replicate data when adding a node if there is warehouse data in the group.\n       \n    \n    \n      –set-group-state\n      Specifies the state of the group as normal and readonly.Normal is read, write / readonly is read only\n       \n    \n  \n\n\nAdd Node to Group 2\n\nAdd another node to Warehouse Group2.\n\n# Add warehouse node to group1.              \n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --add-node=\"192.168.0.83:5411\"  \\\n        --node-type=warehouse --deployer=\"192.168.0.83:5201\" --port-no=\"5410\"       \\\n        --home-path=\"/home/machbase/warehouse_g2\" --package-name=machbase           \\\n        --replication=\"192.168.0.83:5412\" --group=\"group2\" --no-replicate\n  \n# Run installed node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --startup-node=\"192.168.0.83:5411\"\n\n\n\n  \n    \n      Option items\n      Description\n      Example\n    \n  \n  \n    \n      –add-node\n      Specifies the node name to be added as “IP: PORT”.The PORT value is set to the CLUSTER_LINK_PORT_NO value.\n      192.168.0.83:5411\n    \n    \n      –node-type\n      Specifies the node type.There are five types: coordinator, deployer, lookup, broker, and warehouse.\n      warehouse\n    \n    \n      –deployer\n      Registers the deployer node information of the server to be installed.\n      192.168.0.83:5201\n    \n    \n      –port-no\n      Specifies the working port of ‘machbased’.Since the value was set to 5656 on the Broker, a different port must be specified if it is installed on the same server. The warehouse port number is set as 5400.This port is used when connecting to client and machsql.\n      5410\n    \n    \n      –home-path\n      Specifies the path to install. To distinguish the groups, set them in order of warehouse_g1, g2, g3.\n      /home/machbase/warehouse_g2\n    \n    \n      –package-name\n      Sets the package name specified when adding the package.\n      machbase\n    \n    \n      –replication\n      Specifies the node in charge of replication as “IP: PORT”.The port value is set to the warehouse port number 5412.\n      192.168.0.83:5412\n    \n    \n      –group\n      Specifies the Group name.\n      group2\n    \n    \n      –no-replicate\n      Specifies whether to replicate data when adding a node if there is warehouse data in the group.\n       \n    \n    \n      –set-group-state\n      Specifies the state of the group as normal and readonly.Normal is read, write / readonly is read only\n       \n    \n  \n\n\nDelete Warehouse\n\nDelete the warehouse node from the Coordinator node.\n\n# Delete warehouse node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --remove-node=\"192.168.0.83:5401\"\n\n\nShut Down/Stop Warehouse\n\nThere is a way to shut down / kill the warehouse node at the Coordinator node.\n\n# Terminate warehouse node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --shutdown-node=\"192.168.0.83:5401\"\n  \n# Stop warehouse node.\n$MACHBASE_COORDINATOR_HOME/bin/machcoordinatoradmin --kill-node=\"192.168.0.83:5401\"\n\n\nOtherwise, the process can be shut down / killed directly from the server where the warehouse is installed.\n\n# Terminate warehouse node.\n$MACHBASE_HOME/bin/machadmin -s\n  \n# Stop warehouse node.\n$MACHBASE_HOME/bin/machadmin -k"
					}
					
				
		
				
					,
					
					"feature-tables-log-input-append-html": {
						"id": "feature-tables-log-input-append-html",
						"title": "Append",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/input/append.html",
						"content": "This is a fast real-time data input API provided by Machbase.\n\nIt can be entered using C, C ++, C #, Java, Python, PHP, or Javascript.\n\nRefer to the SDK Guide for details."
					}
					
				
		
				
					,
					
					"feature-tables-backup-mount-html": {
						"id": "feature-tables-backup-mount-html",
						"title": "Backup and Mount",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/backup-mount.html",
						"content": "It is very important to backup your data on a regular basis. This chapter describes how to back up Machbase data and restore backed up data. Machbase also provides a mount function that allows you to retrieve backed up data without restoring it. The mount function can be executed very quickly when the backed up data needs to be read.\n\n\n  Backup Overview\n  Database Mount\n  Backup and Recovery"
					}
					
				
		
				
					,
					
					"feature-tables-backup-mount-backup-recover-html": {
						"id": "feature-tables-backup-mount-backup-recover-html",
						"title": "Backup and Recovery",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/backup-mount/backup-recover.html",
						"content": "Database Backup\nThere are two methods of Machbase backup:\n\n  Full backup of current DB\n  Select and backup only specific tables\n\n\nSyntax:\n\nBACKUP [ DATABASE | TABLE table_name ]  [ time_duration ] INTO [ DISK ] = 'path/backup_name';\ntime_duration = FROM start_time TO end_time\npath = 'absolute_path' or  'relative_path'\nBACKUP [ DATABASE | TABLE table_name ] AFTER 'previous_backup_dir'\n\n\nExample:\n\n# Directory backup\n       BACKUP DATABASE INTO DISK = 'backup_dir_name';\n \n# Set backup duration\n      - Directory backup\n       BACKUP DATABASE FROM TO_DATE('2015-07-14 00:00:00','YYYY-MM-DD HH24:MI:SS')\n                         TO TO_DATE('2015-07-14 23:59:59','YYYY-MM-DD HH24:MI:SS')\n                         INTO DISK = '/home/machbase/backup_20150714'\n\n\nWhen executing the backup command, the backup type, duration condition, and backup destination path must be defined. To back up the entire database, you must specify “DATABASE”. To back up specific tables, specify “TABLE” as the backup type. When backing up specific tables, you must specify the table name.\n\nYou can specify the backup destination using the DURATION conditional statement. Specify the start time and end time of the backup target data in the FROM and TO clauses. In the example above, “2015-07-14 00:00:00” is defined as FROM and “2015-07-14 23:59:59” is defined as TO, so the user will be able to see all data for July 14, 2015 Quot; Duration If you do not specify a time condition, “1970-01-01 00:00:00” is set to FROM and the current time at which it is executed is set in the TO clause.\n\nThe DURATION clause cannot be used in the DATABASE including the tag table and the tag table, and to back up only the added data, you must perform the incremental backup (Backup …. AFTER ‘previous_backup’) statement.\nSpecify “IBFILE” to create a backup file as a single file, or “DISK” as a backup option to create multiple files and directories. Note that when specifying a backup path, backup files are created under “$ MACHBASE_HOME / dbs” when a relative path is specified. To specify an absolute path, you must always set a path that starts with “/”.\n\nDatabase Recovery\n\nWhen restoring data from a backup file, it can not be executed by a query command, and the command “machadmin -r” should be executed in a situation where the database does not operate. Before running the backup, you should review for the following conditions:\n\n\n  Is the Machbase database down?\n  Is the current database deleted and replaced with the data to be recovered, and is the deletion of the current database allowed?\n  For incremental backup, previous full backup file is required.\n\n\nSyntax:\n\nmachadmin -r backup_database_path\n\n\nExample:\n\nbackup database into disk = '/home/machbase/backup';\n\nmachadmin -k\nmachadmin -d\nmachadmin -r /home/machbase/backup;"
					}
					
				
		
				
					,
					
					"sdk-cli-odbc-example-html": {
						"id": "sdk-cli-odbc-example-html",
						"title": "CLI/ODBC Example",
						"version": "all",
						"categories": "",
						"url": " /sdk/cli-odbc-example.html",
						"content": "Index\n\n\n  Application Development\n    \n      Check CLI Installation\n      Makefile Creation Guide\n      Compile and Link\n    \n  \n  Sample Program\n    \n      Connection Example\n      Data Input and Output Example\n      Prepare Execute Example\n      Extension Function Append Example\n      Acquiring Table Column Information Example\n      Multi-Thread Append Example\n    \n  \n\n\nApplication Development\n\nCheck CLI Installation\n\nIf the following files are included in include and lib of the directory where Machbase is installed, the environment in which the application can be developed is complete.\n\nMach@localhost:~/machbase_home$ ls -l include lib install/\ninclude:\ntotal 176\n-rwxrwxr-x 1 mach mach 31449 Jun 18 19:26 machbase_sqlcli.h\n \ninstall/:\ntotal 12\n-rw-rw-r-- 1 mach mach 1667 Jun 18 19:26 machbase_env.mk\n \nlib:\ntotal 16196\n-rw-rw-r-- 1 mach mach  78603 Jun 18 19:26 machbase.jar\n-rw-rw-r-- 1 mach mach 964290 Jun 18 19:26 libmachbasecli.a\n\n\nMakefile Creation Guide\n\nmach@localhost:~/machbase_home$ cd sample/\nmach@localhost:~/machbase_home/sample$ cd cli/\nmach@localhost:~/machbase_home/sample/cli$ ls\nMakefile sample1_connect.c\n\n\nIf the Machbase package was installed, the sample program will be installed in the following path.\n\ninclude $(MACHBASE_HOME)/install/machbase_env.mk\nINCLUDES += $(LIBDIR_OPT)/$(MACHBASE_HOME)/include\n \nall : sample1_connect\n \nsample1_connect : sample1_connect.o\n$(LD_CC) $(LD_FLAGS) $(LD_OUT_OPT)$@ $&lt; $(LIB_OPT)machbasecli$(LIB_AFT) $(LIBDIR_OPT)$(MACHBASE_HOME)/lib $(LD_LIBS)\n \nsample1_connect.o : sample1_connect.c\n$(COMPILE.cc) $(CC_FLAGS) $(INCLUDES) $(CC_OUT_OPT)$@ $&lt;\n \nclean :\nrm -f sample1_connect\n\n\nCompile and Link\n\nExecuting the following for a given sample creates an executable file.\n\nmach@localhost:~/machbase_home/sample/cli$ make\ngcc -c -g -W -Wall -rdynamic -O3 -finline-functions -fno-omit-frame-pointer -fno-strict-aliasing -m64 -mtune=k8 -g -W -Wall -rdynamic -O3 -finline-functions -fno-omit-frame-pointer -fno-strict-aliasing -m64 -mtune=k8 -I/home/machbase/machbase_home/include -I. -L//home/machbase/machbase_home/include -osample1_connect.o sample1_connect.c\ngcc -m64 -mtune=k8 -L/home/machbase/machbase_home/lib -osample1_connect sample1_connect.o -lmachbasecli -L/home/machbase/machbase_home/lib -lm -lpthread -ldl -lrt -rdynamic\nmach@localhost:~/machbase_home/sample/cli$ ls -al\ntotal 1196\ndrwxrwxr-x 2 mach mach 4096 Jun 18 20:15 .\ndrwxrwxr-x 4 mach mach 4096 Jun 18 19:26 ..\n-rw-rw-r-- 1 mach mach 483 Jun 18 19:26 Makefile\n-rwxrwxr-x 1 mach mach 1196943 Jun 18 20:15 sample1_connect\n-rw-rw-r-- 1 mach mach 549 Jun 18 19:26 sample1_connect.c\n-rw-rw-r-- 1 mach mach 8168 Jun 18 20:15 sample1_connect.o\n\n\n\n  You can write your application as necessary by modifying the sample Makefile above\n\n\nSample Program\n\nConnection Example\n\nWe will create an example program to connect using the CLI.\nThe sample file name is sample1_connect.c.\nMACHBASE_PORT_NO must be the same as the PORT_NO value in the $MACHBASE_HOME/conf/machbase.conf file.\n\nsample1_connect.c\n\nIf you register sample1_connect.c in Makefile, compile and run it, it will appear as follows.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample1_connect\nconnected ...\n\n\nData Input and Output Example\n\nIn the example source below, we created a table using the CREATE TABLE statement, arbitrarily create simple data values, input data using the INSERT statement, and output the data using the SELECT statement. You will be able to see how to configure each type when entering and checking values ​​directly.\nThe sample file name is sample2_insert.c.\n\nsample2_insert.c\n\nIf you register sample2_insert.c in the Makefile, compile and run it, it will appear as follows.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample2_insert\n \nconnected ...\n1 record inserted\n2 record inserted\n3 record inserted\n4 record inserted\n5 record inserted\n6 record inserted\n7 record inserted\n8 record inserted\n9 record inserted\n===== 0 ========\nseq = 9, score = 18, total = 270000, percentage = 0.00, ratio = 3.3e-05, id = id-9, srcip = 192.168.0.9, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0009, regdate = 2015-03-31 15:26:09, log = text log-9, image = 62696E61727920696D6167652D39\n===== 1 ========\nseq = 8, score = 16, total = 240000, percentage = 0.00, ratio = 3.3e-05, id = id-8, srcip = 192.168.0.8, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0008, regdate = 2015-03-31 15:26:08, log = text log-8, image = 62696E61727920696D6167652D38\n===== 2 ========\nseq = 7, score = 14, total = 210000, percentage = 0.00, ratio = 3.3e-05, id = id-7, srcip = 192.168.0.7, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0007, regdate = 2015-03-31 15:26:07, log = text log-7, image = 62696E61727920696D6167652D37\n===== 3 ========\nseq = 6, score = 12, total = 180000, percentage = 0.00, ratio = 3.3e-05, id = id-6, srcip = 192.168.0.6, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0006, regdate = 2015-03-31 15:26:06, log = text log-6, image = 62696E61727920696D6167652D36\n===== 4 ========\nseq = 5, score = 10, total = 150000, percentage = 0.00, ratio = 3.3e-05, id = id-5, srcip = 192.168.0.5, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0005, regdate = 2015-03-31 15:26:05, log = text log-5, image = 62696E61727920696D6167652D35\n===== 5 ========\nseq = 4, score = 8, total = 120000, percentage = 0.00, ratio = 3.3e-05, id = id-4, srcip = 192.168.0.4, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0004, regdate = 2015-03-31 15:26:04, log = text log-4, image = 62696E61727920696D6167652D34\n===== 6 ========\nseq = 3, score = 6, total = 90000, percentage = 0.00, ratio = 3.3e-05, id = id-3, srcip = 192.168.0.3, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0003, regdate = 2015-03-31 15:26:03, log = text log-3, image = 62696E61727920696D6167652D33\n===== 7 ========\nseq = 2, score = 4, total = 60000, percentage = 0.00, ratio = 3.3e-05, id = id-2, srcip = 192.168.0.2, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0002, regdate = 2015-03-31 15:26:02, log = text log-2, image = 62696E61727920696D6167652D32\n===== 8 ========\nseq = 1, score = 2, total = 30000, percentage = 0.00, ratio = 3.3e-05, id = id-1, srcip = 192.168.0.1, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0001, regdate = 2015-03-31 15:26:01, log = text log-1, image = 62696E61727920696D6167652D31\n\n\nPrepare Execute Example\n\nLet’s write an example program that binds and INSERTs data.\nYou can enter a value by binding data in Machbase. When you use this, you need to specify the types of data values ​​clearly. In case of long string types, you must specify the length value.\nThe following example shows how to bind data for each type.\nThe file name is sample3_prepare.c.\n\nsample3_prepare.c\n\nIf you register sample3_prepare.c in the Makefile, compile and run it, it will appear as follows.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample3_prepare\n \nconnected ...\n1 prepared record inserted\n2 prepared record inserted\n3 prepared record inserted\n4 prepared record inserted\n5 prepared record inserted\n6 prepared record inserted\n7 prepared record inserted\n8 prepared record inserted\n9 prepared record inserted\n===== 0 ========\nseq = 9, score = 18, total = 270000, percentage = 1.11, ratio = 3.7037e-05, id = id-9, srcip = 192.168.0.9, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0009, regdate = 1970-01-01 09:00:00, log = log-9, image = 696D6167652D39\n===== 1 ========\nseq = 8, score = 16, total = 240000, percentage = 1.12, ratio = 3.75e-05, id = id-8, srcip = 192.168.0.8, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0008, regdate = 1970-01-01 09:00:00, log = log-8, image = 696D6167652D38\n===== 2 ========\nseq = 7, score = 14, total = 210000, percentage = 1.14, ratio = 3.80952e-05, id = id-7, srcip = 192.168.0.7, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0007, regdate = 1970-01-01 09:00:00, log = log-7, image = 696D6167652D37\n===== 3 ========\nseq = 6, score = 12, total = 180000, percentage = 1.17, ratio = 3.88889e-05, id = id-6, srcip = 192.168.0.6, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0006, regdate = 1970-01-01 09:00:00, log = log-6, image = 696D6167652D36\n===== 4 ========\nseq = 5, score = 10, total = 150000, percentage = 1.20, ratio = 4e-05, id = id-5, srcip = 192.168.0.5, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0005, regdate = 1970-01-01 09:00:00, log = log-5, image = 696D6167652D35\n===== 5 ========\nseq = 4, score = 8, total = 120000, percentage = 1.25, ratio = 4.16667e-05, id = id-4, srcip = 192.168.0.4, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0004, regdate = 1970-01-01 09:00:00, log = log-4, image = 696D6167652D34\n===== 6 ========\nseq = 3, score = 6, total = 90000, percentage = 1.33, ratio = 4.44444e-05, id = id-3, srcip = 192.168.0.3, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0003, regdate = 1970-01-01 09:00:00, log = log-3, image = 696D6167652D33\n===== 7 ========\nseq = 2, score = 4, total = 60000, percentage = 1.50, ratio = 5e-05, id = id-2, srcip = 192.168.0.2, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0002, regdate = 1970-01-01 09:00:00, log = log-2, image = 696D6167652D32\n===== 8 ========\nseq = 1, score = 2, total = 30000, percentage = 2.00, ratio = 6.66667e-05, id = id-1, srcip = 192.168.0.1, dstip = 2001:0DB8:0000:0000:0000:0000:1428:0001, regdate = 1970-01-01 09:00:00, log = log-1, image = 696D6167652D31\n\n\nExtension Function Append Example\n\nIn Machbase, the append protocol is provided by reading a large amount of data from a file and inputting it at a high speed. Let’s write an example program that uses this Append protocol.\nFirst, let’s look at an example of how to append to the various types provided by Machbase. The Append method has its own settings for each type. Therefore, if you know how to use every method, you will be able to write programs more efficiently. All the methods are listed in the example code at the bottom.\nThe file name is sample4_append1.c.\n\nsample4_append.c\n\nIf you register sample4_append1.c in the Makefile, compile and run it, it will appear as follows.\n\n[mach@localhost cli]$ make sample4_append1\ngcc -c -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -I/home/mach/machbase_home/include -I. -L//home/mach/machbase_home/include -osample4_append1.o sample4_append1.c\ngcc -m64 -mtune=k8 -L/home/mach/machbase_home/lib -osample4_append1 sample4_append1.o -lmachcli -L/home/mach/machbase_home/lib -lm -lpthread -ldl -lrt -rdynamic\n[mach@localhost cli]$ ./sample4_append1\nconnected ...\nappend open ok\nappend close ok\nsuccess : 13, failure : 0\ntimegap = 48 microseconds for 13 records\n270833.33 records/second\n[mach@localhost cli]$\n \nYou can check what is inserted after MACH_SQL.\n \nMach&gt; select * from CLI_SAMPLE;\nSHORT1 INTEGER1 LONG1 FLOAT1 DOUBLE1\n-----------------------------------------------------------------------------------------------------------\nDATETIME1 VARCHAR1 IP IP2\n------------------------------------------------------------------------------------------------------------------------------\nTEXT1\n------------------------------------------------------------------------------------\nBIN1\n------------------------------------------------------------------------------------\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 203.212.222.111 NULL\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXX\nFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFA\nFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFA\nFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFAFA\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 203.212.222.111 NULL\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXX\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 203.212.222.111 7F7F:7F7F:7F7F:7F7F:7F7F:7F7F:7F7F:7F7F\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 203.212.222.111 NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 192.168.0.1 NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR 127.0.0.1 NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 MY VARCHAR NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2000-12-31 00:00:00 000:000:000 NULL NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2014-05-23 17:41:28 000:000:000 NULL NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n2015-04-09 16:44:11 134:256:000 NULL NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n1970-01-01 09:00:01 000:000:000 NULL NULL NULL\nNULL\nNULL\n2 4 6 8.4 10.9\n1970-01-01 09:00:00 000:000:000 NULL NULL NULL\nNULL\nNULL\n[12] row(s) selected.\n\n\nNow let’s use a fast append method using a file. This is an example useful for fast input of large amounts of logs, packets, etc. used in business. The file name is sample4_append2.c.\nYou have to save the data to be entered in advance in data.txt.\n\n./make_data\n\n\nModifying the given make_data.c gives you the opportunity to create a data.txt file for your situation.\n\nmake_data.c\n\nIf you register sample4_append2.c in the Makefile, compile and run it, it will appear as follows.\n\n[mach@localhost cli]$ make\ngcc -c -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -I/home/mach/machbase_home/include -I. -L//home/mach/machbase_home/include -osingle_append2.o single_append2.c\ngcc -m64 -mtune=k8 -L/home/mach/machbase_home/lib -osingle_append2 single_append2.o -lmachcli -L/home/mach/machbase_home/lib -lm -lpthread -ldl -lrt -rdynamic\n[mach@localhost cli]$ ./single_append2\nconnected ...\ntable created\nappend open ok\nappend data start\n....................................................................................................\nappend data end\nappend close ok\nsuccess : 1000000, failure : 0\ntimegap = 1641503 microseconds for 1000000 records\n609197.79 records/second\n\n\nAcquiring Table Column Information Example\n\nThere are a number of ways to obtain table column information, but we will look at how to use SQLDescribeCol and SQLColumns.\n\nSQLDescribeCol\n\nThe sample file name is sample5_describe.c.\n\nsample5_describe.c\n\nIf you add the above file and run make, you can see the contents of the column as shown below.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample5_describe\nconnected ...\n----------------------------------------------------------------\nName Type Length\n----------------------------------------------------------------\nSEQ 5 5\nSCORE 4 10\nTOTAL -5 19\nPERCENTAGE 6 27\nRATIO 8 27\nID 12 10\nSRCIP 2104 15\nDSTIP 2106 60\nREG_DATE 9 31\nTLOG 2100 67108864\nIMAGE -2 67108864\n----------------------------------------------------------------\n[mach@localhost cli]$\n\n\nSQLColumns\n\nSQLColumns is a function that can find the information of the columns existing in the current table. Machbase also supports the above functions and can be used to find out the information of each column.\n\nThe file name is sample6_columns.c.\n\nsample6_columns.c\n\nAdd the above file and run make. The results are as follows.\n\n[mach@localhost cli]$ make\n \n[mach@localhost cli]$ ./sample6_columns\nconnected ...\n--------------------------------------------------------------------------------\nName Type TypeName Length\n--------------------------------------------------------------------------------\n_ARRIVAL_TIME 93 DATE 31\nSEQ 5 SMALLINT 5\nSCORE 4 INTEGER 10\nTOTAL -5 BIGINT 19\nPERCENTAGE 6 FLOAT 27\nRATIO 8 DOUBLE 27\nID 12 VARCHAR 10\nSRCIP 2104 IPV4 15\nDSTIP 2106 IPV6 60\nREG_DATE 93 DATE 31\nTLOG 2100 TEXT 67108864\nIMAGE -2 BINARY 67108864\n--------------------------------------------------------------------------------\n\n\nMulti-Thread Append Example\n\nAn example of using multiple threads in one program to append to multiple tables.\n\nThe file name is sample8_multi_session_multi_table.c.\n\nsample8_multi_session_multi_table.c\n\nAdd the make code and run the executable file. Because the threads are used, the output order may be different. The results are as follows.\n\n[mach@localhost cli]$ make sample8_multi_session_multi_table\ngcc -c -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -g -W -Wall -rdynamic -fno-inline -m64 -mtune=k8 -I/home/mach/machbase_home/include -I. -L//home/mach/machbase_home/include -osample8_multi_session_multi_table.o sample8_multi_session_multi_table.c\ngcc -m64 -mtune=k8 -L/home/mach/machbase_home/lib -osample8_multi_session_multi_table sample8_multi_session_multi_table.o -lmachcli  -L/home/mach/machbase_home/lib -lm -lpthread -ldl -lrt -rdynamic\n[mach@localhost cli]$ ./sample8_multi_session_multi_table\nconnectDB success.\ncreateTables success.\n[0]connectDB success.\n[1]connectDB success.\n[2]connectDB success.\n[1-0]appendOpen success.\n[0-0]appendOpen success.\n[2-0]appendOpen success.\n[1-1]appendOpen success.\n[2-1]appendOpen success.\n[0-1]appendOpen success.\n[1-2]appendOpen success.\n[2-2]appendOpen success.\nfile open success - [1][suffle_data2.txt]\nfile open success - [2][suffle_data3.txt]\n[0-2]appendOpen success.\nfile open success - [0][suffle_data1.txt]\n.......................................................................................\n \n[1-0]appendClose start...\n..\n[0-0]appendClose start...\nappend result success : 100000, failure : 0\n[1-0]appendClose success\n[1-1]appendClose start...\nappend result success : 100000, failure : 0\n[1-1]appendClose success\n[1-2]appendClose start...\nappend result success : 100000, failure : 0\n[1-2]appendClose success\nappend result success : 100000, failure : 0\n[0-0]appendClose success\n[0-1]appendClose start...\n.append result success : 100000, failure : 0\n[0-1]appendClose success\n[0-2]appendClose start...\nappend result success : 100000, failure : 0\n[0-2]appendClose success\n \n[2-0]appendClose start...\nappend result success : 100000, failure : 0\n[2-0]appendClose success\n[2-1]appendClose start...\nappend result success : 100000, failure : 0\n[2-1]appendClose success\n[2-2]appendClose start...\nappend result success : 100000, failure : 0\n[2-2]appendClose success\n[1]disconnected.\n[2]disconnected.\n[0]disconnected.\n1 thread join\n2 thread join\n3 thread join\n\n\nYou can see the result through machsql as below.\n\n[mach@localhost cli]$ machsql\n \n=================================================================\n     Machbase Client Query Utility\n     Release Version 3.5.0\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase Server Addr (Default:127.0.0.1) :\nMachbase User ID  (Default:SYS)\nMachbase User Password : manager\nMACH_CONNECT_MODE=INET, PORT=5656\nMach&gt; select count(*) from table_f1;\ncount(*)\n-----------------------\n300000\n[1] Row Selected.\nMach&gt; select count(*) from table_f2;\ncount(*)\n-----------------------\n300000\n[1] row(s) selected.\nMach&gt; select count(*) from table_event;\ncount(*)\n-----------------------\n300000\n[1] row(s) selected."
					}
					
				
		
				
					,
					
					"sdk-cli-odbc-html": {
						"id": "sdk-cli-odbc-html",
						"title": "CLI/ODBC",
						"version": "all",
						"categories": "",
						"url": " /sdk/cli-odbc.html",
						"content": "Index\n\n\n  Standard CLI Functions\n  Connection String for Connecting\n  Extension CLI Function (APPEND)\n    \n      Understanding Append Protocol\n      Append Data Transfer\n      Append Data Error Check\n      Additional Options for Checking Server Errors\n      Leaving Trace Log When Server Error Occurs\n      APPEND Function Description\n        \n          SQLAppendOpen\n          SQLAppendData (deprecated)\n          SQLAppendDataByTime(deprecated)\n          SQLAppendDataV2\n          SQLAppendDataByTimeV2\n          SQLAppendFlush\n          SQLAppendClose\n          SQLAppendSetErrorCallback\n          SQLSetConnectAppendFlush\n          SQLSetStmtAppendInterval\n          Error Check and Description\n        \n      \n    \n  \n  Column wise parameter binding\n  Supported Strings\n\n\nCLI is  a software development standard defined in ISO/IEC 9075-3: 2003.\n\nThe CLI defines functions and specifications for how to pass SQL to the database and how to receive and analyze the results. This CLI was developed in the early 1990s and was developed exclusively for C and COBOL languages, and its specifications have been maintained to date.\n\nThe most widely known standard interface to date is ODBC (Open Database Connectivity), which provides a way for a client program to access a database regardless of the type of database. The current ODBC API version is 3.52 and is defined in ISO and X/Open standards.\n\nStandard CLI Functions\n\nSee the following links for usage of the standard functions.\n\n  Wikipedia\n  Open Group Document\n\n\nYou can refer to the following function.\n\n\n  \n    \n      SQLAllocConnect\n      SQLDisconnect\n      SQLGetDescField\n      SQLPrepare\n    \n  \n  \n    \n      SQLAllocEnv\n      SQLDriverConnect\n      SQLGetDescRec\n      SQLPrimaryKeys\n    \n    \n      SQLAllocHandle\n      SQLExecDirect\n      SQLGetDiagRec\n      SQLStatistics\n    \n    \n      SQLAllocStmt\n      SQLExecute\n      SQLGetEnvAttr\n      SQLRowCount\n    \n    \n      SQLBindCol\n      SQLFetch\n      SQLGetFunctions\n      SQLSetConnectAttr\n    \n    \n      SQLBindParameter+\n      SQLFreeConnect\n      SQLGetInfo\n      SQLSetDescField\n    \n    \n      SQLColAttribute\n      SQLFreeEnv\n      SQLGetStmtAttr\n      SQLSetDescRec\n    \n    \n      SQLColumns\n      SQLFreeHandle\n      SQLGetTypeInfo\n      SQLSetEnvAttr\n    \n    \n      SQLConnect\n      SQLFreeStmt\n      SQLNativeSQL\n      SQLSetStmtAttr\n    \n    \n      SQLCopyDesc\n      SQLGetConnectAttr\n      SQLNumParams\n      SQLStatistics\n    \n    \n      SQLDescribeCol\n      SQLGetData\n      SQLNumResultCols\n      SQLTables\n    \n  \n\n\nConnection String for Connecting\n\nTo connect through the CLI, you need to create a connection string. The contents of each are as follows.\n\n\n  \n    \n      Connection String Item Name\n      Item Description\n    \n  \n  \n    \n      DSN\n      Specifies the data source name.ODBC specifies the section name of the file containing the resource, and CLI specifies the server name or IP address.\n    \n    \n      DBNAME\n      Describes the DB name of Machbase.\n    \n    \n      SERVER\n      Indicates the host name or IP address of the server where Machbase is located.\n    \n    \n      NLS_USE\n      Sets the language type to use with each other (currently unused, kept for future expansion).\n    \n    \n      UID\n      User ID\n    \n    \n      PWD\n      User password\n    \n    \n      PORT_NO\n      Port number to connect to\n    \n    \n      PORT_DIR\n      Specifies the file path to use when connecting to a Unix domain from Unix.(It is specified when modified from the server, and it works even if it is not specified by default.)\n    \n    \n      CONNTYPE\n      Specifies the connection method between the client and the server.1: Connection with TCP / IP INET2: Connect to Unix Domain\n    \n    \n      CONNTYPE\n      Specifies the connection method between the client and the server.1: Connection with TCP / IP INET2: Connect to Unix Domain\n    \n    \n      COMPRESS\n      Indicates whether to compress the Append protocol.If this value is 0, it is transmitted without compression.If this value is any value greater than 0, it is compressed only if the Append record is larger than its value.Ex) COMPRESS = 512Only when the record size is larger than 512, it is compressed and operates.For remote connection, compression improves transmission performance.\n    \n    \n      SHOW_HIDDEN_COLS\n      Decides whether to show the hidden column (_arrival_time) when executing it with select *.If it is 0, it is not shown. If it is 1, information of the corresponding column is output.\n    \n    \n      CONNECTION_TIMEOUT\n      Sets how long to wait on the first connection.The default setting is 30 seconds.This value is set higher if the server response on the first connection is slower than 30 seconds.\n    \n    \n      SOCKET_TIMEOUT\n      This is a timeout that occurs when Protocol I/O takes time.The client checks and waits , then performing Disconnect.Same as Read Timeout of ORACLE. (In MYSQL and MSSQL, uses SOCKET_TIMEOUT as same as Machbase.)Set SOCKET_TIMEOUT=NN (seconds) in Connection String, and the default value is set to 30 minutes (1800).\n    \n    \n      ALTERNATIVE_SERVERS\n      When using the cluster version, it is a setting to have the information of several brokers additionally.When multiple brokers are registered, even if the connected broker is terminated, the data is continuously input after connecting to another broker.Multiple brokers can be registered, and the values of : are separated by commas.ex) ALTERNATIVE_SERVERS=192.168.0.10:20320,192.168.0.11:20320;\n    \n  \n\n\nAn example of CLI connection is as follows.\n\nsprintf(connStr,\"SERVER=127.0.0.1;COMPRESS=512;UID=SYS;PWD=MANAGER;CONNTYPE=1;PORT_NO=%d\", MACHBASE_PORT_NO);\n \nif (SQL_ERROR == SQLDriverConnect( gCon, NULL, (SQLCHAR *)connStr, SQL_NTS, NULL, 0, NULL, SQL_DRIVER_NOPROMPT )) {\n   ...\n}\n\n\nExtension CLI Function (APPEND)\n\nThe CLI extension function is a function for implementing the Append protocol provided to input data to the Machbase server at high speed.\n\nThis function consists of four functions: channel open, channel data input, channel flush, and channel closing.\n\nUnderstanding Append Protocol\n\nThe Append protocol provided by Machbase works asynchronously. The term asynchronous means that the response to a specific job requested by the client to the server does not completely synchronize with each other but occurs at the moment when an arbitrary event occurs. That is, even if a client has performed an append, you can not immediately get or verify the results of that execution, and you can check it at any time when the server is ready. For this reason, developers who develop applications using the Append protocol should have an understanding of the following internal behaviors. The following discussion is about how and when a client detects asynchronous errors that occur in the server.\n\nAppend Data Transfer\n\nIn a typical call such as SQLExecute or SQLExecDirect (), Machbase uses a synchronous scheme that returns the results back to the client immediately. However, SQLAppendDataV2 () does not send a request immediately after user data is entered. Instead, it waits until all of the client communication buffers are full, and then it sends the data to the client all at once. The reason for this design is that the input data of the client using Append assumes tens to hundreds of thousands of records per second, so it utilizes the buffering method for high-speed data transmission. For this reason, if the user wants to transmit the contents of the buffer at will, the user can input data explicitly by calling SQLAppendFlush () function.\n\nAppend Data Error Check\n\nAs mentioned earlier, the Append protocol is buffered and operates asynchronously. In particular, it is very important to understand when and how an error is detected because it takes a method to detect an error only when an error occurs, without receiving any response when an error does not occur in the server. In addition, since the cost of detecting an error is relatively large, it is very inefficient to check each time a record is input, and currently Machbase is designed to detect an error only in the following cases explicitly. When an error is detected, the error callback function set by the user is called every time.\n\n  Checks after all the transmit buffers are full and the data has been explicitly sent to the server,\n  Checks after explicitly sending data to the server from within SQLAppendFlush ()\n  Checks just before shut down from within SQLAppendClose ()\n\n\nIn other words, it is basically designed to detect errors only in the above three cases, and is designed to minimize the occurrence of I/O.\n\nAdditional Options for Checking Server Errors\n\nIn order to achieve the maximum performance, the default error detection technique can be more frequently checked and utilized by the user if desired. This can be done by adjusting the last argument to the SQLAppendOpen () function, aErrorCheckCount. When this value is 0, it does not perform any checking operation and operates basically. However, if this value is greater than 0, SQLAppendData () is explicitly checked for errors every time it is called. In other words, if this value is 10, you pay the cost of checking for errors every 10 appends. Therefore, when this value is small, system resources for error detection are used much, so it should be adjusted to an appropriate number.\n\nLeaving Trace Log When Server Error Occurs\n\nIf you want to leave a trace log for the append data where an error occurs, set the prepared property DUMP_APPEND_ERROR to 1 on the server. With this setting, the specification of the record that generated the error in the mach.trc file is written to the file. However, if the number of errors is excessive, the amount of system resources used will increase drastically, which may degrade the overall performance of Machbase.\n\nAPPEND Function Description\n\nSQLAppendOpen\n\nSQLRETURN SQLAppendOpen(SQLHSTMT   aStatementHandle,\n                        SQLCHAR   *aTableName,\n                        SQLINTEGER aErrorCheckCount );\n\n\nThis function opens a channel for the target table. If this channel is not closed afterwards, it is kept open continuously.\nA maximum of 1024 statements can be set for one connection. You can use SQLAppendOpen for each statement.\n\n\n  aStatementHandle: Represents the handle of the Statement to be appended.\n  aTableName: Indicates the name of the table to which Append will be performed.\n  aErrorCheckCount: Decides whether to check the server for errors whenever several data are input. If this value is 0, no error is checked arbitrarily.\n\n\nSQLAppendData (deprecated)\nSQLRETURN  SQLAppendData(SQLHSTMT StatementHandle, void *aData[]);\n\n\nThis function is a function that inputs data for the channel.\n\n\n  aData is an array containing pointers to the data to be input. The number of arrays must match the number of columns held by the table specified at Open.\n  The return value can be SQL_SUCCESS, SQL_SUCCESS_WITH_INFO, or SQL_ERROR.\nIn particular, if SQL_SUCCESS_WITH_INFO is returned, there may be errors such as a lengthy input column being truncated, so check the result again.\n\n\nConfiguration According to Data Type\n\nNumeric and character types\n\n  Types such as float, double, short, int, long long, and char * work well with pointers to their values.\n\n\nAddress type\n\n\n  0x04, 0x7f, 0x00, 0x00, 0x01 are entered in this order.\n  In the case of ipv4, it is passed as an array of 5-byte unsigned char.\n  The first byte is set to 4, the next 4 bytes are set to consecutive address values.\n  For example, in the case of 127.0.0.1, five byte arrays 0x04, 0x7f, 0x00, 0x00, and 0x01 are entered in order.\n\n\n// For tables with four column information (short (16), int (32), long (64), varchar)\n \ntestAppendIPFunc()\n{\n   short val1 = 0;\n   int   val2 = 1;\n   long long  val3 = 2;  \n   char *val4 = \"my string\";\n   void *valueArray[4];\n \n   valueArray[0] = (void *)&amp;val1;\n   valueArray[1] = (void *)&amp;val2;\n   valueArray[2] = (void *)&amp;val3;\n   valueArray[3] = (void *)val4;\n \n   SQLAppendData(aStmt, valueArray);\n}\n\n\nConfiguration According to Data Type\n\ndatetime type\n\n\n  Since Machbase internally has a nano-unit time resolution value, it must be converted when setting the time on the client, and it is expressed as a 64-bit unsigned integer value.\nTherefore, for proper conversion, you need to add nano values ​​after converting to seconds using the UNIX library mktime.\n※ Machbase time = (total time (seconds) since January 1, 1970) * 1,000,000,000 + milli-second * 1,000,000 + micro-second * 1000 + nano-second;\n\n\n// Code if Date String is entered as \"Year - Month - Date: Minute: Second Millis: Micro: Nano\"\n \ntestAppendDateStrFunc(char *aDateString)\n{\n    int yy, int mm, int dd, int hh, int mi, int ss;\n    unsigned long t1;\n    void *valueArray[5];\n    sscanf(aDateString, \"%d-%d-%d %d:%d:%d %d:%d:%d\",\n        &amp;yy, &amp;mm, &amp;dd, &amp;hh, &amp;mi, &amp;ss, &amp;mmm, &amp;uuu, &amp;nnn);\n    sTm.tm_year = yy - 1900;\n    sTm.tm_mon = mm - 1;\n    sTm.tm_mday = dd;\n    sTm.tm_hour = hh;\n    sTm.tm_min = mi;\n    sTm.tm_sec = ss;\n    t1 = mktime(&amp;sTm);\n    t1 = t1 * 1000000000L;\n    t1 = t1 + (mmm*1000000L) + (uuu*1000) + nnn;\n \n    valueArray[4] = &amp;t1;\n    SQLAppendData(aStmt, valueArray);\n}\n\n\nSQLAppendDataByTime(deprecated)\n\nSQLRETURN  SQLAppendDataByTime(SQLHSTMT StatementHandle, SQLBIGINT aTime, void *aData[]);\n\n\nThis function is a function to input data for the corresponding channel, and the value of _arrival_time stored in the DB can be set to a specific time value instead of the current time.\nFor example, you want to enter the date in the log file a month ago as the date.\n\n\n  aTime is a time value set to _arrival_time.\n  aData is an array containing pointers to the data to be input.\n  The number of arrays must match the number of columns held by the table specified at Open.\n\n\nFor the rest, refer to the SQLAppendData () function.\n\n// For tables with four column information (short (16), int (32), long (64), varchar)\n \ntestAppendFuncWithTime()\n{\n   long long sTime = 1;\n   short val1 = 0;\n   int   val2 = 1;\n   long long  val3 = 2;  \n   char *val4 = \"my string\";\n   void *valueArray[4];\n \n   valueArray[0] = (void *)&amp;val1;\n   valueArray[1] = (void *)&amp;val2;\n   valueArray[2] = (void *)&amp;val3;\n   valueArray[3] = (void *)val4;\n \n   SQLAppendDataByTime(aStmt, sTime, valueArray);\n}\n\n\nSQLAppendDataV2\n\nSQLRETURN  SQLAppendDataV2(SQLHSTMT StatementHandle, SQL_APPEND_PARAM *aData);\n\n\nThis function is a newly introduced Append function since Machbase 2.0. It is a convenient function that improves the input method inconvenient in existing functions.\nIn the case of TEXT and BINARY type introduced in 2.0 especially, input is possible only in SQLAppendDataV2 () function.\n\n\n  Can input NULL for each type\n  Can input string length when inputting VARCHAR\n  Can input binary and string data when inputting IPv4 or IPv6\n  Can specify data length for TEXT, BINARY type\n\n\nThe function arguments are structured as follows.\n\n\n  aData is a pointer to an array of arguments called SQL_APPEND_PARAM. The number of this array must match the number of columns held by the table specified at Open.\n  The return value can be SQL_SUCCESS, SQL_SUCCESS_WITH_INFO, or SQL_ERROR. In particular, if SQL_SUCCESS_WITH_INFO is returned, there may be errors such as a lengthy input column being truncated, so check the result again.\n\n\nBelow is the definition of SQL_APPEND_PARAM that will actually be used in V2 , which is included in machbase_sqlcli.h.\n\ntypedef struct machAppendVarStruct\n{\n    unsigned int mLength;\n    void *mData;\n} machAppendVarStruct;\n \n/* for IPv4, IPv6 as bin or string representation */\ntypedef struct machbaseAppendIPStruct\n{\n    unsigned char   mLength; /* 0:null, 4:ipv4, 6:ipv6, 255:string representation */\n    unsigned char   mAddr[16];\n    char           *mAddrString;\n} machbaseAppendIPStruct;\n \n/* Date time*/\ntypedef struct machbaseAppendDateTimeStruct\n{\n    long long       mTime;\n#if defined(SUPPORT_STRUCT_TM)\n    struct tm       mTM;\n#endif\n    char           *mDateStr;\n    char           *mFormatStr;\n} machbaseAppendDateTimeStruct;\n \ntypedef union machbaseAppendParam\n{\n    short                        mShort;\n    unsigned short               mUShort;\n    int                          mInteger;\n    unsigned int                 mUInteger;\n    long long                    mLong;\n    unsigned long long           mULong;\n    float                        mFloat;\n    double                       mDouble;\n    machbaseAppendIPStruct       mIP;\n    machbaseAppendVarStruct      mVar;     /* for all varying type */\n    machbaseAppendVarStruct      mVarchar; /* alias */\n    machbaseAppendVarStruct      mText;    /* alias */\n    machbaseAppendVarStruct      mBinary;  /* binary */\n    machbaseAppendVarStruct      mBlob;    /* reserved alias */\n    machbaseAppendVarStruct      mClob;    /* reserved alias */\n    machbaseAppendDateTimeStruct mDateTime;\n} machbaseAppendParam;\n \n#define SQL_APPEND_PARAM machbaseAppendParam\n\n\nAs you can see from the above, there is a structure in which a shared structure machbaseAppendParam which internally contains one argument. The length and value for the data and string can be explicitly entered for each data type. Examples of actual use are as follows.\n\nFixed-Length Numeric Type Input\n\nFixed-length numeric types are short, ushort, integer, uinteger, long, ulong, float, and double. This type can be entered by directly assigning a value to the structure member of SQL_APPEND_PARAM.\n\n\n  \n    \n      Database Type\n      NULL Macro\n      SQL_APPEND_PARAM Member\n    \n  \n  \n    \n      SHORT\n      SQL_APPEND_SHORT_NULL\n      mShort\n    \n    \n      USHORT\n      SQL_APPEND_USHORT_NULL\n      mUShort\n    \n    \n      INTEGER\n      SQL_APPEND_INTEGER_NULL\n      mInteger\n    \n    \n      UINTEGER\n      SQL_APPEND_UINTEGER_NULL\n      mUInteger\n    \n    \n      LONG\n      SQL_APPEND_LONG_NULL\n      mLong\n    \n    \n      ULONG\n      SQL_APPEND_ULONG_NULL\n      mULong\n    \n    \n      FLOAT\n      SQL_APPEND_FLOAT_NULL\n      mFloat\n    \n    \n      DOUBLE\n      SQL_APPEND_DOUBLE_NULL\n      mDouble\n    \n  \n\n\nThe following is an example of entering actual values.\n\n// Assume that the Table Schema consists of eight columns, SHORT, USHORT, INTEGER, UINTEGER, LONG, ULONG, FLOAT, and DOUBLE, respectively.\n \nvoid testAppendExampleFunc()\n{\n    SQL_APPEND_PARAM sParam[8];\n \n    /* fixed column */\n    sParam[0].mShort = SQL_APPEND_SHORT_NULL;\n    sParam[1].mUShort = SQL_APPEND_USHORT_NULL;\n    sParam[2].mInteger = SQL_APPEND_INTEGER_NULL;\n    sParam[3].mUInteger = SQL_APPEND_UINTEGER_NULL;\n    sParam[4].mLong = SQL_APPEND_LONG_NULL;\n    sParam[5].mULong = SQL_APPEND_ULONG_NULL;\n    sParam[6].mFloat = SQL_APPEND_FLOAT_NULL;\n    sParam[7].mDouble = SQL_APPEND_DOUBLE_NULL;\n \n    SQLAppendDataV2(Stmt, sParam);\n \n    /* FIXED COLUMN Value */\n    sParam[0].mShort = 2;\n    sParam[1].mUShort = 3;\n    sParam[2].mInteger = 4;\n    sParam[3].mUInteger = 5;\n    sParam[4].mLong = 6;\n    sParam[5].mULong = 7;\n    sParam[6].mFloat = 8.4;\n    sParam[7].mDouble = 10.9;\n \n    SQLAppendDataV2(Stmt, sParam);\n}\n\n\nDate Type Input\n\nBelow is an example of inputting data of DATETIME type. Several macros are available for convenience.\n\nPerforms operations on the mDateTime member in SQL_APPEND_PARAM. The following macro can specify a date by setting a 64-bit integer value called mTime in the mDateTime structure.\n\ntypedef struct machbaseAppendDateTimeStruct\n{\nlong long       mTime;\n#if defined(SUPPORT_STRUCT_TM)\nstruct tm       mTM;\n#endif\nchar           *mDateStr;\nchar           *mFormatStr;\n} machbaseAppendDateTimeStruct;\n\n\n\n  \n    \n      Macro\n      Description\n    \n  \n  \n    \n      SQL_APPEND_DATETIME_NOW\n      Enters the current client time.\n    \n    \n      SQL_APPEND_DATETIME_STRUCT_TM\n      Sets a value to mTM, the struct tm structure of mDateTime, and inputs the value to the database.\n    \n    \n      SQL_APPEND_DATETIME_STRING\n      Sets a value for the string type of mDateTime and enters it into the database.mDateStr: real date string value assignedmFormatStr: format string assignment for date string\n    \n    \n      SQL_APPEND_DATETIME_NULL\n      Enters the value of the date column as NULL.\n    \n    \n      Any 64-bit Value\n      This value is entered as the actual datetime.This value represents an integer value in nanoseconds since January 1, 1970.For example, if this value is 1 billion (1,000,000,000), it represents 0: 1: 1 on January 1, 1970. (GMT)\n    \n  \n\n\n\n// Assume that the table schema consists of eight columns, SHORT, USHORT, INTEGER, UINTEGER, LONG, ULONG, FLOAT, and DOUBLE, respectively.\n \nvoid testAppendDateTimeFunc()\n{\n    SQL_mach_PARAM sParam[1];\n    /* NULL Insert */\n    sParam[0].mDateTime.mTime   = SQL_APPEND_DATETIME_NULL;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* Current Time */\n    sParam[0].mDateTime.mTime      = SQL_APPEND_DATETIME_NOW;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* nano second since 1970/01/01 */\n    sParam[0].mDateTime.mTime      = 1234;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* String format time */\n    sParam[0].mDateTime.mTime      = SQL_APPEND_DATETIME_STRING;\n    sParam[0].mDateTime.mDateStr   = \"23/May/2014:17:41:28\";\n    sParam[0].mDateTime.mFormatStr = \"DD/MON/YYYY:HH24:MI:SS\";\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* struct tm based time */\n    sParam[0].mDateTime.mTime      = SQL_APPEND_DATETIME_STRUCT_TM;\n    sParam[0].mDateTime.mTM.tm_year = 2000 - 1900;\n    sParam[0].mDateTime.mTM.tm_mon  =  11;\n    sParam[0].mDateTime.mTM.tm_mday  = 31;\n    SQLAppendDataV2(Stmt, sParam);\n}\n\n\nInternet Address Type Input\n\nThe following is an example of inputting IPv4 and IPv6 type data. There are also several macros available for your convenience. Performs operations on the mLength member in SQL_APPEND_PARAM.\n\n/* for IPv4, IPv6 as bin or string representation */\ntypedef struct machbaseAppendIPStruct\n{\nunsigned char   mLength; /* 0:null, 4:ipv4, 6:ipv6, 255:string representation */\nunsigned char   mAddr[16];\nchar           *mAddrString;\n} machbaseAppendIPStruct;\n\n\n\n  \n    \n      Macro (set on mLength)\n      Description\n    \n  \n  \n    \n      SQL_APPEND_IP_NULL\n      Enters a NULL value in the corresponding column\n    \n    \n      SQL_APPEND_IP_IPV4\n      mAddr has IPv4\n    \n    \n      SQL_APPEND_IP_IPV6\n      mAddr has IPv6\n    \n    \n      SQL_APPEND_IP_STRING\n      mAddrString has an address string.\n    \n  \n\n\nThe following is an example of entering actual values for each case.\n\nvoid testAppendIPFunc()\n{\nSQL_APPEND_PARAM sParam[1];\n/* NULL */\nsParam[0].mIP.mLength  = SQL_APPEND_IP_NULL;\nSQLAppendDataV2(Stmt, sParam);\n\n    /* Direct array access */\n    sParam[0].mIP.mLength  = SQL_APPEND_IP_IPV4;\n    sParam[0].mIP.mAddr[0] = 127;\n    sParam[0].mIP.mAddr[1] = 0;\n    sParam[0].mIP.mAddr[2] = 0;\n    sParam[0].mIP.mAddr[3] = 1;\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* IPv4 from binary */\n    sParam[0].mIP.mLength  = SQL_APPEND_IP_IPV4;\n    *(in_addr_t *)(sParam[0].mIP.mAddr) = inet_addr(\"192.168.0.1\");\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* IPv4 : ipv4 from string */\n    sParam[0].mIP.mLength     = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = \"203.212.222.111\";\n    SQLAppendDataV2(Stmt, sParam);\n \n    /* IPv4 : ipv4 from invalid string */\n    sParam[0].mIP.mLength     = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = \"ip address is not valid\";\n    SQLAppendDataV2(Stmt, sParam);                           // invalid IP value\n \n    /* IPv6 : ipv6 from binary bytes */\n    sParam[0].mIP.mLength  = SQL_APPEND_IP_IPV6;\n    sParam[0].mIP.mAddr[0]  = 127;\n    sParam[0].mIP.mAddr[1]  = 127;\n    sParam[0].mIP.mAddr[2]  = 127;\n    sParam[0].mIP.mAddr[3]  = 127;\n    sParam[0].mIP.mAddr[4]  = 127;\n    sParam[0].mIP.mAddr[5]  = 127;\n    sParam[0].mIP.mAddr[6]  = 127;\n    sParam[0].mIP.mAddr[7]  = 127;\n    sParam[0].mIP.mAddr[8]  = 127;\n    sParam[0].mIP.mAddr[9]  = 127;\n    sParam[0].mIP.mAddr[10] = 127;\n    sParam[0].mIP.mAddr[11] = 127;\n    sParam[0].mIP.mAddr[12] = 127;\n    sParam[0].mIP.mAddr[13] = 127;\n    sParam[0].mIP.mAddr[14] = 127;\n    sParam[0].mIP.mAddr[15] = 127;\n    SQLAppendDataV2(Stmt, sParam);\n \n    sParam[0].mIP.mLength     = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = \"::127.0.0.1\";\n    SQLAppendDataV2(Stmt, sParam);\n \n    sParam[0].mIP.mLength     = SQL_APPEND_IP_STRING;\n    sParam[0].mIP.mAddrString = \"FFFF:FFFF:1111:2222:3333:4444:7733:2123\";\n    SQLAppendDataV2(Stmt, sParam);\n}\n\n\nVariable Data Types (Character and Binary Data) Input\n\nVariable data types include VARCHAR and TEXT, and BLOB and CLOB. In existing functions, only VARCHAR was supported, and there was no way for the user to enter the length of the string. For that reason, we had to get the length through the strlen () function each time, but from function V2, the user can directly specify the length for the variable data type. Thus, if the user knows the length in advance, data can be input more quickly. Internally, the variable data type is a structure. However, for convenience of development, members are created separately for each data type.\n\ntypedef struct machAppendVarStruct\n{\nunsigned int mLength;\nvoid *mData;\n} machAppendVarStruct;\n\n\nWhen inputting a variable data type, set the length of the data to mLength and set the primitive data pointer to mData. If mLength is greater than the defined schema, it is automatically truncated. At this time, SQLAppendDataV2 () returns SQL_SUCCESS_WITH_INFO and also fills the internal structure with a related warning message. To see this warning message, use SQLError () function.\n\n\n  \n    \n      Database Type\n      NULL Macro\n      SQL_APPEND_PARAM Member(mVar is acceptable)\n    \n    \n      VARCHAR\n      SQL_APPEND_VARCHAR_NULL\n      mVarchar\n    \n    \n      TEXT\n      SQL_APPEND_TEXT_NULL\n      mText\n    \n    \n      BINARY\n      SQL_APPEND_BINARY_NULL\n      mBinary\n    \n    \n      BLOB\n      SQL_APPEND_BLOB_NULL\n      mBlob\n    \n    \n      CLOB\n      SQL_APPEND_CLOB_NULL\n      mClob\n    \n  \n\n\nThe following is an example of entering actual values for each environment. Assumes that there is one VARCHAR column.\n\nCREATE TABLE ttt (name VARCHAR(10));\n\n\nvoid testAppendVarcharFunc()\n{\n    SQL_mach_PARAM sParam[1];\n \n    /*  VARCHAR : NULL */\n    sParam[0].mVarchar.mLength = SQL_APPEND_VARCHAR_NULL\n    SQLAppendDataV2(Stmt, sParam); /* OK */\n \n    /*  VARCHAR : string */\n    strcpy(sVarchar, \"MY VARCHAR\");\n    sParam[0].mVarchar.mLength = strlen(sVarchar);\n    sParam[0].mVarchar.mData   = sVarchar;\n    SQLAppendDataV2(Stmt, sParam); /* OK */\n \n    /*  VARCHAR : Truncation! */\n    strcpy(sVarchar, \"MY VARCHAR9\"); /* Truncation! */\n    sParam[0].mVarchar.mLength = strlen(sVarchar);\n    sParam[0].mVarchar.mData   = sVarchar;\n    SQLAppendDataV2(Stmt, sParam);  /* SQL_SUCCESS_WITH_INFO */\n}\n\n\nSQLAppendDataByTimeV2\n\nSQLRETURN  SQLAppendDataByTimeV2(SQLHSTMT StatementHandle, SQLBIGINT aTime, SQL_APPEND_PARAM  *aData);\n\n\nThis function is a function to input data for the corresponding channel, and the value of _arrival_time stored in the DB can be set to a specific time value instead of the current time. For example, you want to enter the date in the log file a month ago as the date.\n\n\n  aTime is the time value to be set to _arrival_time. You must enter the nano second value from January 1, 1970 to the present. Also, input values ​​must be sorted in order from the past to the present.\n  aData is an array containing pointers to the data to be input. The number of arrays must match the number of columns held by the table specified at Open.\n\n\nFor the rest, refer to the SQLAppendDataV2 () function.\n\nSQLAppendFlush\n\nSQLRETURN SQLAppendFlush(SQLHSTMT StatementHandle);\n\n\nThis function immediately sends the data accumulated in the current channel buffer to the Machbase server.\n\nSQLAppendClose\nSQLRETURN SQLAppendClose(SQLHSTMT   aStmtHandle,\n                         SQLBIGINT* aSuccessCount,\n                         SQLBIGINT* aFailureCount);\n\n\nThis function closes the currently open channel. If an unopened channel exists, an error occurs.\n\n\n  aSuccessCount: The number of successful Append records.\n  aFailureCount: The number of failed Append records.\n\n\nSQLAppendSetErrorCallback\n\nSQLRETURN SQLAppendSetErrorCallback(SQLHSTMT aStmtHandle, SQLAppendErrorCallback aFunc);\n\n\nThis function sets the callback function that is called when an error occurs during append. If you do not set this function, the client will ignore any errors that occur in the server.\n\n\n  aStmtHandle: Specifies a Statement to check for errors.\n  aFunc: Specifies the function pointer to call on Append failure.\n\n\nThe prototype for SQLAppendErrorCallback is:\n\ntypedef void (*SQLAppendErrorCallback)(SQLHSTMT aStmtHandle,\n                                     SQLINTEGER aErrorCode,\n                                     SQLPOINTER aErrorMessage,\n                                         SQLLEN aErrorBufLen,\n                                     SQLPOINTER aRowBuf,\n                                         SQLLEN aRowBufLen);\n\n\n\n  aStatementHandle: the statement handle that generated the error\n  aErrorCode: 32-bit error code that caused the error\n  aErrorMessage: string for the error code\n  aErrorBufLen: the length of aErrorMessage\n  aRowBuf: a string containing the detailed description of the record that caused the error\n  aRowBufLen: length of aRowBuf\n\n\nExample of Using Error Callback (dumpError)\n\nvoid dumpError(SQLHSTMT    aStmtHandle,\nSQLINTEGER  aErrorCode,\nSQLPOINTER  aErrorMessage,\nSQLLEN      aErrorBufLen,\nSQLPOINTER  aRowBuf,\nSQLLEN      aRowBufLen)\n{\nchar       sErrMsg[1024] = {0, };\nchar       sRowMsg[32 * 1024] = {0, };\n\n    if (aErrorMessage != NULL)\n    {\n        strncpy(sErrMsg, (char *)aErrorMessage, aErrorBufLen);\n    }\n \n    if (aRowBuf != NULL)\n    {\n        strncpy(sRowMsg, (char *)aRowBuf, aRowBufLen);\n    }\n \n    fprintf(stdout, \"Append Error : [%d][%s]\\n[%s]\\n\\n\", aErrorCode, sErrMsg, sRowMsg);\n}\n\n\n......\n\n    if( SQLAppendOpen(m_IStmt, TableName, aErrorCheckCount) != SQL_SUCCESS )\n    {\n        fprintf(stdout, \"SQLAppendOpen error\\n\");\n        exit(-1);\n    }\n    // Setting Callback.\n    assert(SQLAppendSetErrorCallback(m_IStmt, dumpError) == SQL_SUCCESS);\n \n    doAppend(sMaxAppend);\n \n    if( SQLAppendClose(m_IStmt, &amp;sSuccessCount, &amp;sFailureCount) != SQL_SUCCESS )\n    {\n        fprintf(stdout, \"SQLAppendClose error\\n\");\n        exit(-1);\n    }\n}\n\n\nSQLSetConnectAppendFlush\n\nSQLRETURN SQL_API SQLSetConnectAppendFlush(SQLHDBC hdbc, SQLINTEGER option)\n\n\nThe data input by Append is written to the communication buffer and is sent to the server when the user calls the SQLAppendFlush function in the waiting state or the communication buffer becomes full. You can use this function if you want the user to send data by append to the server at regular intervals even if the buffer is not full. This function computes the difference between the last transmitted time and the current time every 100ms, and transfers the contents of the communication buffer to the server when the specified time (1 second if not set) has passed.\n\nThe parameters are:\n\n\n  hdbc: DB connection handle.\n  If option: 0, auto flush is off; otherwise, auto flush is on.\n\n\nExecuting on an unconnected hdbc will result in an error.\n\nSQLSetStmtAppendInterval\nSQLRETURN SQL_API SQLSetStmtAppendInterval(SQLHSTMT hstmt, SQLINTEGER fValue)\n\n\nUses SQLSetConnectAppendFlush to turn off automatic flushing or flushing for a particular statement when you turn on flushing on a time unit.\n\nThe parameters are:\n\n\n  hstmt: This is the statement handle that you want to adjust the flush interval.\n  fValue: The value to which you want to adjust the flush interval. If 0, flush is not performed and the unit is ms. Set to a multiple of 100 since the thread that determines whether to flush every 100ms is executed. It does not automatically flush at exactly the right time. 1000 is the default value.\n\n\nExecution of this function will succeed even if time-based flush is not running.\n\nError Check and Description\n\nThis is a description of the code and how to check for errors when using the Append related functions. If the return value in the CLI function is not SQL_SUCCESS, you can check the error message using the following code.\n\nSQLINTEGER errNo;\nint msgLength;\nchar sqlState[6];\nchar errMsg[1024];\n\nif (SQL_SUCCESS == SQLError ( env, con, stmt, (SQLCHAR *)sqlState, &amp;errNo,\n(SQLCHAR *)errMsg, 1024, &amp;msgLength ))\n{\n//set five length error code\nprintf(\"ERROR-%05d: %s\\n\", errNo, errMsg);\n}\n\n\nThe error message returned from the Append related function is as follows.\n\n\n  \n    \n      function\n      message\n      description\n    \n  \n  \n    \n      SQLAppendOpen\n      statement is already opened.\n      Occurs when SQLAppendOpen is executed in duplicate.\n    \n    \n      Failed to close stream protocol.\n      Stream protocol termination failed.\n    \n    \n      Failed to read protocol.\n      A network read error occurred.\n    \n    \n      cannot read column meta.\n      Invalid column meta information structure\n    \n    \n      cannot allocate memory.\n      An internal buffer memory allocation error occurred.\n    \n    \n      cannot allocate compress memory.\n      Compressed buffer memory allocation error occurred.\n    \n    \n      invalid return after reading column meta.\n      Return value has an error.\n    \n    \n      SQLAppendData\n      statement is not opened.\n      Called AppendData without AppendOpen.\n    \n    \n      column() truncated :\n      Occurs when you enter data that is larger than the size specified in the varchar type column.\n    \n    \n      Failed to add binary.\n      Write error in communication buffer occurred.\n    \n    \n      SQLAppendClose\n      statement is not opened.\n      Not in AppendOpen state.\n    \n    \n      Failed to close stream protocol.\n      Stream protocol termination failed.\n    \n    \n      Failed to close buffer protocol.\n      Buffer protocol termination failed.\n    \n    \n      cannot read column meta.\n      The column meta information structure is incorrect.\n    \n    \n      invalid return after reading column meta.\n      Return value has an error.\n    \n    \n      SQLAppendFlush\n      statement is not opened.\n      Not in AppendOpen state\n    \n    \n      Failed to close stream protocol.\n      A network write error occurred.\n    \n    \n      SQLSetErrorCallback\n      statement is not opened.\n      Not in AppendOpen state.\n    \n    \n      Protocol Error (not APPEND_DATA_PROTOCOL)\n      Communication buffer read result is not APPEND_DATA_PROTOCOL value.\n    \n    \n      SQLAppendDataV2\n      Invalid date format or date string.\n      Occurs when the datetime type is wrong.\n    \n    \n      statement is not opened.\n      Not in AppendOpen state\n    \n    \n      column() truncated :\n      This occurs when you enter data that is larger than the size specified in the binary type column.\n    \n    \n      column() truncated :\n      Occurs when you enter data that is larger than the size specified in the varchar and text type column.\n    \n    \n      Failed to add stream.\n      Write error in communication buffer occurred.\n    \n    \n      IP address length is invalid.\n      The mLength value of the IPv4, IPv6 type structure is specified incorrectly.\n    \n    \n      IP string is invalid.\n      Not in IPv4 or IPv6 format.\n    \n    \n      Unknown data type has been specified.\n      Not the data type used by Machbase.\n    \n  \n\n\nColumn wise parameter binding\n\nThe SQLAppend function, which is used to enter a large amount of data into Machbase quickly, can be used only when entering a log / tag table, and the SQLAppend function cannot be used to perform a bulk update on a lookup or volatile table.\nFor this purpose, Machbase 5.5 and later versions support column wise parameter binding. (Row wise format parameter binding is not yet supported.)\nSet SQL_ATTR_PARAM_BIND_TYPE in the argument Attribute of the function SQLSetStmtAttr () and SQL_PARAM_BIND_BY_COLUMN in the parameter param.\nFor each column to bind, set the parameter to an array and the indicator variable to an array. Then call SQLBindParameter () with this parameter.\nThe figure below shows how columnar binding works for each parameter array.\n\n\n  \n    \n      Column A(parameter A)\n      Column B(parameter B)\n      Column C(parameter C)\n    \n  \n  \n    \n      Value_Array\n      Indicator/length array\n      Value_Array\n      Indicator/length array\n      Value_Array\n      Indicator/length array\n    \n  \n\n\n#define DESC_LEN 51\n#define ARRAY_SIZE 10\nSQLCHAR * Statement = \"INSERT INTO Parts (PartID, Description, Price) VALUES (?, ?, ?)\";\n \n/* Array of parameters to bind */\nSQLUINTEGER PartIDArray[ARRAY_SIZE];\nSQLCHAR DescArray[ARRAY_SIZE][DESC_LEN];\nSQLREAL PriceArray[ARRAY_SIZE];\n/* Array of predicate variables to bind */\nSQLINTEGER PartIDIndArray[ARRAY_SIZE], DescLenOrIndArray[ARRAY_SIZE], PriceIndArray[ARRAY_SIZE];\nSQLUSMALLINT i, ParamStatusArray[ARRAY_SIZE];\nSQLUINTEGER ParamsProcessed;\n \n// Set the SQL_ATTR_PARAM_BIND_TYPE statement attribute to use\n// column-wise binding.\nSQLSetStmtAttr(hstmt, SQL_ATTR_PARAM_BIND_TYPE, SQL_PARAM_BIND_BY_COLUMN, 0);\n// Specify the number of elements in each parameter array.\nSQLSetStmtAttr(hstmt, SQL_ATTR_PARAMSET_SIZE, ARRAY_SIZE, 0);\n// Specify an array in which to return the status of each set of\n// parameters.\nSQLSetStmtAttr(hstmt, SQL_ATTR_PARAM_STATUS_PTR, ParamStatusArray, 0);\n// Specify an SQLUINTEGER value in which to return the number of sets of\n// parameters processed.\nSQLSetStmtAttr(hstmt, SQL_ATTR_PARAMS_PROCESSED_PTR, &amp;ParamsProcessed, 0);\n// Bind the parameters in column-wise fashion.\nSQLBindParameter(hstmt, 1, SQL_PARAM_INPUT, SQL_C_ULONG, SQL_INTEGER, 5, 0,\n    PartIDArray, 0, PartIDIndArray);\nSQLBindParameter(hstmt, 2, SQL_PARAM_INPUT, SQL_C_CHAR, SQL_CHAR, DESC_LEN - 1, 0,\n    DescArray, DESC_LEN, DescLenOrIndArray);\nSQLBindParameter(hstmt, 3, SQL_PARAM_INPUT, SQL_C_FLOAT, SQL_REAL, 7, 0,\n    PriceArray, 0, PriceIndArray);\n\n\nSupported Strings\n\nMachbase stores string data using UTF-8 by default.\nIn the case of Windows that inputs/outputs strings in methods other than UTF-8, ODBC converts them as follows.\n\n\n  \n    \n      OS\n      Unicode/Non-Unicode\n      String Conversion\n      Note\n    \n  \n  \n    \n      Windows\n      Unicode (UTF-16)\n      UTF-16 ⟷ UTF-8\n      N/A\n    \n    \n      Windows\n      Non-Unicode (MBCS)\n      MBCS ⟷ UTF-8\n      Use the default string of Non-Unicode application in Windows settings\n    \n    \n      Linux\n      UTF-8\n      N/A\n      UTF-8 only supported"
					}
					
				
		
				
					,
					
					"install-cluster-cluster-command-install-html": {
						"id": "install-cluster-cluster-command-install-html",
						"title": "Cluster Edition Installation(Command-line)",
						"version": "all",
						"categories": "",
						"url": " /install/cluster/cluster-command-install.html",
						"content": "(1) Coordinator / Deployer Installation, Add Package\n  (2) Lookup / Broker / Warehouse Installation"
					}
					
				
		
				
					,
					
					"install-cluster-cluster-env-html": {
						"id": "install-cluster-cluster-env-html",
						"title": "Preparing for Cluster Edition Installation",
						"version": "all",
						"categories": "",
						"url": " /install/cluster/cluster-env.html",
						"content": "Index\n\n\n  Confirm and Change File LIMIT\n  Server Time Synchronization\n  Change Network Kernel Parameters\n  Create User\n\n\nConfirm and Change File LIMIT\n\nTo increase the maximum number of files that can be opened, do the following.\n\n\n  Modify the file /etc/security/limits.conf\n\n\nsudo vi /etc/security/limits.conf\n*       hard   nofile      65535\n*       soft   nofile      65535\n\n\n\n  Reboot.\n\n\nsudo reboot\n# or\nsudo shutdown -r now\n\n\n\n  Check the results. If the output is 65535, it has been successfully changed.\n\n\nunlimit -Sn\n\n\nServer Time Synchronization\n\nYou must synchronize the server time between each host. If they are already synchronized, check for confirmation.\n\n\n  Synchronize the time on all servers with the time server.\n\n\n# Synchronize with the following command.                      \n/usr/bin/rdate -s time.bora.net &amp;&amp; /sbin/clock -w\n\n\n\n  If the time server can not be used, modify it with a direct command.\n\n\n# Modify with the following command.                                 \ndate -s \"2017-10-31 11:15:30\"\n\n\n\n  Check the modified time\n\n\n# Check with the following command.                                 \ndate\n\n\nChange Network Kernel Parameters\n\n\n  Check the current set value.\n\n\nCheck with the following command.                                 \nsysctl -a | egrep 'mem_(max|default)|tcp_.*mem'\n\n\n\n  Change the setting value with the following command (for 64GB Memory).\n\n\nsysctl -w net.core.rmem_default = \"33554432\"     # 32MB\nsysctl -w net.core.wmem_default = \"33554432\"\nsysctl -w net.core.rmem_max     = \"268435456\"    # 256MB\nsysctl -w net.core.wmem_max     = \"268435456\"  \nsysctl -w net.ipv4.tcp_rmem     = \"262144 33554432 268435456\"\nsysctl -w net.ipv4.tcp_wmem     = \"262144 33554432 268435456\"\n \n# 8388608 Page * 4KB = 32GB\nsysctl -w net.ipv4.tcp_mem      = \"8388608 8388608 8388608\"\n\n\n\n  To keep the changes, add them to the /etc/sysctl.conf file and restart the host OS.\n\n\n# Modify the file /etc/sysctl.conf.\nnet.core.rmem_default = \"33554432\"\nnet.core.wmem_default = \"33554432\"\nnet.core.rmem_max     = \"268435456\"\nnet.core.wmem_max     = \"268435456\"\nnet.ipv4.tcp_rmem     = \"262144 33554432 268435456\"\nnet.ipv4.tcp_wmem     = \"262144 33554432 268435456\"\nnet.ipv4.tcp_mem      = \"8388608 8388608 8388608\"\n\n\nCreate User\n\n\n  Create a Linux OS user ‘machbase’ for Machbase installation. The user account directory is created as: /home/machbase.\n\n\n$ sudo useradd machbase --home-dir \"/home/machbase\"\n\n\n\n  Set password (machbase)\n\n\nsudo passwd machbase"
					}
					
				
		
				
					,
					
					"install-cluster-html": {
						"id": "install-cluster-html",
						"title": "Cluster Installation",
						"version": "all",
						"categories": "",
						"url": " /install/cluster.html",
						"content": "Preparing for Cluster Edition Installation\n  Cluster Edition Installation(Command-line)"
					}
					
				
		
				
					,
					
					"intro-edition-cluster-html": {
						"id": "intro-edition-cluster-html",
						"title": "Cluster Edition",
						"version": "all",
						"categories": "",
						"url": " /intro/edition/cluster.html",
						"content": "Index\n\n\n  Why Should We Use the Cluster Edition?\n  Terms\n  Structure\n    \n      Classifications of Node\n      Coordinator\n      Special Node: Deployer\n      Warehouse\n      Node Port Management\n    \n  \n  Save/Select Data\n    \n      Save Data\n      Select Data\n    \n  \n  Replication\n    \n      Coordinator Replication\n      Lookup Replication\n      Broker Replication\n      The Broker is not a Replication target.\n      Warehouse Replication\n    \n  \n  How To Recover\n  Not Supported Features\n    \n      Query Statement\n      Clause/Function\n    \n  \n  Supported Hardware and Operating Systems\n\n\nThe Cluster Edition is a product with high input speed and standard SQL  that can process large data input / inquiries in a distributed environment that not even the Machbase Fog Edition can process, such as on multiple servers either on premise or in the cloud.\n\nWhy Should We Use the Cluster Edition?\n\nMachbase offers Fog Edition which inputs time series data at extremely high speed. However, the following disadvantages exist:\n\n\n  Because it consists of a single process, it lacks in high availability (HA).\n  Because one process is dedicated to analyzing data, there is a limit to increasing the performance of large data analysis.\n\n\nTo overcome these shortcomings, there is a need for a higher end distributed product that can ensure availability and scalability  when storing and analyzing large amounts of data. The Machbase Cluster Edition meets these requirements.\n\nTerms\n\nHost\nRepresents one physical server, or one OS instance in the cloud/VM.\n\nNode\nRepresents the Machbase Process residing on the server.\n\nThe Process type is the same as the Node types below.\n\n\n  Coordinator\n  Deployer\n  Lookup\n  Broker\n  Warehouse\n\n\nStructure\n\nIn the Machbase Cluster Edition, several nodes that reside in the Host constitute one cluster.\n\n\n\nHigh Availability\nThe service can be continued even if one of all the internal nodes is interrupted.\n\nHigh Scalability\nData storage can be distributed, and parallel analysis is possible from the distributed data, so\n\nperformance increases as the cluster grows.\n\nClassifications of Node\nEach Node can be classified as follows:\n\n\n  \n    \n      Classification\n      Description\n      Process Name\n    \n  \n  \n    \n      Coordinator\n      The process of managing all general purpose servers and nodes\n      machcoordinatord\n    \n    \n      Deployer\n      The process that resides on each host Responsible for installing, upgrading, and monitoring the Broker / Warehouse.\n      machdeployerd\n    \n    \n      Lookup\n      The process of Lookup table data.\n      machlookupd\n    \n    \n      Broker\n      The process of welcoming an actual client program. Serves to distribute client data insert / data lookup queries to the warehouse.\n      machbased\n    \n    \n      Warehouse\n      The process that stores the actual data Stores some of the entire cluster data and executes the commands received from the Broker.\n      machbased\n    \n  \n\n\nCoordinator\n\nCoordinator is a process that manages the state of all nodes, and can be a maximum of two.\n\nFirst, the generated Coordinator is called the Primary Coordinator, and the other is called the Secondary Coordinator, and only the Primary Coordinator manages the state of all the Nodes.\n\nWhen the Primary Coordinator is down, the Secondary Coordinator is upgraded to Primary Coordinator.\n\nSpecial Node: Deployer\nIt is managed by the Coordinator, but it is simply  a process  that installs / removes Broker / Warehouse / Lookup Nodes.\n\nNormally, only one Deployer can be added to the Host at a time when installing Nodes, but multiple Deployers can be added for installation performance.\n\n\n\n\n\nExample of Installation From Server\nFigure (a) below shows the installation of two Coordinators, two Brokers, four Warehouse Active, and four Warehouse Standby Nodes on four generic servers.\n\nAs shown in the figure,  you can distinguish each node with ‘hostname: port’ followed by the host name of the general-purpose server and the assigned port number.\n\n\n\nLookup\n\nLookup is for data management in Lookup Table.\n\nBroker\n\nThe Broker delivers the Client’s commands to the Warehouse, and then collects the results of the Warehouse and transmits them back into the Client.\n\n\n  When entering data, the Broker makes sure the data enters evenly into the Warehouse.\n  When retrieving data, the Broker fetches the data to the Warehouse and collects and delivers all the results.\n\n\nThe Broker does not have the data in the Log Table, but it has the data in the Volatile Table.\n\nWarehouse\n\nThe Warehouse will store the Log Table data directly, and will act as the actual execution of the command delivered by the Broker.\n\nLike the Broker, there is direct client access to the Warehouse, but the data can not be input / updated / deleted; the Warehouse data can only be retrieved.\n\nWarehouse Group\n\nThe Warehouse can specify the Group to which it belongs.\n\n\n  When the Broker inputs data, all Warehouses in the same Group receive the same records.\n  Even if a specific Warehouse of the group is dropped, there are no issues with data retrieval.\n  When a new Warehouse is added to a group, the same records are maintained through Replication.\n\n\nStatus of Warehouse Group\n\n\n  \n    \n      Status\n      INSERT/APPEND\n      SELECT\n    \n  \n  \n    \n      Normal\n      O\n      O\n    \n    \n      Readonly\n      X\n      O\n    \n  \n\n\nThe conditions that change to Readonly state are as follows.\n\n\n  During INSERT / APPEND, if some Warehouses in the Group fail to be input\n    \n      Because there is a data inconsistency between the failed Warehouses and the successful Warehouses,\nthe failed Warehouse is placed in the Scrapped state and the group is moved to the Readonly state to avoid receiving further input. (warning)\n    \n  \n  When a new Warehouse is added\n    \n      If the input is received even while the replication process is in progress, the state is changed to the Readonly state because the end of the replication is unknown. (warning)\n    \n  \n\n\nNode Port Management\nEach Node must have several ports open, which are distinguished as follows:\n\n\n\n\n  \n    \n      Port Classification\n      Description\n      Required Node\n    \n  \n  \n    \n      Service Port\n      Port directly connected by client\n      Broker / Warehouse\n    \n    \n      Cluster Port\n      Port for communication between Nodes\n      All Nodes\n    \n    \n      Replication Port\n      Port for communication between warehouses for replication\n      Warehouse\n    \n    \n      Admin Port\n      Port for communication for management purpose\n      Coordinator / Deployer\n    \n  \n\n\nCommands That Can Be Executed After Direct Connection\n\nThe following table lists the possible and not possible commands to connect directly to each Node. \nAll nodes are accessible via the client, but there are queries that are not possible depending on the type of Node.\n\n\n  \n    \n       \n      Broker (Leader)\n      Broker (non-leader)\n      Warehouse Standby\n    \n  \n  \n    \n      Client Connection\n      O\n      O\n      O\n    \n    \n      DDL\n      O\n      X\n      X\n    \n    \n      DELETE\n      O\n      O\n      X\n    \n    \n      INSERT\n      O\n      O\n      X\n    \n    \n      APPEND\n      O\n      O\n      X\n    \n    \n      SELECT\n      O\n      O\n      O\n    \n  \n\n\nSave/Select Data\n\nMachbase Cluster Edition can distribute the data and collect the results computed by distributed query execution. This section explains how to store and lookup the table type.\n\nSave Data\n\nLog Table\n\nWhen data is entered into the Log Table through the Broker, it is distributed to all warehouses. (The data is not stored in the Broker that performs the input.) The Coordinator determines the database size of each warehouse, and the Broker distributes the data based on that.\n\nIf data is entered directly into the Log Table via the Warehouse, it is stored only in the corresponding Warehouse. Can be selected to avoid performance degradation due to distributed algorithm and network bottleneck.\n\nVolatile Table\n\nWhen a Broker enters data into a Volatile Table, it is stored in the corresponding Broker. In other words, no data is entered or synchronized with other Brokers.\n\nThe reason for not supporting replication for Volatile Tables is that if it matches the characteristics of the Volatile Table able to DELETE, it affects the replication performance.\n\nVolatile tables are created only in the Broker, so they can not be entered in the Warehouse.\n\nLookup Table\n\nWhen data is entered into the lookup table through a broker, the entered data is stored in the lookup node and replicated to other brokers through replication.\n\nTag Table\n\nIt is the same as the storage method of the log table. However, when entering data including a new TagID, it can only be entered through the Leader Broker.\n\nSelect Data\n\nLog Table\n\nWhen you view the data in the Log Table through the Broker, queries are distributed to all Warehouses. Each Warehouse actually performs the query, exchanging intermediate results between the Warehouses if necessary. The Broker collects the partial results generated in this way and returns the final result.\n\nWhen viewing the data in the Log Table through the Warehouse, the query is executed only in the corresponding warehouse. This process is identical to query execution in the Fog Edition.\n\nLookup / Volatile Table\n\nWhen viewing data in a Volatile Table through a Broker, the query is executed only by the Broker. This process is identical to query execution in the Fog Edition.\n\nJOIN can not be done through the Warehouse, because Volatile Tables are not created.\n\nJOIN Between Two Tables\n\nWhen joining a Log Table and a Volatile Table through a Broker, the connected Broker and the rest of the Warehouse execute the query at the same time. The Broker distributes the results of the Volatile table to the Warehouse.\n\nThe Warehouse JOINs the data delivered by the broker and returns the result. The Broker collects the partial results generated in this way and returns the final result.\n\nJOIN can not be done through the warehouse, because volatile tables are not created.\n\nReplication\n\nReplication refers to the process of replicating the same node in preparation for failure of an existing node.\n\nCoordinator Replication\n\nUp to two Coordinators can be created in Cluster Edition.\n\nBoth Coordinators continuously maintain Cluster Node information.\n\nEven if either end abnormally, the remaining Coordinator can continue managing the Cluster Node.\n\nWhen the Primary Coordinator is restarted, the existing secondary coordinator is upgraded to primary and the restarting coordinator becomes secondary.\n\nLookup Replication\n\nBasically, the Lookup Master manages the Lookup Table data, but you can add a Lookup Slave to duplicate the data.\n\nBroker Replication\n\nThe Broker is not a Replication target.\n\nTherefore, the data record of the Volatile Table in Broker A is not kept the same in Broker B. (not synchronized)\n\nHowever, because the table / index scheme of the entire Cluster are all the same, if the Volatile Table  VOL_TBL1 exists with Broker A,  Volatile Table  VOL_TBL1 also exists with Broker B.\n\nWarehouse Replication\n\nIf a new Warehouse is added to the Group, the Warehouse is replicated through the following process.\n\n1. The Coordinator starts DDL replication to the new Warehouse.\n2. Group switches to Readonly state.\n3. One of the Warehouses in the group starts data replication to the new warehouse.\n4. When the data replication is completed, the group is switched to the Normal state. In the case of data insert, the Broker guarantees redundancy by sending the same data to the same Group.\n\n\nHow To Recover\n\nEven if the Node terminates abnormally, the service can be continued in the following manner.\n\nFor more information, refer to the Operations Guide.\n\n\n  \n    \n      Type\n      Fail-over Method\n    \n  \n  \n    \n      Coordinator\n      Even if the Primary Coordinator is abnormally terminated, the Secondary Coordinator becomes the Primary Coordinator and the cluster management can continue.Even if the Coordinator is terminated in the worst case scenario, the entire service (data insert / inquiry) can be continued without the cluster management.(Of course, when the Broker or Warehouse is shut down at this time, you can not manage the cluster.)\n    \n    \n      Deployer\n      Node operation (ADD, REMOVE ..) can not be performed on the corresponding Host, and statistical information of the host can not be collected.\n    \n    \n      Lookup\n      When a failure occurs in Lookup Master, Lookup Monitor automatically detects and changes one of the Lookup Slaves to Lookup Master to enable continuous service use.If there is no Lookup Slave in the past, it is recommended that at least one Lookup Slave exists for stable HA because data replication is not possible.\n    \n    \n      Broker\n      Even if the Broker is terminated, the service can continue if another Broker exists.However, because the connection to the client that has been terminated is disconnected, it must be reconnected to another Broker.\n    \n    \n      Warehouse\n      If there is another / other Warehouse (s) in the Group, the Warehouse (s) will participate in SELECT and APPEND.\n    \n  \n\n\nNot Supported Features\n\nQuery Statement\n\nTABLESPACE\n\nCurrently, the Cluster Edition does not distinguish between table spaces.\n\nBACKUP / MOUNT\n\nCurrently, the Cluster Edition does not distinguish between databases.\n\nLOAD IN FILE\n\nThe ability to read and distribute CSV files is currently not implemented.\n\nALTER TABLE FORGERY CHECK\n\nResult File can not be collected in one place as client’s user data is checked for any changes.\n\nClause/Function\n\nUNION ALL\n\nExecution units are complex and are currently not supported.\n\nGROUP_CONCAT() function\n\nThe entire contents of the CONCAT for the subgroups collected by each Warehouse can not be processed as a simple accumulation.\n(ORDER BY in GROUP CONCAT)\n\nTS_CHANGE_COUNT() function\n\nThe TS_CHANGE_COUNT results for the subgroups collected in each Warehouse can not be processed as simple accumulations.\n\nIn addition, TS_CHANGE_COUNT () is significant if the entire result is sorted, but if the results are distributed in the Warehouse, it is meaningless.\n\nSupported Hardware and Operating Systems\n\n\n  \n    \n      CPU\n      Intel Core i Series (Nehalem~) or higher recommended\n    \n  \n  \n    \n      Memory\n      2 GB or more recommended for each Node to be installed\n    \n    \n      Operating System\n      Linux (Any distribution)"
					}
					
				
		
				
					,
					
					"feature-tables-tag-create-drop-html": {
						"id": "feature-tables-tag-create-drop-html",
						"title": "Creating and Dropping Tag table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/create-drop.html",
						"content": "Index\n\n\n  Creation of Tag table\n    \n      Additional Sensor Column\n      Additional metadata columns\n      Setting Table Property\n      Dropping tag table\n    \n  \n\n\nThe user must write ‘TAG’ as the table type. By manipulating this table, sensor data can be utilized in various ways.\n\nUnlike the previous version, tag table name does not need to be “TAG”, and can be freely specified.\n\nNote that there is no TAG table when the database is first installed.\n\nSince the TAG table is, in basic, intended to store sensor data, the following three essential items must be included.\n\n  Tag name\n  Input time\n  Sensor value\n\n\nHowever, the Machbase TAG table is accompanied by keywords for the above required columns, as it allows input of the above three and additional columns.\n\nStarting from version 7.5, the SUMMARIZED keyword in the tag value is optional.\n\n  Tag name : PRIMARY KEY\n  Input time : BASETIME\n\n\nThis tag name is used as tag meta information described in the next section.\n\nCreation of Tag table\n\nThe simplest tag table is generated as follows.\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME, value DOUBLE);\n[ERR-02253: Mandatory column definition (PRIMARY KEY / BASETIME) is missing.]\n==&gt; If you omit some keywords on creating tag table, error occurs.\n \nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nExecuted successfully.\n==&gt; To use statistical information, the SUMMARIZED keyword must be added to the tag value.\n \nMach&gt; desc tag;\n[ COLUMN ]              \n----------------------------------------------------------------\nNAME      TYPE        LENGTH\n----------------------------------------------------------------\nNAME      varchar         20\nTIME      datetime       31\nVALUE    double          17\n \nMach&gt; CREATE TAG TABLE other_tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE);\nExecuted successfully.\n \nMach&gt; desc OTHER_TAG;\n[ COLUMN ]              \n----------------------------------------------------------------\nNAME      TYPE        LENGTH\n----------------------------------------------------------------\nNAME      varchar         20\nTIME      datetime       31\nVALUE    double          17\n\n\n==&gt; If you omit some keywords on creating tag table, error occurs. To improve performance, an internal table is created which divided into four partitions.\n\nAdditional Sensor Column\n\nIn reality, it is sometimes difficult to solve a given problem with just three columns when using the TAG table..\n\nIn particular, since the information of the sensor data to be input may be a specific group or an Internet address as well as a name, a time, and a value, the following can be added.\n\nMach&gt; create tag table TAG (name varchar(20) primary key, time datetime basetime, value double, grpid short, myip ipv4) ;\nExecuted successfully.\n \nMach&gt; desc tag;\n[ COLUMN ]              \n----------------------------------------------------------------\nNAME             TYPE        LENGTH\n----------------------------------------------------------------\nNAME             varchar         20\nTIME             datetime        31\nVALUE            double          17\nGRPID            short            6       &lt;=== added column\nMYIP             ipv4            15       &lt;=== added column\n\n\nNote, however, that in older versions, including 5.5, values of type VARCHAR can not fit into the supplementary column.\n\nMach&gt; create tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized, myname varchar(100)) ;\n[ERR-01851: Variable length columns are not allowed in tag table.]\n\n\nIn the case of string type, the above error occurs. In versions 5.6 and later, VARCHAR is also supported for additional columns in the TAG table.\n\nAdditional metadata columns\n\nIt is not only possible to add sensor columns to the TAG table, but also to input information dependent on each tag name.\n\nSince this information does not need to be redundantly stored in the sensor data, it is necessary to add a separate column definition syntax METADATA (…) for efficient management.\n\nMach&gt; create tag table TAG (name varchar(20) primary key, time datetime basetime, value double)\n   2  metadata (room_no integer, tag_description varchar(100));\n\n\nHere, room_no and tag_description are information dependent on name. For example, you can input this information.\n\n\n  \n    \n      name\n      room_no\n      tag_description\n    \n  \n  \n    \n      temp_001\n      1\n      It reads current temperature as Celsius\n    \n    \n      humid_001\n      1\n      It reads current humidity as percentage\n    \n  \n\n\nAfter input, you can query with TAG table through SELECT.\n\nMach&gt; SELECT name, time, value, tag_description FROM tag LIMIT 1;\nname                  time                            value\n--------------------------------------------------------------------------------------\ntag_description\n------------------------------------------------------------------------------------\ntemp_001              2019-03-01 09:52:17 000:000:000 25.3\n\n\nIt reads current temperature as Celsius\n\nSetting Table Property\n\nWhen creating tag table, user can set 3 types of property.\n\n\n  \n    \n      NAME\n      EXPLANATION\n      VALUE\n    \n  \n  \n    \n      TAG_PARTITION_COUNT\n      User can specify the number of partition to control memory and CPU usage.\n      - Default: 4 - Min: 1 - Max: 1024\n    \n    \n      TAG_DATA_PART_SIZE\n      User can specify data size to control memory and CPU usage for each partition.User can specify in BYTE, can ALIGN in to MB.\n      - Default: 16MB (16 * 1024 * 1024) - Min: 1MB (1024 * 1024) - Max: 1GB (1024 * 1024 * 1024)\n    \n    \n      TAG_STAT_ENABLE\n      User can specify it to activate the function that save statistical information for each TAG ID.\n      - Default : 1 - Min: 0  (disable) - Max: 1 (enable)\n    \n  \n\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE) TAG_PARTITION_COUNT=1;\nExecuted successfully.\n \nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE) TAG_DATA_PART_SIZE=1048576;\nExecuted successfully.\n \nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE) TAG_STAT_ENABLE=0;\nExecuted successfully.\n \nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED) TAG_PARTITION_COUNT=2, TAG_STAT_ENABLE=1;\nExecuted successfully.\n\n\nDropping tag table\n\nIf you need to recreate the generated tag table, or if you need to free up disk space, you can use the following DROP command to drop it.\n\nNote that all data related to the TAG table, ie tag data, metadata tables are also dropped.\n\nMach&gt; DROP TABLE tag;\nDropped successfully.\n \nMach&gt; DESC tag;\ntag does not exist."
					}
					
				
		
				
					,
					
					"feature-tables-volatile-create-manage-html": {
						"id": "feature-tables-volatile-create-manage-html",
						"title": "Creating and Managing Volatile Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/volatile/create-manage.html",
						"content": "The creation and deletion methods of volatile tables are as follows.\n\nCreate\n\ncreate volatile table vtable (id1 integer, name varchar(20));\n\n\nDrop\n\ndrop table vtable;"
					}
					
				
		
				
					,
					
					"feature-tables-lookup-create-manage-html": {
						"id": "feature-tables-lookup-create-manage-html",
						"title": "Creating and Managing Lookup Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/lookup/create-manage.html",
						"content": "The method of creating the reference table is as follows.\n\nCreating Lookup Table\n\nCREATE LOOKUP TABLE lktable (id INTEGER PRIMARY KEY, name VARCHAR(20));\n\n\nLookup Table must specify Primary key.\n\nDeleting Lookup Table\n\nDROP TABLE lktable;"
					}
					
				
		
				
					,
					
					"feature-tables-log-create-manage-html": {
						"id": "feature-tables-log-create-manage-html",
						"title": "Creating and Managing Log Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/create-manage.html",
						"content": "Index\n\n\n  Creating Log Table\n  Deleting Log Table\n\n\nThe log table can be simply generated as follows. Let’s create a table called sensor_data and delete it.\n\nData types compatible with Machbase can be found in the SQL Reference Types.\n\nCreating Log Table\n\nCreate a log table with the ‘CREATE TABLE’ syntax.\n\nMach&gt; CREATE TABLE sensor_data\n      (\n          id VARCHAR(32),\n          val DOUBLE\n       );\nCreated successfully.\n \nMach&gt; DROP TABLE sensor_data;\nDropped successfully.\n\n\nDeleting Log Table\n\nDelete log table with ‘DROP TABLE’ statement.\n\nMach&gt; DROP TABLE sensor_data;\nDropped successfully.\n \n-- TRUNCATE deletes only data and keeps table.\nMach&gt; TRUNCATE TABLE sensor_data;\nTruncated successfully."
					}
					
				
		
				
					,
					
					"feature-tables-tag-create-select-html": {
						"id": "feature-tables-tag-create-select-html",
						"title": "Creating and Selecting in Rollup Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/create-select.html",
						"content": "Index\n\n\n  Creating Rollup Table\n  Start/Stop Rollup Table\n  Collect Rollup Instantly\n  Drop Rollup\n  Data Sample\n  Get ROLLUP AVG\n  Get ROLLUP MIN/MAX Value\n  Get ROLLUP SUM/COUNT\n  Get ROLLUP Sum of Squares\n  Grouping at Various Time Intervals\n  Using ROLLUP for JSON type\n\n\nCreating Rollup Table\n\nWhen user create tag table, Rollup does not created default, user must create by themselves. Syntax is as follow.\n\n\n\n\n  rollup name : rollup table’s name (Can be freely created with string up to 40)\n  source table name : Name of source table which rollup will aggregate data.\n  *number sec/min/hour : time and time unit for aggregate\n  src_table_column : rollup target data column name\n    \n      Only numeric type columns are allowed\n      If the source table is a rollup table, it is ommited and automatically designated as the rollup target column of the source table\n    \n  \n  ex) 1 sec aggregate : 1 sec\nex) 30 seconds aggregate : 30 sec\nex) 1 minute aggregate : 1 min\nex) 1 hour aggregate : 1 hour\n  constraint\n    \n      Source table for aggregation can specify only tag table or rollup table\n      Only one rollup table can be exist to source table for aggregation\n      If source table for aggregation is rollup table, time for rollup table must be bigger than time for source table. And it must be multiple.\n    \n  \n\n\nExample for Creating rollup table\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE, strvalue VARCHAR(20));\nExecuted successfully.\n \n-- creating 1 second rollup for tag table\nMach&gt; CREATE ROLLUP _tag_rollup_sec ON tag(value) INTERVAL 1 SEC;\n  \n-- creating 1 minute rollup for tag table\nMach&gt; CREATE ROLLUP _tag_rollup_min ON tag(value) INTERVAL 1 MIN;\n  \n-- creating 1 hour rollup for tag table\nMach&gt; CREATE ROLLUP _tag_rollup_hour ON tag(value) INTERVAL 1 HOUR;\n  \n-- creating 30 seconds rollup for tag table\nMach&gt; CREATE ROLLUP _tag_rollup_30sec ON tag(value) INTERVAL 30 SEC;\n  \n-- creating 10 minutes rollup for roll up table above\nMach&gt; CREATE ROLLUP _tag_rollup_10min ON _tag_rollup_30sec INTERVAL 10 MIN;\n \n-- Error when creating rollup for non-numeric type columns\nMach&gt; CREATE ROLLUP _tag_rollup_sec ON tag(strvalue) INTERVAL 1 SEC;\n[ERR-02671: Invalid type for ROLLUP column (STRVALUE).]\n\n\nStart/Stop Rollup Table\n\nEven user create rollup, rollup thread does not start automatically so user must start rollup by themselves.\n\n-- Start specific rollup\nEXEC ROLLUP_START(rollup_name)\n \n-- Stop specific rollup\nEXEC ROLLUP_STOP(rollup_name)\n\n\nCollect Rollup Instantly\n\nrollup basically aggregate data in set time unit.\n\n\n  ex) If it is 1 hour rollup, it aggregate data once in hour and rest in rest time.\nUser can aggregate data in force.\n\n\n-- Execute specific rollup flush immediately\nEXEC ROLLUP_FORCE(rollup_name)\n\n\nDrop Rollup\n\nDrop Rollup.\n\nDROP ROLLUP rollup_name\n\n\n\n  rollup_name : Name of rollup\n  constraint: if there is an rollup table that reference rollup table that will be deleted as source tree, it will cause error. User must delete it in reverse order.\n\n\nmach&gt; create tag table tag (name varchar(20) primary key, time datetime basetime, value double summarized);\nmach&gt; create rollup _tag_rollup_1 on tag(value) interval 1 sec;\nmach&gt; create rollup _tag_rollup_2 on _tag_rollup_1 interval 1 min;\nmach&gt; create rollup _tag_rollup_3 on _tag_rollup_2 interval 1 hour;\n  \nWhen created as above, the reference order is as follows.\n  \ntag -&gt; _tag_rollup_1 -&gt; _tag_rollup_2 -&gt; _tag_rollup_3\n  \nAt this time, if you try to delete the tag table or rollup in the middle, an error occurs.\n  \nmach&gt; drop rollup tag\n&gt; [ERR-02651: Dependent ROLLUP table exists.]\nmach&gt; drop rollup _tag_rollup_1\n&gt; [ERR-02651: Dependent ROLLUP table exists.]\n  \nUser must delete them in the following order to delete them normally.\n  \nmach&gt; drop rollup _tag_rollup_3;\nmach&gt; drop rollup _tag_rollup_2;\nmach&gt; drop rollup _tag_rollup_1;\nmach&gt; drop table tag;\n\n\nSyntax\n\nSELECT TIME ROLLUP 3 SECOND, AVG(VALUE) FROM TAG WHERE ...;\n\n\nAs above, if the ROLLUP clause is appended after the Datetime type column specified as the BASETIME attribute, the rollup table is selected.\n\n[BASETIME_COLUMN] ROLLUP [PERIOD] [TIME_UNIT]\n\n\n\n  BASETIME_COLUMN : Datetime column of the TAG table specified by the BASETIME attribute\n  PERIOD : DATE_TRUNC() can specify a range for each unit of time available. (see below)\n  TIME_UNIT : Any time unit available in the DATE_TRUNC() function can be used. (see below)\n\n\nDepending on the selection of TIME_UNIT, the searched rollup table is different.\n\n\n  \n    \n      unit of time(Abbreviation)\n      range of time\n      rollup table\n    \n  \n  \n    \n      nanosecond (nsec)\n      1000000000 (1 sec)\n      SECOND\n    \n    \n      microsecond (usec)\n      60000000 (60 sec)\n      SECOND\n    \n    \n      milisecond (msec)\n      60000 (60 sec)\n      SECOND\n    \n    \n      second (sec)\n      86400 (1 day)\n      SECOND\n    \n    \n      minute (min)\n      1440 (1 day)\n      MINUTE\n    \n    \n      hour\n      24 (1 day)\n      HOUR\n    \n    \n      day\n      1\n      HOUR\n    \n    \n      month\n      1\n      HOUR\n    \n    \n      year\n      1\n      HOUR\n    \n  \n\n\nSince using the ROLLUP clause directly performs a rollup table lookup, to use an aggregate function, it has the following characteristics.\n\n\n  The aggregate function must be called on the numeric type column. However, only the six aggregate functions (SUM, COUNT, MIN, MAX, AVG, SUMSQ) supported by the rollup table are supported.\n  GROUP BY must be done directly with the BASETIME column to be ROLLUP.\n    \n      You can use the ROLLUP clause with the same meaning as it is.\n      Alternatively, an alias may be attached to the ROLLUP clause, and the alias may be written in GROUP BY.\n    \n  \n\n\nSELECT   time rollup 3 sec mtime, avg(value)\nFROM     TAG\nGROUP BY time rollup 3 sec mtime;\n \n-- or\nSELECT   time rollup 3 sec mtime, avg(value)\nFROM     TAG\nGROUP BY mtime;\n\n\nData Sample\n\nBelow is sample data for rollup test.\n\ncreate tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\n \ninsert into tag metadata values ('TAG_0001');\n \ninsert into tag values('TAG_0001', '2018-01-01 01:00:01 000:000:000', 1);\ninsert into tag values('TAG_0001', '2018-01-01 01:00:02 000:000:000', 2);\ninsert into tag values('TAG_0001', '2018-01-01 01:01:01 000:000:000', 3);\ninsert into tag values('TAG_0001', '2018-01-01 01:01:02 000:000:000', 4);\ninsert into tag values('TAG_0001', '2018-01-01 01:02:01 000:000:000', 5);\ninsert into tag values('TAG_0001', '2018-01-01 01:02:02 000:000:000', 6);\n \ninsert into tag values('TAG_0001', '2018-01-01 02:00:01 000:000:000', 1);\ninsert into tag values('TAG_0001', '2018-01-01 02:00:02 000:000:000', 2);\ninsert into tag values('TAG_0001', '2018-01-01 02:01:01 000:000:000', 3);\ninsert into tag values('TAG_0001', '2018-01-01 02:01:02 000:000:000', 4);\ninsert into tag values('TAG_0001', '2018-01-01 02:02:01 000:000:000', 5);\ninsert into tag values('TAG_0001', '2018-01-01 02:02:02 000:000:000', 6);\n \ninsert into tag values('TAG_0001', '2018-01-01 03:00:01 000:000:000', 1);\ninsert into tag values('TAG_0001', '2018-01-01 03:00:02 000:000:000', 2);\ninsert into tag values('TAG_0001', '2018-01-01 03:01:01 000:000:000', 3);\ninsert into tag values('TAG_0001', '2018-01-01 03:01:02 000:000:000', 4);\ninsert into tag values('TAG_0001', '2018-01-01 03:02:01 000:000:000', 5);\ninsert into tag values('TAG_0001', '2018-01-01 03:02:02 000:000:000', 6);\n\n\nFor one tag, different values ​​in seconds were input for 3 hours.\n\nGet ROLLUP AVG\n\nBelow is the case of getting average of seconds, minutes, hours of tag table.\n\nMach&gt; SELECT time rollup 1 sec mtime, avg(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           avg(value)\n---------------------------------------------------------------\n2018-01-01 01:00:01 000:000:000 1\n2018-01-01 01:00:02 000:000:000 2\n2018-01-01 01:01:01 000:000:000 3\n2018-01-01 01:01:02 000:000:000 4\n2018-01-01 01:02:01 000:000:000 5\n2018-01-01 01:02:02 000:000:000 6\n2018-01-01 02:00:01 000:000:000 1\n2018-01-01 02:00:02 000:000:000 2\n2018-01-01 02:01:01 000:000:000 3\n2018-01-01 02:01:02 000:000:000 4\n2018-01-01 02:02:01 000:000:000 5\n2018-01-01 02:02:02 000:000:000 6\n2018-01-01 03:00:01 000:000:000 1\n2018-01-01 03:00:02 000:000:000 2\n2018-01-01 03:01:01 000:000:000 3\n2018-01-01 03:01:02 000:000:000 4\n2018-01-01 03:02:01 000:000:000 5\n2018-01-01 03:02:02 000:000:000 6\n[18] row(s) selected.\n \nMach&gt; SELECT time rollup 1 min mtime, avg(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           avg(value)\n---------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 1.5\n2018-01-01 01:01:00 000:000:000 3.5\n2018-01-01 01:02:00 000:000:000 5.5\n2018-01-01 02:00:00 000:000:000 1.5\n2018-01-01 02:01:00 000:000:000 3.5\n2018-01-01 02:02:00 000:000:000 5.5\n2018-01-01 03:00:00 000:000:000 1.5\n2018-01-01 03:01:00 000:000:000 3.5\n2018-01-01 03:02:00 000:000:000 5.5\n[9] row(s) selected.\n \nMach&gt; SELECT time rollup 1 hour mtime, avg(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           avg(value)\n---------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 3.5\n2018-01-01 02:00:00 000:000:000 3.5\n2018-01-01 03:00:00 000:000:000 3.5\n[3] row(s) selected.\n\n\nGet ROLLUP MIN/MAX Value\n\nBelow is the case of getting min/max value of seconds, minutes, hours of tag table. The difference between others, you can get minimum value and maximum value at the same time with just one query.\n\nMach&gt; SELECT time rollup 1 hour mtime, min(value), max(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           min(value)                  max(value)\n--------------------------------------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 1                           6\n2018-01-01 02:00:00 000:000:000 1                           6\n2018-01-01 03:00:00 000:000:000 1                           6\n[3] row(s) selected.\n \nMach&gt; SELECT time rollup 1 min mtime, min(value), max(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           min(value)                  max(value)\n--------------------------------------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 1                           2\n2018-01-01 01:01:00 000:000:000 3                           4\n2018-01-01 01:02:00 000:000:000 5                           6\n2018-01-01 02:00:00 000:000:000 1                           2\n2018-01-01 02:01:00 000:000:000 3                           4\n2018-01-01 02:02:00 000:000:000 5                           6\n2018-01-01 03:00:00 000:000:000 1                           2\n2018-01-01 03:01:00 000:000:000 3                           4\n2018-01-01 03:02:00 000:000:000 5                           6\n[9] row(s) selected.\n\n\nGet ROLLUP SUM/COUNT\n\nBelow is the case of getting sum/count value. Also you can get  sum value and count value at the same time with just one query.\n\nMach&gt; SELECT time rollup 1 min  mtime, sum(value), count(value) FROM TAG WHERE name = 'TAG_0001' group by mtime order by mtime;\nmtime                           sum(value)                  count(value)\n-------------------------------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 3                           2\n2018-01-01 01:01:00 000:000:000 7                           2\n2018-01-01 01:02:00 000:000:000 11                          2\n2018-01-01 02:00:00 000:000:000 3                           2\n2018-01-01 02:01:00 000:000:000 7                           2\n2018-01-01 02:02:00 000:000:000 11                          2\n2018-01-01 03:00:00 000:000:000 3                           2\n2018-01-01 03:01:00 000:000:000 7                           2\n2018-01-01 03:02:00 000:000:000 11                          2\n[9] row(s) selected.\n\n\nGet ROLLUP Sum of Squares\n\nBelow is the case of getting sum of squares in rollup.\n\nMach&gt; SELECT time ROLLUP 1 SEC mtime, SUMSQ(value) FROM tag GROUP BY mtime ORDER BY mtime;\nmtime                           SUMSQ(value)               \n---------------------------------------------------------------\n2018-01-01 01:00:01 000:000:000 1                          \n2018-01-01 01:00:02 000:000:000 4                          \n2018-01-01 01:01:01 000:000:000 9                          \n2018-01-01 01:01:02 000:000:000 16                         \n2018-01-01 01:02:01 000:000:000 25                         \n2018-01-01 01:02:02 000:000:000 36                         \n2018-01-01 02:00:01 000:000:000 1                          \n2018-01-01 02:00:02 000:000:000 4                          \n2018-01-01 02:01:01 000:000:000 9                          \n2018-01-01 02:01:02 000:000:000 16                         \n2018-01-01 02:02:01 000:000:000 25                         \n2018-01-01 02:02:02 000:000:000 36                         \n2018-01-01 03:00:01 000:000:000 1                          \n2018-01-01 03:00:02 000:000:000 4                          \n2018-01-01 03:01:01 000:000:000 9                          \n2018-01-01 03:01:02 000:000:000 16                         \n2018-01-01 03:02:01 000:000:000 25                         \n2018-01-01 03:02:02 000:000:000 36                         \n[18] row(s) selected.\n \nMach&gt; SELECT time ROLLUP 1 MIN mtime, SUMSQ(value) FROM tag GROUP BY mtime ORDER BY mtime;\nmtime                           SUMSQ(value)               \n---------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 5                          \n2018-01-01 01:01:00 000:000:000 25                         \n2018-01-01 01:02:00 000:000:000 61                         \n2018-01-01 02:00:00 000:000:000 5                          \n2018-01-01 02:01:00 000:000:000 25                         \n2018-01-01 02:02:00 000:000:000 61                         \n2018-01-01 03:00:00 000:000:000 5                          \n2018-01-01 03:01:00 000:000:000 25                         \n2018-01-01 03:02:00 000:000:000 61                         \n[9] row(s) selected.\n\n\nGrouping at Various Time Intervals\n\nThe advantage of the ROLLUP clause is that it is not necessary to intentionally use DATE_TRUNC() to vary the time interval.\n\nTo get the sum of the 3-second interval and the number of data, you can do as follows.\nSince the example time range is only 0 sec, 1 sec, and 2 sec, it can be seen that they all converge to 0 sec. As a result, it matches the ‘rollup by minute’ query result.\n\nMach&gt; SELECT time rollup 3 sec  mtime, sum(value), count(value) FROM TAG WHERE name = 'TAG_0001' GROUP BY mtime ORDER BY mtime;\nmtime                           sum(value)                  count(value)\n-------------------------------------------------------------------------------------\n2018-01-01 01:00:00 000:000:000 3                           2\n2018-01-01 01:01:00 000:000:000 7                           2\n2018-01-01 01:02:00 000:000:000 11                          2\n2018-01-01 02:00:00 000:000:000 3                           2\n2018-01-01 02:01:00 000:000:000 7                           2\n2018-01-01 02:02:00 000:000:000 11                          2\n2018-01-01 03:00:00 000:000:000 3                           2\n2018-01-01 03:01:00 000:000:000 7                           2\n2018-01-01 03:02:00 000:000:000 11                          2\n\n\nUsing ROLLUP for JSON type\n\nStarting from version 7.5, ROLLUP can be used for JSON types.\nYou can create by adding the JSON PATH and OPERATOR at create statement.\nA ROLLUP can be created for each PATH in one JSON column.\n\n-- create tag table\nCREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, jval JSON);\n  \n-- insert data\ninsert into tag values ('tag-01', '2022-09-01 01:01:01', \"{ \\\"x\\\": 1, \\\"y\\\": 1.1}\");\ninsert into tag values ('tag-01', '2022-09-01 01:01:02', \"{ \\\"x\\\": 2, \\\"y\\\": 1.2}\");\ninsert into tag values ('tag-01', '2022-09-01 01:01:03', \"{ \\\"x\\\": 3, \\\"y\\\": 1.3}\");\ninsert into tag values ('tag-01', '2022-09-01 01:01:04', \"{ \\\"x\\\": 4, \\\"y\\\": 1.4}\");\ninsert into tag values ('tag-01', '2022-09-01 01:01:05', \"{ \\\"x\\\": 5, \\\"y\\\": 1.5}\");\ninsert into tag values ('tag-01', '2022-09-01 01:02:00', \"{ \\\"x\\\": 6, \\\"y\\\": 1.6}\");\ninsert into tag values ('tag-01', '2022-09-01 01:03:00', \"{ \\\"x\\\": 7, \\\"y\\\": 1.7}\");\ninsert into tag values ('tag-01', '2022-09-01 01:04:00', \"{ \\\"x\\\": 8, \\\"y\\\": 1.8}\");\ninsert into tag values ('tag-01', '2022-09-01 01:05:00', \"{ \\\"x\\\": 9, \\\"y\\\": 1.9}\");\ninsert into tag values ('tag-01', '2022-09-01 01:06:00', \"{ \\\"x\\\": 10, \\\"y\\\": 2.0}\");\n  \n-- create rollup\nCREATE ROLLUP _tag_rollup_jval_x_sec ON tag(jval-&gt;'$.x') INTERVAL 1 SEC;\nCREATE ROLLUP _tag_rollup_jval_y_sec ON tag(jval-&gt;'$.y') INTERVAL 1 SEC;\n\n\nYou can also use selecting ROLLUP in the same way.\n\nMach&gt; SELECT time ROLLUP 2 SEC mtime, MIN(jval-&gt;'$.x'), MAX(jval-&gt;'$.x'), SUM(jval-&gt;'$.x'), COUNT(jval-&gt;'$.x'), SUMSQ(jval-&gt;'$.x') FROM tag GROUP BY mtime ORDER BY mtime;\nmtime                           min(jval-&gt;'$.x')            max(jval-&gt;'$.x')            sum(jval-&gt;'$.x')            count(jval-&gt;'$.x')   sumsq(jval-&gt;'$.x')        \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n2022-09-01 01:01:00 000:000:000 1                           1                           1                           1                    1                         \n2022-09-01 01:01:02 000:000:000 2                           3                           5                           2                    13                        \n2022-09-01 01:01:04 000:000:000 4                           5                           9                           2                    41                        \n2022-09-01 01:02:00 000:000:000 6                           6                           6                           1                    36                        \n2022-09-01 01:03:00 000:000:000 7                           7                           7                           1                    49                        \n2022-09-01 01:04:00 000:000:000 8                           8                           8                           1                    64                        \n2022-09-01 01:05:00 000:000:000 9                           9                           9                           1                    81                        \n2022-09-01 01:06:00 000:000:000 10                          10                          10                          1                    100                       \n[8] row(s) selected.\n  \nMach&gt; SELECT time ROLLUP 2 SEC mtime, MIN(jval-&gt;'$.y'), MAX(jval-&gt;'$.y'), SUM(jval-&gt;'$.y'), COUNT(jval-&gt;'$.y'), SUMSQ(jval-&gt;'$.y') FROM tag GROUP BY mtime ORDER BY mtime\nmtime                           min(jval-&gt;'$.y')            max(jval-&gt;'$.y')            sum(jval-&gt;'$.y')            count(jval-&gt;'$.y')   sumsq(jval-&gt;'$.y')        \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n2022-09-01 01:01:00 000:000:000 1.1                         1.1                         1.1                         1                    1.21                      \n2022-09-01 01:01:02 000:000:000 1.2                         1.3                         2.5                         2                    3.13                      \n2022-09-01 01:01:04 000:000:000 1.4                         1.5                         2.9                         2                    4.21                      \n2022-09-01 01:02:00 000:000:000 1.6                         1.6                         1.6                         1                    2.56                      \n2022-09-01 01:03:00 000:000:000 1.7                         1.7                         1.7                         1                    2.89                      \n2022-09-01 01:04:00 000:000:000 1.8                         1.8                         1.8                         1                    3.24                      \n2022-09-01 01:05:00 000:000:000 1.9                         1.9                         1.9                         1                    3.61                      \n2022-09-01 01:06:00 000:000:000 2                           2                           2                           1                    4                         \n[8] row(s) selected."
					}
					
				
		
				
					,
					
					"tools-utils-csv-html": {
						"id": "tools-utils-csv-html",
						"title": "csvimport / csvexport",
						"version": "all",
						"categories": "",
						"url": " /tools/utils/csv.html",
						"content": "‘csvimport’ and ‘csvexport’ are tools used to import/export CSV files to the Machbase server.\n\nThe options have been simplified for simpler use of the CSV file using the machloader.\n\nIn addition to the options described below, all options available in machloader are available.\n\nIndex\n\n\n  csvimport\n  csvexport\n\n\ncsvimport\n\nCSV files can be easily entered into the server using csvimport.\n\nBasic Usage\n\nEnter the table name and data file name according to the following options.\n\nOptions:\n\n-t: table name specification option\n-d: data file naming options\n* You can do this with just the table name and data file name without specifying the option.\n\n\nExample:\n\ncsvimport -t table_name -d table_name.csv\ncsvimport table_name file_path\ncsvimport file_path table_name\n\n\nCSV Header Exception\n\nUse the following option to enter the CSV file except for the header at the time of input.\n\nOptions:\n\n-H: Will not recognize the first line of the csv file as a header.\n\n\nExample:\n\ncsvimport -t table_name -d table_name.csv -H\n\n\nAutomatic Table Creation\n\nIf a table is not created to be entered at the time of input, the table can be created at the same time through the following options.\n\nOption\n\n-C: Automatically creates the table during import. Column names are automatically created as c0, c1, .... The created column is varchar (32767) type.\n-H: Creates column name with csv header name during import.\n\n\nExample:\n\ncsvimport -t table_name -d table_name.csv -C\ncsvimport -t table_name -d table_name.csv -C -H\n\n\ncsvexport\n\nThe database table data can be easily exported to the CSV file with ‘csvexport’.\n\nBasic Usage\n\nOption:\n\n-t: table name specification option\n-d: data file naming options\n* You can do this with just the table name and data file name without specifying the option.\n\n\nExample:\n\ncsvexport -t table_name -d table_name.csv\ncsvexport table_name file_path\ncsvexport file_path table_name\n\n\nUsing CSV Header\n\nWith the following option, you can add a header to the CSV file to be exported with a column name.\n\nOption:\n\n-H: Creates the header of the csv file with the table column name.\n\n\nExample:\n\ncsvexport -t table_name -d table_name.csv -H"
					}
					
				
		
				
					,
					
					"feature-tables-log-extract-data-retrieval-html": {
						"id": "feature-tables-log-extract-data-retrieval-html",
						"title": "Data Retrieval",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/extract/data-retrieval.html",
						"content": "Index\n\n\n  Basic access\n  View Conditional Clause\n\n\nYou can retrieve data in ANSI standard SQL.\n\nThe following example shows a search without creating an index.\n\nIn other words, the last input data is outputed first.\n\nFor more information, see the SELECT section of the SQL Reference.\n\nBasic access\n\nSELECT * FROM table_name;\n\n\nIndex\nBasic access\nView Conditional Clause\nMach&gt; SELECT * FROM mach_log;\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.1     NULL                            NULL       \nNULL                                                                             \n192.168.0.2     2014-06-15 19:50:03 484:382:010 82         \nerror code = 20, critical warning                                                \n192.168.0.2     2014-06-15 19:50:03 484:382:008 57         \nerror code = 20                                                                  \n192.168.0.1     2014-06-15 19:50:03 484:382:006 99         \nerror code = 10, critical bug                                                    \n192.168.0.1     2014-06-15 19:50:03 484:382:004 55         \nerror code = 10                                                                  \n192.168.0.2     2014-06-15 19:50:03 484:382:002 31       \nnormal state                                                                     \n192.168.0.1     2014-06-15 19:50:03 484:382:000 32         \nnormal state                                                                     \n[7] row(s) selected.\nMach&gt;\n\n\nView Conditional Clause\n\nSELECT column_name,column_name\nFROM table_name\nWHERE column_name operator value;\n\n\nMach&gt; SELECT * FROM mach_log WHERE device = '192.168.0.1';\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.1     NULL                            NULL       \nNULL                                                                             \n192.168.0.1     2014-06-15 19:50:36 488:663:006 99         \nerror code = 10, critical bug                                                    \n192.168.0.1     2014-06-15 19:50:36 488:663:004 55         \nerror code = 10                                                                  \n192.168.0.1     2014-06-15 19:50:36 488:663:000 32         \nnormal state                                                                     \n[4] row(s) selected.\n \nMach&gt; SELECT * FROM mach_log WHERE device = '192.168.0.1' AND temp &gt; 30 AND temp &lt; 50;\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.1     2014-06-15 19:50:36 488:663:000 32         \nnormal state                                                                     \n[1] row(s) selected.\n \nMach&gt; SELECT * FROM mach_log where device &gt; '192.168.0.1';\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.2     2014-06-15 19:50:36 488:663:010 82         \nerror code = 20, critical warning                                                \n192.168.0.2     2014-06-15 19:50:36 488:663:008 57         \nerror code = 20                                                                  \n192.168.0.2     2014-06-15 19:50:36 488:663:002 31         \nnormal state                                                                     \n[3] row(s) selected.\n \nMach&gt; SELECT * FROM mach_log WHERE msg LIKE '%error%';\nDEVICE          TM                              TEMP       \n----------------------------------------------------------------\nMSG                                                                              \n------------------------------------------------------------------------------------\n192.168.0.2     2014-06-15 19:50:36 488:663:010 82         \nerror code = 20, critical warning                                                \n192.168.0.2     2014-06-15 19:50:36 488:663:008 57         \nerror code = 20                                                                  \n192.168.0.1     2014-06-15 19:50:36 488:663:006 99         \nerror code = 10, critical bug                                                    \n192.168.0.1     2014-06-15 19:50:36 488:663:004 55         \nerror code = 10                                                                  \n[4] row(s) selected."
					}
					
				
		
				
					,
					
					"sql-ref-datatypes-html": {
						"id": "sql-ref-datatypes-html",
						"title": "Datatypes",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/datatypes.html",
						"content": "Index\n\n\n  Data Type Table\n  SQL DataType Table\n\n\nData Type Table\n\n\n  \n    \n      Type Name\n      Description\n      Value Range\n      NULL Value\n    \n  \n  \n    \n      short\n      16-bit signed integer data type\n      -32767 ~ 32767\n      -32768\n    \n    \n      ushort\n      16-bit unsigned integer type data type\n      0 ~ 65534\n      65535\n    \n    \n      integer\n      32-bit signed integer data type\n      -2147483647 ~ 2147483647\n      -2147483648\n    \n    \n      uinteger\n      32-bit unsigned integer data type\n      0 ~ 4294967294\n      4294967295\n    \n    \n      long\n      64-bit signed integer data type\n      -9223372036854775807 ~ 9223372036854775807\n      -9223372036854775808\n    \n    \n      ulong\n      64-bit unsigned integer data type\n      0~18446744073709551614\n      18446744073709551615\n    \n    \n      float\n      32-bit floating point data type\n      -\n      -\n    \n    \n      double\n      64-bit floating point data type\n      -\n      -\n    \n    \n      datetime\n      Time and date\n      1970-01-01 00:00:00 000:000:000 ~\n      -\n    \n    \n      varchar\n      Variable-length character strings (UTF-8)\n      Length : 1 ~ 32768 (32K)\n      -\n    \n    \n      ipv4\n      Version 4 Internet address type (4 bytes)\n      “0.0.0.0” ~ “255.255.255.255”\n      -\n    \n    \n      ipv6\n      Version 6 Internet address type (16 bytes)\n      “0000:0000:0000:0000:0000:0000:0000:0000” ~ “FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF”\n      -\n    \n    \n      text\n      Text data type (keyword index can be generated)\n      Length : 0 ~ 64M\n      -\n    \n    \n      binary\n      Binary data type  (index creation not possible)\n      Length: 0 ~ 64M\n      -\n    \n    \n      json\n      json data type\n      json data length : 1 ~ 32768 (32K)json path length : 1 ~ 512\n      -\n    \n  \n\n\nshort\n\nThis is the same as the 16-bit signed integer data of the C language. For the minimum negative value, it is recognized as NULL. May be displayed as “int16”.\n\ninteger\n\nThis is the same as 32-bit signed integer data in C language. For the minimum negative value, it is recognized as NULL. May be displayed as “int32” or “int”.\n\nlong\n\nThis is the same as 64-bit signed integer data in C language. For the minimum negative value, it is recognized as NULL. May be displayed as “int64”.\n\nfloat\n\nThis is equivalent to the C 32-bit floating-point data type float. For a positive maximum value, it is recognized as NULL.\n\ndouble\n\nThis is equivalent to the 64-bit floating-point data type double of C language. For a positive maximum value, it is recognized as NULL.\n\ndatetime\n\nIn Machbase, this type maintains the nano value of the time elapsed since midnight January 1, 1970.\n\nThus, Machbase provides the ability to process values ​​up to nano units for all datetime type related functions.\n\nvarchar\n\nThis is a variable string data type and can be generated up to 32K bytes in length.\n\nThis length criterion is based on one character in English, so it is different from the actual number of characters to be output in UTF-8 and should be set to an appropriate length.\n\nIPv4\n\nThis type is a type that can store addresses used in Internet Protocol version 4.\n\nIt is internally represented using 4 bytes, and can be expressed from “0.0.0.0” to “255.255.255.255”.\n\nIPv6\n\nThis type is a type that can store addresses used in Internet Protocol version 6.\n\n16 bytes are internally represented and can be expressed from “0000: 0000: 0000: 0000: 0000: 0000: 0000: 0000” to “FFFF: FFFF: FFFF: FFFF: FFFF: FFFF: FFFF: FFFF” .\nSince the abbreviation type is also supported when inputting data, it can be expressed as follows using the symbol :.\n\n\n  ”:: FFFF: 1232”: all leading with zeros\n  ”:: FFFF: 192.168.0.3”: Support for IPv4 type compatibility\n  ”:: 192.168.3.1”: Support for deprecated IPv4 type compatibility\n\n\ntext\n\nThis type is a data type for storing text or documents beyond the size of a VARCHAR.\n\nThis data type can be searched through keyword indexes and can store up to 64 megabytes of text. \nThis type is mainly used to store and retrieve large text files as separate columns.\n\nbinary\n\nThis type is a supported type for storing unstructured data in columns.\n\nIt is used to store binary data such as image, video, or audio. Indexes can not be created for this type. \nThe maximum data size for storing is up to 64 megabytes, the same as the TEXT type.\n\njson\n\nThis type is a data type for storing json data.\n\nJson is a format to store data object, consisting of “Key-Value” pairs, into text format.\n\nThe maximum size of data is 32K bytes which is same as varchar type.\n\nSQL Datatype Table\n\nThe following table shows the SQL data types and C data types corresponding to the mark base data types.\n\n\n  \n    \n      Machbase Datatype\n      Machbase CLI Datatype\n      SQL Datatype\n      C Datatype\n      Basic types for C\n      Description\n    \n  \n  \n    \n      short\n      SQL_SMALLINT\n      SQL_SMALLINT\n      SQL_C_SSHORT\n      int16_t (short)\n      16-bit signed integer data type\n    \n    \n      ushort\n      SQL_USMALLINT\n      SQL_SMALLINT\n      SQL_C_USHORT\n      uint16_t (unsigned short)\n      16-bit unsigned integer type data type\n    \n    \n      integer\n      SQL_INTEGER\n      SQL_INTEGER\n      SQL_C_SLONG\n      int32_t (int)\n      32-bit signed integer data type\n    \n    \n      uinteger\n      SQL_UINTEGER\n      SQL_INTEGER\n      SQL_C_ULONG\n      uint32_t (unsigned int)\n      32-bit unsigned integer data type\n    \n    \n      long\n      SQL_BIGINT\n      SQL_BIGINT\n      SQL_C_SBIGINT\n      int64_t (long long)\n      64-bit signed integer data type\n    \n    \n      ulong\n      SQL_UBIGINT\n      SQL_BIGINT\n      SQL_C_UBIGINT\n      uint64_t (unsigned long long)\n      64-bit unsigned integer data type\n    \n    \n      float\n      SQL_FLOAT\n      SQL_REAL\n      SQL_C_FLOAT\n      float\n      32-bit floating point data type\n    \n    \n      double\n      SQL_DOUBLE\n      SQL_FLOAT, SQL_DOUBLE\n      SQL_C_DOUBLE\n      double\n      64-bit floating point data type\n    \n    \n      datetime\n      SQL_TIMESTAMPSQL_TIME\n      SQL_TYPE_TIMESTAMPSQL_BIGINTSQL_TYPE_TIME\n      SQL_C_TYPE_TIMESTAMPSQL_C_UBIGINTSQL_C_TIME\n      char * (YYYY-MM-DD HH24:MI:SS)int64_t (timestamp: nano seconds)struct tm\n      Time and date\n    \n    \n      varchar\n      SQL_VARCHAR\n      SQL_VARCHAR\n      SQL_C_CHAR\n      char *\n      String\n    \n    \n      ipv4\n      SQL_IPV4\n      SQL_VARCHAR\n      SQL_C_CHAR\n      char * (enter ip string)unsigned char[4]\n      Version 4 Internet address type\n    \n    \n      ipv6\n      SQL_IPV6\n      SQL_VARCHAR\n      SQL_C_CHAR\n      char * (enter ip string)unsigned char[16]\n      Version 6 Internet address type\n    \n    \n      text\n      SQL_TEXT\n      SQL_LONGVARCHAR\n      SQL_C_CHAR\n      char *\n      Text\n    \n    \n      binary\n      SQL_BINARY\n      SQL_BINARY\n      SQL_C_BINARY\n      char *\n      Binary data\n    \n    \n      json\n      SQL_JSON\n      SQL_JSON\n      SQL_C_CHAR\n      json_t\n      json data type"
					}
					
				
		
				
					,
					
					"feature-tables-backup-mount-dbm-html": {
						"id": "feature-tables-backup-mount-dbm-html",
						"title": "Database Mount",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/backup-mount/dbm.html",
						"content": "MOUNT\n  UNMOUNT\n  Read data from a mounted database\n\n\nContinuously storing a large amount of data in order to analyze the data greatly increases its volume, resulting in the following problems:\n\n\n  Increased disk cost by storing a large volume of data\n  Running into equipment disk limits for data analysis\n\n\nTo solve the problem, it is necessary to back up old data and periodically delete it. If you need to read old data later and you perform data recovery to read the backed up database, it will not only take a long time to recover, but it will also take a long time to convert the database to offline, There is a problem that requires separate equipment to continue the service. Machase supports the MOUNT command to solve this problem.\n\nThe MOUNT command reads the backed up data while the database is in service and creates a new database independent of the currently running database. One server can add multiple backed-up databases to retrieve data at the same time, but the mounted database is read-only and can not add or delete data.\n\nThe Database MOUNT command allows you to read both the backup data and the main database contents simultaneously. Therefore, the mounted database can retrieve data in the same way as the existing data retrieval method.\n\nTo execute the MOUNT instruction, the following conditions must be met.\n\n  The backup database version and the metadata version must be compatible.\n  You can not create tables, create/delete indexes, or add/delete data to a mounted backup database.\n\n\nInformation about mounted databases can be obtained from the V$STORAGE_MOUNT_DATABASES meta table.\n\nMOUNT\n\nTo run the mount command, the backup database pathname and the name of the database to be mounted must be entered.\nThe backup database path sets the location of the directory executed by the backup command. The name of the database to be mounted must be given a separate name to distinguish it from the active database.\nThe backup database pathname can be an absolute pathname (a pathname beginning with the “/” character), or a relative pathname based on $MACHBASE_HOME/dbs with the same rules as the backup command.\n\nSyntax:\n\nMOUNT DATABASE 'backup_database_path' TO mount_name;\n\n\nExample:\n\nMOUNT DATABASE '/home/machbase/backup' TO mountdb;\n\n\nUNMOUNT\n\nIf the mounded database data no longer needs to be read, use the UNMOUNT command to release the mounted state.\n\nSyntax:\n\nUNMOUNT DATABASE mount_name;\n\n\nExample:\n\nUNMOUNT DATABASE mountdb;\n\n\nReading Data from Mounted Database\n\nWhen retrieving data from a mounted database, use the same SQL statement as before.\nOnly the SYS user can read the mounted data. To specify the mounted database table in an SQL statement, the mount_name and user_name must be set connected to a “.” character.\n\nSyntax:\n\nSELECT column_name FROM mount_name.user_name.table_name;\n\n\nExample:\n\nSELECT * FROM mountdb.sys.backuptable;"
					}
					
				
		
				
					,
					
					"sql-ref-ddl-html": {
						"id": "sql-ref-ddl-html",
						"title": "DDL",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/ddl.html",
						"content": "Index\n\n\n  Index\n  CREATE TABLESPACE\n    \n      DATA DISK\n    \n  \n  CREATE TABLE\n    \n      Table Type\n      Data Column\n      Table Property\n      Column Property\n      MINMAX Cache Concept\n      MINMAX Cache Column\n      Primary Key\n      AUTO_DEL Clauses\n    \n  \n  SEQUENCE for Lookup Table\n    \n      Machbase Edition support Sequence\n      Table type support Sequence\n      Configuring Sequence when Creating Lookup Tables\n      Use of sequence column\n    \n  \n  CREATE TAG TABLE\n  CREATE INDEX\n    \n      Index Type\n      KEYWORD Index\n      LSM Index\n      LSM Index Property\n      BITMAP Index\n      RED-BLACK Index\n      Index Property\n    \n  \n  DROP TABLESPACE\n  DROP TABLE\n    \n      DROP INDEX\n    \n  \n  ALTER TABLESPACE\n    \n      ALTER TABLESPACE MODIFY DATADISK\n    \n  \n  ALTER TABLE\n    \n      ALTER TABLE SET\n      ALTER TABLE ADD COLUMN\n      ALTER TABLE DROP COLUMN\n      ALTER TABLE RENAME COLUMN\n      ALTER TABLE MODIFY COLUMN\n      ALTER TABLE FLUSH\n      ALTER TABLE FLUSH INDEX\n      ALTER TABLE RECLAIM STORAGE\n      ALTER TABLE RENAME TO\n      ALTER TABLE AUTO DELETE\n      ALTER TABLE ADD RETENTION\n      ALTER TABLE DROP RETENTION\n    \n  \n  TRUNCATE TABLE\n  CREATE ROLLUP\n  DROP ROLLUP\n  CREATE RETENTION\n  DROP RETENTION\n\n\nCREATE TABLESPACE\n\ncreate_tablespace_stmt:\n\n\n\ndatadisk_list:\n\n\n\ndata_disk:\n\n\n\ndata_disk_property:\n\n\n\ncreate_tablespace_stmt ::= 'CREATE TABLESPACE' tablespace_name 'DATADISK' datadisk_list\ndatadisk_list ::= data_disk ( ',' data_disk )*\ndata_disk ::= disk_name data_disk_property\ndata_disk_property ::= '(' 'DISK_PATH' '=' '\"' path '\"' ( ',' 'PARALLEL_IO' '=' number )? ')'\n\n\n-- Example\ncreate tablespace tbs1 datadisk disk1 (disk_path=\"\"); -- $MACHBASE_HOME/dbs/  (Create in $MACHBASE_HOME/dbs/)\ncreate tablespace tbs1 datadisk disk1 (disk_path=\"tbs1_disk1\"); -- $MACHBASE_HOME/dbs/tbs1_disk1  (Created in $MACHBASE_HOME/dbs/tbs1_disk1. tbs1_disk1 folder must exist)\ncreate tablespace tbs2 datadisk disk1 (disk_path=\"tbs2_disk1\", parallel_io = 5);\ncreate tablespace tbs1 datadisk disk1 (disk_path=\"tbs1_disk1\", parallel_io = 10), disk2 (disk_path=\"tbs1_disk2\"), disk3 (disk_path=\"tbs1_disk3\");\n\n\nThe CREATE TABLESPACE statement creates a tablespace in $MACHBASE_HOME/dbs/ where the indexes of the log table or log table will be stored.\n\nTablespace can have multiple disks. When each Partition File that stores data of Table and Index is stored, it is distributed and stored in Data Disks belonging to Tablespace.\nIf two or more disks are used, the index and table files are distributed and stored on each disk, and I/O is performed in parallel on each device. As the number of disks increases, disk I / O throughput increases, and a large amount of data can be stored on the disk quickly\nAlso, if tables and index tablespace are separately created and different disks are defined, I/O of table and index can be logically separated without reconfiguration of physical disk.\n\nDATA DISK\n\nDefines disk belonging to a tablespace. Each Disk has the following properties.\n\n\n  \n    \n      Property\n      Description\n    \n  \n  \n    \n      data_disk_property\n      Specifies the attributes of the disk.\n    \n    \n      disk_name\n      Specifies the name of the Disk object. It is used to change the attributes of the Disk object through Alter Tablespace syntax later.\n    \n    \n      disk_path\n      Specifies the Directory Path of the disk. This Directory must be created. When a path is specified as a relative path, PATH is searched based on $MACHBASE_HOME/dbs. For example, if PATH = ‘disk1’, Disk Path is recognized as $MACHBASE_HOME/dbs/disk1.\n    \n    \n      parallel_io\n      Determines how many disk IO requests are allowed to be paralleled. (DEF: 3, MIN: 1, MAX: 128)\n    \n  \n\n\nCREATE TABLE\n\ncreate_table_stmt:\n\n\n\ncolumn_list:\n\n\n\ncolumn_property_list:\n\n\n\ntable_property_list:\n\n\n\ncolumn_type:\n\n\n\nauto_del:\n\n\n\ncreate_table_stmt ::= 'CREATE' ( 'LOG' | 'VOLATILE' | 'LOOKUP' )? 'TABLE' table_name '(' column_list ')' table_property_list?\ncolumn_list ::= column_name column_type column_property_list? 'PRIMARY KEY'? 'NOT NULL'? ( ',' column_name column_type column_property_list? 'PRIMARY KEY'? 'NOT NULL'? )*\ncolumn_property_list ::= 'PROPERTY' '(' ( 'PART_PAGE_COUNT' | 'PAGE_VALUE_COUNT' | 'MINMAX_CACHE_SIZE' | 'MAX_CACHE_PART_COUNT' ) '=' value ( ',' ( 'PART_PAGE_COUNT' | 'PAGE_VALUE_COUNT' | 'MINMAX_CACHE_SIZE' | 'MAX_CACHE_PART_COUNT' ) '=' value )* ')'\ntable_property_list ::=  ( 'CHECK_FORGERY' | 'TAG_PARTITION_COUNT' ) '=' value\ncolumn_type ::= 'SHORT' | 'USHORT' | 'INTEGER' | 'UINTEGER' | 'LONG' | 'ULONG' | 'FLOAT' | 'DOUBLE' | 'DATETIME' | 'VARCHAR' '(' size ')' | 'IPV4' | 'IPV6' | 'TEXT' | 'BINARY'\n\n\n-- Create ctest table with 5 columns\nMach&gt; create table ctest (id integer, name varchar(20), sipv4 ipv4,dipv6 ipv6,comment text);\nCreated successfully.\n\n\nTable Type\n\n\n  \n    \n      Table Type\n      Description\n    \n  \n  \n    \n      LOG_TABLE\n      If there is no keyword between CREATE TABLE, a log table is created.\n    \n    \n      VOLATILE_TABLE\n      VOLATILE_TABLE is a temporary table in which all data resides in temporary memory and joins the log table to improve the results,The Machbase server disappears as soon as it is shut down.\n    \n    \n      LOOKUP_TABLE\n      Like VOLATILE_TABLE, LOOKUP_TABLE can perform fast query processing by storing all the data in memory.\n    \n  \n\n\nData Column\n\nA table column is usually a string, but it can be enclosed in single quotes (‘) or double quotes (“) to contain special characters.\n\nMach&gt; CREATE TABLE special_tbl ( with.dot INTEGER );\n[ERR-02010: Syntax error: near token (.dot INTEGER )).]\nCREATE TABLE special_tbl ( \"with.dot\" INTEGER ); -- (Possible)\nCREATE TABLE special_tbl ( 'with.dot' INTEGER ); -- (Possible)\n\n\nWhen querying the column through a SELECT query, it must be enclosed in either single or double quotes.\n\n\n  When enclosed in single quotation marks, the name of the SELECT query result column is output with single quotation marks.\n  When enclosed in double quotes, the name of the SELECT query result column is output as it is.\n\n\n\nMach&gt; SELECT 'with.dot' FROM special_tbl;\n'with.dot'\n--------------\n[0] row(s) selected.\n \nMach&gt; SELECT \"with.dot\" FROM special_tbl;\nwith.dot\n--------------\n[0] row(s) selected.\n\n\nTable Property\n\nSpecifies the attributes for the table.\n\n\n  \n    \n      Property Name\n      Available Table Types\n    \n  \n  \n    \n      TAG_PARTITION_COUNT\n      TAG TABLE\n    \n    \n      TAG_DATA_PART_SIZE\n      TAG TABLE\n    \n    \n      TAG_STAT_ENABLE\n      TAG TABLE\n    \n  \n\n\nTAG_PARTITION_COUNT(Default:4)\nA supported attribute for the TAG table, determines how many partition tables will store the TAG table internally. It should be set according to the number of tags or the performance of the server.\n\nTAG_DATA_PART_SIZE(Default:16MB)\nA supported attribute for the TAG table, determines the data size for each partition table.\n\nTAG_STAT_ENABLE(Default:1)\nA supported attribute for the TAG Table, determines whether to store statistical information for each TAG ID.\n\nColumn Property\n\nSpecifies the attribute for the column.\n\n\n  \n    \n      Property Name\n      Available Table Types\n    \n  \n  \n    \n      PART_PAGE_COUNT\n      LOG TABLE\n    \n    \n      PAGE_VALUE_COUNT\n      LOG TABLE\n    \n    \n      MAX_CACHE_PART_COUNT\n      LOG TABLE\n    \n    \n      MINMAX_CACHE_SIZE\n      LOG TABLE\n    \n  \n\n\nPART_PAGE_COUNT\nThis property represents the number of pages a partition has. The number of values ​​that a partition has is PART_PAGE_COUNT * PAGE_VALUE_COUNT.\n\nPAGE_VALUE_COUNT\nThis property represents the number of values ​​that a page has.\n\nMAX_CACHE_PART_COUNT (Default : 0)\nThis property sets the cache area for performance.\nWhen Machbase accesses a partition, it first looks for a structure that contains the meta information of that partition in memory. It determines how many partition information it contains in memory. Larger size will help performance, but memory usage will increase. The minimum value is 1 and the maximum value is 65535.\n\nMINMAX_CACHE_SIZE (Default : 10240)\nThis property specifies how much cache memory to use for the MINMAX of the corresponding column. The default is 100MB for _ARRIVAL_TIME, the 0th hidden column. However, other columns are specified as 10KB by default. This size can be changed after the creation of the table through the “ALTER TABLE MODIFY” statement.\n\nNOT NULL Constraint\nSpecifies NOT NULL if the column value does not allow NULL, and omit it if it is allowed (Default).\nYou can change the constraint with the ALTER TABLE MODIFY COLUMN command to drop or add this constraint defined after the creation of the table.\n\n# Column c1 is not null and c2 is created without not null constraint.\nCREATE TABLE t1(c1 INTEGER NOT NULL, c2 VARCHAR(200));\n\n\nPre-defined System Columns\nWhen you create a table using the Create Table statement, the system creates two additional predefined system columns. _ARRIVAL_TIME and _RID columns.\n\nThe _ARRIVAL_TIME column is inserted into the DATETIME column based on the system time at which data is inserted into the INSERT statement or AppendData, and the value can be used as the unique key of the generated record. The value of this column can be inserted by specifying the value in the machloader or INSERT statement if the order is guaranteed (in the order of past-present). When data is retrieved using the DURATION conditional expression, data is retrieved based on the value of this column.\nThe _RID column is created by the system as a unique value for a particular record. The data type of this column is a 64-bit integer. For this column, the user can not specify a value and can not create an index. It is automatically generated at the time of data INSERT. You can retrieve records by the value of the _RID column.\n\ncreate volatile table t1111 (i1 integer);\nCreated successfully.\nMach&gt; desc t1111;\n \n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\n_ARRIVAL_TIME                 datetime            8              \nI1                            integer             4              \n \nMach&gt;insert into t1111 values (1);\n1 row(s) inserted.\nMach&gt;select _rid from t1111;\n_rid                \n-----------------------\n0                   \n[1] row(s) inserted.\n \nMach&gt;select i1 from t1111 where _rid = 0;\ni1         \n--------------\n1          \n[1] row(s) selected.\n\n\nMINMAX Cache Concept\n\nIn general, in the Disk DBMS, when a specific value is searched using the index, the disk is accessed to access the disk area including the index, and the final disk page including the corresponding value is searched.\n\nOn the other hand, Machbase is a chronologically partitioned structure in order to maintain time series information, which means that a particular piece of index information is divided into chunks of files in chronological order. Therefore, when a Machbase index is used, an index file fragmented by such a partition is sequentially searched.\nIf the range of data to be searched is divided into 1000 partitions, it means that 1000 files should be opened and retrieved every time. Although it is designed as an efficient columnar database structure, the MINMAX_CACHE structure is a way to improve the performance because the I/O cost is proportional to the number of index partitions.\nMINMAX_CACHE is a structure that holds the index file information of the partition in memory, and is a contiguous memory space that keeps the minimum and maximum values ​​of the column in memory. By maintaining such structure, when a partition containing a specific value is searched, if the value is smaller than the minimum value of the index or is larger than the maximum value, the corresponding partition can be skipped altogether, thereby enabling high-performance data analysis.\n\n\n\nAs shown in the figure above, to find the value 85, only the partitions 1 and 5 included in MIN/MAX among the 5 partitions are actually searched, and the partitions 2, 3 and 4 are skipped altogether.\n\nMINMAX Cache Column\n\nYou can decide whether to use MINMAX Cache for a particular column when creating the table.\n\nIf the minmax_cache_size is set to a value other than 0, the MINMAX Cache will be active when the index is searched for that column and will not be active if  MINMAX_CACHE_SIZE = 0.\nPlease note the following when using this MINMAX Cache.\n\n\n  MINMAX Cache does not need to explicitly create an index on the column.\n  As default for all columns, MINMAX_CACHE_SIZE is set to 10KB and the Alter Table syntax can be used to reset the memory size to a reasonable size.\n  The hidden column _arrival_time is 100MB by default and automatically uses MINMAX Cache memory.\n  In the case of VARCHAR type, MINMAX Cache is not covered. Therefore, if you explicitly specify whether the VARCHAR type is cached, an error will occur.\n  When the corresponding table is created, the MINMAX_CACHE_SIZE maximum memory can be used as much as the property is set. As the number of partitions grows, the memory grows gradually and increases by the maximum memory above.\n  If there are no records in the table, MINMAX Cache memory is not allocated at all.\n\n\nBelow is an example of table creation using actual MINMAX.\n\n\n-- MINMAX_CACHE_SIZE = 0 for VARCHAR is allowed semantically.\nCREATE TABLE ctest (id INTEGER, name VARCHAR(100) PROPERTY(MINMAX_CACHE_SIZE = 0));\nCreated successfully.\nMach&gt;\n \n-- Cache applied to id column.\nCREATE TABLE ctest2 (id INTEGER PROPERTY(MINMAX_CACHE_SIZE = 10240), name VARCHAR(100) PROPERTY(MINMAX_CACHE_SIZE = 0));\nCreated successfully.\nMach&gt;\n \n-- Applied to id1, id2, and id3.\nCREATE TABLE ctest3 (id1 INTEGER PROPERTY(MINMAX_CACHE_SIZE = 10240), name VARCHAR(100) PROPERTY(MINMAX_CACHE_SIZE = 0), id2 LONG PROPERTY(MINMAX_CACHE_SIZE = 1024), id3 IPV4 PROPERTY(MINMAX_CACHE_SIZE = 1024), id4 SHORT);\nCreated successfully.\nMach&gt;\n \n-- MINMAX_CACHE_SIZE is specified in column units or set to 0.\nCREATE TABLE ctest4 (id1 INTEGER PROPERTY(MINMAX_CACHE_SIZE=10240), name VARCHAR(100) PROPERTY(MINMAX_CACHE_SIZE=0), id2 LONG PROPERTY(MINMAX_CACHE_SIZE=10240), id3 IPV4 PROPERTY(MINMAX_CACHE_SIZE=0), id4 SHORT);\nCreated successfully.\nMach&gt;\n\n\nPrimary Key\n\nThis is a constraint that can be assigned to a Volatile/Lookup table column. The Volatile / Lookup table does not always need to have a primary key, but you can not use the INSERT ON DUPLICATE KEY UPDATE statement without a primary key.\n\nWhen a primary key is assigned, a red-black tree index corresponding to the primary key is generated.\n\nAUTO_DEL Clauses\n\nSupported version 5.5 or later.\n\nThis feature limits the amount of data to maintain disk storage space. It is only supported for log table. It is set by using AUTO_DEL clause before specifying table property in CREATE TABLE statement. The AUTO_DEL clause can be set based on the storage time or the number of records.\n\nCREATE TABLE t1 (c1 INT) KEEP 30 DAY AFTER APPEND INTERVAL 5 SECOND;\n\n\nThe above example deletes data that is more than 30 days old if there is more input five seconds after the automatic deletion is performed. If the interval specified is too long, auto delete is performed for a long time, which may affect the input. If it is too short, it may affect overall system performance.\n\nThe following example shows how to specify the automatic deletion function as the number of data to save.\n\nCREATE TABLE t1 (c1 INT) KEEP 3 RECORD AFTER APPEND INTERVAL 5 RECORD;\n\n\nIt checks the number of records in the table for every 5 input data and automatically deletes it and keep only 3 records if there are more than three.\n\nSEQUENCE for Lookup Table\n\nSequence was added to generate a unique record of the Lookup table and determine the order in which the data is entered.\n\nThis feature was added to solve problems such as difficulty in distinguishing the order of records if the datetime values overlap in the lookup table and application errors due to data duplication.\n\nMachbase Edition support Sequence\n\nAll type of machbase editions are supported. (Edge / Fog / Cluster)\n\nTable type support Sequence\n\nLookup Table\n\nConfiguring Sequence when Creating Lookup Tables\n\nWhen creating a lookup table with a CreateTable SQL statement, simply specify that you want to set the Sequence by adding a PROPERTY clause to the column to be used as the Sequence.\n\nThe columns to be set in Sequence only support LONG datatype (64bit, unsigned) and no other.\n\nIn addition, the start value of Sequence can be set, but if it is set to 1, Sequence starts from 1. (No support for 0 or negative numbers)\n\nCREATE LOOKUP TABLE table_name (v1 LONG PROPERTY(SEQUENCE=1), v2 VARCHAR(10));\n\n\nUse of sequence column\n\nThe Sequence column of the Lookup table is basically the same as a regular Long column and when used in this way, the Sequence value does not automatically increase.\n\nIt is allowed to enter values directly into the Sequence column, and even duplicate values can be entered.\n\nInstead, if you want to use the Sequence function, you should use a newly added Sequence-only function called nextval to increase the Sequence value.\n\nInternally, it stores the largest value of a column set to Sequence, so when you enter it later using nextval Function, the largest value of the Sequence column value +1 is stored.\n\nExample of Sequence column\n-- Insert the following Sequence value using nextval Function in the Sequence column.\nINSERT INTO table_name (v1, v2) values (nextval(v1), 'aaaa');\n   \n-- Insert a value directly into the Sequence column\nINSERT INTO table_name (v1, v2) values (100, 'aaaa');\n   \n-- Insert a the computational value in the Sequence column.\nINSERT INTO table_name (v1, v2) values (1 + 99, 'aaaa');\n   \n-- Success Select of Lookup Tables with Sequence Columns\nSELECT v1, v2 FROM table_name;\n  \n-- Invalid Select for Sequence column (nextval column can only be used in insert query)\nSELECT nextval(v1), v2 FROM table_name;\n\n\nCREATE TAG TABLE\n\ncreate_tag_table_stmt:\n\n\n\ntag_column_list:\n\n\n\ncreate_tag_table_stmt ::= 'CREATE' 'TAGDATA' 'TABLE' table_name '(' tag_column_list ( 'metadata' '(' column_list ')' )? ')'\ntag_column_list ::= column_name column_type column_property_list? 'PRIMARY KEY'? 'BASETIME'? 'SUMMARIZED'? 'NOT NULL'? ( ',' column_name column_typecolumn_property_list? 'PRIMARY KEY'? 'BASETIME'? 'SUMMARIZED'? 'NOT NULL'? )*\n\n\nThe primary key, basetime, and summarized must be included in the tag table creation.\n\n-- Example\nCREATE TAGDATA TABLE tag (name varchar(20) primary key, time datetime basetime, value double summarized);\nCREATE TAGDATA TABLE tag (name varchar(20) primary key, int_column int, time datetime basetime, value double summarized, value2 float);\nCREATE TAGDATA TABLE tag (name varchar(20) primary key, time datetime basetime, value double summarized, value2 float) METADATA (i1 int);\n\n\nCREATE INDEX\n\ncreate_index_stmt:\n\n\n\nindex_type:\n\n\n\ntable_space:\n\n\n\nindex_property_list:\n\n\n\ncreate_index_stmt ::= 'CREATE' 'INDEX' index_name 'ON' table_name '(' column_name ')' index_type? table_space? index_property_list?\nindex_type ::= 'INDEX_TYPE' ( 'KEYWORD' | 'BITMAP' | 'REDBLACK' )\ntable_space ::= 'TABLESPACE' table_space_name\nindex_property_list ::= ( 'MAX_LEVEL' | 'PAGE_SIZE' | 'BLOOM_FILTER' | 'BITMAP_ENCODE' | 'PART_VALUE_COUNT' ) '=' value\n\n\nIndex Type\n\nSpecifies the Index Type to be created. If it is not Keyword Index, Index Type is created as Default Index Type according to Table Type if Index Type is not specified.\n\n\n  \n    \n      Table Type\n      Default Index Type\n    \n  \n  \n    \n      Volatile Table\n      REDBLACK\n    \n    \n      Lookup Table\n      REDBLACK\n    \n    \n      Log Table\n      LSM\n    \n  \n\n\nKEYWORD Index\n\nThis can be created only for varchar and text column of log table. It can be created for only one column.\n\nLSM Index\n\nLSM (Log Structure Merge) Index is an index optimized for storing and searching Big Data. The partitions of the LSM indexes are maintained for each level, and the lower level partitions are merged to move to the upper level. Lower partitions used to create a higher level partition are deleted.\n\nThis Index Level Partition Building is performed by Background Thread. The upper level partitions are merged with the lower level partitions and are created as one partition, so there are the following advantages when searching through the index.\n\n\n  \n    If the key is duplicated, the disk space for key storage is saved because it is stored only once.\n  \n  \n    Searching for multiple partitions reduces the cost of opening and closing the file when searching for one index partition, and the number of index pages accessed is also reduced.\n  \n\n\nLSM Index Property\n\n\n  \n    \n      Item\n      Description\n    \n  \n  \n    \n      MAX_LEVEL(DEFAULT = 3, MIN = 0, MAX = 3 )\n      The maximum level of the LSM Index, and the current value of 3 is the maximum value. And the maximum number of records of one partition can not exceed 200 million. The partition size of each level is the number of values ​​of the previous partition * 10. For example, if MAX_LEVEL = 3 and PART_VALUE_COUNT is 100,000, then Level 0 = 100,000, Level 1 = 1,000,0000, Level 2 = 10,000,000, and Level 3 = 100,000,000. If the Partition Size of the last level exceeds 200 million, index creation will fail.\n    \n    \n      PAGE_SIZE(DEFAULT = 512 * 1024, MIN = 32 * 1024,MAX = 1 * 1024 * 1024)\n      Specifies the size of the page in which the index key value and bitmap value are stored. Default is 512K.\n    \n    \n      BLOOM_FILTER(DEFAULT = 1, DISABLE = 0, ENABLE(DEFAULT) = non_zero_integer)\n      Sets whether to set Bloom filter on index. Setting the Bloom filter allows you to quickly search for values ​​that do not exist in the index, but it will increase the time it takes to create the index.If you use only Range conditions, Bloom filters are not available and need not be created.If you do not want to create a Bloom filter, you should set BLOOM_FILTER = 0.\n    \n    \n      BITMAP_ENCODE(DEFAULT = EQUAL, RANGE)\n      Sets the bitmap type of the index.If BITMAP_ENCODE = EQUAL (default), generates a bitmap for the same value as the key value. If BITMAP = RANGE, generates a bitmap according to the range of the key value.It is better to set as BITMAP_ENCODE = EQUAL when using = as the query condition, and BITMAP_ENCODE = RANGE when using the specific range value as the query condition.In the case of BITMAP = RANGE, the cost of creation increases slightly compared to EQUAL.\n    \n  \n\n\nBITMAP Index\n\nThis is an index for data analysis and can be created only in the log table. It can be created on all columns except varchar, text, and binary, and can only be created on a single column.\n\nRED-BLACK Index\n\nThis is a memory index for real-time data retrieval. It can be created only in the Volatile/Lookup table. It can be created in all columns of this table and can only be created for a single column.\n\nIndex Property\n\nThe properties that can be applied in the LSM Index are as follows.\n\nPART_VALUE_COUNT\nIndicates the number of rows stored in the Partition of Index.\n\n-- Example\n-- Index applied to c1 column.\nCREATE INDEX index1 on table1 ( c1 )\n-- Keyword index applied to var_column of varchar type, and page_size unit is 100000.\nCREATE INDEX index2 on table1 (var_column) INDEX_TYPE KEYWORD PAGE_SIZE=100000;\n\n\nDROP TABLESPACE\n\ndrop_tablespace_stmt:\n\n\n\ndrop_table_stmt ::= 'DROP TABLESPACE' tablespace_name\n\n\nDeletes the specified tablespace. However, if the object created in Tablespace exists, deletion fails.\n\n-- Example\nDROP TABLESPACE TablespaceName;\n\n\nDROP TABLE\n\ndrop_table_stmt:\n\n\n\ndrop_table_stmt ::= 'DROP TABLE' table_name\n\n\nDeletes the specified table. However, if there is another session in which the table is being searched, it fails with an error.\n\n-- Example\nDROP TABLE TableName;\n\n\nDROP INDEX\n\ndrop_index_stmt:\n\n\n\ndrop_index_stmt ::= 'DROP INDEX' index_name\n\n\nDeletes the specified index. However, if there is another session in which the table is being searched, it fails with an error.\n\n-- Example\nDROP INDEX IndexName;\n\n\nALTER TABLESPACE\n\nThe ALTER TABLESPACE statement is used to change the information associated with the specified tablespace.\n\nALTER TABLESPACE MODIFY DATADISK\n\nThis syntax is used to change the properties of DATADISK in Tablespace.\n\nalter_tablespace_stmt:\n\n\n\nalter_tablespace_stmt ::= 'ALTER TABLESPACE' table_name 'MODIFY DATADISK' disk_name 'SET' 'PARALLEL_IO' '=' value\n\n\n-- Example\nALTER TABLESPACE tbs1 MODIFY DATADISK disk1 SET PARALLEL_IO = 10;\n\n\nALTER TABLE\n\nThe ALTER TABLE statement is used to change the schema information of the specified table, and only the Log Table is available.\n\nALTER TABLE SET\n\nThis syntax changes the properties of a table. Currently there are no dynamically changeable properties.\n\nALTER TABLE ADD COLUMN\n\nalter_table_add_stmt:\n\n\n\nalter_table_add_stmt ::= 'ALTER TABLE' table_name 'ADD COLUMN' '(' column_name column_type ( 'DEFAULT' value )? ')'\n\n\nThis syntax is the ability to add a specific column to the table in real time. You can add the name and type of the column, and set the default data values ​​through the DEFAULT clause.\n\n-- Example-1\nalter table atest2 add column (id4 float);\n \n-- Example-2\nalter table atest2 add column (id6 double  default 5);\nalter table atest2 add column (id7 ipv4  default '192.168.0.1');\nalter table atest2 add column (id8 varchar(4) default 'hello');\n\n\nALTER TABLE DROP COLUMN\n\nalter_table_drop_stmt:\n\n\n\nalter_table_drop_stmt ::= 'ALTER TABLE' table_name 'DROP COLUMN' '(' column_name ')'\n\n\nThis syntax is to delete a specific column in the table in real time.\n\n-- Example\nalter table atest2 drop column (id4);\nalter table atest2 drop column (id8);\n\n\nALTER TABLE RENAME COLUMN\n\nalter_table_column_rename_stmt:\n\n\n\nalter_table_column_rename_stmt ::= 'ALTER TABLE' table_name 'RENAME COLUMN' old_column_name 'TO' new_column_name\n\n\nThis syntax is a function that changes a specific column name in a table.\n\n-- Example\nalter table atest2 rename column id7 to id7_rename;\n\n\nALTER TABLE MODIFY COLUMN\n\nalter_table_modify_stmt:\n\n\n\nalter_table_modify_stmt ::= 'ALTER TABLE' table_name 'MODIFY COLUMN' ( '(' column_name 'VARCHAR' '(' new_size ')' ')' | column_name ( 'NOT'? 'NULL' | 'SET' 'MINMAX_CACHE_SIZE' '=' value ) )\n\n\nThis syntax changes the properties of a particular column of a table. Currently it is possible to modify MINMAX CACHE attributes and NOT NULL constraints for column lengths and other types of VARCHAR types.\n\nVARCHAR SIZE\n\nThis syntax supports changing the column length of VARCHAR type only. This operation can not be reduced in length to preserve existing data, and should always be increased.\n\nALTER TABLE table_name MODIFY COLUMN (column_name VARCHAR(new_size));\n\n\n-- Example: Assume TABLE is created like this.\n-- create table atest5 (id integer, name varchar(5), id3 double, id4 float);\n \n-- Error occurred: Can not change to another type,\nalter table atest5 modify column (id varchar(10));\n \n-- Error occurred: VARCHAR length can not be made smaller.\nalter table atest5 modify column (name varchar(3));\n \n-- Error occurred: Maximum size of VARCHAR can not exceed 32767.\nalter table atest5 modify column (name varchar(32768));\n \n-- Success\nalter table atest5 modify column (name varchar(128));\n\n\nMINMAX_CACHE_SIZE\n\nThis syntax changes MINMAX_CACHE_SIZE for a particular column.\n\nALTER TABLE table_name MODIFY COLUMN column_name SET MINMAX_CACHE_SIZE=value;\n\n\n-- Example: Assume TABLE is created like this.\ncreate table atest9 (id integer, name varchar(100));\n \n-- Error: Does not apply to VARCHAR.\nalter table atest9 modify column name set minmax_cache_size=0;\n[ERR-02139 : MINMAX CACHE is not allowed for VARCHAR column(NAME).]\n \n-- Change success\nalter table atest9 modify column id set minmax_cache_size=10240;\n\n\nNOT NULL\n\nAdds a NOT NULL constraint to the column. If you add a NOT NULL constraint, the DDL operation fails for columns with NULL values.\nIf you want to allow NULL values ​​in a column, use the MODIFY COLUMN NULL command in the next section.\n\nALTER TABLE table_name MODIFY COLUMN column_name NOT NULL;\n\n\n-- Add NOT NULL constraint to t1.c1.\nalter table t1 modify column c1 not null;\n\n\nNULL\n\nReleases the NOT NULL constraint. Performance improvement due to min_max cache of LSM index can not be obtained. NULL values ​​can be input.\n\nALTER TABLE table_name MODIFY COLUMN column_name NULL;\n\n\n-- Release NOT NULL constraint at t1.c1.\nalter table t1 modify column c1 null;\n\n\nALTER TABLE FLUSH\n\nalter_table_flush_stmt:\n\n\n\nalter_table_flush_stmt ::= 'ALTER TABLE' table_name 'FLUSH'\n\n\nWaits until the input data for the specified table is fully reflected in the data file.\n\nALTER TABLE FLUSH INDEX\n\nalter_table_flush_index_stmt:\n\n\n\nalter_table_flush_index_stmt ::= 'ALTER TABLE' table_name 'FLUSH' 'INDEX' ( index_name | 'ALL' )\n\n\nWaits until the index data of the specified table is completely reflected in the index file.\n\n-- Wait until data for all indexes of bulktable is reflected in index files.\nalter table bulktable flush index all;\n \n-- Wait until data of c1_idx index of bulktable is reflected in index file.\nalter table bulktable flush index c1_idx;\n\n\nALTER TABLE RECLAIM STORAGE\n\nalter_table_reclaim_storage_stmt:\n\n\n\nalter_table_reclaim_storage_stmt ::= 'ALTER TABLE' table_name 'RECLAIM STORAGE'\n\n\nDelete unused data from the Tag table to secure available space.\nIf the system property DISK_TAG_AUTO_RECLAIM is 1 (default), it is automatically performed without special execution.\nWhen this value is set to 0, the storage space may be secured by performing this query at a desired time. Only available for Tag table.\n\n-- Reclaim the storage space of the tag table.\nalter table tag reclaim storage;\n\n\nALTER TABLE RENAME TO\n\nalter_table_rename_stmt:\n\n\n\nalter_table_rename_stmt ::= 'ALTER TABLE' table_name 'RENAME TO' new_name\n\n\nChanges the name of the table.\nMetatables can not be renamed, and you can not use the $ character in the name to be changed. Table renaming is only possible for log tables.\n\n-- Change name of user table to employee.\nALTER TABLE user RENAME TO employee\n\n\nALTER TABLE AUTO DELETE\n\nalter_table_auto_del_stmt:\n\nalter_table_auto_del_stmt ::=  'ALTER TABLE' table_name ('KEEP' 'DISABLE' || 'KEEP' value ('DAY' | 'HOUR' | 'MINUTE' | 'SECOND' | 'RECORD') 'AFTER APPEND' 'INTERVAL' value ('DAY' | 'HOUR' | 'MINUTE' | 'SECOND' | 'RECORD') )\n\n\n\n\nSet the automatic deletion period of the table or determine whether to use it.\nAuto deleting is only possible for log tables.\n\n-- The automatic deletion of the logtbl table keeps only 30 days of data and changes it to check every 5 seconds after the append.\nALTER TABLE logtbl keep 30 day after append interval 5 second;\n\nALTER TABLE ADD RETENTION\n\nalter_table_add_retention_stmt:\n\nalter_table_add_retention_stmt ::=  'ALTER TABLE' table_name 'ADD RETENTION' policy_name\n\n\n\n\nALTER TABLE tag ADD RETENTION policy_1d_1h;\n\n\nALTER TABLE DROP RETENTION\n\nalter_table_drop_retention_stmt:\n\nalter_table_drop_retention_stmt ::=  'ALTER TABLE' table_name 'DROP RETENTION'\n\n\n\n\nALTER TABLE tag DROP RETENTION;\n\n\nTRUNCATE TABLE\n\ntruncate_table_stmt:\n\n\n\ntruncate_table_stmt ::= 'TRUNCATE TABLE' table_name\n\n\n-- Delete all data in ctest table.\nMach&gt; truncate table ctest;\nTruncated successfully.\n\n\nDeletes all data in the specified table. However, if there is another session in which the table is being searched, it fails with an error.\n\nCREATE ROLLUP\n\ncreate_rollup_stmt:\n\n\n\ncreate_rollup_stmt ::= 'CREATE ROLLUP' rollup_name 'ON' src_table_name '('src_table_column')' 'INTERVAL' number ('SEC' | 'MIN' | 'HOUR')\n\n\n-- Creates a rollup targeting the value column of the tag table.\nMach&gt; CREATE ROLLUP _rollup_tag_value_sec ON tag(value) INTERVAL 1 SEC;\nExecuted successfully\n\n\nDROP ROLLUP\n\ndrop_rollup_stmt:\n\n\n\ndrop_rollup_stmt ::= 'DROP ROLLUP' rollup_name\n\n\n-- drop rollup.\nMach&gt; DROP ROLLUP _rollup_tag_value_sec;\nExecuted successfully\n\n\nCREATE RETENTION\n\ncreate_retention_stmt:\n\n\n\ncreate_retention_stmt ::= 'CREATE RETENTION' policy_name 'DURATION' duration ( 'MONTH' | 'DAY' ) 'INTERVAL' interval ( 'DAY' | 'HOUR' )\n\n\nMach&gt; CREATE RETENTION policy_1d_1h DURATION 1 DAY INTERVAL 1 HOUR;\nExecuted successfully\n\n\nDROP RETENTION\n\ndrop_retention_stmt:\n\n\n\ndrop_retention_stmt ::= 'DROP RETENTION' policy_name\n\n\nMach&gt; DROP RETENTION policy_1d_1h;\nExecuted successfully"
					}
					
				
		
				
					,
					
					"feature-tables-volatile-delete-html": {
						"id": "feature-tables-volatile-delete-html",
						"title": "Deleting Volatile Data",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/volatile/delete.html",
						"content": "Delete Data\n\nVolatile tables can delete data using the primary key value condition in the condition clause (WHERE clause).\n\n\n  The primary key column must be specified in the volatile table.\n  Only the (Primary key column) = (value) condition is allowed, and can not be used with other conditions.\n  You can not use a column other than the primary key column.\n\n\nMach&gt; create volatile table vtable (id integer primary key, name varchar(20));\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(3, 'north device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(4, 'south device');\n1 row(s) inserted.\nMach&gt; select * from vtable;\nID          NAME                 \n-------------------------------------\n1           west device          \n2           east device          \n3           north device         \n4           south device         \n[4] row(s) inserted.\nMach&gt; delete from vtable where id = 2;\n[1] row(s) deleted.\nMach&gt; select * from vtable;\nID          NAME                 \n-------------------------------------\n1           west device          \n3           north device         \n4           south device         \n[3] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-tables-lookup-delete-html": {
						"id": "feature-tables-lookup-delete-html",
						"title": "Lookup Data Deletion",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/lookup/delete.html",
						"content": "It is the same as the volatile table."
					}
					
				
		
				
					,
					
					"feature-tables-log-delete-html": {
						"id": "feature-tables-log-delete-html",
						"title": "Deletion of Log Data",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/delete.html",
						"content": "Index\n\n\n  Syntax\n  Example\n\n\nThe DELETE statement in Machbase can be performed on the log table.\n\nIn addition, it is not possible to delete data in an arbitrary position in the middle, and it is possible to erase consecutively from the arbitrary position to the last (oldest log) record. This is a policy that takes advantage of the characteristics of log data. It is a DB format representation of the act of deleting a file in order to secure space when it is entered once.\n\nBelow is the type of expression you can use.\n\nSyntax\n\nDELETE FROM table_name;\nDELETE FROM table_name OLDEST number ROWS;\nDELETE FROM table_name EXCEPT number ROWS;\nDELETE FROM table_name EXCEPT number [YEAR | MONTH | WEEK | DAY | HOUR | MINUTE | SECOND];\nDELETE FROM table_name BEFORE datetime_expr;\n\n\nExample\n\n-- Delete all data.\nmach&gt;DELETE FROM devices;\n10 row(s) deleted.\n \n-- Delete oldest 5.\nmach&gt;DELETE FROM devices OLDEST 5 ROWS;\n10 row(s) deleted.\n \n-- Delete all except last 5.\nmach&gt;DELETE FROM devices EXCEPT 5 ROWS;\n15 row(s) deleted.\n \n-- Delete all data from before June 1, 2018.\nmach&gt;DELETE FROM devices BEFORE TO_DATE('2018-06-01', 'YYYY-MM-DD');\n50 row(s) deleted."
					}
					
				
		
				
					,
					
					"feature-tables-tag-manipulate-delete-html": {
						"id": "feature-tables-tag-manipulate-delete-html",
						"title": "Delete tag data",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/manipulate/delete.html",
						"content": "Index\n\n\n  Tag data deletion constraints\n    \n      Delete all tags before a specific time\n      Delete all data\n    \n  \n  Delete ROLLUP Data\n    \n      Example of deleting rollup data\n    \n  \n\n\nTag data deletion constraints\n\nMachbase only supports deletion of data before a specific time.\n\nUnsupported tag data deletion condition\n\n\n  Delete specific tag ID data\n  Delete data for a specific time range\n  Delete specific time range data for a specific tag\n\n\nSupported tag data deletion condition\n\n\n  Delete all tags before a specific time\n  Delete all data\n\n\nDelete all tags before a specific time\n\nIf you specify the time of the BEFORE statement, all tags before that time are deleted.\n\nDELETE FROM TAG BEFORE TO_DATE('Time-string');\n\n\n# Original Data\nMach&gt; select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\nTAG_0001 2018-01-01 01:00:00 000:000:000 1\nTAG_0001 2018-01-02 02:00:00 000:000:000 2\nTAG_0001 2018-01-03 03:00:00 000:000:000 3\nTAG_0001 2018-01-04 04:00:00 000:000:000 4\nTAG_0001 2018-01-05 05:00:00 000:000:000 5\nTAG_0001 2018-01-06 06:00:00 000:000:000 6\nTAG_0001 2018-01-07 07:00:00 000:000:000 7\nTAG_0001 2018-01-08 08:00:00 000:000:000 8\nTAG_0001 2018-01-09 09:00:00 000:000:000 9\nTAG_0001 2018-01-10 10:00:00 000:000:000 10\nTAG_0002 2018-02-01 01:00:00 000:000:000 11\nTAG_0002 2018-02-02 02:00:00 000:000:000 12\nTAG_0002 2018-02-03 03:00:00 000:000:000 13\nTAG_0002 2018-02-04 04:00:00 000:000:000 14\nTAG_0002 2018-02-05 05:00:00 000:000:000 15\nTAG_0002 2018-02-06 06:00:00 000:000:000 16\nTAG_0002 2018-02-07 07:00:00 000:000:000 17\nTAG_0002 2018-02-08 08:00:00 000:000:000 18\nTAG_0002 2018-02-09 09:00:00 000:000:000 19\nTAG_0002 2018-02-10 10:00:00 000:000:000 20\n[20] row(s) selected.\n \nMach&gt; delete from tag before to_date('2018-02-01');\n10 row(s) deleted.\n \nMach&gt; select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\nTAG_0002 2018-02-01 01:00:00 000:000:000 11\nTAG_0002 2018-02-02 02:00:00 000:000:000 12\nTAG_0002 2018-02-03 03:00:00 000:000:000 13\nTAG_0002 2018-02-04 04:00:00 000:000:000 14\nTAG_0002 2018-02-05 05:00:00 000:000:000 15\nTAG_0002 2018-02-06 06:00:00 000:000:000 16\nTAG_0002 2018-02-07 07:00:00 000:000:000 17\nTAG_0002 2018-02-08 08:00:00 000:000:000 18\nTAG_0002 2018-02-09 09:00:00 000:000:000 19\nTAG_0002 2018-02-10 10:00:00 000:000:000 20\n[10] row(s) selected.\n\n\nDelete all data\n\nIf there are no conditions, all data is deleted.\n\n# Original Data\nMach&gt; select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\nTAG_0001 2018-01-01 01:00:00 000:000:000 1\nTAG_0001 2018-01-02 02:00:00 000:000:000 2\nTAG_0001 2018-01-03 03:00:00 000:000:000 3\nTAG_0001 2018-01-04 04:00:00 000:000:000 4\nTAG_0001 2018-01-05 05:00:00 000:000:000 5\nTAG_0001 2018-01-06 06:00:00 000:000:000 6\nTAG_0001 2018-01-07 07:00:00 000:000:000 7\nTAG_0001 2018-01-08 08:00:00 000:000:000 8\nTAG_0001 2018-01-09 09:00:00 000:000:000 9\nTAG_0001 2018-01-10 10:00:00 000:000:000 10\nTAG_0002 2018-02-01 01:00:00 000:000:000 11\nTAG_0002 2018-02-02 02:00:00 000:000:000 12\nTAG_0002 2018-02-03 03:00:00 000:000:000 13\nTAG_0002 2018-02-04 04:00:00 000:000:000 14\nTAG_0002 2018-02-05 05:00:00 000:000:000 15\nTAG_0002 2018-02-06 06:00:00 000:000:000 16\nTAG_0002 2018-02-07 07:00:00 000:000:000 17\nTAG_0002 2018-02-08 08:00:00 000:000:000 18\nTAG_0002 2018-02-09 09:00:00 000:000:000 19\nTAG_0002 2018-02-10 10:00:00 000:000:000 20\n[20] row(s) selected.\n \nMach&gt; delete from tag;\n20 row(s) deleted.\n \nMach&gt; select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\n[0] row(s) selected.\n\n\nDelete ROLLUP Data\n\nExample of deleting rollup data\n\nDELETE FROM TAG ROLLUP BEFORE TO_DATE('Time-string');\n\n\nif you specify the time of the BEFORE statement, all rollup data before that time are deleted.\n\nif you don’t specify the  time of the BEFORE statement, all rollup data is deleted."
					}
					
				
		
				
					,
					
					"sql-ref-dml-html": {
						"id": "sql-ref-dml-html",
						"title": "DML",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/dml.html",
						"content": "Index\n\n\n  INSERT\n    \n      INSERT ON DUPLICATE KEY UPDATE\n    \n  \n  INSERT SELECT\n  UPDATE\n    \n      UPDATE METADATA\n    \n  \n  DELETE\n  DELETE WHERE\n  LOAD DATA INFILE\n\n\nINSERT\n\ninsert_stmt:\n\n\n\ninsert_column_list:\n\n\n\nvalue_list:\n\n\n\nset_list:\n\n\n\ninsert_stmt ::= 'INSERT INTO' table_name ( '(' insert_column_list ')' )? 'METADATA'? 'VALUES' '(' value_list ')' ( 'ON DUPLICATE KEY UPDATE' ( 'SET' set_list )? )?\ninsert_column_list ::= column_name ( ',' column_name )*\nvalue_list ::= value ( ',' value )*\nset_list ::= column_name '=' value ( ',' column_name '=' value )*\n\n\ncreate table test (number int,name varchar(20));\nCreated successfully.\ninsert into test values (1,\"test\");\n1 row(s) inserted.\ninsert into test(name,number) values (\"test\",2);\n1 row(s) inserted.\n\n\nThis is the syntax for entering values ​​into a specific table. One unusual thing is that columns not specified in Column_List are all filled with NULL values. This is a policy considering the characteristics of log files adopted for convenience of input and efficiency of storage space.\nMETADATA is only available for tag tables.\n\nINSERT ON DUPLICATE KEY UPDATE\n\nMachbase supports syntax similar to the commonly known UPSERT function.\n\nA special syntax that can be used when entering a value into a Lookup/Volatile table with a primary key specified. If the table already contains data with a duplicate primary key value, the value of the existing data is changed. \nOf course, when there is no duplicated key value data, it is inserted as new data.\n\nTo use this syntax, the primary key must be specified in the volatile table.\n\nIf the column value of the inserted data is different from the column value of the updated data, or if it is desired to update a column value other than the column value of the inserted data, the SET clause can be further input.\n\n\n  The SET clause consists of ‘column = value’, each separated by a comma.\n  You must not change the default key value in the SET clause.\n\n\nINSERT SELECT\n\ninsert_select_stmt:\n\n\n\ninsert_select_stmt ::= 'INSERT INTO' table_name ( '(' insert_column_list ')' )? select_stmt\n\n\nThis statement inserts the result of the SELECT statement on a specific table. In basic, it is similar to other DBMSs, but with the following differences.\n\n  The _ARRIVAL_TIME column value is entered as the time value at the time the INSERT SELECT statement is executed unless specified in the select and INSERT column lists.\n  If the input value to be inserted for a VARCHAR type column is greater than the maximum length of the column, the maximum length of the corresponding column is entered without error.\n  If type conversion is possible (numeric -&gt; numeric), it is inserted according to the input column value.\n  ROLLBACK does not occur if an error occurs during execution.\n  If you insert a value in the _ARRIVAL_TIME column, the new value will not be entered if it has a time before the existing value.\n\n\ncreate table t1 (i1 integer, i2 varchar(60), i3 varchar(5));\nCreated successfully.\n \ninsert into t1 values (1, 'a', 'ddd' );\n1 row(s) inserted.\ninsert into t1 values (2, 'kkkkkkkkkkkkkkkkkkkkk', 'c');\n1 row(s) inserted.\n \ninsert into t1 select * from t1;\n2 row(s) inserted.\ncreate table t2 (i1 integer, i2 varchar(60), i3 varchar(5));\n \ninsert into t2 (_arrival_time, i1, i2, i3) select _arrival_time, * from t1;\n4 row(s) inserted.\n\n\nUPDATE\n\n\n  This function is available from 5.5.\n\n\nupdate_stmt:\n\n\n\nupdate_expr_list:\n\n\n\nupdate_expr:\n\n\n\nupdate_stmt ::= 'UPDATE' table_name ( 'METADATA' )? 'SET' update_expr_list 'WHERE' primary_key_column '=' value\nupdate_expr_list ::= update_expr ( ',' update_expr)*\nupdate_expr ::= column '=' value\n\n\nINSERT ON DUPLICATE UPDATE syntax is also provided, rather than UPSERT via a KEY UPDATE.\nYou can also use the Primary Key to enter values ​​into the specified Lookup/Volatile table. In the WHERE clause, you must create a matching predicate for the primary key.\n\nUPDATE METADATA\n\nOnly for the TAGDATA table, when you want to update the metadata.\n\nUPDATE TAG METADATA SET ...\n\n\n\n  The metadata of the TAGDATA table can not be entered/modified through INSERT ON DUPLICATE KEY UPDATE.\n\n\nDELETE\n\ndelete_stmt:\n\n\n\ntime_unit:\n\n\n\ndelete_stmt ::= 'DELETE FROM' table_name ( 'OLDEST' number 'ROWS' | 'EXCEPT' number ( 'ROWS' | time_unit ) | 'BEFORE' datetime_expression )? 'NO WAIT'?\ntime_unit ::= 'DURATION' number time_unit ( ( 'BEFORE' | 'AFTER' ) number time_unit )?\n\n\nThe DELETE statement in Machbase can be performed on the log table. In addition, it is not possible to delete data in an arbitrary position in the middle, and it is possible to erase consecutively from the arbitrary position to the last (oldest log) record.\n\nThis is a policy that takes advantage of the characteristics of log data. It is a DB format representation of the act of deleting a file in order to secure space when it is entered once.\n\nThe syntax of DURATION, OLDEST, and EXCEPT cannot be used for TAG and Rollup tables.\n\n-- Delete all.\nDELETE FROM devices;\n \n-- Delete oldest last N rows.\nDELETE FROM devices OLDEST N ROWS;\n \n-- Delete all except recent N rows.\nDELETE FROM devices EXCEPT N ROWS;\n \n-- Delete all except N matches from now on.\nDELETE FROM devices EXCEPT N DAY;\n \n-- Delete all data from before June 1, 2014.\nDELETE FROM devices BEFORE TO_DATE('2014-06-01', 'YYYY-MM-DD');\n \n-- Delete tag data from before June 1, 2014.\nDELETE FROM tag BEFORE TO_DATE('2014-06-01', 'YYYY-MM-DD');\n \n-- Delete tag rollup data from before June 1, 2014.\nDELETE FROM tag ROLLUP BEFORE TO_DATE('2014-06-01', 'YYYY-MM-DD');\n\n\nDELETE WHERE\n\ndelete_where_stmt:\n\n\n\ndelete_where_stmt ::= 'DELETE FROM' table_name 'WHERE' column_name '=' value\n\n\ncreate volatile table t1 (i1 int primary key, i2 int);\nCreated successfully.\ninsert into t1 values (2,2);\n1 row(s) inserted.\ndelete from t1 where i1 = 2;\n1 row(s) deleted.\n\n\n\n  You can only delete records that match the conditions created in the WHERE clause, which can only be performed on volatile tables.\n  The primary key can only be performed on the specified volatile table.\n  The WHERE clause allows only conditions of (primary key column) = (value), and can not be created with other conditions.\n  You can not use a column other than the primary key column in the condition.\n\n\ndelete_from_tag_where_stmt:\n\n\n\ndelete_from_tag_where_stmt ::= 'DELETE FROM' table_name 'WHERE' tag_name '=' value ( and tag_time '&lt;' datetime_expression  )?\n\n\nThe Tag table additionally supports two types of deletion queries as follows.\n\n  Delete by tag name\n  Delete by tag name and tag time\n\n\n-- Delete by tag name\nDELETE FROM tag where tag_name = 'my_tag_2021'\n \n-- Delete by tag name and tag time\nDELETE FROM tag where tag_name = 'my_tag_2021' and tag_time &lt; TO_DATE('2021-07-01', 'YYYY-MM-DD');\n\n\n\n  The time it takes for the deleted row to be physically deleted from the storage space after the deletion query is executed may vary depending on the situation of the DBMS.\n\n\nLOAD DATA INFILE\n\nload_data_infile_stmt:\n\n\n\nload_data_infile_stmt: 'LOAD DATA INFILE' file_name 'INTO TABLE' table_name ( 'TABLESPACE' tbs_name )? ( 'AUTO' ( 'BULKLOAD' | 'HEADUSE' | 'HEADUSE_ESCAPE' ) )? ( ( 'FIELDS' | 'COLUMNS' ) ( 'TERMINATED BY' char )? ( 'ENCLOSED BY' char )? )? ( 'TRIM' ( 'ON' | 'OFF' ) )? ( 'IGNORE' number ( 'LINES' | 'ROWS' ) )? ( 'MAX_LINE_LENGTH' number )? ( 'ENCODED BY' coding_name )? ( 'ON ERROR' ( 'STOP' | 'IGNORE' ) )?\n\n\nCSV format data files are read directly from the server, and tables and columns are created directly from the server according to the options and input them.\nEach option is explained as follows\n\n\n  \n    \n      Options\n      Description\n    \n  \n  \n    \n      AUTO mode_stringmode_string =(BULKLOAD | HEADUSE | HEADUSE_ESCAPE)\n      Creates the corresponding table and automatically generates the column type (varchar type for automatic creation) and the column name.BULKLOAD: Enters one row of data as one column. It is used for data that can not be divided into columns.HEADUSE: Uses the column name as described in the first line of the data file as the column name of the table, and creates as many columns as there are in the line.HEADUSE_ESCAPE: Similar to the HEADUSE option, but appends a ‘’ character to the front and back of the column name to avoid errors that can occur if the column name is the same as the reserved word in the DB. If a special character exists in the column name, changes it to ‘’.\n    \n    \n      (FIELDS|COLUMNS) TERMINATED BY ‘term_char’ESCAPED BY ‘escape_char’\n      Specifies the delimiter (term_char) and escape character (escape_char) for parsing the data line. For common CSV files, the delimiter is , and the escape character is ‘.\n    \n    \n      ENCODED BY coding_namecoding_name ={ UTF8(default) | MS949 | KSC5601 | EUCJP | SHIFTJIS | BIG5 | GB231280 }\n      Specifies the encoding options for the data file. The default value is UTF-8.\n    \n    \n      TRIM (ON | OFF)\n      Removes or maintains the empty space of the column. The default is ON.\n    \n    \n      IGNORE number (LINES | ROWS)\n      Ignores data for a specified number of lines or lines. It is used to ignore header of CSV format file or ignore VCF header.\n    \n    \n      MAX_LINE_LENGTH\n      Specifies the maximum length of one line. The default value is 512K. If the data is larger, you can specify a larger value.\n    \n    \n      ON ERROR (STOP | IGNORE)\n      Specifies the action to take when an error occurs during input. If it is STOP, input is stopped. If it is IGNORE, the line where error occurred is skipped and input is continued.The default is IGNORE.\n    \n  \n\n\n-- Use default field delimiter(,)  field encloser (\") to input data.\nLOAD DATA INFILE '/tmp/aaa.csv' INTO TABLE Sample_data ;\n \n-- Create NEWTABLE with one column and enter one line as one column.\nLOAD DATA INFILE '/tmp/bbb.csv' INTO TABLE NEWTABLE AUTO BULKLOAD;\n \n-- Create NEWTABLE using first line of csv as column information, and input it into table.\nLOAD DATA INFILE '/tmp/bbb.csv' INTO TABLE NEWTABLE AUTO HEADUSE;\n  \n-- First line is ignored and field delimiter is ; and enclosing character is specified by '.\nLOAD DATA INFILE '/tmp/ccc.csv' INTO TABLE Sample_data FIELDS TERMINATED BY ';' ENCLOSED '\\''  IGNORE 1 LINES ON ERROR IGNORE;\n\n\n\n  If the AUTO option is not used, all columns of the table must be created as VARCHAR or TEXT types."
					}
					
				
		
				
					,
					
					"install-linux-docker-install-html": {
						"id": "install-linux-docker-install-html",
						"title": "Docker Installation",
						"version": "all",
						"categories": "",
						"url": " /install/linux/docker-install.html",
						"content": "Installing Docker\n\nMachbase provides a Docker image. Assuming that Docker is already installed, use the following process of installing Machbase on Docker.\n\nTo install Docker , refer to the Docker Installation Page . Docker Hub for Machbase is available on this page.\n\n$ docker pull machbase/machbase\nUsing default tag: latest\nlatest: Pulling from machbase/machbase\n3a291d7fe8d1: Pull complete\nf1e7bd0ef2d1: Pull complete\n78632f9cbb53: Pull complete\nf4f6c5358244: Pull complete\na3e04b27f9cd: Pull complete\na3ed95caeb02: Pull complete\ne03e135c0eda: Pull complete\n26612cd7ebc1: Pull complete\nb61e71cf4bc2: Pull complete\n09c9c411b936: Pull complete\n2b1cdec8c664: Pull complete\nfd9a9a288691: Pull complete\nd8852dedc8a1: Pull complete\ncba7e30dbb6f: Pull complete\nc7ead0fa7c49: Pull complete\n6af02fe4c01f: Pull complete\nd18db958464f: Pull complete\n1fb93627ec0f: Pull complete\n265b8b73294a: Pull complete\nf122e6396b46: Pull complete\n3b2f248fb414: Pull complete\n07ed5a8f0935: Pull complete\n44ec57c5ed31: Pull complete\n59383e5f4c61: Pull complete\n542101ec7002: Pull complete\nDigest: sha256:aa6a982d35946b3fb33930de91cad61bfe7d3e9a559080526ed8e9a511c82c2b\nStatus: Downloaded newer image for machbase/machbase:latest\n\n\n# Check the installed Machbase image.\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nmachbase/machbase   latest              dfb90844e7da        2 months ago        1.09 GB\n\n\n# Execute the Machbase image.\n$ docker run -it machbase/machbase\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDatabase created successfully.\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for Machbase server start.\nMachbase server started successfully.\nSERVER HAS BEEN RESET\nSERVER STARTED, PID : 56\n     Connection URL : http://172.17.0.2:5001\nmachbase@5ba45a22d140:~$"
					}
					
				
		
				
					,
					
					"sdk-dotnet-html": {
						"id": "sdk-dotnet-html",
						"title": ".NET Connector",
						"version": "all",
						"categories": "",
						"url": " /sdk/dotnet.html",
						"content": "Index\n\n\n  Class\n    \n      MachConnection : DbConnection\n      MachCommand : DbCommand\n      MachDataReader : DbDataReader\n      MachParameterCollection : DbParameterCollection\n      MachParameter : DbParameter\n      MachException : DbException\n      MachAppendWriter\n      ErrorDelegateFuncType\n      MachAppendException : MachException\n      MachTransaction\n    \n  \n  Sample Code\n    \n      Connection\n      Executing Queries\n      Executing SELECT\n      Parameter Binding\n      APPEND\n    \n  \n\n\nThe .NET (C #) Connector library that supports some features of the ADO.NET driver is provided.\n\nThe library location is  $MACHBASE_HOME/lib/ provided as a DLL type. It provides different DLLs depending on the .NET version.\n\n\n  .NET Framework 4.0: machNetConnector.dll\n  .NET Core 2.0: machNetConnectorCore.dll\n\n\nClass\n\n\n    Features not listed below may not be implemented yet or may not work correctly.\n      If you call a method or field that is not a named instance, it generates NotImplementedException or a NotSupportedException.\n\n\nMachConnection : DbConnection\n\nThis class is responsible for linking with Machbase. Because it inherits IDisposable like DbConnection, it supports disassociation through Dispose () or automatic disposition of object using using () statement.\n\n\n  \n    \n      Constructor\n      Description\n    \n  \n  \n    \n      MachConnection(string aConnectionString)\n      Creates a MachConnection with a Connection String as input.\n    \n  \n\n\n\n  \n    \n      Method\n      Description\n    \n  \n  \n    \n      Open()\n      Attempts to connect to the connection string.\n    \n    \n      Close()\n      Closes the connection when connecting.\n    \n    \n      BeginDbTransaction(IsolationLevel isolationLevel)\n      (Not yet implemented) MACHBASE does not support this object because there is no special transaction.\n    \n    \n      CreateDbCommand()\n      (Not yet implemented) Explicitly induces MachCommands to be created\n    \n    \n      ChangeDatabase(string databaseName)\n      (Not yet implemented) MACHBASE has no DATABASE classification.\n    \n  \n\n\n\n  \n    \n      Field\n      Description\n    \n  \n  \n    \n      State\n      Represents a System.Data.ConnectionState value.\n    \n    \n      StatusString\n      Indicates the state to be performed by the connected MachCommand.This is used internally to decorate the Error Message and it is not appropriate to check the status of the query with this value because it indicates the state in which the operation started.\n    \n    \n      Database\n      (Not yet implemented)\n    \n    \n      DataSource\n      (Not yet implemented)\n    \n    \n      ServerVersion\n      (Not yet implemented)\n    \n  \n\n\n\n     Connection String\n      Each item is separated by a semicolon (;). \n      Many of the keywords in the same section have the same meaning.\n  \n \n   \n     Keyword\n     Description\n     Example\n     Default value\n   \n \n \n   \n     DSNSERVERHOST\n     Hostname\n     DSN=localhostSERVER=192.168.0.1\n     \n   \n   \n     PORTPORT_NO\n     Port No.\n     PORT=5656\n     5656\n   \n   \n     USERIDUSERNAMEUSERUID\n     User ID\n     USER=SYS\n     SYS\n   \n   \n     PASSWORDPWD\n     User password\n     PWD=manager\n     \n   \n   \n     CONNECT_TIMEOUTConnectionTimeoutconnectTimeout\n     Maximum connection time\n     CONNECT_TIMEOUT\n     60 second\n   \n   \n     COMMAND_TIMEOUTcommandTimeout\n     Maximum time to perform each command\n     COMMAND_TIMEOUT\n     60 second\n   \n \n\n        As an example, we can prepare the following string.\n      String sConnString = String.Format(\"DSN={0};PORT_NO={1};UID=;PWD=MANAGER;CONNECT_TIMEOUT=10000;COMMAND_TIMEOUT=50000\", SERVER_HOST, SERVER_PORT);\n\n\nMachCommand : DbCommand\n\nA class that performs SQL commands or APPEND using MachConnection. \nSince it inherits IDisposable like DbCommand, it supports object disposal through Dispose () or automatic disposal of object using using () statement\n\n\n  \n    \n      Constructor\n      Description\n    \n  \n  \n    \n      MachCommand(string aQueryString, MachConnection)\n      Creates by typing the query to be executed along with the MachConnection object to be connected.\n    \n    \n      MachCommand(MachConnection)\n      Creates a MachConnection object to connect to. Use only if there is no query to perform (eg APPEND).\n    \n  \n\n\n\n  \n    \n      Method\n      Description\n    \n  \n  \n    \n      void CreateParameter() /void CreateDbParameter()\n      Creates a new MachParameter.\n    \n    \n      void Cancel()\n      (Not yet implemented)\n    \n    \n      void Prepare()\n      (Not yet implemented)\n    \n    \n      MachAppendWriterAppendOpen(aTableName, aErrorCheckCount = 0, MachAppendOption = None)\n      \n        Starts APPEND. Returns a MachAppendWriter object.\n        - aTableName: Target table name\n        - aErrorCheckCount: Each time the cumulative number of records entered by APPEND-DATA matches, it is checked whether it is sent to the server or not.\n          In other words, you are setting the automatic APPEND-FLUSH point.\n        - MachAppendOption: Currently only one option is provided.\n          - MachAppendOption.None: No options are attached.\n          - MachAppendOption.MicroSecTruncated: When inputting the value of a DateTime object, enter the value expressed only up to microsecond.\n          (The Ticks value of a DateTime object is expressed up to 100 nanoseconds.)\n      \n    \n    \n      voidAppendData(MachAppendWriter aWriter, List&lt;object&gt; aDataList)\n      \n        Through the MachAppendWriter object, it takes a list containing the data and enters it into the database.\n        - In the order of the data in the List, each datatype must match the datatype of the column represented in the table.\n        - If the data in the List is insufficient or overflows, an error occurs.\n         \n           &nbsp;When representing a time value with a ulong object, simply do not enter the Tick value of the DateTime object. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In that value, you must enter a value that excludes the DateTime Tick value that represents 1970-01-01.\n        \n      \n    \n    \n      voidAppendDataWithTime(MachAppendWriter aWriter, List&lt;object&gt; aDataList, DateTime aArrivalTime)\n      Method that explicitly puts an _arrival_time value into a DateTime object in AppendData().\n    \n    \n      voidAppendDataWithTime(MachAppendWriter aWriter, List&lt;object&gt; aDataList, ulong aArrivalTimeLong)\n      Method that can explicitly put _arrival_time value into a ulong object in AppendData().Refer to AppendData() above for problems that may occur when typing a ulong value as an _arrival_time value.\n    \n    \n      void AppendFlush(MachAppendWriter aWriter)\n      \n        The data entered by AppendData() is immediately sent to the server to force data insert.\n        The more frequently the call is made, the lower the data loss rate due to the system error and the faster the error check, although the performance is lowered.\n        The less frequently the call is made, the more likely the data loss will occur and the error checking will be delayed, but the performance will increase significantly.\n      \n    \n    \n      void AppendClose(MachAppendWriter aWriter)\n      Closes APPEND. Internally, after calling AppendFlush(), the actual protocol is internally finished.\n    \n    \n      int ExecuteNonQuery()\n      Performs the input query. Returns the number of records affected by the query.\n        It is usually used when performing queries except SELECT.\n      \n    \n    \n      object ExecuteScalar()\n      Performs the input query. Returns the first value of the query targetlist as an object.\n        It is usually used when you want to perform a SELECT query, especially a SELECT (Scalar Query) with only one result, and get the result without a DbDataReader.\n      \n    \n    \n      DbDataReader ExecuteDbDataReader(CommandBehavior aBehavior)\n      Executes the input query, generates a DbDataReader that can read the result of the query, and returns it.\n    \n  \n\n\n\n  \n    \n      Field\n      Description\n    \n  \n  \n    \n      Connection / DbConnection\n      Connected MachConnection.\n    \n    \n      ParameterCollection / DbParameterCollection\n      The MachParameterCollection to use for the Binding purpose.\n    \n    \n      CommandText\n      Query string.\n    \n    \n      CommandTimeout\n      The amount of time it takes to perform a particular task, waiting for a response from the server.It follows the values ​​set in MachConnection, where you can only reference values.\n    \n    \n      FetchSize\n      The number of records to fetch from the server at one time . The default value is 3000.\n    \n    \n      IsAppendOpened\n      Determines if Append is already open when APPEND is at work\n    \n    \n      CommandType\n      (Not yet implemented)\n    \n    \n      DesignTimeVisible\n      (Not yet implemented)\n    \n    \n      UpdatedRowSource\n      (Not yet implemented)\n    \n  \n\n\nMachDataReader : DbDataReader\n\nThis is a class that reads fetch results. Only objects created with MachCommand.ExecuteDbDataReader () that can not be explicitly created are available.\n\n\n  \n    \n      Method\n      Description\n    \n  \n  \n    \n      string GetName(int ordinal)\n      Returns the ordinal column name.\n    \n    \n      string GetDataTypeName(int ordinal)\n      Returns the datatype name of the ordinal column.\n    \n    \n      Type GetFieldType(int ordinal)\n      Returns the datatype of the ordinal column.\n    \n    \n      int GetOrdinal(string name)\n      Returns the index at which the column name is located.\n    \n    \n      object GetValue(int ordinal)\n      Returns the ordinal value of the current record.\n    \n    \n      bool IsDBNull(int ordinal)\n      Returns whether the ordinal value of the current record is NULL.\n    \n    \n      int GetValues(object[] values)\n      Sets all the values ​​of the current record and returns the number.\n    \n    \n      xxxx GetXXXX(int ordinal)\n      Returns the ordinal column value according to the datatype (XXXX). - Boolean - Byte - Char - Int16 / 32/64 - DateTime - String - Decimal - Double - Float\n    \n    \n      bool Read()\n      Reads the next record. Returns False if the result does not exist.\n    \n    \n      DataTable GetSchemaTable()\n      (Not supported)\n    \n    \n      bool NextResult()\n      (Not supported)\n    \n  \n\n\n\n  \n    \n      Field\n      Description\n    \n  \n  \n    \n      FetchSize\n      The number of records to fetch from the server at one time. The default is 3000, which can not be modified here.\n    \n    \n      FieldCount\n      Number of result columns.\n    \n    \n      this[int ordinal]\n      Equivalent to object GetValue (int ordinal).\n    \n    \n      this[string name]\n      Equivalent to object GetValue(GetOrdinal(name).\n    \n    \n      HasRows\n      Indicates whether the result is present.\n    \n    \n      RecordsAffected\n      Unlike MachCommand, here, it represents Fetch Count.\n    \n  \n\n\nMachParameterCollection : DbParameterCollection\n\nThis is a class that binds parameters needed by MachCommand.\n\nIf you do this after binding, the values ​​are done together.\n\n\n    Since the concept of Prepared Statement is not implemented, execution performance after Binding is the same as the performance performed first.\n\n\n\n  \n    \n      Method\n      Description\n    \n  \n  \n    \n      MachParameterAdd(string parameterName, DbType dbType)\n      Adds the MachParameter, specifying the parameter name and type. Returns the added MachParameter object.\n    \n    \n      int Add(object value)\n      Adds a value. Returns the index added.\n    \n    \n      void AddRange(Array values)\n      Adds an array of simple values.\n    \n    \n      MachParameterAddWithValue(string parameterName, object value)\n      Adds the parameter name and its value. Returns the added MachParameter object.\n    \n    \n      bool Contains(object value)\n      Determines whether or not the corresponding value is added.\n    \n    \n      bool Contains(string value)\n      Determines whether or not the corresponding parameter name is added.\n    \n    \n      void Clear()\n      Deletes all parameters.\n    \n    \n      int IndexOf(object value)\n      Returns the index of the corresponding value.\n    \n    \n      int IndexOf(string parameterName)\n      Returns the index of the corresponding parameter name.\n    \n    \n      void Insert(int index, object value)\n      Adds the value to a specific index.\n    \n    \n      void Remove(object value)\n      Deletes the parameter including the value.\n    \n    \n      void RemoveAt(int index)\n      Deletes the parameter located at the index.\n    \n    \n      void RemoveAt(string parameterName)\n      Deletes the parameter with that name.\n    \n  \n\n\n\n  \n    \n      Field\n      Description\n    \n  \n  \n    \n      Count\n      Number of parameters\n    \n    \n      this[int index]\n      Indicates the MachParameter at index.\n    \n    \n      this[string name]\n      Indicates the MachParameter of the order in which the parameter names match.\n    \n  \n\n\nMachParameter : DbParameter\n\nThis is a class that contains the information that binds the necessary parameters to each MachCommand.\n\nNo special methods are supported.\n\n\n  \n    \n      Field\n      Description\n    \n  \n  \n    \n      ParameterName\n      Parameter name\n    \n    \n      Value\n      Value\n    \n    \n      Size\n      Value size\n    \n    \n      Direction\n      ParameterDirection (Input / Output / InputOutput / ReturnValue)The default value is Input.\n    \n    \n      DbType\n      DB Type\n    \n    \n      MachDbType\n      MACHBASE DB TypeMay differ from DB Type.\n    \n    \n      IsNullable\n      Whether nullable\n    \n    \n      HasSetDbType\n      Whether DB Type is specified\n    \n  \n\n\nMachException : DbException\n\nThis is a class that displays errors that appear in Machbase.\n\nAn error message is set, and all error messages  can be found in  MachErrorMsg .\n\n\n  \n    \n      Field\n      Description\n    \n  \n  \n    \n      int MachErrorCode\n      Error code provided by MACHBASE\n    \n  \n\n\nMachAppendWriter\n\nAPPEND is supported as a separate class using MachCommand.\n\nThis is a class to support MACHBASE Append Protocol, not ADO.NET standard.\nIt is created with MachCommand’s AppendOpen () without a separate constructor.\n\n\n  \n    \n      Method\n      Description\n    \n  \n  \n    \n      void SetErrorDelegator(ErrorDelegateFuncType aFunc)\n      Specifies the ErrorDelegateFunc to call when an error occurs.\n    \n  \n\n\n\n  \n    \n      Field\n      Description\n    \n  \n  \n    \n      SuccessCount\n      Number of successful records. Is set after AppendClose().\n    \n    \n      FailureCount\n      The number of records that failed input. Set after AppendClose ().\n    \n    \n      Option\n      MachAppendOption received input during AppendOpen()\n    \n  \n\n\nErrorDelegateFuncType\n\npublic delegate void ErrorDelegateFuncType(MachAppendException e);\n\n\nIn MachAppendWriter, you can specify a function to detect errors occurring on the MACHBASE server side during APPEND.\n\nIn .NET, this function type is specified as a Delegator Function.\n\nMachAppendException : MachException\n\nSame as MachException, except that:\n\n\n  An error message is received from the server side.\n  A data buffer in which an error has occurred can be obtained. (comma-separated) can be used to process and re-append or record data.\n\n\nThe exception is only available within the ErrorDelegateFunc.\n\n\n  \n    \n      Method\n      Description\n    \n  \n  \n    \n      GetRowBuffer()\n      A data buffer in which an error has occurred can be obtained.\n    \n  \n\n\nMachTransaction\n\nNot supported.\n\nSample Code\n\nConnection\n\nYou can create a MachConnection and use Open () - Close ().\n\nString sConnString = String.Format(\"DSN={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nMachConnection sConn = new MachConnection(sConnString);\nsConn.Open();\n//... do something\nsConn.Close();\n\n\nIf you use the using statement, you do not need to call Close (), which is a connection closing task.\n\nString sConnString = String.Format(\"DSN={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    sConn.Open();\n    //... do something\n} // you don't need to call sConn.Close();\n\n\nExecuting Queries\n\nCreate a MachCommand and perform the query.\n\nString sConnString = String.Format(\"DSN={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    String sQueryString = \"CREATE TABLE tab1 ( col1 INTEGER, col2 VARCHAR(20) )\";\n    MachCommand sCommand = new MachCommand(sQueryString , sConn)\n    try\n    {\n        sCommand.ExecuteNonQuery();\n    }\n    catch (MachException me)\n    {\n        throw me;\n    }\n}\n\n\nAgain, using the using statement, MachCommand release can be done immediately.\n\nString sConnString = String.Format(\"DSN={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    String sQueryString = \"CREATE TABLE tab1 ( col1 INTEGER, col2 VARCHAR(20) )\";\n    using(MachCommand sCommand = new MachCommand(sQueryString , sConn))\n    {\n        try\n        {\n            sCommand.ExecuteNonQuery();\n        }\n        catch (MachException me)\n        {\n            throw me;\n        }\n    }\n}\n\n\nExecuting SELECT\n\nYou can get a MachDataReader by executing a MachCommand with a SELECT query.\n\nYou can fetch the records one by one through the MachDataReader.\n\nString sConnString = String.Format(\"DSN={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    String sQueryString = \"SELECT * FROM tab1;\";\n    using(MachCommand sCommand = new MachCommand(sQueryString , sConn))\n    {\n        try\n        {\n            MachDataReader sDataReader = sCommand.ExecuteReader();\n            while (sDataReader.Read())\n            {\n                for (int i = 0; i &lt; sDataReader.FieldCount; i++)\n                {\n                    Console.WriteLine(String.Format(\"{0} : {1}\",\n                                                    sDataReader.GetName(i),\n                                                    sDataReader.GetValue(i)));\n                }\n            }\n        }\n        catch (MachException me)\n        {\n            throw me;\n        }\n    }\n}\n\n\nParameter Binding\n\nYou can create a MachParameterCollection and then link it to a MachCommand.\n\nString sConnString = String.Format(\"DSN={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    string sSelectQuery = @\"SELECT *\n        FROM tab2\n        WHERE CreatedDateTime &lt; @CurrentTime\n        AND CreatedDateTime &gt;= @PastTime\";\n \n    using (MachCommand sCommand = new MachCommand(sSelectQuery, sConn))\n    {\n        DateTime sCurrtime = DateTime.Now;\n        DateTime sPastTime = sCurrtime.AddMinutes(-1);\n \n        try\n        {\n            sCommand.ParameterCollection.Add(new MachParameter { ParameterName = \"@CurrentTime\", Value = sCurrtime });\n            sCommand.ParameterCollection.Add(new MachParameter { ParameterName = \"@PastTime\", Value = sPastTime });\n \n            MachDataReader sDataReader = sCommand.ExecuteReader();\n \n            while (sDataReader.Read())\n            {\n                for (int i = 0; i &lt; sDataReader.FieldCount; i++)\n                {\n                    Console.WriteLine(String.Format(\"{0} : {1}\",\n                                                    sDataReader.GetName(i),\n                                                    sDataReader.GetValue(i)));\n                }\n            }\n        }\n        catch (MachException me)\n        {\n            throw me;\n        }\n    }\n}\n\n\nAPPEND\n\nWhen you run AppendOpen () on a MachCommand, you get a MachAppendWriter object.\n\nUsing this object and MachCommand, you can get a list of one input record and perform an AppendData ().\nAppendFlush () will reflect the input of all records, and AppendClose () will end the entire Append process.\n\nString sConnString = String.Format(\"DSN={0};PORT_NO={1};UID=;PWD=MANAGER;\", SERVER_HOST, SERVER_PORT);\nusing (MachConnection sConn = new MachConnection(sConnString))\n{\n    using (MachCommand sAppendCommand = new MachCommand(sConn))\n    {\n        MachAppendWriter sWriter = sAppendCommand.AppendOpen(\"tab2\");\n        sWriter.SetErrorDelegator(AppendErrorDelegator);\n \n        var sList = new List&lt;object&gt;();\n        for (int i = 1; i &lt;= 100000; i++)\n        {\n            sList.Add(i);\n            sList.Add(String.Format(\"NAME_{0}\", i % 100));\n \n            sAppendCommand.AppendData(sWriter, sList);\n \n            sList.Clear();\n \n            if (i % 1000 == 0)\n            {\n                sAppendCommand.AppendFlush();\n            }\n        }\n \n        sAppendCommand.AppendClose(sWriter);\n        Console.WriteLine(String.Format(\"Success Count : {0}\", sWriter.SuccessCount));\n        Console.WriteLine(String.Format(\"Failure Count : {0}\", sWriter.FailureCount));\n    }\n}\n\n\nprivate static void AppendErrorDelegator(MachAppendException e)\n{\n    Console.WriteLine(\"{0}\", e.Message);\n    Console.WriteLine(\"{0}\", e.GetRowBuffer());\n}"
					}
					
				
		
				
					,
					
					"feature-tables-tag-duplication-removal-html": {
						"id": "feature-tables-tag-duplication-removal-html",
						"title": "Duplication removal",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/duplication-removal.html",
						"content": "Configuration of duplication removal\nWhen creating the TAG table, the duration for duplicate removal is passed as a table property. The maximum configurable duration for duplicate removal is 30 days.\n\n-- If the newly inserted data duplicates existing data within a day from system time those data will be deleted.\n  \nCREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED) TAG_DUPLICATE_CHECK_DURATION=1;\n\n\nThe property of the duplication removal is shown in the table m$sys_table_property.\nSELECT * FROM m$sys_table_property WHERE id={table_id} AND name = 'TAG_DUPLICATE_CHECK_DURATION';\n\n\nData insert/select example (duplication removal duration is one day)\n-- Total inserted data are 6 and 4 of them are duplicates but 1 duplicated record was inserted one day before \n-- system time(1970-01-03 09:00:00 000:000:003). \n-- Newly inserted duplicated data within the configured duration (1 day) are not displayed.\n\nINSERT INTO tag VALUES('tag1', '1970-01-01 09:00:00 000:000:001', 0);\nINSERT INTO tag VALUES('tag1', '1970-01-02 09:00:00 000:000:001', 0);    \nINSERT INTO tag VALUES('tag1', '1970-01-02 09:00:00 000:000:002', 0);\nINSERT INTO tag VALUES('tag1', '1970-01-02 09:00:00 000:000:002', 1);\nINSERT INTO tag VALUES('tag1', '1970-01-03 09:00:00 000:000:003', 0);\nINSERT INTO tag VALUES('tag1', '1970-01-01 09:00:00 000:000:001', 0);\n    \nSELECT * FROM tag WHERE name = 'tag1';\nNAME                  TIME                            VALUE                       \n--------------------------------------------------------------------------------------\ntag1                  1970-01-01 09:00:00 000:000:001 0\ntag1                  1970-01-02 09:00:00 000:000:001 0                           \ntag1                  1970-01-02 09:00:00 000:000:002 0\ntag1                  1970-01-03 09:00:00 000:000:003 0      \ntag1                  1970-01-01 09:00:00 000:000:001 0\n  \n\n\nConstrain of duplication removal\n\n  Once the duplication removal policy is set during table creation, it cannot be modified thereafter.\n  The duplication removal setting can be configured on a daily basis, with a maximum limit of 30 days.\n  If the existing input data has already been deleted, any subsequent occurrence of the same data will not be considered as a duplicate for the purpose of duplication removal."
					}
					
				
		
				
					,
					
					"intro-edition-edge-html": {
						"id": "intro-edition-edge-html",
						"title": "Edge Edition",
						"version": "all",
						"categories": "",
						"url": " /intro/edition/edge.html",
						"content": "Necessity\n\nThe Edge Edition refers to the Machbase line of products that operate at high speeds on small devices with limited resources.\n\nSince late 2010, Edge equipment with a certain amount of computing power has been released, and various businesses are being developed through the Edge equipment in the field of Industrial IoT.\n\nIt is common to monitor the state of the production equipment in a specific environment, store data generated in a robot or place where a large amount of sensing is required, and transmit the data to the parent server when necessary.\n\nHowever, in order to operate smoothly on these small-scale Edge devices, the key is to store data at high speed and extract data in real time.\n\nMachbase enables this requirement through its innovative technology and supports the following hardware.\n\nSupported Hardware Examples\n\n\n  \n    \n       \n      Rasberry PI 3\n      Samsung ARTIK 7\n      LattePanda\n      nvidia Jetson TX2\n    \n  \n  \n    \n      Form\n      \n      \n      \n      \n    \n    \n      CPU\n      ARM Cortex-A53 (64bit)\n      ARM Cortex-A53 (64bit)\n      Intel ATOM (x5-Z8350)\n      ARM Cortex-A57 (64bit)\n    \n    \n      MEMORY\n      1 GB\n      1 GB\n      4GB\n      8GB\n    \n  \n\n\nFuntional Limitations\n\nIt has the following functional limitations as it has to operate on a small-scale equipment with limited resources.\n\n\n  No rollup\n  No backup\n  No mount/restore"
					}
					
				
		
				
					,
					
					"intro-editions-html": {
						"id": "intro-editions-html",
						"title": "Introduction of Machbase Products",
						"version": "all",
						"categories": "",
						"url": " /intro/editions.html",
						"content": "Machbase has the following 2 types of products that can be applied to each type of business environment.\n\n\n  Standard Edition\n  Cluster Edition"
					}
					
				
		
				
					,
					
					"feature-tables-volatile-ex-html": {
						"id": "feature-tables-volatile-ex-html",
						"title": "Volatile Table Utilization Example",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/volatile/ex.html",
						"content": "Save Sensor Data Current Value\n\nThe data of the volatile table exist only in the memory, and the update operation by the primary key is very fast. Using this feature, it creates a table that stores the current values ​​of the sensors that change very quickly. An example of a table creation script is shown below.\n\ncreate volatile table sensor_current (sensor_id varchar(40) primary key, value double);\n\n\nInput and Update Volatile Data\n\nSince the table has been created, the current value of the sensor can be reflected through data input and update operations. The input sensor value is determined based on the primary key sensor_id column as to whether to perform input or update. Input or update can be performed with the following query.\n\ninsert into sensor_current values('SENSOR_001',100.0) on duplicate key update set value=100.0;\n\n\nThe data input in the above query statement updates the value column value of the record with the sensor_id value ‘SENSOR_001’, which is the column corresponding to the primary key, to 100.0. If there is no data, insert a new record according to the syntax of the insert statement.\n\nVolatile Data Search\n\nTo find the current value of specific sensor data, search using the following query. You can perform searches using the same syntax as a regular SQL query.\n\nSELECT value FROM sensor_current WHERE sensor_id = 'SENSOR_001'"
					}
					
				
		
				
					,
					
					"feature-tables-lookup-ex-html": {
						"id": "feature-tables-lookup-ex-html",
						"title": "Lookup Table Utilization Example",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/lookup/ex.html",
						"content": "Create Lookup Table\n\nThe lookup table can be updated and is used to add data that is not in the original log data through a join. The following example shows an example of creating a log table and a lookup table.\n\n-- Create log table.\ncreate table weblog (addr ipv4, msg varchar(100));\n-- Input sample data.\ninsert into weblog values ('127.0.0.1', 'a test msessage');\n-- Create lookup table.\ncreate lookup table dnslookup (addr ipv4 primary key, hostname varchar (100));\n\n\nLet’s insert or update the data in the lookup table.\n\ninsert into dnslookup values ('127.0.0.1', 'localhost') on duplicate key update set hostname = '127.0.0.1'\n\n\nYou can retrieve data from lookup tables and log tables through join.\n\nselect msg, hostname from weblog, dnslookup where weblog.addr = dnslookup.addr;"
					}
					
				
		
				
					,
					
					"feature-tables-log-ex-html": {
						"id": "feature-tables-log-ex-html",
						"title": "Example of Log Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/ex.html",
						"content": "Index\n\n\n  Log Data Insert\n  Log Data Retrieval\n  Create and View Index\n  Time Series Data Retrieval\n  Internet Address Type Data Retrieval\n\n\nInstalling the Machbase package provides a tutorial that creates a log table, populates the generated table with the log data, and displays the log data.\n\nYou can find it in the path below.\n\n[machbase@localhost tutorials]$ cd $MACHBASE_HOME/tutorials\n[machbase@localhost tutorials]$ ls -l\ntotal 0\ndrwxrwxr-x 2 machbase machbase 103 Oct 30 16:10 backup_mount\ndrwxrwxr-x 2 machbase machbase  44 Oct 30 16:10 connect_r\ndrwxrwxr-x 2 machbase machbase 177 Oct 30 16:10 csvload\ndrwxrwxr-x 2 machbase machbase  49 Oct 30 16:10 export_data\ndrwxrwxr-x 2 machbase machbase  32 Oct 30 16:10 install_docker_image\ndrwxrwxr-x 2 machbase machbase  49 Oct 30 16:10 ip_address\ndrwxrwxr-x 2 machbase machbase  75 Oct 30 16:10 searchtext\ndrwxrwxr-x 2 machbase machbase  93 Oct 30 16:10 time_series\n[machbase@localhost tutorials]$\n\n\nCreate Log Table\n\nThe log data to be input is a file in the following csv format.\n\n[machbase@localhost csvload]$ cd $MACHBASE_HOME/tutorials/csvload\n[machbase@localhost csvload]$ more sample_data.csv\n2015-05-20 06:00:00,63.214.191.124,2296,122.195.164.32,5416,12,GET /twiki/bin/view/Main/TWikiGroups?rev=1.2 HTTP/1.1,200,5162\n2015-05-20 06:00:07,212.237.153.79,6203,71.129.68.118,8859,67,GET /twiki/bin/view/Main/WebChanges HTTP/1.1,200,40520\n2015-05-20 06:00:07,243.9.49.80,344,122.195.164.32,6203,46,GET /twiki/bin/view/Main/TWikiGroups?rev=1.2 HTTP/1.1,200,5162\n2015-05-20 06:00:07,232.191.241.129,5377,174.47.129.59,1247,17,GET /mailman/listinfo/hsdivision HTTP/1.1,200,6291\n2015-05-20 06:00:07,121.67.24.216,2296,212.237.153.79,6889,68,GET /twiki/bin/view/TWiki/WebTopicEditTemplate HTTP/1.1,200,3732\n2015-05-20 06:00:07,31.224.72.52,450,100.46.183.122,10541,20,GET /twiki/bin/view/Main/WebChanges HTTP/1.1,200,40520\n2015-05-20 06:00:07,210.174.159.227,6180,173.149.119.202,6927,2,GET /twiki/bin/rdiff/TWiki/AlWilliams?rev1=1.2&amp;rev2=1.1 HTTP/1.1,200,5234\n2015-05-20 06:00:07,210.174.159.227,10124,16.194.51.72,10512,69,GET /twiki/bin/rdiff/TWiki/AlWilliams?rev1=1.2&amp;rev2=1.1 HTTP/1.1,200,5234\n2015-05-20 06:00:07,60.48.99.15,12333,85.183.139.166,12020,64,GET /robots.txt HTTP/1.1,200,68\n\n\nCheck each field value of log data and create a table. You can create it in machsql using ‘CREATE TABLE’ syntax.\n\nCREATE TABLE SAMPLE_TABLE\n(\n    srcip        IPV4,\n    srcport      INTEGER,\n    dstip        IPV4,\n    dstport      INTEGER,\n    protocol     SHORT,\n    eventlog     VARCHAR(1204),\n    eventcode    SHORT,\n    eventsize    LONG\n);\n\n\nAlternatively, you can create a table creation script file and run machsql on the OS command line.\n\n[machbase@localhost csvload]$ machsql -s localhost -u sys -p manager -f create_sample_table.sql\n=================================================================\n     Machbase Client Query Utility\n     Release Version x.x.x.official\n     Copyright 2014 MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMACHBASE_CONNECT_MODE=INET, PORT=5656\nType 'help' to display a list of available commands.\nMach&gt; CREATE TABLE SAMPLE_TABLE\n(\n    srcip        IPV4,\n    srcport      INTEGER,\n    dstip        IPV4,\n    dstport      INTEGER,\n    protocol     SHORT,\n    eventlog     VARCHAR(1204),\n    eventcode    SHORT,\n    eventsize    LONG\n);\nCreated successfully. \n\n\nLog Data Insert\n\nSince the log data is a csv format file, you can load it using csvimport.\n\nThe first field in the log file is the date, which specifies the option to enter this value into the _arrival_time column.\n\n[machbase@localhost csvload]$ csvimport -t sample_table -d sample_data.csv -a -F \"_arrival_time YYYY-MM-DD HH24:MI:SS\"\n-----------------------------------------------------------------\n     Machbase Data Import/Export Utility.\n     Release Version x.x.x.official\n     Copyright 2014, MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n-----------------------------------------------------------------\nNLS            : US7ASCII            EXECUTE MODE   : IMPORT\nTARGET TABLE   : sample_table        DATA FILE      : sample_data.csv\nIMPORT_MODE    : APPEND              FILED TERM     : ,\nROW TERM       :\n                   ENCLOSURE      : \"\nESCAPE         : \"                   ARRIVAL_TIME   : TRUE\nENCODING       : NONE                HEADER         : FALSE\nCREATE TABLE   : FALSE\n \n Progress bar                       Imported records        Error records\n                                             1000000                    0\n \nImport time         :  0 hour  0 min  5.728 sec\nLoad success count  : 1000000\nLoad fail count     : 0\n \n[machbase@localhost csvload]$\n\n\nLog Data Retrieval\n\nCheck the data in machsql.\n\n[machbase@localhost csvload]$ machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version x.x.x.official\n     Copyright 2014 MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase server address (Default:127.0.0.1) :\nMachbase user ID  (Default:SYS)\nMachbase User Password :\nMACHBASE_CONNECT_MODE=INET, PORT=5656\nType 'help' to display a list of available commands.\nMach&gt; show tables;\nNAME                                                                              TYPE\n-----------------------------------------------------------------------------------------------\nSAMPLE_TABLE                                                                      LOG\n[1] row(s) selected.\n \nMach&gt; desc sample_table;\n[ COLUMN ]\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH\n----------------------------------------------------------------\nSRCIP                         ipv4                15\nSRCPORT                       integer             11\nDSTIP                         ipv4                15\nDSTPORT                       integer             11\nPROTOCOL                      short               6\nEVENTLOG                      varchar             1204\nEVENTCODE                     short               6\nEVENTSIZE                     long                20\n \nMach&gt; SELECT COUNT(*) FROM SAMPLE_TABLE;\nCOUNT(*)\n-----------------------\n1000000\n[1] row(s) selected.\n \nMach&gt; SELECT SRCIP, COUNT(*) FROM SAMPLE_TABLE GROUP BY SRCIP ORDER BY 2 DESC LIMIT 10;\nSRCIP           COUNT(*)\n----------------------------------------\n96.128.212.177  13594\n173.149.119.202 13546\n219.229.142.218 13537\n69.99.246.62    13511\n239.81.105.222  13501\n86.45.186.17    13487\n231.146.69.51   13483\n248.168.229.34  13472\n105.9.103.49    13472\n115.18.128.171  13468\n[10] row(s) selected.\nMach&gt;\n\n\nCreate and View Index\n\nCreate a keyword index for the eventlog column of varchar type in the generated sample_table column and search for text.\n\n-- Create eventlog_index index.\nMach&gt; CREATE INDEX eventlog_index ON SAMPLE_TABLE( eventlog) INDEX_TYPE KEYWORD;\nCreated successfully.\nElapsed time: 0.442\n \n-- Check created index.\nMach&gt; desc sample_table;\n[ COLUMN ]\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH\n----------------------------------------------------------------\nSRCIP                         ipv4                15\nSRCPORT                       integer             11\nDSTIP                         ipv4                15\nDSTPORT                       integer             11\nPROTOCOL                      short               6\nEVENTLOG                      varchar             1204\nEVENTCODE                     short               6\nEVENTSIZE                     long                20\n \n[ INDEX ]\n----------------------------------------------------------------\nNAME                          TYPE                COLUMN\n----------------------------------------------------------------\nEVENTLOG_INDEX                KEYWORD_LSM         EVENTLOG\n \n \n-- Retrieve data containing 'view' using SEARCH syntax.\nMach&gt; SELECT EVENTLOG FROM SAMPLE_TABLE WHERE EVENTLOG SEARCH 'view' LIMIT 10;\nEVENTLOG\n------------------------------------------------------------------------------------\nGET /twiki/bin/view/TWiki/ManagingWebs?skin=print HTTP/1.1\nGET /twiki/bin/view/Main/TokyoOffice HTTP/1.1\nGET /twiki/bin/view/TWiki/ManagingWebs?rev=1.22 HTTP/1.1\nGET /twiki/bin/view/Main/DCCAndPostFix HTTP/1.1\nGET /twiki/bin/view/TWiki/WebTopicEditTemplate HTTP/1.1\nGET /twiki/bin/view/Main/TokyoOffice HTTP/1.1\nGET /twiki/bin/view/TWiki/WikiCulture HTTP/1.1\nGET /twiki/bin/view/Main/MikeMannix HTTP/1.1\nGET /twiki/bin/view/TWiki/WikiCulture HTTP/1.1\nGET /twiki/bin/view/TWiki/WikiCulture HTTP/1.1\n[10] row(s) selected.\n \n-- Obtain number of data containing 'robots.txt'.\nMach&gt; SELECT COUNT(*) FROM SAMPLE_TABLE WHERE EVENTLOG SEARCH 'robots.txt';\nCOUNT(*)\n-----------------------\n40283\n[1] row(s) selected.\n \n-- Aggregate data containing 'robots.txt' by SRCIP and output only top 10.\nMach&gt; SELECT SRCIP, COUNT(*) FROM SAMPLE_TABLE WHERE EVENTLOG SEARCH 'robots.txt' GROUP BY SRCIP ORDER BY 2 DESC LIMIT 10;\nSRCIP           COUNT(*)\n----------------------------------------\n81.227.25.139   616\n162.80.44.96    596\n7.234.88.67     595\n227.106.13.91   578\n220.192.100.45  570\n46.201.48.18    570\n231.146.69.51   564\n185.22.195.164  564\n64.58.31.79     561\n50.5.206.126    561\n[10] row(s) selected.\n\n\nTime Series Data Retrieval\n\nMachbase provides a convenient syntax for querying time series data. Learn how to query fast data using DURATION.\n\n-- Check maximum and minimum values entered in _arrival_time column.\nMach&gt; SELECT MIN(_ARRIVAL_TIME), MAX(_ARRIVAL_TIME) FROM SAMPLE_TABLE;\nMIN(_ARRIVAL_TIME)              MAX(_ARRIVAL_TIME)\n-------------------------------------------------------------------\n2015-05-20 06:00:00 000:000:000 2015-05-20 06:40:10 000:000:000\n[1] row(s) selected.\n \n-- Use DATE_TRUNC() to obtain count per minute.\nMach&gt; SELECT DATE_TRUNC('minute', _ARRIVAL_TIME) as TIME, COUNT(*) as COUNT FROM SAMPLE_TABLE GROUP BY TIME ORDER BY TIME;\nTIME                            COUNT\n--------------------------------------------------------\n2015-05-20 06:00:00 000:000:000 32001\n2015-05-20 06:01:00 000:000:000 28000\n2015-05-20 06:02:00 000:000:000 24000\n2015-05-20 06:03:00 000:000:000 32000\n2015-05-20 06:04:00 000:000:000 16000\n2015-05-20 06:05:00 000:000:000 16000\n2015-05-20 06:06:00 000:000:000 32000\n2015-05-20 06:07:00 000:000:000 32000\n2015-05-20 06:08:00 000:000:000 20000\n2015-05-20 06:09:00 000:000:000 24000\n2015-05-20 06:10:00 000:000:000 20000\n2015-05-20 06:11:00 000:000:000 20000\n2015-05-20 06:12:00 000:000:000 24000\n2015-05-20 06:13:00 000:000:000 20000\n2015-05-20 06:14:00 000:000:000 32000\n2015-05-20 06:15:00 000:000:000 24000\n2015-05-20 06:16:00 000:000:000 32000\n2015-05-20 06:17:00 000:000:000 28000\n2015-05-20 06:18:00 000:000:000 32000\n2015-05-20 06:19:00 000:000:000 12000\n2015-05-20 06:20:00 000:000:000 24000\n2015-05-20 06:21:00 000:000:000 28000\n2015-05-20 06:22:00 000:000:000 28000\n2015-05-20 06:23:00 000:000:000 24000\n2015-05-20 06:24:00 000:000:000 28000\n2015-05-20 06:25:00 000:000:000 28000\n2015-05-20 06:26:00 000:000:000 32000\n2015-05-20 06:27:00 000:000:000 20000\n2015-05-20 06:28:00 000:000:000 20000\n2015-05-20 06:29:00 000:000:000 20000\n2015-05-20 06:30:00 000:000:000 28000\n2015-05-20 06:31:00 000:000:000 32000\n2015-05-20 06:32:00 000:000:000 32000\n2015-05-20 06:33:00 000:000:000 28000\n2015-05-20 06:34:00 000:000:000 20000\n2015-05-20 06:35:00 000:000:000 24000\n2015-05-20 06:36:00 000:000:000 24000\n2015-05-20 06:37:00 000:000:000 16000\n2015-05-20 06:38:00 000:000:000 24000\n2015-05-20 06:39:00 000:000:000 16000\n2015-05-20 06:40:00 000:000:000 3999\n[41] row(s) selected.\n \n-- Use DURATION statement to specify time range one minute before specified time reference.\nMach&gt; SELECT MIN(_ARRIVAL_TIME), MAX(_ARRIVAL_TIME), COUNT(*) as COUNT FROM SAMPLE_TABLE DURATION 1 MINUTE BEFORE TO_DATE('2015-05-20 06:30:00');\nMIN(_ARRIVAL_TIME)              MAX(_ARRIVAL_TIME)              COUNT\n-----------------------------------------------------------------------------------------\n2015-05-20 06:29:05 000:000:000 2015-05-20 06:29:45 000:000:000 20000\n[1] row(s) selected.\n \n-- Use DURATION syntax to specify time range after one minute from specific time reference.\nMach&gt; SELECT MIN(_ARRIVAL_TIME), MAX(_ARRIVAL_TIME), COUNT(*) as COUNT FROM SAMPLE_TABLE DURATION 1 MINUTE AFTER TO_DATE('2015-05-20 06:30:00');\nMIN(_ARRIVAL_TIME)              MAX(_ARRIVAL_TIME)              COUNT\n-----------------------------------------------------------------------------------------\n2015-05-20 06:30:04 000:000:000 2015-05-20 06:30:57 000:000:000 28000\n[1] row(s) selected.\n \n \n-- Use DURATION statement to specify FROM to TO time range.\nMach&gt; SELECT MIN(_ARRIVAL_TIME), MAX(_ARRIVAL_TIME), COUNT(*) as COUNT FROM SAMPLE_TABLE DURATION FROM TO_DATE('2015-05-20 06:20:00') TO TO_DATE('2015-05-20 06:30:00');\nMIN(_ARRIVAL_TIME)              MAX(_ARRIVAL_TIME)              COUNT\n-----------------------------------------------------------------------------------------\n2015-05-20 06:20:03 000:000:000 2015-05-20 06:29:45 000:000:000 252000\n[1] row(s) selected.\n\n\nInternet Address Type Data Retrieval\n\nMachbase provides data types for Internet addresses and can be conveniently searched.\n\n-- Set IP band in Netmask format and inquire.\nMach&gt; SELECT COUNT(*) FROM SAMPLE_TABLE WHERE SRCIP CONTAINED '100.195.159.0/24';\nCOUNT(*)\n-----------------------\n13097\n[1] row(s) selected.\n \n \n-- Equal (=) search is also possible using '*'.\nMach&gt; SELECT COUNT(*) FROM SAMPLE_TABLE WHERE SRCIP = '100.195.159.*';\nCOUNT(*)\n-----------------------\n13097\n[1] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-tables-tag-ex-html": {
						"id": "feature-tables-tag-ex-html",
						"title": "An example of tag table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/ex.html",
						"content": "Index\n\n\n  Introduction\n  Data conversion flowchart\n  Tag table creation and tag meta loading\n  Create table for PLC data loading\n  Loading PLC data\n  Tag meta name generation rules\n  Loading Tag table data\n\n\nIntroduction\n\nIntroduction\nThe TAG table can load the structure type of the file in which general sensor data is stored.\n\nThe most common types of text storage files are &lt;value, value, value&gt; &lt;value, value, value&gt;  which is a random file content that just lists a number of numeric values .\n\nIn the case of a file containing time, there are &lt;time, value, value, value&gt; &lt;time, value, value, value&gt; &lt;repeat ..&gt;.\n\nThe data in these files is created when a device called PLC (programmable logic controller) collects data from one or more sensors continuously over a long period of time.\n\nThe following is the picture of PLC example file.\n\n\n\nNow we will load this file into Machbase’s TAG table.\n\nData conversion flowchart\n\n\n\nAs you can see in the figure above, we will load the raw CSV file into Machbase’s log table and convert it into a tag table.\n\nTag table creation and tag meta loading\n\nCreate TAG table as shown below and load the tag names (tag meta) stored in the CSV file at once using a tool called tagmetaimport.\n\nNot only options that are described below, but also every options available in machloader are able to use.\n\nMach&gt; create tag table tag (name varchar(32) primary key, time datetime basetime, value double\nsummarized);\nExecuted successfully.\nElapsed time: 3.032\n \n$ cat tag_meta.csv\nMTAG_V00\nMTAG_V01\nMTAG_C00\nMTAG_C01\nMTAG_C02\nMTAG_C03\nMTAG_C04\nMTAG_C05\nMTAG_C06\nMTAG_C07\nMTAG_C08\nMTAG_C09\nMTAG_C10\nMTAG_C11\nMTAG_C12\nMTAG_C13\nMTAG_C14\nMTAG_C15\n \n$ tagmetaimport -d tag_meta.csv\nImport time : 0 hour 0 min 0.340 sec\nLoad success count : 18\n\n\n(If you changed Machbase Port number from basic port number, you should use changed port number by using -P option on tagmeraimport.)\nAs shown above, 18 tag meta information were loaded successfully.\n\nCreate table for PLC data loading\n\nExecute the following query to create the log table.\n\ncreate table plc_tag_table(\n    tm datetime,\n    V0 DOUBLE ,\n    V1 DOUBLE ,\n    C0 DOUBLE ,\n    C1 DOUBLE ,\n    C2 DOUBLE ,\n    C3 DOUBLE ,\n    C4 DOUBLE ,\n    C5 DOUBLE,\n    C6 DOUBLE ,\n    C7 DOUBLE ,\n    C8 DOUBLE ,\n    C9 DOUBLE ,\n    C10 DOUBLE ,\n    C11 DOUBLE ,\n    C12 DOUBLE ,\n    C13 DOUBLE ,\n    C14 DOUBLE ,\n    C15 DOUBLE\n);\n\n\n\n   Note that this table is a log type of table (do not get confused by file names). In Machbase, if you do not specify a separate table type, the default type of table is log.\n\n\nLoading PLC data\n\nInput the plc_tag.csv file, which contains 2 million original PLC data, using the machloader as PLC input in the log table plc_tag_table created above.\n\nIn the plc_tag.csv file, the first column is time, then V0, V1, … Columns are divided up to C15.\n\nAs for the data pattern in 1 second, about 100 pieces of data are input from 0 to 99 milliseconds, there is no input from 100 milliseconds to 999 milliseconds, and the same pattern is input for the next 1 second.\n\n$ machloader -t plc_tag_table -i -d plc_tag.csv -F \"tm YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn\"\n-----------------------------------------------------------------\nMachbase Data Import/Export Utility.\nRelease Version 5.5.0.official\nCopyright 2014, MACHBASE Corporation or its subsidiaries.\nAll Rights Reserved.\n-----------------------------------------------------------------\nNLS : US7ASCII EXECUTE MODE : IMPORT\nTARGET TABLE : plc_tag_table DATA FILE : 4_plc_tag.csv\nIMPORT MODE : APPEND FIELD TERM : ,\nROW TERM : \\n ENCLOSURE : \"\nESCAPE : \\ ARRIVAL_TIME : FALSE\nENCODING : NONE HEADER : FALSE\nCREATE TABLE : FALSE\nProgress bar Imported records Error records\n============================== 2000000 0\nImport time : 0 hour 0 min 26.544 sec\nLoad success count : 2000000\nLoad fail count : 0\n\n\nTag meta name generation rules\n\nNow you insert data into the tag table in order to  see the data through the Tag Analyzer.\n\nFor this, the insert-select statement will insert every data in plc_tag_table in tag table.\n\nThe name of each tag should be matched so it is determined as follows.\n\n\n  \n    \n      Column name of log table\n      Tag name values of tag table\n    \n  \n  \n    \n      V0\n      MTAG_V00\n    \n    \n      V1\n      MTAG_V01\n    \n    \n      C0\n      MTAG_C00\n    \n    \n      C1\n      MTAG_C01\n    \n    \n      …\n       \n    \n    \n      C15\n      MTAG_C15\n    \n  \n\n\nLoading Tag table data\n\nIt’s time to load the actual data into the tag table.\n\nQuery at the below insert data in to tag table sequentially.\n\nMach&gt; insert into tag select 'MTAG_V00', tm, v0 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 4.898\nMach&gt; insert into tag select 'MTAG_V01', tm, v1 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 5.577\nMach&gt; insert into tag select 'MTAG_C00', tm, c0 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.327\nMach&gt; insert into tag select 'MTAG_C01', tm, c1 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.445\nMach&gt; insert into tag select 'MTAG_C02', tm, c2 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.898\nMach&gt; insert into tag select 'MTAG_C03', tm, c3 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.078\nMach&gt; insert into tag select 'MTAG_C04', tm, c4 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.799\nMach&gt; insert into tag select 'MTAG_C05', tm, c5 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.210\nMach&gt; insert into tag select 'MTAG_C06', tm, c6 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 9.232\nMach&gt; insert into tag select 'MTAG_C07', tm, c7 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.398\nMach&gt; insert into tag select 'MTAG_C08', tm, c8 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.432\nMach&gt; insert into tag select 'MTAG_C09', tm, c9 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 6.734\nMach&gt; insert into tag select 'MTAG_C10', tm, c10 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.692\nMach&gt; insert into tag select 'MTAG_C11', tm, c11 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 8.628\nMach&gt; insert into tag select 'MTAG_C12', tm, c12 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 8.229\nMach&gt; insert into tag select 'MTAG_C13', tm, c13 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 9.517\nMach&gt; insert into tag select 'MTAG_C14', tm, c14 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.231\nMach&gt; insert into tag select 'MTAG_C15', tm, c15 from plc_tag_table;\n2000000 row(s) inserted.\nElapsed time: 7.830\n\n\nA total of 36 millions of records are created."
					}
					
				
		
				
					,
					
					"feature-tables-volatile-extract-html": {
						"id": "feature-tables-volatile-extract-html",
						"title": "Volatile Data Extraction",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/volatile/extract.html",
						"content": "Data Retrieval\n\nAs with other table types, data retrieval can be performed as follows.\n\nMach&gt; create volatile table vtable (id integer primary key, name varchar(20));\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(3, 'north device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(4, 'south device');\n1 row(s) inserted.\nMach&gt; select * from vtable;\nID          NAME                 \n-------------------------------------\n1           west device          \n2           east device          \n3           north device         \n4           south device         \n[4] row(s) selected.\nMach&gt; select * from vtable where id = 1;\nID          NAME                 \n-------------------------------------\n1           west device          \n[1] row(s) selected.\nMach&gt; select * from vtable where name like 'west%';\nID          NAME                 \n-------------------------------------\n1           west device          \n[1] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-tables-lookup-extract-html": {
						"id": "feature-tables-lookup-extract-html",
						"title": "Lookup Data Extraction",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/lookup/extract.html",
						"content": "The data is extracted using SQL statement and the usage method is the same as the volatile table."
					}
					
				
		
				
					,
					
					"feature-tables-log-extract-html": {
						"id": "feature-tables-log-extract-html",
						"title": "Log Data Extraction",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/extract.html",
						"content": "Machbase can extract data using standard ANSI SQL syntax and also provides an extended syntax to conveniently manipulate time series data.\n\n\n  Data Retrieval\n  Time Series Data Retrieval\n  Text Search\n  Simple Join\n  Network Data Type / Operator"
					}
					
				
		
				
					,
					
					"feature-tables-tag-manipulate-extract-html": {
						"id": "feature-tables-tag-manipulate-extract-html",
						"title": "Extract tag data",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/manipulate/extract.html",
						"content": "Index\n\n\n  Sample Schema\n  Extract all TAG data\n  Extract data for a specific tag name\n  Query for time range\n  Time range search for multiple tags\n  Search data over a certain value\n  Display Statistical Information By Specific Tag ID\n  Extraction by Using RESTful API\n    \n      Prepare for RESTful API\n      RESTful API calling convention\n      Sample for Fetching single tag data by using CURL\n      fetching multi tag data by using CURL\n    \n  \n  Specifying the search direction using hints\n    \n      Forward Search\n      Backward Search\n      Setting basic scan direction property\n    \n  \n\n\nMachbase provides high-speed tag data extraction, especially for the time range of a specific tag.\n\nSample Schema\n\nIn the following example, we created a TAG table and created two tags as shown below.\n\nFor each tag, data from January 1, 2018 to February 10, 2018 were inserted.\n\ncreate tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\n \ninsert into tag metadata values ('TAG_0001');\ninsert into tag metadata values ('TAG_0002');\n \ninsert into tag values('TAG_0001', '2018-01-01 01:00:00 000:000:000', 1);\ninsert into tag values('TAG_0001', '2018-01-02 02:00:00 000:000:000', 2);\ninsert into tag values('TAG_0001', '2018-01-03 03:00:00 000:000:000', 3);\ninsert into tag values('TAG_0001', '2018-01-04 04:00:00 000:000:000', 4);\ninsert into tag values('TAG_0001', '2018-01-05 05:00:00 000:000:000', 5);\ninsert into tag values('TAG_0001', '2018-01-06 06:00:00 000:000:000', 6);\ninsert into tag values('TAG_0001', '2018-01-07 07:00:00 000:000:000', 7);\ninsert into tag values('TAG_0001', '2018-01-08 08:00:00 000:000:000', 8);\ninsert into tag values('TAG_0001', '2018-01-09 09:00:00 000:000:000', 9);\ninsert into tag values('TAG_0001', '2018-01-10 10:00:00 000:000:000', 10);\n \ninsert into tag values('TAG_0002', '2018-02-01 01:00:00 000:000:000', 11);\ninsert into tag values('TAG_0002', '2018-02-02 02:00:00 000:000:000', 12);\ninsert into tag values('TAG_0002', '2018-02-03 03:00:00 000:000:000', 13);\ninsert into tag values('TAG_0002', '2018-02-04 04:00:00 000:000:000', 14);\ninsert into tag values('TAG_0002', '2018-02-05 05:00:00 000:000:000', 15);\ninsert into tag values('TAG_0002', '2018-02-06 06:00:00 000:000:000', 16);\ninsert into tag values('TAG_0002', '2018-02-07 07:00:00 000:000:000', 17);\ninsert into tag values('TAG_0002', '2018-02-08 08:00:00 000:000:000', 18);\ninsert into tag values('TAG_0002', '2018-02-09 09:00:00 000:000:000', 19);\ninsert into tag values('TAG_0002', '2018-02-10 10:00:00 000:000:000', 20);\n\n\nExtract all TAG data\n\nMach&gt; select * from tag;\nNAME TIME VALUE\n--------------------------------------------------------------------------------------\nTAG_0001 2018-01-01 01:00:00 000:000:000 1\nTAG_0001 2018-01-02 02:00:00 000:000:000 2\nTAG_0001 2018-01-03 03:00:00 000:000:000 3\nTAG_0001 2018-01-04 04:00:00 000:000:000 4\nTAG_0001 2018-01-05 05:00:00 000:000:000 5\nTAG_0001 2018-01-06 06:00:00 000:000:000 6\nTAG_0001 2018-01-07 07:00:00 000:000:000 7\nTAG_0001 2018-01-08 08:00:00 000:000:000 8\nTAG_0001 2018-01-09 09:00:00 000:000:000 9\nTAG_0001 2018-01-10 10:00:00 000:000:000 10\nTAG_0002 2018-02-01 01:00:00 000:000:000 11\nTAG_0002 2018-02-02 02:00:00 000:000:000 12\nTAG_0002 2018-02-03 03:00:00 000:000:000 13\nTAG_0002 2018-02-04 04:00:00 000:000:000 14\nTAG_0002 2018-02-05 05:00:00 000:000:000 15\nTAG_0002 2018-02-06 06:00:00 000:000:000 16\nTAG_0002 2018-02-07 07:00:00 000:000:000 17\nTAG_0002 2018-02-08 08:00:00 000:000:000 18\nTAG_0002 2018-02-09 09:00:00 000:000:000 19\nTAG_0002 2018-02-10 10:00:00 000:000:000 20\n[20] row(s) selected.\n\n\nIf there is no special condition as described above, data can be extracted for each tag arranged in each time order.\n\nExtract data for a specific tag name\n\nBelow is an example of data with TAG name TAG_0002.\n\nMach&gt; select * from tag where name='TAG_0002';\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0002              2018-02-01 01:00:00 000:000:000 11                         \nTAG_0002              2018-02-02 02:00:00 000:000:000 12                         \nTAG_0002              2018-02-03 03:00:00 000:000:000 13                         \nTAG_0002              2018-02-04 04:00:00 000:000:000 14                         \nTAG_0002              2018-02-05 05:00:00 000:000:000 15                         \nTAG_0002              2018-02-06 06:00:00 000:000:000 16                         \nTAG_0002              2018-02-07 07:00:00 000:000:000 17                         \nTAG_0002              2018-02-08 08:00:00 000:000:000 18                         \nTAG_0002              2018-02-09 09:00:00 000:000:000 19                         \nTAG_0002              2018-02-10 10:00:00 000:000:000 20                         \n[10] row(s) selected.\n\n\nQuery for time range\n\nThe following is a query of a time range for TAG_0002 and receives data.\n\n\n   It is common to give time range by using between clause. Of course, using ‘&lt;’ or ‘&gt;’ to get time range will get same result.\n\n\nMach&gt; select * from tag where name = 'TAG_0002' and time between to_date('2018-02-01') and to_date('2018-02-05');\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0002              2018-02-01 01:00:00 000:000:000 11                         \nTAG_0002              2018-02-02 02:00:00 000:000:000 12                         \nTAG_0002              2018-02-03 03:00:00 000:000:000 13                         \nTAG_0002              2018-02-04 04:00:00 000:000:000 14                         \n[4] row(s) selected.\n \nMach&gt; select * from tag where name = 'TAG_0002' and time &gt; to_date('2018-02-01') and time &lt; to_date('2018-02-05');\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0002              2018-02-01 01:00:00 000:000:000 11                         \nTAG_0002              2018-02-02 02:00:00 000:000:000 12                         \nTAG_0002              2018-02-03 03:00:00 000:000:000 13                         \nTAG_0002              2018-02-04 04:00:00 000:000:000 14                         \n[4] row(s) selected.\n\n\nTime range search for multiple tags\n\nBelow is an example of retrieving the same time range data for two or more tags.\n\nIf you want to get fast results for a large number of tags at the same time, it is preferable to perform the following type of query.\n\nMach&gt; select * from tag where name in ('TAG_0002', 'TAG_0001') and time between to_date('2018-01-05') and to_date('2018-02-05');\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0001              2018-01-05 05:00:00 000:000:000 5                          \nTAG_0001              2018-01-06 06:00:00 000:000:000 6                          \nTAG_0001              2018-01-07 07:00:00 000:000:000 7                          \nTAG_0001              2018-01-08 08:00:00 000:000:000 8                          \nTAG_0001              2018-01-09 09:00:00 000:000:000 9                          \nTAG_0001              2018-01-10 10:00:00 000:000:000 10                         \nTAG_0002              2018-02-01 01:00:00 000:000:000 11                         \nTAG_0002              2018-02-02 02:00:00 000:000:000 12                         \nTAG_0002              2018-02-03 03:00:00 000:000:000 13                         \nTAG_0002              2018-02-04 04:00:00 000:000:000 14                         \n[10] row(s) selected.\n\n\nSearch data over a certain value\n\nThe conditions for the tag value can also be given as follows.\n\nFiltering was performed for those values greater than 12 and less than 15 among the values of TAG_0002.\n\nMach&gt; select * from tag where name = 'TAG_0002' and value &gt; 12 and value &lt; 15 and time between to_date('2018-02-01') and to_date('2018-02-05');\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0002              2018-02-03 03:00:00 000:000:000 13                         \nTAG_0002              2018-02-04 04:00:00 000:000:000 14                         \n[2] row(s) selected.\n\n\nDisplay Statistical Information By Specific Tag ID\n\nWhen we create tag table, virtual table that aggregate simple statistic information by tag table’s tag ID is created.\n\nvirtual table name is v${tag table name}_stat.\n\nIf user uses that table, user can get tag table’s statistic information quickly.\n\nStatistical information target column is automatically designated as the third column.\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nExecuted successfully.\n \nMach&gt; DESC v$tag_stat;\n[ COLUMN ]                             \n----------------------------------------------------------------------------------------------------\nNAME                                                        NULL?    TYPE                LENGTH       \n----------------------------------------------------------------------------------------------------\nNAME                                                                 varchar             100                \nROW_COUNT                                                            ulong               20                 \nMIN_TIME                                                             datetime            31             \nMAX_TIME                                                             datetime            31             \nMIN_VALUE                                                            double              17                 \nMIN_VALUE_TIME                                                       datetime            31             \nMAX_VALUE                                                            double              17                 \nMAX_VALUE_TIME                                                       datetime            31             \nRECENT_ROW_TIME                                                      datetime            31\n\n\nIf there is no SUMMARIZED keyword in the third column, VALUE-related information (MIN_VALUE, MAX_VALUE, MIN_VALUE_TIME, MAX_VALUE_TIME) is not saved.\n\nStatistic information that is colleced is as follow.\n\n\n  \n    \n      Column name\n      Information\n    \n  \n  \n    \n      NAME\n      Tag ID’s name\n    \n    \n      ROW_COUNT\n      Number of Rows\n    \n    \n      MIN_TIME\n      The smallest basetime column value among the corresponding tag ID rows\n    \n    \n      MAX_TIME\n      The biggest basetime column value among the corresponding tag ID rows\n    \n    \n      MIN_VALUE\n      The smallest summarized column value among the corresponding tag ID rows\n    \n    \n      MIN_VALUE_TIME\n      The basetime column value that is inserted with MIN_VALUE\n    \n    \n      MAX_VALUE\n      The biggest summarized column value among the corresponding tag ID rows\n    \n    \n      MAX_VALUE_TIME\n      The basetime column value that is inserted with MAX_VALUE\n    \n    \n      RECENT_ROW_TIME\n      The basetime column value that is inserted most recently\n    \n  \n\n\nExample of select is as follow.\n\n\n  When a SUMMARIZED column exists\n\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nExecuted successfully.\n  \nMach&gt; INSERT INTO tag VALUES('tag-0', TO_DATE('2021-08-12'), 10);\nMach&gt; INSERT INTO tag VALUES('tag-0', TO_DATE('2021-08-13'), 10);\nMach&gt; INSERT INTO tag VALUES('tag-0', TO_DATE('2021-08-14'), 20);\nMach&gt; INSERT INTO tag VALUES('tag-0', TO_DATE('2021-08-11'), 5);\nMach&gt; INSERT INTO tag VALUES('tag-1', TO_DATE('2022-08-12'), 100);\nMach&gt; INSERT INTO tag VALUES('tag-1', TO_DATE('2022-08-11'), 200);\nMach&gt; INSERT INTO tag VALUES('tag-1', TO_DATE('2022-08-10'), 50);\n  \nMach&gt; SELECT * FROM v$tag_stat;\nNAME                                                                              ROW_COUNT            MIN_TIME                        MAX_TIME                        MIN_VALUE                 \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMIN_VALUE_TIME                  MAX_VALUE                   MAX_VALUE_TIME                  RECENT_ROW_TIME               \n---------------------------------------------------------------------------------------------------------------------------------\ntag-0                                                                             4                    2021-08-11 00:00:00 000:000:000 2021-08-14 00:00:00 000:000:000 5                         \n2021-08-11 00:00:00 000:000:000 20                          2021-08-14 00:00:00 000:000:000 2021-08-11 00:00:00 000:000:000\ntag-1                                                                             3                    2022-08-10 00:00:00 000:000:000 2022-08-12 00:00:00 000:000:000 50                        \n2022-08-10 00:00:00 000:000:000 200                         2022-08-11 00:00:00 000:000:000 2022-08-10 00:00:00 000:000:000\n[2] row(s) selected.\n  \n2. When a SUMMARIZED column does not exist\nMach&gt; CREATE TAG TABLE other_tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE);\nExecuted successfully.\n  \nMach&gt; INSERT INTO other_tag VALUES('tag-0', TO_DATE('2021-08-12'), 10);\nMach&gt; INSERT INTO other_tag VALUES('tag-0', TO_DATE('2021-08-13'), 10);\nMach&gt; INSERT INTO other_tag VALUES('tag-0', TO_DATE('2021-08-14'), 20);\nMach&gt; INSERT INTO other_tag VALUES('tag-0', TO_DATE('2021-08-11'), 5);\nMach&gt; INSERT INTO other_tag VALUES('tag-1', TO_DATE('2022-08-12'), 100);\nMach&gt; INSERT INTO other_tag VALUES('tag-1', TO_DATE('2022-08-11'), 200);\nMach&gt; INSERT INTO other_tag VALUES('tag-1', TO_DATE('2022-08-10'), 50);\n  \nMach&gt; SELECT * FROM v$other_tag_stat;\nNAME                                                                              ROW_COUNT            MIN_TIME                        MAX_TIME                        MIN_VALUE                 \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nMIN_VALUE_TIME                  MAX_VALUE                   MAX_VALUE_TIME                  RECENT_ROW_TIME               \n---------------------------------------------------------------------------------------------------------------------------------\ntag-0                                                                             4                    2021-08-11 00:00:00 000:000:000 2021-08-14 00:00:00 000:000:000 NULL                      \nNULL                            NULL                        NULL                            2021-08-11 00:00:00 000:000:000\ntag-1                                                                             3                    2022-08-10 00:00:00 000:000:000 2022-08-12 00:00:00 000:000:000 NULL                      \nNULL                            NULL                        NULL                            2022-08-10 00:00:00 000:000:000\n[2] row(s) selected.\n\n\nExtraction by Using RESTful API\n\nPrepare for RESTful API\n\nSpecify the values of the properties blow and start the server.\n\nmachbase.conf\n\nHTTP_ENABLE = 1\nHTTP_PORT_NO = 5678\n\n\nRESTful API calling convention\n\nSELECT FORM\n\n{MWA URL}/machiot-rest-api/datapoints/raw/{TagName}/{Start}/{End}/{Direction}/{Count}/{Offset}/ \n \nTagName    : Tag Name. multiple tag available(Seperated by ',')\nStart, End : range, YYYY-MM-DD HH24:MI:SS or YYYY-MM-DD or YYYY-MM-DD HH24:MI:SS,mmm (mmm: millisecond, When omitted start is 000, End is 999, micro and nano is 999)\nWhen using real string, put 'T' between time and date to remove blank.\nDirection  : 0(ascending), support in future (time increase)\nCount      : LIMIT, whole if 0\nOffset     : offset (default = 0)\n\n\nSample for Fetching single tag data by using CURL\n\nCall for machbase installed in 192.168.0.148 as follow, the data can be retrieved from the web.\n\nSingle Tag\n\n$ curl -G \"http://192.168.0.148:5001/machiot-rest-api/v1/datapoints/raw/TAG_0001/2018-01-01T00:00:00/2018-01-06T00:00:00\"\n \n{\"ErrorCode\": 0,\n \"ErrorMessage\": \"\",\n \"Data\": [{\"DataType\": \"DOUBLE\",\n \"ErrorCode\": 0,\n \"TagName\": \"TAG_0001\",\n \"CalculationMode\": \"raw\",\n \"Samples\": [{\"TimeStamp\": \"2018-01-01 01:00:00 000:000:000\", \"Value\": 1.0, \"Quality\": 1},\n             {\"TimeStamp\": \"2018-01-02 02:00:00 000:000:000\", \"Value\": 2.0, \"Quality\": 1},\n             {\"TimeStamp\": \"2018-01-03 03:00:00 000:000:000\", \"Value\": 3.0, \"Quality\": 1},\n             {\"TimeStamp\": \"2018-01-04 04:00:00 000:000:000\", \"Value\": 4.0, \"Quality\": 1},\n             {\"TimeStamp\": \"2018-01-05 05:00:00 000:000:000\", \"Value\": 5.0, \"Quality\": 1}]}]\n}\n\n\nfetching multi tag data by using CURL\n\nFollow is sample for fetching two tag values.\n\n$ curl -G \"http://192.168.0.148:5001/machiot-rest-api/datapoints/raw/TAG_0001,TAG_0002/2018-01-05T00:00:00/2018-02-05T00:00:00\"\n{\"ErrorCode\": 0,\n \"ErrorMessage\": \"\",\n \"Data\": [{\"DataType\": \"DOUBLE\",\n           \"ErrorCode\": 0,\n           \"TagName\": \"TAG_0001,TAG_0002\",\n           \"CalculationMode\": \"raw\",\n           \"Samples\": [{\"TimeStamp\": \"2018-01-05 05:00:00 000:000:000\", \"Value\": 5.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-06 06:00:00 000:000:000\", \"Value\": 6.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-07 07:00:00 000:000:000\", \"Value\": 7.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-08 08:00:00 000:000:000\", \"Value\": 8.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-09 09:00:00 000:000:000\", \"Value\": 9.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-01-10 10:00:00 000:000:000\", \"Value\": 10.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-02-01 01:00:00 000:000:000\", \"Value\": 11.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-02-02 02:00:00 000:000:000\", \"Value\": 12.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-02-03 03:00:00 000:000:000\", \"Value\": 13.0, \"Quality\": 1},\n                       {\"TimeStamp\": \"2018-02-04 04:00:00 000:000:000\", \"Value\": 14.0, \"Quality\": 1}\n]}]}\n\n\nSpecifying the serach direction using hints\n\nIn general, the tag table can be searched starting with the oldest record. If you want to search from the most recently inserted record, you can use hints to control the search direction.\n\nForward Search\n\ndefault, search by using ‘/*+ SCAN_FORWARD(table_name) */’ hint.\n\nMach&gt; SELECT * FROM tag WHERE t_name='TAG_99' LIMIT 10;\nT_NAME                T_TIME                          T_VALUE                    \n--------------------------------------------------------------------------------------\nTAG_99                2017-01-01 00:00:49 500:000:000 0                          \nTAG_99                2017-01-01 00:01:39 500:000:000 1                          \nTAG_99                2017-01-01 00:02:29 500:000:000 2                          \nTAG_99                2017-01-01 00:03:19 500:000:000 3                          \nTAG_99                2017-01-01 00:04:09 500:000:000 4                          \nTAG_99                2017-01-01 00:04:59 500:000:000 5                          \nTAG_99                2017-01-01 00:05:49 500:000:000 6                          \nTAG_99                2017-01-01 00:06:39 500:000:000 7                          \nTAG_99                2017-01-01 00:07:29 500:000:000 8                          \nTAG_99                2017-01-01 00:08:19 500:000:000 9                          \n[10] row(s) selected.\nElapsed time: 0.001\n \nMach&gt; SELECT /*+ SCAN_FORWARD(tag) */  * FROM tag WHERE t_name='TAG_99' LIMIT 10;\nT_NAME                T_TIME                          T_VALUE                    \n--------------------------------------------------------------------------------------\nTAG_99                2017-01-01 00:00:49 500:000:000 0                          \nTAG_99                2017-01-01 00:01:39 500:000:000 1                          \nTAG_99                2017-01-01 00:02:29 500:000:000 2                          \nTAG_99                2017-01-01 00:03:19 500:000:000 3                          \nTAG_99                2017-01-01 00:04:09 500:000:000 4                          \nTAG_99                2017-01-01 00:04:59 500:000:000 5                          \nTAG_99                2017-01-01 00:05:49 500:000:000 6                          \nTAG_99                2017-01-01 00:06:39 500:000:000 7                          \nTAG_99                2017-01-01 00:07:29 500:000:000 8                          \nTAG_99                2017-01-01 00:08:19 500:000:000 9                          \n[10] row(s) selected.\nElapsed time: 0.001\nMach&gt;\n\n\nBackward Search\n\nsearch by using ‘/*+ SCAN_BACKWARD(table_name) */’ hint.\n\nMach&gt; SELECT /*+ SCAN_BACKWARD(tag) */ * FROM tag WHERE t_name='TAG_99' LIMIT 10;\nT_NAME                T_TIME                          T_VALUE                    \n--------------------------------------------------------------------------------------\nTAG_99                2017-02-27 20:53:19 500:000:000 9                          \nTAG_99                2017-02-27 20:52:29 500:000:000 8                          \nTAG_99                2017-02-27 20:51:39 500:000:000 7                          \nTAG_99                2017-02-27 20:50:49 500:000:000 6                          \nTAG_99                2017-02-27 20:49:59 500:000:000 5                          \nTAG_99                2017-02-27 20:49:09 500:000:000 4                          \nTAG_99                2017-02-27 20:48:19 500:000:000 3                          \nTAG_99                2017-02-27 20:47:29 500:000:000 2                          \nTAG_99                2017-02-27 20:46:39 500:000:000 1                          \nTAG_99                2017-02-27 20:45:49 500:000:000 0                          \n[10] row(s) selected.\nElapsed time: 0.001\nMach&gt;\n\n\nSetting basic scan direction property\n\nBy using TABLE_SCAN_DIRECTION property, user can set tag table scan direction when there is no hint in select query."
					}
					
				
		
				
					,
					
					"intro-features-html": {
						"id": "intro-features-html",
						"title": "Machbase Features",
						"version": "all",
						"categories": "",
						"url": " /intro/features.html",
						"content": "Index\n\n\n  Support for Various Table Structures\n  Hardware Support for Various Sizes\n  Tag Analyzer: Data Visualization Solution Support\n  Write Once, Read Many\n  Lock-free Architecture Support\n  High Speed Data Storage\n  STREAM Function Support\n  Configuring Real-Time Index\n  Real-Time Data Compression\n  Outstanding Query Performance\n  Time Series Data Characteristics SQL Syntax Support\n  Supports Text Search Function\n  Optional Deletion Support\n  Automated Data Collection\n\n\nSupport for Various Table Structures\n\nMachbase provides four table types for users according to one’s usage. (Tag, Log, Volatile, Lookup)\n\nThis is because client requirements for storing sensor data are very diverse and one business does not have just one specific data pattern.\nTherefore, it’s important to understand these business requirements and select the appropriate tables for them.\nThe table below shows the characteristics of each table.\n\n\n  \n    \n      Table Type\n      Tag Table\n      Log Table\n      Volatile Table\n      Lookup Table\n    \n  \n  \n    \n      PURPOSE\n      Optimized for processing sensor time series data in the form of &lt;sensor name, time, sensor value&gt;\n      Optimized for processing PLC log time series data  (text included)\n      Real-time processing of volatile memory data\n      Manages master data that can be stored permanently\n    \n    \n      DESCRIPTION\n      Used when storing sensor data at high speed, extracting corresponding data at high speed, or creating statistical tables in real-time  Mainly stores real-time sensor data\n      Used when storing log data including text and analyzing it in the form of general DBMS  Mainly stores historical user data\n      Used when Insert, Delete, Update, Select is required for memory-based performance (tens of thousands per second)  All data is lost when the system is shut down.  Mainly used for key-value based monitoring.\n      Used to permanently store user-editable master data.  SELECT has high-speed performance, but INSERT, UPDATE, and DELETE provide disk-based performance.\n    \n    \n      TABLE STRUCTURE\n      &lt;Sensor name, time, sensor value&gt; is the  basic type, with the ability for assigning additional columns.\n      Any schema possible\n      Any schema possible (Primary Key can be assigned)\n       \n    \n    \n      INSERT (INPUT) PERFORMANCE\n      Millions per second\n      Millions per second\n      Tens of Thousands per second\n      Hundreds per second\n    \n    \n      SELECT\n      Sensor Name + Limited Time Range\n      All inquiries possible\n       \n       \n    \n    \n      DELETE\n      Real time deletion of data before an arbitrary point\n      Real time deletion of arbitrary point / interval data\n      Primary Key Record Delete Support (※ Primary Key Designation Required)\n       \n    \n    \n      UPDATE\n      Not supported (※ Only Metadata column is editable)\n      Not supported\n      Primary Key Update Support (※ Primary Key Designation Required)\n       \n    \n    \n      STORAGE SIZE LIMITS\n      Disk limit\n      Memory limit\n       \n       \n    \n    \n      INDEX STRUCTURE\n      Three-step partitioning real-time index(※ default creation)\n      LSM index\n      Red/black memory index\n       \n    \n    \n      STREAM SUPPORT\n      Target only (save target)\n      Both source / target (read and save target)\n      Not possible\n       \n    \n    \n      CONSIDERATIONS\n      Consider enough storage to erase historical data\n      Consider as temporary storage for Tag input\n      Consider memory limit\n       \n    \n  \n\n\nHardware Support for Various Sizes\n\nMachbase provides various product editions according to user environments as listed below.\n\nStandard Edition\n\nThis product is used to achieve high-speed data processing on a single server.\n\nIt runs on Windows or Linux operating systems based on Intel x86 CPU and provides very fast sensor data storage and analysis that other DBMSs can not provide.\n\nIn most cases, it is used to store real-time data input from hundreds or more of edge devices and to perform secondary analysis.\n\nCluster Edition\n\nThis product was developed for the purpose of storing large-scale sensor data for large manufacturing plants.\n\nA number of physical servers operate in clusters to store more than 10 million data per second in semiconductor or display, power generation, and steel production processes.\n\nIt is used in an increasingly data-rich environment where data capacity needs to be continuously maintained.\n\nTag Analyzer: Data Visualization Solution Support\n\nMachbase provides real-time visualization of tens of billions of sensor data stored in Machbase (since Version 5).\n\nIn other words, an arbitrary tag ID is designated, and the trend chart for the period in which the ID is input can be instantaneously checked on the web-based basis.\n\nIn addition, it provides not only simple tag data but also a statistical chart during that period, so statistical analysis is possible beyond simple visualization.\n\n\n\nWrite Once, Read Many\n\nSensor data is rarely edited or deleted once it is entered into the database.\n\nTherefore, Machbase is designed so that once the key time series data is inputted to maximize the characteristics of the machine data, an UPDATE can not occur.\n\nOnce the log data has been entered, it cannot be altered or deleted by malicious users, so there should be no concerns.\n\nLock-free Architecture Support\n\nThe most important aspect in sensor data processing is that data input, update, delete operation and read operation should be processed as independent as possible without conflicts.\n\nBecause of this, Machbase is designed not to allocate any locks for the SELECT operation, and it is designed with a high performance structure that never conflicts with the operation of input or deletion changes.\n\nTherefore, even when hundreds of thousands of data are entered and some of them are deleted in real time, the SELECT operation can speed up statistical operations on millions of records.\n\nHigh Speed Data Storage\n\nMachbase provides data storage performance that is exponentially faster than conventional databases. Even if there are many indexes in a specific table, data can be received from at least 300,000 to at most 2 million per second.\n\nThis is possible because Machbase is designed to optimize time series data.\n\nSTREAM Function Support\n\nSince Machbase Version 5, Standard Edition provides STREAM functionality to support real-time data filtering.\n\nThis STREAM performs a condition evaluation on real-time data input in DBMS at high speed and transmits the result to an arbitrary table.\n\nThis function is very useful for generating a warning when the value of a certain sensor exceeds a specific range or real time evaluation of internally input data is needed.\n\nConfiguring Real-Time Index\n\nMachbase innovatively improves on conventional database structure (where the more indexes you have the slower your data insert performance is) and can build indexes in near real-time, even with hundreds of thousands of data inserts per second.\n\nThis feature is a key technology for analyzing time series data, such as machine data, because it provides a powerful functional foundation for instant retrieval of actual data as it occurs.\n\nReal-Time Data Compression\n\nThe characteristic of time series data such as machine data is that data is generated constantly. This inevitably means that not only will the storage space of the database becomes eventually inadequate, but it will not have enough data to process.\n\nIn particular, although conventional databases input data at a high speed, as the number of indexes increases, the occupied data space also greatly increases. Therefore, conventional databases are quite unsuitable for storing and analyzing machine data.\nMachbase uses two innovative real-time compression techniques to compress and store up to a hundred times more data without any setbacks in performance.\n\nLogical Real-Time Data Compression Technology Support\nFirst, Machbase supports logical real-time data compression technology.\n\nThis is based on the data redundancy of the machine data derived from a column-type database. It is an innovative technique to reduce the data storage space by coding redundant data as the number of data having the same value increases, which allows high redundancy data to be compressed hundreds of times the original amount.\n\nPhysical Data Compression Technology (Patented Technology)\nThe second is Machbase’s patented physical data compression technology.\n\nThis is a technology that reduces the amount of physical data to be stored by dividing a physical data block to be stored in a disk into a predetermined size partition, compressing it into a disk separately, and further reducing the I/O cost caused by the system. This helps to increase the efficiency of the storage space by compressing the actual logically compressed data once more.\n\nOutstanding Query Performance\nThe innovative and technological superiority of Machbase is that the search and statistical analysis of millions or tens of millions of previously stored historical data is very fast, even with the simultaneous input of hundreds of thousands of data per second.\n\nThis is possible because of Machbase’s own indexing technology that provides superior performance for both insertion and analysis, and will play a key role in real-time business decision making.\n\nUnlike conventional databases, Machbase can process two or more indexes in a single query, which can be expected to perform several times faster when processing data in parallel.\n\nThe following is an example of using two or more indexes in a single query.\nSELECT * FROM table1 WHERE c1 = 1 and c2 = 2;\n\n\nTime Series Data Characteristics SQL Syntax Support\n\nIn the case of sensor data, the newest data is several times more valuable than the older data, and also the “access frequency” of the latest data is characterized as being several times more compared to old data.\n\nFor this reason, Machbase supports time series data features through two types of tables: Tag and Log.\n\nLog Table\n\nThe log table supported by Machbase has the following features.\n\nFirst, it automatically saves input time\nWhenever a record is stored in the database, a timestamp in nanoseconds is stored as a field called _arrival_time.\n\nThis means that all records stored by Machbase can be searched for or given condition on a time basis.\n\nSecond, it prioritizes lookup of recent data\n\nWhen retrieving data, the latest time is output before the old time. That is, when SELECT is performed, the latest data is output first.\n\nThe result is the descending sort based on the _arrival_time column mentioned earlier.\n\nThird, the DURATION keyword\n\nThe DURATION keyword is provided to enable quick lookup of specific time range data based on input time.\n\nIn the case of machine data analysis, these characteristics are provided at the SQL level because they often specify a specific time range.\n\nThis makes it easy to analyze data without stating “where” clause to complex time operators.\n\n-- Example 1)  View data statistics from 10 minutes ago\nSELECT SUM(traffic) FROM t1 DURATION 10 MINUTE;\n \n-- Example 2) View data statistics for 30 minutes from 1 hour ago\nSELECT SUM(traffic) FROM t1 DURATION 30 MINUTE BEFORE 1 HOUR;\n\n\nTag Table\n\nThe tag table that is supported from Machbase 5.0 has the following features.\n\nFirst, high-speed TAGID / time condition search performance\n\nThe tag table is excellent at any time and any ID based search performance.\n\nIt boasts ultra-fast data extraction performance that can not be achieved with existing RDBMSs, ensuring the same speed even when billions of sensor data are stored.\n\nSecond, the high-speed tag data input\n\nThe tag table supports high-speed data input.\n\nAs in the previous log table, data can be input without difficulty even with the input of hundreds of thousands of sensor data per second.\n\nThird, real-time statistics function\n\nThe tag table supports real-time statistics function.\n\nMachbase automatically generates five types of statistics in real time for the data stored in this tag table and provides a function to access them in real time.\n\nSupports Text Search Function\n\nOne of the most important practical uses for users to store and use logarithmic time series data is to determine if a specific event occurred at a particular point in time.\n\nTime-series data processing is possible at a specific point in time, but in most cases the occurrence of a specific event requires searching for a specific “word” in a text field stored in a particular column.\n\nHowever, in a traditional database, in order to search for a word in a specific field, the exact match or LIKE clause is used to check the condition of some initial character through B + Tree. In most cases, this results in a very slow response.\n\nThat’s why searching for a particular word in a conventional database is very weak and frustrating.\n\nOn the other hand, with Machbase, the SEARCH keyword based on the log table is provided to enable real-time word search.\n\nThis makes it possible to quickly search for any error text generated from the equipment.\n\n-- Example 1) Output record containing Error or 102 in msg field\nSELECT id, ipv4 FROM devices WHERE msg SEARCH 'Error' or msg SEARCH '102';\n \n-- Example 2) Output record containing Error and 102 in msg field\nSELECT id, ipv4 FROM devices WHERE msg SEARCH 'Error 102';\n\n\nOptional Deletion Support\n\nIn the case of sensor data, it is true that deletion operations are rarely generated after insertion.\n\nHowever, with embedded devices, there is a limited storage space that is not carefully managed by users.\n\nIn this case, if a ‘disk full’ occurs or a failure occurs due to machine data, the company could suffer a lot of damage.\n\nMachbase provides the ability to delete records for a given condition in this environment.\n\nTherefore, embedded developers can use CRON or periodic programs to easily manage Machbase to not keep data over a certain size.\n\nFor Log Tables\n\nThe following commands are supported:\n\n-- Example 1) Delete oldest last 100.\nDELETE FROM devices OLDEST 100 ROWS;\n \n-- Example 2) Delete all but 1000 most recent.\nDELETE FROM devices EXCEPT 1000 ROWS;\n \n-- Example 3) Delete all of them from now on except one day.\nDELETE FROM devices EXCEPT 1 DAY;\n \n-- Example 4) Delete all data from before June 1, 2014.\nDELETE FROM devices BEFORE TO_DATE('2014-06-01', 'YYYY-MM-DD');\n\n\nFor Tag Tables\n\nThe following command is supported:\n\n-- Delete all data from before June 15, 2016.\nDELETE FROM tag BEFORE TO_DATE('2016-06-15', 'YYYY-MM-DD');\n\n\nAutomated Data Collection\n\nMachbase provides a “Collector” function that reads data from scattered machine data log files and automatically transfers them.\n\nIt not only collects pre-formatted data such as syslog and web server logs, but also provides a function that can be easily converted and automatically collected even if the log format is arbitrarily defined by the user."
					}
					
				
		
				
					,
					
					"feed-xml": {
						"id": "feed-xml",
						"title": "",
						"version": "all",
						"categories": "",
						"url": " /feed.xml",
						"content": "&lt;img src=&quot;/en/assets/img/logo.png&quot; alt=&quot;Machbase manual&quot;&gt;\n    machbase-manual documents.\n\n    /en/\n    \n    Wed, 19 Jul 2023 09:32:07 +0900\n    Wed, 19 Jul 2023 09:32:07 +0900\n    Jekyll v3.9.3"
					}
					
				
		
				
					,
					
					"sql-ref-func-html": {
						"id": "sql-ref-func-html",
						"title": "Functions",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/func.html",
						"content": "Index\n\n\n  ABS\n  ADD_TIME\n  AVG\n  BITAND / BITOR\n  COUNT\n  DATE_TRUNC\n  DAYOFWEEK\n  DECODE\n  FIRST / LAST\n  FROM_UNIXTIME\n  FROM_TIMESTAMP\n  GROUP_CONCAT\n  INSTR\n  LEAST / GREATEST\n  LENGTH\n  LOWER\n  LPAD / RPAD\n  LTRIM / RTRIM\n  MAX\n  MIN\n  NVL\n  ROUND\n  ROWNUM\n  SERIESNUM\n  STDDEV / STDDEV_POP\n  SUBSTR\n  SUBSTRING_INDEX\n  SUM\n  SUMSQ\n  SYSDATE / NOW\n  TO_CHAR\n  TO_DATE\n  TO_DATE_SAFE\n  TO_HEX\n  TO_IPV4 / TO_IPV4_SAFE\n  TO_IPV6 / TO_IPV6_SAFE\n  TO_NUMBER / TO_NUMBER_SAFE\n  TO_TIMESTAMP\n  TRUNC\n  TS_CHANGE_COUNT\n  UNIX_TIMESTAMP\n  UPPER\n  VARIANCE / VAR_POP\n  YEAR / MONTH / DAY\n  ISNAN / ISINF\n  Support Type of Built-In Function\n  JSON-related function\n  JSON Operator\n\n\nABS\n\nThis function works on a numeric column, converts it to a positive value, and returns the value as a real number.\n\nABS(column_expr)\n\n\nMach&gt; CREATE TABLE abs_table (c1 INTEGER, c2 DOUBLE, c3 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO abs_table VALUES(1, 1.0, '');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO abs_table VALUES(2, 2.0, 'sqltest');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO abs_table VALUES(3, 3.0, 'sqltest');\n1 row(s) inserted.\n \nMach&gt; SELECT ABS(c1), ABS(c2) FROM abs_table;\nSELECT ABS(c1), ABS(c2) from abs_table;\nABS(c1)                     ABS(c2)\n-----------------------------------------------------------\n3                           3\n2                           2\n1                           1\n[3] row(s) selected.\n\n\nADD_TIME\n\nThis function performs a date and time operation on a given datetime column. Supports increment/decrement operations up to year, month, day, hour, minute, and second, and does not support operations on milli, micro, and nanoseconds. The Diff format is: “Year/Month/Day Hour:Minute:Second”. Each item has a positive or negative value.\n\nADD_TIME(column,time_diff_format)\n\n\nMach&gt; CREATE TABLE add_time_table (id INTEGER, dt DATETIME);\nCreated successfully.\n \nMach&gt; INSERT INTO  add_time_table VALUES(1, TO_DATE('1999-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(2, TO_DATE('2000-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(3, TO_DATE('2012-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(4, TO_DATE('2013-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(5, TO_DATE('2014-12-30 11:22:33 444:555:666'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  add_time_table VALUES(6, TO_DATE('2014-12-30 23:22:33 444:555:666'));\n1 row(s) inserted.\n \nMach&gt; SELECT ADD_TIME(dt, '1/0/0 0:0:0') FROM add_time_table;\nADD_TIME(dt, '1/0/0 0:0:0')\n----------------------------------\n2015-12-30 23:22:33 444:555:666\n2015-12-30 11:22:33 444:555:666\n2014-11-11 01:02:03 004:005:006\n2013-11-11 01:02:03 004:005:006\n2001-11-11 01:02:03 004:005:006\n2000-11-11 01:02:03 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '0/0/0 1:1:1') FROM add_time_table;\nADD_TIME(dt, '0/0/0 1:1:1')\n----------------------------------\n2014-12-31 00:23:34 444:555:666\n2014-12-30 12:23:34 444:555:666\n2013-11-11 02:03:04 004:005:006\n2012-11-11 02:03:04 004:005:006\n2000-11-11 02:03:04 004:005:006\n1999-11-11 02:03:04 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '1/1/1 0:0:0') FROM add_time_table;\nADD_TIME(dt, '1/1/1 0:0:0')\n----------------------------------\n2016-01-31 23:22:33 444:555:666\n2016-01-31 11:22:33 444:555:666\n2014-12-12 01:02:03 004:005:006\n2013-12-12 01:02:03 004:005:006\n2001-12-12 01:02:03 004:005:006\n2000-12-12 01:02:03 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '-1/0/0 0:0:0') FROM add_time_table;\nADD_TIME(dt, '-1/0/0 0:0:0')\n----------------------------------\n2013-12-30 23:22:33 444:555:666\n2013-12-30 11:22:33 444:555:666\n2012-11-11 01:02:03 004:005:006\n2011-11-11 01:02:03 004:005:006\n1999-11-11 01:02:03 004:005:006\n1998-11-11 01:02:03 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '0/0/0 -1:-1:-1') FROM add_time_table;\nADD_TIME(dt, '0/0/0 -1:-1:-1')\n----------------------------------\n2014-12-30 22:21:32 444:555:666\n2014-12-30 10:21:32 444:555:666\n2013-11-11 00:01:02 004:005:006\n2012-11-11 00:01:02 004:005:006\n2000-11-11 00:01:02 004:005:006\n1999-11-11 00:01:02 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(dt, '-1/-1/-1 0:0:0') FROM add_time_table;\nADD_TIME(dt, '-1/-1/-1 0:0:0')\n----------------------------------\n2013-11-29 23:22:33 444:555:666\n2013-11-29 11:22:33 444:555:666\n2012-10-10 01:02:03 004:005:006\n2011-10-10 01:02:03 004:005:006\n1999-10-10 01:02:03 004:005:006\n1998-10-10 01:02:03 004:005:006\n[6] row(s) selected.\n \nMach&gt; SELECT * FROM add_time_table WHERE dt &gt; ADD_TIME(TO_DATE('2014-12-30 11:22:33 444:555:666'), '-1/-1/-1 0:0:0');\nID          DT\n-----------------------------------------------\n6           2014-12-30 23:22:33 444:555:666\n5           2014-12-30 11:22:33 444:555:666\n[2] row(s) selected.\n \nMach&gt; SELECT * FROM add_time_table WHERE dt &gt; ADD_TIME(TO_DATE('2014-12-30 11:22:33 444:555:666'), '-1/-2/-1 0:0:0');\nID          DT\n-----------------------------------------------\n6           2014-12-30 23:22:33 444:555:666\n5           2014-12-30 11:22:33 444:555:666\n4           2013-11-11 01:02:03 004:005:006\n[3] row(s) selected.\n \nMach&gt; SELECT ADD_TIME(TO_DATE('2000-12-01 00:00:00 000:000:001'), '-1/0/0 0:0:-1') FROM add_time_table;\nADD_TIME(TO_DATE('2000-12-01 00:00:00 000:000:001'), '-1/0/0 0:0:-1')\n------------------------------------------\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n1999-11-30 23:59:59 000:000:001\n[6] row(s) selected.\n \nMach&gt; SELECT * FROM add_time_table WHERE dt &gt; ADD_TIME(TO_DATE('2014-12-30 11:22:33 444:555:666'), '-1/-2/-1 0:0:0');\nID          DT\n-----------------------------------------------\n6           2014-12-30 23:22:33 444:555:666\n5           2014-12-30 11:22:33 444:555:666\n4           2013-11-11 01:02:03 004:005:006\n[3] row(s) selected.\n\n\nAVG\n\nThis function is an aggregate function that operates on a numeric column and prints the average value of that column.\n\nAVG(column_name)\n\n\nMach&gt; CREATE TABLE avg_table (id1 INTEGER, id2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO avg_table VALUES(1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(1, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(2, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(2, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO avg_table VALUES(null, 4);\n1 row(s) inserted.\n \nMach&gt; SELECT id1, AVG(id2) FROM avg_table GROUP BY id1;\nid1         AVG(id2)\n-------------------------------------------\n2                2\nNULL             4\n1                2\n\n\nBITAND / BITOR\n\nThis function converts two input values ​​to a 64-bit signed integer and returns the result of bitwise and/or. The input value must be an integer and the output value is a 64-bit signed integer.\n\nFor integer values ​​less than 0, it is recommended to use only uinteger and ushort types, because different results may be obtained depending on the platform.\n\nBITAND (&lt;expression1&gt;, &lt;expression2&gt;)\nBITOR (&lt;expression1&gt;, &lt;expression2&gt;)\n\n\nMach&gt; CREATE TABLE bit_table (i1 INTEGER, i2 UINTEGER, i3 FLOAT, i4 DOUBLE, i5 SHORT, i6 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO bit_table VALUES (-1, 1, 1, 1, 2, 'aaa');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO bit_table VALUES (-2, 2, 2, 2, 3, 'bbb');\n1 row(s) inserted.\n \nMach&gt; SELECT BITAND(i1, i2) FROM bit_table;\nBITAND(i1, i2)\n-----------------------\n2\n1\n[2] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITAND(i2, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n-1          1           1                           1                           2           aaa\n[1] row(s) selected.\n \nMach&gt; SELECT BITOR(i5, 1) FROM bit_table WHERE BITOR(i5, 1) = 3;\nBITOR(i5, 1)\n-----------------------\n3\n3\n[2] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITOR(i2, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n-1          1           1                           1                           2           aaa\n[1] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITAND(i3, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n[ERR-02037 : Function [BITAND] argument data type is mismatched.]\n[0] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITAND(i4, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n[ERR-02037 : Function [BITAND] argument data type is mismatched.]\n[0] row(s) selected.\n \nMach&gt; SELECT BITAND(i5, 1) FROM bit_table WHERE BITAND(i5, 1) = 1;\nBITAND(i5, 1)\n-----------------------\n1\n[1] row(s) selected.\n \nMach&gt; SELECT * FROM bit_table WHERE BITOR(i6, 1) = 1;\nI1          I2          I3                          I4                          I5          I6\n---------------------------------------------------------------------------------------------------------------\n[ERR-02037 : Function [BITOR] argument data type is mismatched.]\n[0] row(s) selected.\n \nMach&gt; SELECT BITOR(i1, i2) FROM bit_table;\nBITOR(i1, i2)\n-----------------------\n-2\n-1\n[2] row(s) selected.\n \nMach&gt; SELECT BITAND(i1, i3) FROM bit_table;\nBITAND(i1, i3)\n-----------------------\n[ERR-02037 : Function [BITAND] argument data type is mismatched.]\n[0] row(s) selected.\n \nMach&gt; SELECT BITOR(i1, i6) FROM bit_table;\nBITOR(i1, i6)\n-----------------------\n[ERR-02037 : Function [BITOR] argument data type is mismatched.]\n[0] row(s) selected.\n\n\nCOUNT\n\nThis function is an aggregate function that obtains the number of records in a given column.\n\nCOUNT(column_name)\n\n\nMach&gt; CREATE TABLE count_table (id1 INTEGER, id2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO count_table VALUES(1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(1, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(2, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(2, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO count_table VALUES(null, 4);\n1 row(s) inserted.\n \nMach&gt; SELECT COUNT(*) FROM count_table;\nCOUNT(*)\n-----------------------\n7\n[1] row(s) selected.\n \nMach&gt; SELECT COUNT(id1) FROM count_table;\nCOUNT(id1)\n-----------------------\n6\n[1] row(s) selected.\n\n\nDATE_TRUNC\n\nThis function returns a given datetime value as a new datetime value that is displayed only up to ‘time unit’ and ‘time range’.\n\nDATE_TRUNC (field, date_val [, count])\n\n\nMach&gt; CREATE TABLE trunc_table (i1 INTEGER, i2 DATETIME);\nCreated successfully.\n \nMach&gt; INSERT INTO trunc_table VALUES (1, TO_DATE('1999-11-11 1:2:0 4:5:1'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (2, TO_DATE('1999-11-11 1:2:0 5:5:2'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (3, TO_DATE('1999-11-11 1:2:1 6:5:3'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (4, TO_DATE('1999-11-11 1:2:1 7:5:4'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (5, TO_DATE('1999-11-11 1:2:2 8:5:5'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (6, TO_DATE('1999-11-11 1:2:2 9:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (7, TO_DATE('1999-11-11 1:2:3 10:5:7'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO trunc_table VALUES (8, TO_DATE('1999-11-11 1:2:3 11:5:8'));\n1 row(s) inserted.\n \nMach&gt; SELECT COUNT(*), DATE_TRUNC('second', i2) tm FROM trunc_table group by tm ORDER BY 2;\nCOUNT(*)             tm\n--------------------------------------------------------\n2                    1999-11-11 01:02:00 000:000:000\n2                    1999-11-11 01:02:01 000:000:000\n2                    1999-11-11 01:02:02 000:000:000\n2                    1999-11-11 01:02:03 000:000:000\n[4] row(s) selected.\n \nMach&gt; SELECT COUNT(*), DATE_TRUNC('second', i2, 2) tm FROM trunc_table group by tm ORDER BY 2;\nCOUNT(*)             tm\n--------------------------------------------------------\n4                    1999-11-11 01:02:00 000:000:000\n4                    1999-11-11 01:02:02 000:000:000\n[2] row(s) selected.\n \nMach&gt; SELECT COUNT(*), DATE_TRUNC('nanosecond', i2, 2) tm FROM trunc_table group by tm ORDER BY 2;\nCOUNT(*)             tm\n--------------------------------------------------------\n1                    1999-11-11 01:02:00 004:005:000\n1                    1999-11-11 01:02:00 005:005:002\n1                    1999-11-11 01:02:01 006:005:002\n1                    1999-11-11 01:02:01 007:005:004\n1                    1999-11-11 01:02:02 008:005:004\n1                    1999-11-11 01:02:02 009:005:006\n1                    1999-11-11 01:02:03 010:005:006\n1                    1999-11-11 01:02:03 011:005:008\n[8] row(s) selected.\n \nMach&gt; SELECT COUNT(*), DATE_TRUNC('nsec', i2, 1000000000) tm FROM trunc_table group by tm ORDER BY 2; //DATE_TRUNC('sec', i2, 1) 과같음\nCOUNT(*)             tm\n--------------------------------------------------------\n2                    1999-11-11 01:02:00 000:000:000\n2                    1999-11-11 01:02:01 000:000:000\n2                    1999-11-11 01:02:02 000:000:000\n2                    1999-11-11 01:02:03 000:000:000\n[4] row(s) selected.\n\n\nThe allowable time ranges for time units and time units are as follows.\n\n\n  \n    \n      Time Unit\n      Time Range\n    \n  \n  \n    \n      nanosecond (nsec)\n      1000000000 (1 second)\n    \n    \n      microsecond (usec)\n      60000000 (60 seconds)\n    \n    \n      milisecond (msec)\n      60000 (60 seconds)\n    \n    \n      second (sec)\n      86400 (1 day)\n    \n    \n      minute (min)\n      1440 (1 day)\n    \n    \n      hour\n      24 (1 day)\n    \n    \n      day\n      1\n    \n    \n      month\n      1\n    \n    \n      year\n      1\n    \n  \n\n\nFor example, if you type in DATE_TRUNC(‘second’, time, 120), the value returned will be displayed every two minutes and is the same as DATE_TRUNC(‘minute’, time, 2).\n\nDAYOFWEEK\n\nThis function returns a natural number representing the day of the week for a given datetime value.\n\nReturns a semantically equivalent value for TO_CHAR (time, ‘DAY’), but returns an integer here.\n\nDAYOFWEEK(date_val)\n\n\nThe returned natural number represents the next day of the week.\n\n\n  \n    \n      Return Value\n      Day of Week\n    \n  \n  \n    \n      0\n      Sunday\n    \n    \n      1\n      Monday\n    \n    \n      2\n      Tuesday\n    \n    \n      3\n      Wednesday\n    \n    \n      4\n      Thursday\n    \n    \n      5\n      Friday\n    \n    \n      6\n      Saturday\n    \n  \n\n\nDECODE\n\nThis function compares the given Column value with Search, and returns the next return value if it is the same. If there is no satisfactory Search value, it returns the default value. If Default is omitted, NULL is returned.\n\nDECODE(column, [search, return],.. default)\n\n\nMach&gt; CREATE TABLE decode_table (id1 VARCHAR(11));\nCreated successfully.\n \nMach&gt; INSERT INTO decode_table VALUES('decodetest1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO decode_table VALUES('decodetest2');\n1 row(s) inserted.\n \nMach&gt; SELECT id1, DECODE(id1, 'decodetest1', 'result1', 'decodetest2', 'result2', 'DEFAULT') FROM decode_table;\nid1          DECODE(id1, 'decodetest1', 'result1', 'decodetest2', 'result2', 'DEFAULT')\n---------------------------------------------------------\ndecodetest2  result2\ndecodetest1  result1\n[2] row(s) selected.\n \nMach&gt; SELECT id1, DECODE(id1, 'codetest', 2, 99) FROM decode_table;\nid1          DECODE(id1, 'codetest', 2, 99)\n-----------------------------------------------\ndecodetest2  99\ndecodetest1  99\n[2] row(s) selected.\n \nMach&gt; SELECT DECODE(id1, 'decodetest1', 2) FROM decode_table;\nDECODE(id1, 'decodetest1', 2)\n--------------------------------\nNULL\n2\n[2] row(s) selected.\n \nMach&gt; SELECT DECODE(id1, 'codetest', 2) FROM decode_table;\nDECODE(id1, 'codetest', 2)\n-----------------------------\nNULL\nNULL\n[2] row(s) selected.\n\n\nFIRST / LAST\n\nThis function is an aggregate function that returns the specific value of the highest (or last) record in the sequence in which the ‘reference value’ in each group is in order.\n\n\n  FIRST: Returns a specific value from the most advanced record in the sequence\n  LAST: Returns a specific value from the last record in the sequence\n\n\nFIRST(sort_expr, return_expr)\nLAST(sort_expr, return_expr)\n\n\nMach&gt; create table firstlast_table (id integer, name varchar(20), group_no integer);\nCreated successfully.\nMach&gt; insert into firstlast_table values (1, 'John', 0);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (2, 'Grey', 1);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (5, 'Ryan', 0);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (4, 'Andrew', 0);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (7, 'Kyle', 1);\n1 row(s) inserted.\nMach&gt; insert into firstlast_table values (6, 'Ross', 1);\n1 row(s) inserted.\n \nMach&gt; select group_no, first(id, name) from firstlast_table group by group_no;\ngroup_no    first(id, name)\n-------------------------------------\n1           Grey\n0           John\n[2] row(s) selected.\n \n \nMach&gt; select group_no, last(id, name) from firstlast_table group by group_no;\ngroup_no    last(id, name)\n-------------------------------------\n1           Kyle\n0           Ryan\n\n\nFROM_UNIXTIME\n\nThis function converts a 32-bit UNIXTIME value entered as an integer to a datetime datatype value. (UNIX_TIMESTAMP converts datetime data to 32-bit UNIXTIME integer data.)\n\nFROM_UNIXTIME(unix_timestamp_value)\n\n\nMach&gt; SELECT FROM_UNIXTIME(315540671) FROM TEST;\nFROM_UNIXTIME(315540671)\n----------------------------------\n1980-01-01 11:11:11 000:000:000\n \nMach&gt; SELECT FROM_UNIXTIME(UNIX_TIMESTAMP('2001-01-01')) FROM unix_table;\nFROM_UNIXTIME(UNIX_TIMESTAMP('2001-01-01'))\n------------------------------------------\n2001-01-01 00:00:00 000:000:000\n\n\nFROM_TIMESTAMP\n\nThis function takes a nanosecond value that has passed since 1970-01-01 09:00 and converts it to a datetime data type.\n\n(TO_TIMESTAMP () converts a datetime data type to nanosecond data that has passed since 1970-01-01 09:00.)\n\nFROM_TIMESTAMP(nanosecond_time_value)\n\n\nMach&gt; SELECT FROM_TIMESTAMP(1562302560007248869) FROM TEST;\nFROM_TIMESTAMP(1562302560007248869)\n--------------------------------------\n2019-07-05 13:56:00 007:248:869\n\n\nBoth sysdate and now represent nanosecond values elapsed since 1970-01-01 09:00 at the current time, so you can use FROM_TIMESTAMP () immediately.\n\nOf course, the results are the same without using them. This can be useful if you have sysdate and now operations in nanoseconds.\n\nMach&gt; select sysdate, from_timestamp(sysdate) from test_tbl;\nsysdate                         from_timestamp(sysdate)\n-------------------------------------------------------------------\n2019-07-05 14:00:59 722:822:443 2019-07-05 14:00:59 722:822:443\n[1] row(s) selected.\n \nMach&gt; select sysdate, from_timestamp(sysdate-1000000) from test_tbl;\nsysdate                         from_timestamp(sysdate-1000000)\n-------------------------------------------------------------------\n2019-07-05 14:01:05 130:939:525 2019-07-05 14:01:05 129:939:525      -- 1 ms (1,000,000 ns) 차이가 발생함\n[1] row(s) selected.\n\n\nGROUP_CONCAT\n\nThis function is an aggregate function that outputs the value of the corresponding column in the group in a string.\n\n\n  　This function cannot be used in Cluster Edition.\n\n\nGROUP_CONCAT(\n     [DISTINCT] column\n     [ORDER BY { unsigned_integer | column }\n     [ASC | DESC] [, column ...]]\n     [SEPARATOR str_val]\n)\n\n\n\n  DISTINCT: Duplicate values ​​are not appended if duplicate values ​​are attached.\n  ORDER BY: Arranges the sequence of column values ​​to be attached according to the specified column values.\n  SEPARATOR: A delimiter string used to append column values. The default value is a comma (,).\n\n\nThe syntax notes are as follows.\n\n\n  \n    \n      \n        \n          You can specify only one column, and if you want to specify more than one column, you must use the TO_CHAR () function and the CONCAT operator (\n           \n          ) to make one expression.\n        \n      \n    \n  \n  ORDER BY can specify other columns besides the columns to be joined, and can specify multiple columns.\n  You must enter a string constant in SEPARATOR, and you can not enter a string column.\n\n\nMach&gt; CREATE TABLE concat_table(id1 INTEGER, id2 DOUBLE, name VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO concat_table VALUES (1, 2, 'John');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (2, 1, 'Ram');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (3, 2, 'Zara');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (4, 2, 'Jill');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (5, 1, 'Jack');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO concat_table VALUES (6, 1, 'Jack');\n1 row(s) inserted.\n \n \nMach&gt; SELECT GROUP_CONCAT(name) AS G_NAMES FROM concat_table GROUP BY id2;\nG_NAMES                                                                                                          \n------------------------------------------------------------------------------------\nJack,Jack,Ram                                                                                                    \nJill,Zara,John                                                                                                   \n[2] row(s) selected.\n \nMach&gt; SELECT GROUP_CONCAT(DISTINCT name) AS G_NAMES FROM concat_table GROUP BY Id2;\nG_NAMES                                                                                                          \n------------------------------------------------------------------------------------\nJack,Ram                                                                                                         \nJill,Zara,John                                                                                                   \n[2] row(s) selected.\n \nMach&gt; SELECT GROUP_CONCAT(name SEPARATOR '.') G_NAMES FROM concat_table GROUP BY Id2;\nG_NAMES                                                                                                          \n------------------------------------------------------------------------------------\nJack.Jack.Ram                                                                                                    \nJill.Zara.John                                                                                                   \n[2] row(s) selected.\n \nMach&gt; SELECT GROUP_CONCAT(name ORDER BY id1) G_NAMES, GROUP_CONCAT(id1 ORDER BY id1) G_SORTID FROM concat_table GROUP BY id2;\nG_NAMES                                                                                                          \n------------------------------------------------------------------------------------\nG_SORTID                                                                                                         \n------------------------------------------------------------------------------------\nRam,Jack,Jack                                                                                                    \n2,5,6                                                                                                            \nJohn,Zara,Jill                                                                                                   \n1,3,4                                                                                                            \n[2] row(s) selected.\n\n\nINSTR\n\nThis function returns the index of the number of characters in the string entered together. The index starts at 1.\n\n\n  If no string pattern is found, 0 is returned.\n  If the length of the string pattern to find is 0 or NULL, NULL is returned.\n\n\nINSTR(target_string, pattern_string)\n\n\nMach&gt; CREATE TABLE string_table(c1 VARCHAR(20));\nCreated successfully.\n \nMach&gt; INSERT INTO string_table VALUES ('abstract');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO string_table VALUES ('override');\n1 row(s) inserted.\n \nMach&gt; SELECT c1, INSTR(c1, 'act') FROM string_table;\nc1                    INSTR(c1, 'act')\n------------------------------------------\noverride              0\nabstract              6\n[2] row(s) selected.\n\n\nLEAST / GREATEST\n\nBoth functions return the smallest value (LEAST) or the largest value (GREATEST) if you specify multiple columns or values ​​as input parameters.\n\nIf the input value is 1 or absent, it is treated as an error. If the input value is NULL, NULL is returned. Therefore, if the input value is a column, it must be converted in advance using a function.\nIf a column (BLOB, TEXT) that can not be compared with the input value is included or type conversion is not possible for comparison, comparison is processed as an error.\n\nLEAST(value_list, value_list,...)\nGREATEST(value_list, value_list,...)\n\n\nMach&gt; CREATE TABLE lgtest_table(c1 INTEGER, c2 LONG, c3 VARCHAR(10), c4 VARCHAR(5));\nCreated successfully.\n \nMach&gt; INSERT INTO lgtest_table VALUES (1, 2, 'abstract', 'ace');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lgtest_table VALUES (null, 100, null, 'bag');\n1 row(s) inserted.\n \nMach&gt; SELECT LEAST (c1, c2) FROM lgtest_table;\nLEAST (c1, c2)\n-----------------------\nNULL\n1\n[2] row(s) selected.\n \nMach&gt; SELECT LEAST (c1, c2, -1) FROM lgtest_table;\nLEAST (c1, c2, -1)\n-----------------------\nNULL\n-1\n[2] row(s) selected.\n \nMach&gt; SELECT GREATEST(c3, c4) FROM lgtest_table;\nGREATEST(c3, c4)\n--------------------\nNULL\nace\n[2] row(s) selected.\n \nMach&gt; SELECT LEAST(c3, c4) FROM lgtest_table;\nLEAST(c3, c4)\n-----------------\nNULL\nabstract\n[2] row(s) selected.\n \nMach&gt; SELECT LEAST(NVL(c3, 'aa'), c4) FROM lgtest_table;\nLEAST(NVL(c3, 'aa'), c4)\n----------------------------\naa\nabstract\n[2] row(s) selected.\n\n\nLENGTH\n\nThis function gets the length of a string column. The obtained value outputs the number of bytes in English.\n\nLENGTH(column_name)\n\n\nMach&gt; CREATE TABLE length_table (id1 INTEGER, id2 DOUBLE, name VARCHAR(15));\nCreated successfully.\n \nMach&gt; INSERT INTO length_table VALUES(1, 10, 'Around the Horn');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO length_table VALUES(NULL, 20, 'Alfreds Futterkiste');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO length_table VALUES(3, NULL, 'Antonio Moreno');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO length_table VALUES(4, 40, NULL);\n1 row(s) inserted.\n \nMach&gt; select * FROM length_table;\nID1         ID2                         NAME\n-------------------------------------------------------------\n4           40                          NULL\n3           NULL                        Antonio Moreno\nNULL        20                          Alfreds Futterk\n1           10                          Around the Horn\n[4] row(s) selected.\n \nMach&gt; select id1 * 10 FROM length_table;\nid1 * 10\n-----------------------\n40\n30\nNULL\n10\n[4] row(s) selected.\n \nMach&gt; select * FROM length_table Where id1 &gt; 1 and id2 &lt; 50;\nID1         ID2                         NAME\n-------------------------------------------------------------\n4           40                          NULL\n[1] row(s) selected.\n \nMach&gt; select name || ' with null concat' FROM length_table;\nname || ' with null concat'\n------------------------------------\nNULL\nAntonio Moreno with null concat\nAlfreds Futterk with null concat\nAround the Horn with null concat\n[4] row(s) selected.\n \nMach&gt; select LENGTH(name) FROM length_table;\nLENGTH(name)\n---------------\nNULL\n14\n15\n15\n[4] row(s) selected.\n\n\nLOWER\n\nThis function converts an English string to lowercase.\n\nLOWER(column_name)\n\n\nMach&gt; CREATE TABLE lower_table (name VARCHAR(20));\nCreated successfully.\n \nMach&gt; INSERT INTO lower_table VALUES('');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lower_table VALUES('James Backley');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lower_table VALUES('Alfreds Futterkiste');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lower_table VALUES('Antonio MORENO');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO lower_table VALUES (NULL);\n1 row(s) inserted.\n \nMach&gt; SELECT LOWER(name) FROM lower_table;\nLOWER(name)\n------------------------\nNULL\nantonio moreno\nalfreds futterkiste\njames backley\nNULL\n[5] row(s) selected.\n\n\nLPAD / RPAD\n\nThis function adds a character to the left (LPAD) or to the right (RPAD) until the input is of a given length.\n\nThe last parameter, char, can be omitted, or a space ‘ ‘ character if omitted. \nIf the input column value is longer than the given length, the characters are not appended but only the length is taken from the beginning.\n\nLPAD(str, len, padstr)\nRPAD(str, len, padstr)\n\n\nMach&gt; CREATE TABLE pad_table (c1 integer, c2 varchar(15));\nCreated successfully.\n \nMach&gt; INSERT INTO pad_table VALUES (1, 'Antonio');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO pad_table VALUES (25, 'Johnathan');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO pad_table VALUES (30, 'M');\n1 row(s) inserted.\n \nMach&gt; SELECT LPAD(to_char(c1), 5, '0') FROM pad_table;\nLPAD(to_char(c1), 5, '0')\n-----------------------------\n00030\n00025\n00001\n[3] row(s) selected.\n \nMach&gt; SELECT RPAD(to_char(c1), 5, '0') FROM pad_table;\nRPAD(to_char(c1), 5, '0')\n-----------------------------\n30000\n25000\n10000\n[3] row(s) selected.\n \nMach&gt; SELECT LPAD(c2, 5) FROM pad_table;\nLPAD(c2, 5)\n---------------\n    M\nJohna\nAnton\n[3] row(s) selected.\n \nMach&gt; SELECT RPAD(c2, 5) FROM pad_table;\nRPAD(c2, 5)\n---------------\nM\nJohna\nAnton\n[3] row(s) selected.\n \nMach&gt; SELECT RPAD(c2, 10, '***') FROM pad_table;\nRPAD(c2, 10, '***')\n-----------------------\nM*********\nJohnathan*\nAntonio***\n[3] row(s) selected.\n\n\nLTRIM / RTRIM\n\nThis function removes the value corresponding to the pattern string from the first parameter. The LTRIM function checks to see if the characters are in pattern from left to right, the RTRIM function from right to left, and truncates until a character not in pattern is encountered. If all the strings are present in the pattern, NULL is returned.\n\nIf you do not specify a pattern expression, use the space character ‘’ as a basis to remove the space character.\n\nLTRIM(column_name, pattern)\nRTRIM(column_name, pattern)\n\n\nMach&gt; CREATE TABLE trim_table1(name VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO trim_table1 VALUES ('   smith   ');\n1 row(s) inserted.\n \nMach&gt; SELECT ltrim(name) FROM trim_table1;\nltrim(name)\n---------------\nsmith\n[1] row(s) selected.\n \nMach&gt; SELECT rtrim(name) FROM trim_table1;\nrtrim(name)\n---------------\n   smith\n[1] row(s) selected.\n \nMach&gt; SELECT ltrim(name, ' s') FROM trim_table1;\nltrim(name, ' s')\n---------------------\nmith\n[1] row(s) selected.\n \nMach&gt; SELECT rtrim(name, 'h ') FROM trim_table1;\nrtrim(name, 'h ')\n---------------------\n   smit\n[1] row(s) selected.\n \nMach&gt; CREATE TABLE trim_table2 (name VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO trim_table2 VALUES ('ddckaaadkk');\n1 row(s) inserted.\n \nMach&gt; SELECT ltrim(name, 'dc') FROM trim_table2;\nltrim(name, 'dc')\n---------------------\nkaaadkk\n[1] row(s) selected.\n \nMach&gt; SELECT rtrim(name, 'dk') FROM trim_table2;\nrtrim(name, 'dk')\n---------------------\nddckaaa\n[1] row(s) selected.\n \nMach&gt; SELECT ltrim(name, 'dckak') FROM trim_table2;\nltrim(name, 'dckak')\n------------------------\nNULL\n[1] row(s) selected.\n \nMach&gt; SELECT rtrim(name, 'dckak') FROM trim_table2;\nrtrim(name, 'dckak')\n------------------------\nNULL\n[1] row(s) selected.\n\n\nMAX\n\nThis function is an aggregate function that obtains the maximum value of a given numeric column.\n\nMAX(column_name)\n\n\nMach&gt; CREATE TABLE max_table (c INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO max_table VALUES(10);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO max_table VALUES(20);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO max_table VALUES(30);\n1 row(s) inserted.\n \nMach&gt; SELECT MAX(c) FROM max_table;\nMAX(c)\n--------------\n30\n[1] row(s) selected.\n\n\nMIN\n\nThis function is an aggregate function that obtains the minimum value of a corresponding numeric column.\n\nMIN(column_name)\n\n\nMach&gt; CREATE TABLE min_table(c1 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO min_table VALUES(1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO min_table VALUES(22);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO min_table VALUES(33);\n1 row(s) inserted.\n \nMach&gt; SELECT MIN(c1) FROM min_table;\nMIN(c1)\n--------------\n1\n[1] row(s) selected.\n\n\nNVL\n\nThis function returns value if the value of the column is NULL, or the value of the original column if it is not NULL.\n\nNVL(string1, replace_with)\n\n\nMach&gt; CREATE TABLE nvl_table (c1 varchar(10));\nCreated successfully.\n \nMach&gt; INSERT INTO nvl_table VALUES ('Johnathan');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO nvl_table VALUES (NULL);\n1 row(s) inserted.\n \nMach&gt; SELECT NVL(c1, 'Thomas') FROM nvl_table;\nNVL(c1, 'Thomas')\n---------------------\nThomas\nJohnathan\n\n\nROUND\n\nThis function returns the result of rounding off the digits of the input value (input digit +1). If no digits are entered, the rounding is done at position 0. It is possible to enter a negative number in decimals place to round the decimal place.\n\nROUND(column_name, [decimals])\n\n\nMach&gt; CREATE TABLE round_table (c1 DOUBLE);\nCreated successfully.\n \nMach&gt; INSERT INTO round_table VALUES (1.994);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO round_table VALUES (1.995);\n1 row(s) inserted.\n \nMach&gt; SELECT c1, ROUND(c1, 2) FROM round_table;\nc1                          ROUND(c1, 2)\n-----------------------------------------------------------\n1.995                       2\n1.994                       1.99\n\n\nROWNUM\n\nThis function assigns a number to the SELECT query result row.\n\nIt can be used inside Subquery or Inline View that is used inside SELECT query. If you use ROWNUM () function in Inline View in Target List, you need to give Alias ​​to refer to from outside.\n\nROWNUM()\n\n\nAvailable Clauses\n\nThis function can be used in the target list, GROUP BY, or ORDER BY clause of a SELECT query. However, it can not be used in the WHERE and HAVING clauses of a SELECT query. ROWNUM () If you want to control WHERE or HAVING clause with result number, you can use SELECT query with ROWNUM () in Inline View and refer to it in WHERE or HAVING clause.\n\n\n  \n    \n      Available Clauses\n      Unavailable Clauses\n    \n  \n  \n    \n      Target List / GROUP BY / ORDER BY\n      WHERE / HAVING\n    \n  \n\n\nMach&gt; CREATE TABLE rownum_table(c1 INTEGER, c2 DOUBLE, c3 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO rownum_table VALUES(1, 1.0, '');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(2, 2.0, 'Second Row');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(3, 3.3, 'Third Row');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(4, 4.3, 'Fourth Row');\n1 row(s) inserted.\n \nMach&gt; SELECT INNER_RANK, c3 AS NAME\n    2 FROM   (SELECT ROWNUM() AS INNER_RANK, * FROM rownum_table)\n    3 WHERE  INNER_RANK &lt; 3;\nINNER_RANK           NAME\n------------------------------------\n1                    Fourth Row\n2                    Third Row\n[2] row(s) selected.\n\n\nAltering Results Due to Sorting\n\nIf there is an ORDER BY clause in the SELECT query, the result number of ROWNUM () in the target list may not be sequentially assigned. This is because the ROWNUM () operation is performed before the operation of the ORDER BY clause. If you want to give it sequentially, you can use the query containing the ORDER BY clause in Inline View and then call ROWNUM () in the outer SELECT statement.\n\nMach&gt; CREATE TABLE rownum_table(c1 INTEGER, c2 DOUBLE, c3 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO rownum_table VALUES(1, 1.0, '');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(2, 2.0, 'John');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(3, 3.3, 'Sarah');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO rownum_table VALUES(4, 4.3, 'Micheal');\n1 row(s) inserted.\n \nMach&gt; SELECT ROWNUM(), c2 AS SORT, c3 AS NAME\n    2 FROM   ( SELECT * FROM rownum_table ORDER BY c3 );\nROWNUM()             SORT                        NAME\n-----------------------------------------------------------------\n1                    1                           NULL\n2                    2                           John\n3                    4.3                         Micheal\n4                    3.3                         Sarah\n[4] row(s) selected.\n\n\nSERIESNUM\n\nReturns a number indicating how many of the records belong to the series grouped by SERIES BY. The return type is BIGINT type, and always returns 1 if the SERIES BY clause is not used.\n\nSERIESNUM()\n\n\nMach&gt; CREATE TABLE T1 (C1 INTEGER, C2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO T1 VALUES (0, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (2, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (3, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (4, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (5, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (6, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO T1 VALUES (7, 1);\n1 row(s) inserted.\n \n \nMach&gt; SELECT SERIESNUM(), C1, C2 FROM T1 ORDER BY C1 SERIES BY C2 &gt; 1;\nSERIESNUM() C1 C2\n-------------------------------------------------\n1 1 2\n1 2 3\n1 3 2\n2 5 2\n2 6 3\n[5] row(s) selected.\n\n\nSTDDEV / STDDEV_POP\n\nThis function is an aggregate function that returns the (standard) deviation and the population standard deviation of the (input) column. Equivalent to the square root of the VARIANCE and VAR_POP values, respectively.\n\nSTDDEV(column)\nSTDDEV_POP(column)\n\n\nMach&gt; CREATE TABLE stddev_table(c1 INTEGER, C2 DOUBLE);\n \nMach&gt; INSERT INTO stddev_table VALUES (1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO stddev_table VALUES (2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO stddev_table VALUES (3, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO stddev_table VALUES (4, 2);\n1 row(s) inserted.\n \nMach&gt; SELECT c2, STDDEV(c1) FROM stddev_table GROUP BY c2;\nc2                          STDDEV(c1)\n-----------------------------------------------------------\n1                           0.707107\n2                           0.707107\n[2] row(s) selected.\n \nMach&gt; SELECT c2, STDDEV_POP(c1) FROM stddev_table GROUP BY c2;\nc2                          STDDEV_POP(c1)\n-----------------------------------------------------------\n1                           0.5\n2                           0.5\n[2] row(s) selected.\n\n\nSUBSTR\n\nThis function truncates the variable string column data from START to SIZE.\n\n\n  START starts at 1 and returns NULL if it is zero.\n  If SIZE is larger than the size of the corresponding string, only the maximum value of the string is returned.\nSIZE is optional, and if omitted, it is internally specified by the size of the string.\n\n\nSUBSTRING(column_name, start, [length])\n\n\nMach&gt; CREATE TABLE substr_table (c1 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO substr_table values('ABCDEFG');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO substr_table values('abstract');\n1 row(s) inserted.\n \nMach&gt; SELECT SUBSTR(c1, 1, 1) FROM substr_table;\nSUBSTR(c1, 1, 1)\n--------------------\na\nA\n[2] row(s) selected.\n \nMach&gt; SELECT SUBSTR(c1, 3, 3) FROM substr_table;\nSUBSTR(c1, 3, 3)\n--------------------\nstr\nCDE\n[2] row(s) selected.\n \nMach&gt; SELECT SUBSTR(c1, 2) FROM substr_table;\nSUBSTR(c1, 2)\n-----------------\nbstract\nBCDEFG\n[2] row(s) selected.\n \nMach&gt; drop table substr_table;\nDropped successfully.\n \nMach&gt; CREATE TABLE substr_table (c1 VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO substr_table values('ABCDEFG');\n1 row(s) inserted.\n \nMach&gt; SELECT SUBSTR(c1, 1, 1) FROM substr_table;\nSUBSTR(c1, 1, 1)\n--------------------\nA\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTR(c1, 3, 3) FROM substr_table;\nSUBSTR(c1, 3, 3)\n--------------------\nCDE\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTR(c1, 2) FROM substr_table;\nSUBSTR(c1, 2)\n-----------------\nBCDEFG\n[1] row(s) selected.\n\n\nSUBSTRING_INDEX\n\nReturns the duplicate string until the given delim is found by the count entered. If count is a negative value, it checks the delimiter from the end of the input string and returns it from the position where the delimiter was found to the end of the string.\n\nIf you enter count as 0 or there is no delimiter in the string, the function will return NULL.\n\nSUBSTRING_INDEX(expression, delim, count)\n\n\nMach&gt; CREATE TABLE substring_table (url VARCHAR(30));\nCreated successfully.\n \nMach&gt; INSERT INTO substring_table VALUES('www.machbase.com');\n1 row(s) inserted.\n \nMach&gt; SELECT SUBSTRING_INDEX(url, '.', 1) FROM substring_table;\nSUBSTRING_INDEX(url, '.', 1)\n----------------------------------\nwww\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTRING_INDEX(url, '.', 2) FROM substring_table;\nSUBSTRING_INDEX(url, '.', 2)\n----------------------------------\nwww.machbase\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTRING_INDEX(url, '.', -1) FROM substring_table;\nSUBSTRING_INDEX(url, '.', -1)\n----------------------------------\ncom\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTRING_INDEX(SUBSTRING_INDEX(url, '.', 2), '.', -1) FROM substring_table;\nSUBSTRING_INDEX(SUBSTRING_INDEX(url, '.', 2), '.', -1)\n-------------------------------------------\nmachbase\n[1] row(s) selected.\n \nMach&gt; SELECT SUBSTRING_INDEX(url, '.', 0) FROM substring_table;\nSUBSTRING_INDEX(url, '.', 0)\n----------------------------------\nNULL\n[1] row(s) selected.\n\n\nSUM\n\nThis function is an aggregate function that represents the sum of the numeric columns.\n\nSUM(column_name)\n\n\nMach&gt; CREATE TABLE sum_table (c1 INTEGER, c2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO sum_table VALUES(1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(1, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(2, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(2, 3);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO sum_table VALUES(3, 4);\n1 row(s) inserted.\n \nMach&gt; SELECT c1, SUM(c1) from sum_table group by c1;\nc1          SUM(c1)\n------------------------------------\n2           6\n3           3\n1           3\n[3] row(s) selected.\n \nMach&gt; SELECT c1, SUM(c2) from sum_table group by c1;\nc1          SUM(c2)\n------------------------------------\n2           6\n3           4\n1           6\n[3] row(s) selected.\n\n\nSUMSQ\n\nSUMSQ returns the sum of squares of the numeric values.\n\nSUMSQ(value)\n\n\nMach&gt; CREATE TABLE sumsq_table (c1 INTEGER, c2 INTEGER);\nCreated successfully.\n  \nMach&gt; INSERT INTO sumsq_table VALUES (1, 1);\n1 row(s) inserted.\n  \nMach&gt; INSERT INTO sumsq_table VALUES (1, 2);\n1 row(s) inserted.\n  \nMach&gt; INSERT INTO sumsq_table VALUES (1, 3);\n1 row(s) inserted.\n  \nMach&gt; INSERT INTO sumsq_table VALUES (2, 4);\n1 row(s) inserted.\n  \nMach&gt; INSERT INTO sumsq_table VALUES (2, 5);\n1 row(s) inserted.\n  \nMach&gt; SELECT c1, SUMSQ(c2) FROM sumsq_table GROUP BY c1;\nc1          SUMSQ(c2)          \n------------------------------------\n2           41                 \n1           14                 \n[2] row(s) selected.\n\n\nSYSDATE / NOW\n\nSYSDATE is a pseudocolumn, not a function, that returns the system’s current time.\n\nNOW is the same function as SYSDATE and is provided for user convenience.\n\nSYSDATE\nNOW\n\n\nMach&gt; SELECT SYSDATE, NOW FROM t1;\n \nSYSDATE                         NOW\n-------------------------------------------------------------------\n2017-01-16 14:14:53 310:973:000 2017-01-16 14:14:53 310:973:000\n\n\nTO_CHAR\n\nThis function converts a given data type to a string type. Depending on the type, format_string may be specified, but not for binary types.\n\nTO_CHAR(column)\n\n\nTO_CHAR:  Default Datatype\n\nThe default data types are converted to data in the form of strings as shown below.\n\nMach&gt; CREATE TABLE fixed_table (id1 SHORT, id2 INTEGER, id3 LONG, id4 FLOAT, id5 DOUBLE, id6 IPV4, id7 IPV6, id8 VARCHAR (128));\nCreated successfully.\n \nMach&gt; INSERT INTO fixed_table values(200, 19234, 1234123412, 3.14, 7.8338, '192.168.0.1', '::127.0.0.1', 'log varchar');\n1 row(s) inserted.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id1) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id1) || ' ]'\n------------------------------------------------------------------------------------\n[ 200 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id2) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id2) || ' ]'\n------------------------------------------------------------------------------------\n[ 19234 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id3) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id3) || ' ]'\n------------------------------------------------------------------------------------\n[ 1234123412 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id4) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id4) || ' ]'\n------------------------------------------------------------------------------------\n[ 3.140000 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id5) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id5) || ' ]'\n------------------------------------------------------------------------------------\n[ 7.833800 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id6) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id6) || ' ]'\n------------------------------------------------------------------------------------\n[ 192.168.0.1 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id7) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id7) || ' ]'\n------------------------------------------------------------------------------------\n[ 0000:0000:0000:0000:0000:0000:7F00:0001 ]\n[1] row(s) selected.\n \nMach&gt; SELECT '[ ' || TO_CHAR(id8) || ' ]' FROM fixed_table;\n'[ ' || TO_CHAR(id8) || ' ]'\n------------------------------------------------------------------------------------\n[ log varchar ]\n[1] row(s) selected.\n\n\nTO_CHAR : Floating point number\n\n\n  Supported from 5.5.6 version\n\n\nThis function converts the values ​​of float and double into strings.\nFormat expression cannot be used repeatedly, and must be entered in the form of ‘[letter][number]’.\n\n\n  \n    \n      Format Expression\n      Descibe\n    \n  \n  \n    \n      F / f\n      Specifies the number of decimal places for the column value. The maximum numeric value to input is 30.\n    \n    \n      N / n\n      Specify the number of decimal places for the column value, and enter a comma (,) for every three digits of the integer part. The maximum numeric value to input is 30.\n    \n  \n\n\nMach&gt; create table float_table (i1 float, i2 double);\nCreated successfully.\n \nMach&gt; insert into float_table values (1.23456789, 1234.5678901234567890);\n1 row(s) inserted.\n \nMach&gt; select TO_CHAR(i1, 'f8'), TO_CHAR(i2, 'N9') from float_table;\nTO_CHAR(i1, 'f8')       TO_CHAR(i2, 'N9')\n--------------------------------------------------------------\n1.23456788              1,234.567890123\n[1] row(s) selected.\n\n\nTO_CHAR: DATETIME Type\n\nA function that converts the value of a datetime column to an arbitrary string. You can use this function to create and combine various types of strings.\n\nIf format_string is omitted, the default is “YYYY-MM-DD HH24: MI: SS mmm: uuu: nnn”.\n\n\n  \n    \n      Format Expression\n      Descibe\n    \n  \n  \n    \n      YYYY\n      Converts year to a four-digit number.\n    \n    \n      YY\n      Converts year to a two-digit number.\n    \n    \n      MM\n      Converts the month to a two-digit number.\n    \n    \n      MON\n      Converts the month to a three-digit abbreviated alphabet. (eg JAN, FEB, MAY, …)\n    \n    \n      DD\n      Converts the day to a two-digit number.\n    \n    \n      DAY\n      Converts the day of the week to a three-digit abbreviation . (eg SUN, MON, …)\n    \n    \n      IW\n      Converts the week number of a specific year from 1 to 53 (taking into account the day of the week) by the ISO 8601 rule. - The start of one week is Monday. - The first week can be considered as the last week of the previous year. Likewise, the last week can be considered the first week of the next year.    See ISO 8601 more information.\n    \n    \n      WW\n      Converts week number of the particular year from 1 to 53 (Week Number) not taking into account the day of the week.That is, from January 1 to January 7, it is converted to 1.\n    \n    \n      W\n      Converts week number of a given month from 1 to 5 (Number The Week) not taking in to account the day of the week.That is, from March 1 to March 7 is converted to 1.\n    \n    \n      HH\n      Converts the time to a two-digit number.\n    \n    \n      HH12\n      Converts the time to a 2-digit number, from 1 to 12.\n    \n    \n      HH24\n      Converts the time to a 2-digit number, from 1 to 23.\n    \n    \n      HH2, HH3, HH6\n      Cuts the time to the number following HH.That is, when HH6 is used, 0 is expressed from 0 to 5, and 6 is expressed from 6 to 11.This expression is useful for calculating certain time-series statistics on time series.This value is expressed on a 24-hour basis.\n    \n    \n      MI\n      The minute is represented by a two-digit number.\n    \n    \n      MI2, MI5, MI10, MI20, MI30\n      The corresponding minute is cut to the number following MI.That is, when MI30 is used, 0 is expressed from 0 to 29 minutes, and 30 is represented from 30 to 59 minutes.This expression is useful for calculating certain time-series statistics on time series.\n    \n    \n      SS\n      The second is represented by a two-digit number.\n    \n    \n      SS2, SS5, SS10, SS20, SS30\n      The corresponding seconds are truncated to successive digits.That is, when SS30 is used, 0 is expressed from 0 to 29 seconds, and 30 is represented from 30 to 59 seconds.This expression is useful for calculating certain time-series statistics on time series.\n    \n    \n      AM\n      The current time is expressed in AM or PM according to AM and PM, respectively.\n    \n    \n      mmm\n      The millisecond of the time is represented by a three-digit number.The range of values ​​is 0 to 999.\n    \n    \n      uuu\n      The micro second of the time is represented as a three-digit number.The range of values ​​is 0 to 999.\n    \n    \n      nnn\n      The nano second of the time is expressed as a three-digit number.The range of values ​​is 0 to 999.\n    \n  \n\n\nMach&gt; CREATE TABLE datetime_table (id integer, dt datetime);\nCreated successfully.\n \nMach&gt; INSERT INTO  datetime_table values(1, TO_DATE('1999-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  datetime_table values(2, TO_DATE('2012-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  datetime_table values(3, TO_DATE('2013-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  datetime_table values(4, TO_DATE('2014-12-30 11:22:33 444:555:666'));\n1 row(s) inserted.\n \nMach&gt; SELECT id, dt FROM datetime_table WHERE dt &gt; TO_DATE('2000-11-11 1:2:3 4:5:0');\nid          dt\n-----------------------------------------------\n4           2014-12-30 11:22:33 444:555:666\n3           2013-11-11 01:02:03 004:005:006\n2           2012-11-11 01:02:03 004:005:006\n[3] row(s) selected.\n \nMach&gt; SELECT id, dt FROM datetime_table WHERE dt &gt; TO_DATE('2013-11-11 1:2:3') and dt &lt; TO_DATE('2014-11-11 1:2:3');\nid          dt\n-----------------------------------------------\n3           2013-11-11 01:02:03 004:005:006\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt) FROM datetime_table;\nid          TO_CHAR(dt)\n-------------------------------------------------------------------------------------------------\n4           2014-12-30 11:22:33 444:555:666\n3           2013-11-11 01:02:03 004:005:006\n2           2012-11-11 01:02:03 004:005:006\n1           1999-11-11 01:02:03 004:005:006\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY')\n-------------------------------------------------------------------------------------------------\n4           2014\n3           2013\n2           2012\n1           1999\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM')\n-------------------------------------------------------------------------------------------------\n4           2014-12\n3           2013-11\n2           2012-11\n1           1999-11\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM-DD') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM-DD')\n-------------------------------------------------------------------------------------------------\n4           2014-12-30\n3           2013-11-11\n2           2012-11-11\n1           1999-11-11\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM-DD TO_CHAR') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM-DD TO_CHAR')\n-------------------------------------------------------------------------------------------------\n4           2014-12-30 TO_CHAR\n3           2013-11-11 TO_CHAR\n2           2012-11-11 TO_CHAR\n1           1999-11-11 TO_CHAR\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM-DD HH24:MI:SS') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM-DD HH24:MI:SS')\n-------------------------------------------------------------------------------------------------\n4           2014-12-30 11:22:33\n3           2013-11-11 01:02:03\n2           2012-11-11 01:02:03\n1           1999-11-11 01:02:03\n[4] row(s) selected.\n \nMach&gt; SELECT id, TO_CHAR(dt, 'YYYY-MM-DD HH24:MI:SS mmm.uuu.nnn') FROM datetime_table;\nid          TO_CHAR(dt, 'YYYY-MM-DD HH24:MI:SS mmm.\n-------------------------------------------------------------------------------------------------\n4           2014-12-30 11:22:33 444.555.666\n3           2013-11-11 01:02:03 004.005.006\n2           2012-11-11 01:02:03 004.005.006\n1           1999-11-11 01:02:03 004.005.006\n[4] row(s) selected.\n\n\nTO_CHAR: Unsupported Type\nCurrently, TO_CHAR is not supported for binary types.\n\nThis is because it is impossible to convert to plain text. If you want to output it to the screen, you can check it by outputting hexadecimal value through TO_HEX () function.\n\nTO_DATE\n\nThis function converts a string represented by a given format string to a datetime type.\n\nIf format_string is omitted, the default is “YYYY-MM-DD HH24: MI: SS mmm: uuu: nnn”.\n\n-- default format is \"YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn\" if no format exists.\nTO_DATE(date_string [, format_string])\n\n\nMach&gt; CREATE TABLE to_date_table (id INTEGER, dt datetime);\nCreated successfully.\n \nMach&gt; INSERT INTO  to_date_table VALUES(1, TO_DATE('1999-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  to_date_table VALUES(2, TO_DATE('2012-11-11 1:2:3 4:5:6'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  to_date_table VALUES(3, TO_DATE('2014-12-30 11:22:33 444:555:666'));\n1 row(s) inserted.\n \nMach&gt; INSERT INTO  to_date_table VALUES(4, TO_DATE('2014-12-30 23:22:34 777:888:999', 'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn'));\n1 row(s) inserted.\n \nMach&gt; SELECT id, dt FROM to_date_table WHERE dt &gt; TO_DATE('1999-11-11 1:2:3 4:5:0');\nid          dt\n-----------------------------------------------\n4           2014-12-30 23:22:34 777:888:999\n3           2014-12-30 11:22:33 444:555:666\n2           2012-11-11 01:02:03 004:005:006\n1           1999-11-11 01:02:03 004:005:006\n[4] row(s) selected.\n \nMach&gt; SELECT id, dt FROM to_date_table WHERE dt &gt; TO_DATE('2000-11-11 1:2:3 4:5:0');\nid          dt\n-----------------------------------------------\n4           2014-12-30 23:22:34 777:888:999\n3           2014-12-30 11:22:33 444:555:666\n2           2012-11-11 01:02:03 004:005:006\n[3] row(s) selected.\n \nMach&gt; SELECT id, dt FROM to_date_table WHERE dt &gt; TO_DATE('2012-11-11 1:2:3','YYYY-MM-DD HH24:MI:SS') and dt &lt; TO_DATE('2014-11-11 1:2:3','YYYY-MM-DD HH24:MI:SS');\nid          dt\n-----------------------------------------------\n2           2012-11-11 01:02:03 004:005:006\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999', 'YYYY') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999', 'YYYY')\n-----------------------------------------------\n4           1999-01-01 00:00:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12', 'YYYY-MM') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12', 'YYYY-MM')\n-----------------------------------------------\n4           1999.12.01 00:00:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999', 'YYYY') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999', 'YYYY')\n-----------------------------------------------\n4           1999-01-01 00:00:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12', 'YYYY-MM') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12', 'YYYY-MM')\n-----------------------------------------------\n4           1999-12-01 00:00:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12', 'YYYY-MM-DD HH24:MI') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12-31 13:12', 'YYYY-MM-DD HH24:MI')\n-------------------------------------------------------\n4           1999-12-31 13:12:00 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12:32', 'YYYY-MM-DD HH24:MI:SS') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12-31 13:12:32', 'YYYY-MM-DD HH24:MI:SS')\n-------------------------------------------------------\n4           1999-12-31 13:12:32 000:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12:32 123', 'YYYY-MM-DD HH24:MI:SS mmm') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12-31 13:12:32 123', 'YYYY-MM-DD HH24:MI:SS mmm')\n-------------------------------------------------------\n4           1999-12-31 13:12:32 123:000:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12:32 123:456', 'YYYY-MM-DD HH24:MI:SS mmm:uuu') FROM to_date_table LIMIT 1;\nid          TO_DATE('1999-12-31 13:12:32 123:456', 'YYYY-MM-DD HH24:MI:SS mmm:uuu')\n-------------------------------------------------------\n4           1999-12-31 13:12:32 123:456:000\n[1] row(s) selected.\n \nMach&gt; SELECT id, TO_DATE('1999-12-31 13:12:32 123:456:789', 'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn') FROM to_date_table LIMIT 1;\nid           TO_DATE('1999-12-31 13:12:32 123:456:789', 'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn')\n-------------------------------------------------------\n4           1999-12-31 13:12:32 123:456:789\n[1] row(s) selected.\n\n\nTO_DATE_SAFE\n\nSimilar to TO_DATE (), but returns NULL without error if conversion fails.\n\nTO_DATE_SAFE(date_string [, format_string])\n\n\nMach&gt; CREATE TABLE date_table (ts DATETIME);\nCreated successfully.\n \nMach&gt; INSERT INTO date_table VALUES (TO_DATE_SAFE('2016-01-01', 'YYYY-MM-DD'));\n1 row(s) inserted.\nMach&gt; INSERT INTO date_table VALUES (TO_DATE_SAFE('2016-01-02', 'YYYY'));\n1 row(s) inserted.\nMach&gt; INSERT INTO date_table VALUES (TO_DATE_SAFE('2016-12-32', 'YYYY-MM-DD'));\n1 row(s) inserted.\n \nMach&gt; SELECT ts FROM date_table;\nts\n----------------------------------\nNULL\nNULL\n2016-01-01 00:00:00 000:000:000\n[3] row(s) selected.\n\n\nTO_HEX\n\nThis function returns value if the value of the column is NULL, or the value of the original column if it is not NULL. To ensure consistency of output, convert to BIG ENDIAN type for short, int, and long types.\n\nTO_HEX(column)\n\n\nMach&gt; CREATE TABLE hex_table (id1 SHORT, id2 INTEGER, id3 VARCHAR(10), id4 FLOAT, id5 DOUBLE, id6 LONG, id7 IPV4, id8 IPV6, id9 TEXT, id10 BINARY,\nid11 DATETIME);\nCreated successfully.\n \nMach&gt; INSERT INTO hex_table VALUES(256, 65535, '0123456789', 3.141592, 1024 * 1024 * 1024 * 3.14, 13513135446, '192.168.0.1', '::192.168.0.1', 'textext',\n'binary', TO_DATE('1999', 'YYYY'));\n1 row(s) inserted.\n \nMach&gt; SELECT TO_HEX(id1), TO_HEX(id2), TO_HEX(id3), TO_HEX(id4), TO_HEX(id5), TO_HEX(id6), TO_HEX(id7), TO_HEX(id8), TO_HEX(id9), TO_HEX(id10), TO_HEX(id11)\nFROM hex_table;\nTO_HEX(id1)  TO_HEX(id2)  TO_HEX(id3)            TO_HEX(id4)  TO_HEX(id5)        TO_HEX(id6)        TO_HEX(id7)\n-------------------------------------------------------------------------------------------------------------------------\nTO_HEX(id8)                          TO_HEX(id9)\n--------------------------------------------------------------------------------------------------------------------------\nTO_HEX(id10)                                                                      TO_HEX(id11)\n--------------------------------------------------------------------------------------------------------\n0100   0000FFFF   30313233343536373839   D80F4940   1F85EB51B81EE941   0000000325721556   04C0A80001\n06000000000000000000000000C0A80001   74657874657874\n62696E617279                                                                      0CB325846E226000\n[1] row(s) selected.\n\n\nTO_IPV4 / TO_IPV4_SAFE\n\nThis function converts a given string to an IP version 4 type. If the string can not be converted to a numeric value, the TO_IPV4 () function returns an error and terminates the operation.\n\nHowever, in the case of the TO_IPV4_SAFE () function, NULL is returned in case of error, and the operation can continue.\n\nTO_IPV4(string_value)\nTO_IPV4_SAFE(string_value)\n\n\nMach&gt; CREATE TABLE ipv4_table (c1 varchar(100));\nCreated successfully.\n \nMach&gt; INSERT INTO ipv4_table VALUES('192.168.0.1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv4_table VALUES('     192.168.0.2    ');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv4_table VALUES(NULL);\n1 row(s) inserted.\n \nMach&gt; SELECT c1 FROM ipv4_table;\nc1\n------------------------------------------------------------------------------------\nNULL\n     192.168.0.2\n192.168.0.1\n[3] row(s) selected.\n \nMach&gt; SELECT TO_IPV4(c1) FROM ipv4_table;\nTO_IPV4(c1)\n------------------\nNULL\n192.168.0.2\n192.168.0.1\n[3] row(s) selected.\n \nMach&gt; INSERT INTO ipv4_table VALUES('192.168.0.1.1');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_IPV4(c1) FROM ipv4_table limit 1;\nTO_IPV4(c1)\n------------------\n[ERR-02068 : Invalid IPv4 address format (192.168.0.1.1).]\n[0] row(s) selected.\n \nMach&gt; SELECT TO_IPV4_SAFE(c1) FROM ipv4_table;\nTO_IPV4_SAFE(c1)\n-------------------\nNULL\nNULL\n192.168.0.2\n192.168.0.1\n[4] row(s) selected.\n\n\nTO_IPV6 / TO_IPV6_SAFE\n\nThis function converts a given string to an IP version 6 type. If the string can not be converted to a numeric type, the TO_IPV6 () function returns an error and terminates the operation.\n\nHowever, in the case of the TO_IPV6_SAFE () function, NULL is returned in case of error, and the operation can continue.\n\nTO_IPV6(string_value)\nTO_IPV6_SAFE(string_value)\n\n\nMach&gt; CREATE TABLE ipv6_table (id varchar(100));\nCreated successfully.\n \nMach&gt; INSERT INTO ipv6_table VALUES('::0.0.0.0');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('::127.0.0.1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('::127.0' || '.0.2');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('   ::127.0.0.3');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('::127.0.0.4  ');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('   ::FFFF:255.255.255.255   ');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipv6_table VALUES('21DA:D3:0:2F3B:2AA:FF:FE28:9C5A');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_IPV6(id) FROM ipv6_table;\nTO_IPV6(id)\n---------------------------------------------------------------\n21da:d3::2f3b:2aa:ff:fe28:9c5a\n::ffff:255.255.255.255\n::127.0.0.4\n::127.0.0.3\n::127.0.0.2\n::127.0.0.1\n::\n[7] row(s) selected.\n \nMach&gt; INSERT INTO ipv6_table VALUES('127.0.0.10.10');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_IPV6(id) FROM ipv6_table limit 1;\nTO_IPV6(id)\n---------------------------------------------------------------\n[ERR-02148 : Invalid IPv6 address format.(127.0.0.10.10)]\n[0] row(s) selected.\n \nMach&gt; SELECT TO_IPV6_SAFE(id) FROM ipv6_table;\nTO_IPV6_SAFE(id)\n---------------------------------------------------------------\nNULL\n21da:d3::2f3b:2aa:ff:fe28:9c5a\n::ffff:255.255.255.255\n::127.0.0.4\n::127.0.0.3\n::127.0.0.2\n::127.0.0.1\n::\n[8] row(s) selected.\n\n\nTO_NUMBER / TO_NUMBER_SAFE\n\nThis function converts a given string to a numeric double. If the string can not be converted to a numeric value, the TO_NUMBER () function returns an error and terminates the operation.\n\nHowever, in case of TO_NUMBER_SAFE () function, NULL is returned in case of error, and the operation can continue.\n\nTO_NUMBER(string_value)\nTO_NUMBER_SAFE(string_value)\n\n\nMach&gt; CREATE TABLE number_table (id varchar(100));\nCreated successfully.\n \nMach&gt; INSERT INTO number_table VALUES('10');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO number_table VALUES('20');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO number_table VALUES('30');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_NUMBER(id) from number_table;\nTO_NUMBER(id)\n------------------------------\n30\n20\n10\n[3] row(s) selected.\n \nMach&gt; CREATE TABLE safe_table (id varchar(100));\nCreated successfully.\n \nMach&gt; INSERT INTO safe_table VALUES('invalidnumber');\n1 row(s) inserted.\n \nMach&gt; SELECT TO_NUMBER(id) from safe_table;\nTO_NUMBER(id)\n------------------------------\n[ERR-02145 : The string cannot be converted to number value.(invalidnumber)]\n[0] row(s) selected.\n \nMach&gt; SELECT TO_NUMBER_SAFE(id) from safe_table;\nTO_NUMBER_SAFE(id)\n------------------------------\nNULL\n[1] row(s) selected.\n\n\nTO_TIMESTAMP\n\nThis function converts a datetime data type to nanosecond data that has passed since 1970-01-01 09:00.\n\nTO_TIMESTAMP(datetime_value)\n\n\nMach&gt; create table datetime_tbl (c1 datetime);\nCreated successfully.\n \nMach&gt; insert into datetime_tbl values ('2010-01-01 10:10:10');\n1 row(s) inserted.\n \nMach&gt; select to_timestamp(c1) from datetime_tbl;\nto_timestamp(c1)\n-----------------------\n1262308210000000000\n[1] row(s) selected.\n\n\nTRUNC\n\nThe TRUNC function returns the number truncated at the nth place after the decimal point.\n\nIf n is omitted, treat it as 0 and delete all decimal places. If n is negative, it returns the value truncated from n before the decimal point.\n\nTRUNC(number [, n])\n\n\nMach&gt; CREATE TABLE trunc_table (i1 DOUBLE);\nCreated successfully.\n \nMach&gt; INSERT INTO trunc_table VALUES (158.799);\n1 row(s) inserted.\n \nMach&gt; SELECT TRUNC(i1, 1), TRUNC(i1, -1) FROM trunc_table;\nTRUNC(i1, 1)                TRUNC(i1, -1)\n-----------------------------------------------------------\n158.7                       150\n[1] row(s) selected.\n \nMach&gt; SELECT TRUNC(i1, 2), TRUNC(i1, -2) FROM trunc_table;\nTRUNC(i1, 2)                TRUNC(i1, -2)\n-----------------------------------------------------------\n158.79                      100\n[1] row(s) selected.\n\n\nTS_CHANGE_COUNT\n\nThis function is an aggregate function that obtains the number of changes to a particular column value.\n\nThis function can not be used with 1) Join or 2) Inline view because it can be guaranteed that input data is input in chronological order.\nThe current version only supports types except varchar.\n\n\n  This function cannot be used in Cluster Edition.\n\n\nTS_CHANGE_COUNT(column)\n\n\nMach&gt; CREATE TABLE ipcount_table (id INTEGER, ip IPV4);\nCreated successfully.\n \nMach&gt; INSERT INTO ipcount_table VALUES (1, '192.168.0.1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (1, '192.168.0.2');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (1, '192.168.0.1');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (1, '192.168.0.2');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (2, '192.168.0.3');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (2, '192.168.0.3');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (2, '192.168.0.4');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO ipcount_table VALUES (2, '192.168.0.4');\n1 row(s) inserted.\n \nMach&gt; SELECT id, TS_CHANGE_COUNT(ip) from ipcount_table GROUP BY id;\nid          TS_CHANGE_COUNT(ip)\n------------------------------------\n2           2\n1           4\n[2] row(s) selected.\n\n\nUNIX_TIMESTAMP\n\nUNIX_TIMESTAMP is a function that converts a date type value to a 32-bit integer value that is converted by unix’s time () system call. (FROM_UNIXTIME is a function that converts integer data to a date type value on the contrary.)\n\nUNIX_TIMESTAMP(datetime_value)\n\n\nMach&gt; CREATE table unix_table (c1 int);\nCreated successfully.\n \nMach&gt; INSERT INTO unix_table VALUES (UNIX_TIMESTAMP('2001-01-01'));\n1 row(s) inserted.\n \nMach&gt; SELECT * FROM unix_table;\nC1\n--------------\n978274800\n[1] row(s) selected.\n\n\nUPPER\n\nThis function converts the contents of a given English column to uppercase.\n\nUPPER(string_value)\n\n\nMach&gt; CREATE TABLE upper_table(id INTEGER,name VARCHAR(10));\nCreated successfully.\n \nMach&gt; INSERT INTO upper_table VALUES(1, '');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO upper_table VALUES(2, 'James');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO upper_table VALUES(3, 'sarah');\n1 row(s) inserted.\n \nMach&gt; INSERT INTO upper_table VALUES(4, 'THOMAS');\n1 row(s) inserted.\n \nMach&gt; SELECT id, UPPER(name) FROM upper_table;\nid          UPPER(name)\n----------------------------\n4           THOMAS\n3           SARAH\n2           JAMES\n1           NULL\n[4] row(s) selected.\n\n\nVARIANCE / VAR_POP\n\nThis function is an aggregate function that returns the variance of a given numeric column value. The Variance function returns the variance for the sample, and the VAR_POP function returns the variance for the population.\n\nVARIANCE(column_name)\nVAR_POP(column_name)\n\n\nMach&gt; CREATE TABLE var_table(c1 INTEGER, c2 DOUBLE);\nCreated successfully.\n \nMach&gt; INSERT INTO var_table VALUES (1, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO var_table VALUES (2, 1);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO var_table VALUES (1, 2);\n1 row(s) inserted.\n \nMach&gt; INSERT INTO var_table VALUES (2, 2);\n1 row(s) inserted.\n \nMach&gt; SELECT VARIANCE(c1) FROM var_table;\nVARIANCE(c1)\n------------------------------\n0.333333\n[1] row(s) selected.\n \nMach&gt; SELECT VAR_POP(c1) FROM var_table;\nVAR_POP(c1)\n------------------------------\n0.25\n[1] row(s) selected.\n\n\nYEAR / MONTH / DAY\n\nThese functions extract the corresponding year, month, and day from the input datetime column value and return it as an integer type value.\n\nYEAR(datetime_col)\nMONTH(datetime_col)\nDAY(datetime_col)\n\n\nMach&gt; CREATE TABLE extract_table(c1 DATETIME, c2 INTEGER);\nCreated successfully.\n \nMach&gt; INSERT INTO extract_table VALUES (to_date('2001-01-01 12:30:00 000:000:000'), 1);\n1 row(s) inserted.\n \nMach&gt; SELECT YEAR(c1), MONTH(c1), DAY(c1) FROM extract_table;\nyear(c1)    month(c1)   day(c1)\n----------------------------------------\n2001        1           1\n\n\nISNAN / ISINF\n\nThis function determines whether the numeric value received as an argument is a NaN or Inf value. Returns 1 for NaN or Inf values, and 0 otherwise.\n\nISNAN(number)\nISINF(number)\n\n\nMach&gt; SELECT * FROM test;\nI1                          I2                          I3    \n------------------------------------------------------------------------\n1                           1                           1   \nnan                         inf                         0    \nNULL                        NULL                        NULL    \n[3] row(s) selected.\n \n \nMach&gt; SELECT ISNAN(i1), ISNAN(i2), ISNAN(i3), i3 FROM test ;\nISNAN(i1)   ISNAN(i2)   ISNAN(i3)   i3\n-----------------------------------------------------\n0           0           0           1\n1           0           0           0\nNULL        NULL        NULL        NULL\n[3] row(s) selected.\n \nMach&gt; SELECT * FROM test WHERE ISNAN(i1) = 1;\nI1                          I2                          I3\n------------------------------------------------------------------------\nnan                         inf                         0\n[1] row(s) selected.\n\n\nSupport Type of Built-In Function\n\n\n  \n    \n       \n      Short\n      Integer\n      Long\n      Float\n      Double\n      Varchar\n      Text\n      Ipv4\n      Ipv6\n      Datetime\n      Binary\n    \n  \n  \n    \n      ABS\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      ADD_TIME\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n    \n    \n      AVG\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      BITAND / BITOR\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      COUNT\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      DATE_TRUNC\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n    \n    \n      DECODE\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      x\n      o\n      x\n    \n    \n      FIRST / LAST\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      FROM_UNIXTIME\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      FROM_TIMESTAMP\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      GROUP_CONCAT\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      INSTR\n      x\n      x\n      x\n      x\n      x\n      o\n      o\n      x\n      x\n      x\n      x\n    \n    \n      LEAST / GREATEST\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      LENGTH\n      x\n      x\n      x\n      x\n      x\n      o\n      o\n      x\n      x\n      x\n      o\n    \n    \n      LOWER\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      LPAD / RPAD\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      LTRIM / RTRIM\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      MAX\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      MIX\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      NVL\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      o\n      x\n      x\n      x\n    \n    \n      ROUND\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      ROWNUM\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n    \n    \n      SERIESNUM\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n    \n    \n      STDDEV / STDDEV_POP\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      SUBSTR\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      SUBSTRING_INDEX\n      x\n      x\n      x\n      x\n      x\n      o\n      o\n      x\n      x\n      x\n      x\n    \n    \n      SUM\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      SYSDATE / NOW\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_CHAR\n      o\n      o\n      o\n      o\n      o\n      o\n      x\n      o\n      o\n      o\n      x\n    \n    \n      TO_DATE / TO_DATE_SAFE\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_HEX\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n      o\n    \n    \n      TO_IPV4 / TO_IPV4_SAFE\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_IPV6 / TO_IPV6_SAFE\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_NUMBER / TO_NUMBER_SAFE\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TO_TIMESTAMP\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n    \n    \n      TRUNC\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      TS_CHANGE_COUNT\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      o\n      o\n      o\n      x\n    \n    \n      UNIX_TIMESTAMP\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      UPPER\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n      x\n      x\n      x\n      x\n    \n    \n      VARIANCE / VAR_POP\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n    \n      YEAR / MONTH / DAY\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      x\n      o\n      x\n    \n    \n      ISNAN / ISINF\n      o\n      o\n      o\n      o\n      o\n      x\n      x\n      x\n      x\n      x\n      x\n    \n  \n\n\nJSON-related function\n\nThese functions use json data type as an argument.\n\n\n  \n    \n      Function name\n      Explanation\n      Note\n    \n  \n  \n    \n      JSON_EXTRACT(JSON column name, ‘json path’)\n      Return the value as string type.(If the value does not exist, return ERROR.)\n      - JSON object or array : Convert every objects into string type and return it. - String type : Return as is - Numeric type : Convert into string type and return it - boolean type : Return “True” or “False”\n    \n    \n      JSON_EXTRACT_DOUBLE(JSON column name, ‘json path’)\n      Return the value as 64 bits double type.(If the value does not exist, return NULL.)\n      - JSON object or array : Return NULL - String type : Convert and return if possible. Else return NULL. - Numeric type : Return 64bit real number. - boolean type : Return “True” as 1.0 or “False” as 0.0\n    \n    \n      JSON_EXTRACT_INTEGER(JSON column name, ‘json path’)\n      Return the value as 64 bits integer type.(If the value does not exist, return NULL.)\n      - JSON object or array : Return NULL - String type : Convert and return if possible. Else return NULL. - Numeric type : Return 64bit integer. - boolean type : Return “True” as 1 or “False” as 0\n    \n    \n      JSON_EXTRACT_STRING(JSON column name, ‘json path’)\n      Return the value as string type.(If the value does not exist, return NULL.)Returns same results as operator(→)\n      - JSON object or array : Convert every objects into string type and return it. - String type : Return as is  - Numeric type : Convert into string type and return it  - boolean type : Return “True” or “False”\n    \n    \n      JSON_IS_VALID(‘json string’)\n      Check if the json string is valid for the json format\n      - 0 : False - 1 : True\n    \n    \n      JSON_TYPEOF(JSON column name, ‘json path’)\n      Returns the type of the value.\n      - None : Key does not exists. - Object : Object type - Integer : Integer Type - Real : Real number type - String : String type - True/False : Boolean - Array : Array Type - Null : NULL\n    \n  \n\n\nMach&gt; CREATE TABLE jsontbl (name VARCHAR(20), jval JSON);\nCreated successfully.\n \nMach&gt; INSERT INTO jsontbl VALUES(\"name1\", '{\"name\":\"test1\"}');\n1 row(s) inserted.\nMach&gt; INSERT INTO jsontbl VALUES(\"name2\", '{\"name\":\"test2\", \"value\":123}');\n1 row(s) inserted.\nMach&gt; INSERT INTO jsontbl VALUES(\"name3\", '{\"name\":{\"class1\": \"test3\"}}');\n1 row(s) inserted.\nMach&gt; INSERT INTO jsontbl VALUES(\"name4\", '{\"myarray\": [1, 2, 3, 4]}');\n1 row(s) inserted.\nMach&gt; INSERT INTO jsontbl VALUES(\"name5\", '{\"name\":\"error\"');\n[ERR-02233: Error occurred at column (2): (Error in json load.)]\n \nMach&gt; SELECT name, JSON_EXTRACT_STRING(jval, '$.name') FROM jsontbl;\nname                  JSON_EXTRACT_STRING(jval, '$.name')                                              \n-----------------------------------------------------------------------------------------------------------\nname4                 NULL                                                                             \nname3                 {\"class1\": \"test3\"}                                                              \nname2                 test2                                                                            \nname1                 test1                                                                            \n[4] row(s) selected.\n \nMach&gt; SELECT name, JSON_EXTRACT_INTEGER(jval, '$.myarray[1]') FROM jsontbl;\nname                  JSON_EXTRACT_INTEGER(jval, '$.myarray[1]')\n--------------------------------------------------------------------\nname4                 2                                         \nname3                 NULL                                      \nname2                 NULL                                      \nname1                 NULL                                      \n[4] row(s) selected.\n \nMach&gt; SELECT name, JSON_TYPEOF(jval, '$.name') FROM jsontbl;\nname                  JSON_TYPEOF(jval, '$.name')                                                      \n-----------------------------------------------------------------------------------------------------------\nname4                 None                                                                             \nname3                 Object                                                                           \nname2                 String                                                                           \nname1                 String                                                                           \n[4] row(s) selected.\n\n\nJSON Operator\n\n’-&gt;’ operator is used for accessing an object of JSON data.\n\nReturns the same results as JSON_EXTRACT_STRING function.\n\njson_col -&gt; 'json path'\n\n\nMach&gt; SELECT name, jval-&gt;'$.name' FROM jsontbl;\nname                  JSON_EXTRACT_STRING(jval, '$.name')                                              \n-----------------------------------------------------------------------------------------------------------\nname4                 NULL                                                                             \nname3                 {\"class1\": \"test3\"}                                                              \nname2                 test2                                                                            \nname1                 test1                                                                            \n[4] row(s) selected.\n \nMach&gt; SELECT name, jval-&gt;'$.myarray[1]' FROM jsontbl;\nname                  JSON_EXTRACT_INTEGER(jval, '$.myarray[1]')\n--------------------------------------------------------------------\nname4                 2                                         \nname3                 NULL                                      \nname2                 NULL                                      \nname1                 NULL                                      \n[4] row(s) selected.\n \nMach&gt; SELECT name, jval-&gt;'$.name.class1' FROM jsontbl;\nname                  jval-&gt;'$.name.class1'                                                            \n-----------------------------------------------------------------------------------------------------------\nname4                 NULL                                                                             \nname3                 test3                                                                            \nname2                 NULL                                                                             \nname1                 NULL                                                                             \n[4] row(s) selected"
					}
					
				
		
				
					,
					
					"feature-tables-log-input-import-html": {
						"id": "feature-tables-log-input-import-html",
						"title": "Import",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/input/import.html",
						"content": "Index\n\n\n  Importing Data\n  Confirm Data Insert\n  Sample Example\n\n\nWith the machloader tool, you can enter a text file that is separated by a CSV or other delimiter.\n\nSee the machloader documentation for a detailed description of the machloader tool.\n\nCreate Table\n\nCREATE TABLE import_sample\n(\n    srcip     IPV4,\n    srcport   INTEGER,\n    dstip     IPV4,\n    dstport   INTEGER,\n    protocol  SHORT,\n    eventlog  VARCHAR(1024),\n    eventcode SHORT,\n    eventsize LONG\n);\n\n\nImporting Data\n\nUse the machloader tool to enter the csv file.\n\nmachloader  -i  -t  import_sample   -d  sample_data.csv\n\n\nConfirm Data Insert\n\nCheck the input data.\n\nSELECT  COUNT(*)    FROM    import_sample;\n\n\nSample Example\n\nBelow is a sample process using the actual machloader and machsql.\n\nMach&gt; CREATE TABLE import_sample\n     (\n         srcip     IPV4,\n         srcport   INTEGER,\n         dstip     IPV4,\n         dstport   INTEGER,\n         protocol  SHORT,\n         eventlog  VARCHAR(1024),\n         eventcode SHORT,\n         eventsize LONG\n     );\nCreated successfully.\nMach&gt; quit\n\n\n[mach@localhost ~]$ cd $MACHBASE_HOME/sample/quickstart\n[mach@localhost ~]$ ls -l sample_data.csv\n-rw-r--r--- 1 mach mach 110477124 2017-02-23 15:18 sample_data.csv\n \n[mach@localhost ~]$ machloader -i -t import_sample -d sample_data.csv\n-----------------------------------------------------------------\n     Machbase Data Import/Export Utility.\n     Release Version x.x.x.official\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved.\n-----------------------------------------------------------------\nNLS            : US7ASCII            EXECUTE MODE   : IMPORT\nTARGET TABLE   : import_sample\nDATA FILE      : sample_data.csv\nIMPORT_MODE    : APPEND\nFILED TERM     : ,                   ROW TERM       : \\n\nENCLOSURE      : \"                   ARRIVAL_TIME   : FALSE\nENCODING       : NONE                HEADER         : FALSE\nCREATE TABLE   : FALSE\n Progress bar                       Imported records        Error records\n                                             1000000                    0\nImport time         :  0 hour  0 min  2.39 sec\nLoad success count  : 1000000\nLoad fail count     : 0\n[mach@localhost ~]$\n\n\nMach&gt; SELECT COUNT(*) FROM import_sample;\nCOUNT(*)\n-----------------------\n1000000\n[1] row(s) selected.\nMach&gt;"
					}
					
				
		
				
					,
					
					"feature-tables-volatile": {
						"id": "feature-tables-volatile",
						"title": "Creating and Managing Volatile Index",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/volatile/",
						"content": "Create and Use Index\n\nThe volatile table provides a RED-BLACK Tree optimized for real-time search. Indexes can be set for all data types. However, one index can be created for one column, and no composite index is provided.\n\nMach&gt; create volatile table vtable (id integer, name varchar(10));\nCreated successfully.\nMach&gt; create index idx_vrb on vtable (name) index_type redblack;\nCreated successfuly.\nMach&gt; desc vtable;\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nID                            integer             11             \nNAME                          varchar             10                 \n \n[ INDEX ]                             \n----------------------------------------------------------------\nNAME                          TYPE                COLUMN\n----------------------------------------------------------------\nIDX_VRB                       REDBLACK            NAME               \niFlux&gt;\n\n\nPrimary Key Index\n\nWhen a primary key is assigned to a specific column of a volatile table, a RED-BLACK Tree index is automatically generated. In this case, a special index with a Uniqueness attribute is created and does not allow duplicate values.\n\nMach&gt; create volatile table vtable (id integer primary key, name varchar(20));\nCreated successfully.\nMach&gt; desc vtable;\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nID                            integer             11             \nNAME                          varchar             20                 \n \n[ INDEX ]                             \n----------------------------------------------------------------\nNAME                          TYPE                COLUMN\n----------------------------------------------------------------\n__PK_IDX_VTABLE               REDBLACK            ID  \n \niFlux&gt;\n\n\nOther Index Types\n\nThe bitmap or keyword index used in the log table can not be used in a volatile table.\n\nMach&gt; create bitmap   index idx_1237 on vtable(id1);\n[ERR-02069 : Error in index for invalid table. BITMAP Index can only be created for LOG Table.]\nMach&gt; create keyword  index idx_1238 on vtable(name);\n[ERR-02069 : Error in index for invalid table. KEYWORD Index can only be created for LOG Table.]"
					}
					
				
		
				
					,
					
					"feature-tables-lookup": {
						"id": "feature-tables-lookup",
						"title": "Creating and Managing Lookup Index",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/lookup/",
						"content": "As with volatile tables, RED-BLACK indexes are provided as standard, and their usage is the same as for volatile tables. It is possible to create RED-BLACK index like volatile table, and it is impossible to generate keyword and LSM index."
					}
					
				
		
				
					,
					
					"feature-tables-log": {
						"id": "feature-tables-log",
						"title": "Index of Log Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/",
						"content": "Index\n\n\n  Create Index\n  Change Index\n  Delete Index\n\n\nTwo index types can be created in the Machbase log table.\n\nFor more information, refer to the DDL page CREATE INDEX section of the SQL Reference  .\n\n\n  BITMAP Index: bitmap index can be created in every column except Text, Binary type.\n  KEYWORD Index: Used to search strings as it can be generated only for Varchar and Text column.\n\n\nCreate Index\n\nCreate an index on a specific column using the CREATE INDEX statement.\n\nCREATE INDEX index_name ON table_name (column_name) [index_type] [tablespace] [index_prop_list]\n    index_type ::= INDEX_TYPE { LSM | KEYWORD }\n    tablespace ::= TABLESPACE tablesapce_name\n    index_prop_list ::= value_pair, value_pair, ...\n    value_pair ::= property_name = property_value\n\n\nMach&gt; CREATE INDEX id_index ON log_data(id) INDEX_TYPE LSM TABLESPACE tbs_data MAX_LEVEL=3;\nCreated successfully.\n\n\nChange Index\n\nChange the index attribute using the ALTER INDEX statement.\n\nALTER INDEX index_name SET KEY_COMPRESS = { 0 | 1 }\n\n\nMach&gt; ALTER INDEX id_index SET KEY_COMPRESS = 1;\n\n\nDelete Index\n\nDelete the specified index using the DROP INDEX statement. However, if there is another session in which the table is being searched, it will fail with an error.\n\nDROP INDEX index_name;\n\n\nMach&gt; DROP INDEX id_index;\nDropped successfully."
					}
					
				
		
				
					,
					
					"feature-tables-tag": {
						"id": "feature-tables-tag",
						"title": "Index for tag table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/",
						"content": "Index\n\n\n  Create Index\n  Delete Index\n\n\nTAG index types can be created on Machbase TAG table.\n\nFor more information, refer to the DDL section of the SQL Reference  .\n\n\n  TAG Index: TAG index can be created in additional columns in TAG table.\n\n\nCreate Index\n\nCreate an index on a specific column using the CREATE INDEX statement.\n\nCREATE INDEX index_name ON table_name (column_name) [index_type]\n    index_type ::= INDEX_TYPE { TAG }\n\n\nMach&gt; CREATE INDEX id_index ON tag (id) INDEX_TYPE TAG;\nCreated successfully.\n\n\nStarting with version 7.5, indexes can be created for each json path for json type columns only in the tag table.\n\nJust connect the json path with the operator to the existing index creation syntax.\n\nSince the return type of the json operator is VARCHAR, indexes are used only when comparing VARCHARs.\n\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, jval JSON);\nExecuted successfully.\n  \nMach&gt; CREATE INDEX idx_jval_value1 ON tag (jval-&gt;'$.value1');\nCreated successfully.\n  \nMach&gt; CREATE INDEX idx_jval_value2 ON tag (jval-&gt;'$.value2');\nCreated successfully.\n  \nMach&gt; EXPLAIN SELECT * FROM tag WHERE jval-&gt;'$.value1' = '10';\nPLAN                                                                            \n------------------------------------------------------------------------------------\n PROJECT                                                                        \n  TAG READ (RAW)                                                                \n   KEYVALUE INDEX SCAN (_TAG_DATA_0)                                            \n    [KEY RANGE]                                                                 \n     * jval-&gt;'$.value1' = '10'                                                  \n   VOLATILE FULL SCAN (_TAG_META)                                               \n[6] row(s) selected.\n\n\nDelete Index\n\nDelete the specified index using the DROP INDEX statement. However, if there is another session in which the table is being searched, it will fail with an error.\n\nDROP INDEX index_name;\n\n\nMach&gt; DROP INDEX id_index;\nDropped successfully."
					}
					
				
		
				
					,
					
					"": {
						"id": "",
						"title": "Machbase English Manual",
						"version": "all",
						"categories": "",
						"url": " /",
						"content": "Introduction\n\n\n  Introduction of Machbase\n  Machbase Features\n  Introduction of Machbase Products\n\n\nInstallation\n\n\n  Package Overview\n  Linux Installation\n  Windows Installation\n  License Installation\n  Cluster Edition Installation\n\n\nUpgrade\n\n\n  Cluster Edition Upgrade\n\n\nFeature and Tables\n\n\n  Tag Table\n  Log Table\n  Volatile Table\n  Lookup Table\n  STREAM\n  Backup and Mount\n  Data Auto Delete\n\n\nConfig/Monitoring\n\n\n  Meta Table\n  Virtual Table\n  Property\n  Property(Cluster)\n\n\nSQL Reference\n\n\n  Datatypes\n  DDL\n  DML\n  SELECT\n  SELECT Hint\n  User Management\n  Functions\n  System/Session Management\n\n\nSDK\n\n\n  CLI/ODBC\n  CLI/ODBC Example\n  JDBC\n  Python\n  .Net Connector\n  RESTfull API\n  Timezone\n\n\nTools\n\n\n  Utilities\n  machcoordinatoradmin\n  machdeployeradmin\n\n\nFAQ\n\n\n  How to fix properties when an insufficient memory error occurs"
					}
					
				
		
				
					,
					
					"feature-tables-log-input-html": {
						"id": "feature-tables-log-input-html",
						"title": "Log Data Input",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/input.html",
						"content": "There are many ways to input log data into Machbase.\n\n\n  Insert\n  Append\n  Import\n  Load by SQL"
					}
					
				
		
				
					,
					
					"feature-tables-tag-manipulate-input-html": {
						"id": "feature-tables-tag-manipulate-input-html",
						"title": "Input tag data",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/manipulate/input.html",
						"content": "Index\n\n\n  Input using the INSERT statement\n  Load all at once through a CSV file \n    \n      CSV Format (data.csv)\n    \n  \n  Input via the RESTful API\n  Input data via the SDK\n\n\nThere are various way to insert tag data.\n\nInput using the INSERT statement\n\nThe easiest way to insert data is through the INSERT statement.\n\nThis is used for the small data sets, but if you want to load large amounts of data quickly, use another method.\n\nMach&gt; create tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\nExecuted successfully.\n \nMach&gt;  insert into tag metadata values ('TAG_0001');\n1 row(s) inserted.\n \nMach&gt; insert into tag values('TAG_0001', now, 0);\n1 row(s) inserted.\n \nMach&gt; insert into tag values('TAG_0001', now, 1);\n1 row(s) inserted.\n \nMach&gt; insert into tag values('TAG_0001', now, 2);\n1 row(s) inserted.\n \nMach&gt; select * from tag where name = 'TAG_0001';\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\nTAG_0001              2018-12-19 17:41:37 806:901:728 0                          \nTAG_0001              2018-12-19 17:41:42 327:839:368 1                          \nTAG_0001              2018-12-19 17:41:43 812:782:202 2                          \n[3] row(s) selected.\n\n\nWe put the three TAG values as the current time.\n\nLoad all at once through a CSV file\n\nMachbase allows you to load large amounts of CSV files through a csvimport tool.\n\nMore details can be found through practical examples, and are described in brief below.\n\nCSV Format (data.csv)\n\nTAG_0001, 2009-01-28 07:03:34 0:000:000, -41.98\nTAG_0001, 2009-01-28 07:03:34 1:000:000, -46.50\nTAG_0001, 2009-01-28 07:03:34 2:000:000, -36.16 ....\n\n\nPrepare a csv file consisting of &lt;tag name, time, value&gt; as above.\n\nOf course, the tag name TAG_0001 must be present in the tag meta.\n\nUsing the loading program csvimport\n\ncsvimport -t TAG -d data.csv -F \"time YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn\" -l error.log\n\n\nThis loads data.csv into a table called TAG.\n\nThe -F option specifies the time format stored in the data.csv file.\n\nIn addition, -l error.log records the error that occurred when inputting as a separate file.\n\nInput via the RESTful API\n\nFor more detailed usage of the RESTful API, please refer to the following examples.\n\nSyntax of Input API\n\nMachbase provides the RESTful API as follows:\n\n{\"values\":[ [TAG_NAME, TAG_TIME, VALUE], # Specify Tagname,Time,Value. if you define additional columns, the columns and values are follows. [ .... ].... ], \"date_format\":\"Date Format\" # If you moit date_format, 'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn' is used as default. }\n\n\nIt is requested the number of columns in the defined TAG schema to match the above structure.\n\nInput data via the SDK\n\nMachbase provides standard development tools for a variety of languages, including:\n\n\n  C/C++ library\n  Java library\n  Python library\n  C# library\n\n\nThrough these libraries, users can create various application programs according to their environment and input data to Machbase."
					}
					
				
		
				
					,
					
					"feature-tables-volatile-insert-update-html": {
						"id": "feature-tables-volatile-insert-update-html",
						"title": "Inserting and Updating Volatile Data",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/volatile/insert-update.html",
						"content": "Data Insert\n\nThe data insert of the volatile table is as follows.\n\nMach&gt; create volatile table vtable (id integer, name varchar(20));\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(3, 'north device');\n1 row(s) inserted.\nMach&gt; insert into vtable values(4, 'south device');\n1 row(s) inserted.\n\n\nData Append\n\nIt is a fast real-time data input API provided by Machbase.\nC, C++, C#, Java, Python, PHP, Javascript are available for append.\n\nMach&gt; create volatile table vtable (id integer, value double);\n\n\nSQL_APPEND_PARAM sParam[2];\nfor(int i=0; i&lt;10000; i++)\n{\n    sParam[0].mInteger  = i;\n    sParam[1].mDouble   = i;\n    SQLAppendDataV2(stmt, sParam) != SQL_SUCCESS)\n}\n\n\nFor the Cluster Edition Append, must be done by Leader Broker.\n\nDetails are in SDK guide.\n\nData Update\n\nWhen inputting data in a volatile table, data with duplicate primary key values ​​can be updated using the ON DUPLICATE KEY UPDATE clause.\n\nUpdate Data Value to be Inserted\n\nIf the INSERT statement specifies data to be inserted, but there is other data that matches the primary key value of the insert data, the INSERT statement fails and the corresponding data is not inserted. If there is another data that matches the primary key value of the insertion data, and if you wish to update the corresponding data instead of insertion, a  ON DUPLICATE KEY UPDATE clause can be added.\n\n\n  If there is no duplicate primary key data, the contents of the data to be inserted are inserted as is.\n  If there is duplicate primary key data, the existing data is updated with the contents of the data to be inserted.\n\n\nThe constraints for using this function are as follows.\n\n\n  The primary key must be specified in the volatile table.\n  The value to be inserted must include the primary key value.\n\n\nMach&gt; create volatile table vtable (id integer primary key, direction varchar(10), refcnt integer);\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west', 0);\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east', 0);\n1 row(s) inserted.\nMach&gt; select * from vtable;\nID          DIRECTION   REFCNT     \n----------------------------------------\n1           west       0          \n2           east        0          \n[2] row(s) selected.\n \nMach&gt; insert into vtable values(1, 'south', 0);\n[ERR-01418 : The key already exists in the unique index.]\nMach&gt; insert into vtable values(1, 'south', 0) on duplicate key update;\n1 row(s) inserted.\n \nMach&gt; select * from vtable;\nID          DIRECTION   REFCNT     \n----------------------------------------\n1           south        0          \n2           east        0          \n[2] row(s) selected.\n\n\nSpecify Data Value to be Updated\n\nSimilar to above, but if you need to update to a different column value than the data value to be inserted, it can be specified through the ON DUPLICATE KEY UPDATE SET clause. The data value to be updated can be specified under the SET clause.\n\n\n  If the primary key duplication data does not exist, the contents of the embedded data are inserted as it is.\n  If there is duplicate primary key data, the existing data is updated only with the update data specified in the SET clause.\n  The primary key value can not be specified as the data value to be updated.\n  The values ​​of the columns not specified in the SET clause are not updated.\n\n\nMach&gt; create volatile table vtable (id integer primary key, direction varchar(10), refcnt integer);\nCreated successfully.\nMach&gt; insert into vtable values(1, 'west', 0);\n1 row(s) inserted.\nMach&gt; insert into vtable values(2, 'east', 0);\n1 row(s) inserted.\nMach&gt; select * from vtable;\nID          DIRECTION   REFCNT     \n----------------------------------------\n1           west        0          \n2           east        0          \n[2] row(s) selected.\n \nMach&gt; insert into vtable values(1, 'west', 0) on duplicate key update set refcnt = 1;\n1 row(s) inserted.\n \nMach&gt; select * from vtable;\nID          DIRECTION   REFCNT     \n----------------------------------------\n1           west        1          \n2           east        0          \n[2] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-tables-lookup-insert-html": {
						"id": "feature-tables-lookup-insert-html",
						"title": "Lookup Data Insert",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/lookup/insert.html",
						"content": "Most things are the same as the input and update method of the volatile table.\n\nThere is one difference, when data is inserted through APPEND to the lookup table, if the primary key is duplicated, you can update the corresponding row by setting the ‘LOOKUP_APPEND_UPDATE_ON_DUPKEY’ property.\n\nDetails about ‘LOOKUP_APPEND_UPDATE_ON_DUPKEY’, Property guide will help you.\n\nLookup Table Reload\n\nFrom machbase 6.7, Lookup Node manages Lookup table data.\n\nIf you want to reload the lookup table data from the lookup node, you can do this by using the EXEC TABLE_REFRESH command.\n\nEXEC TABLE_REFRESH(lktable);"
					}
					
				
		
				
					,
					
					"feature-tables-log-input-insert-html": {
						"id": "feature-tables-log-input-insert-html",
						"title": "Insert",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/input/insert.html",
						"content": "Index\n\n  Create Table\n  Data Insertion\n  Confirm Data Insertion\n  Entire Process\n\n\nSimilar to other commercial RDBMSs, you can first create the table and enter the data using the INSERT INTO statement.\n\nMachbase provides the ‘machsql’ tool as an interactive query processor.\n\nCreate Table\n\nCREATE TABLE table_name ( column1 datatype, column2 datatype, column3 datatype, .... );\n\n\nCREATE TABLE sensor_data ( id VARCHAR(32), val DOUBLE );\n\n\nData Insertion\n\nINSERT INTO table_name VALUES (value1, value2, value3, ...);\n\n\nINSERT INTO sensor_data VALUES('sensor1', 10.1); INSERT INTO sensor_data VALUES('sensor2', 20.2); INSERT INTO sensor_data VALUES('sensor3', 30.3);\n\n\nConfirm Data Insertion\n\nSELECT column1, column2, ... FROM table_name;\n\n\nSELECT * FROM sensor_data;\n\n\nEntire Process\n\nBelow is an example using machsql.\n\nMach&gt; CREATE TABLE sensor_data (id VARCHAR(32), val DOUBLE);\n Created successfully.\nMach&gt; INSERT INTO sensor_data VALUES('sensor1', 10.1);\n 1 row(s) inserted.\nMach&gt; INSERT INTO sensor_data VALUES('sensor2', 20.2);\n 1 row(s) inserted.\nMach&gt; INSERT INTO sensor_data VALUES('sensor3', 30.3);\n 1 row(s) inserted.\nMach&gt; SELECT * FROM sensor_data;\nID VAL\n-----------------------------------------------------------------\nsensor3 30.3\nsensor2 20.2\nsensor1 10.1\n[3] row(s) selected."
					}
					
				
		
				
					,
					
					"intro-introduction-html": {
						"id": "intro-introduction-html",
						"title": "Introductions of Machbase",
						"version": "all",
						"categories": "",
						"url": " /intro/introduction.html",
						"content": "Machbase is a new generation columnar time series database that ingests and stores large amounts of log or time stamped data. Machbase is designed for collecting, in real time, “machine or sensor data” from various IoT environments while simultaneously allowing the data to be queried and analyzed using standard SQL and APIs at very high speeds. Machbase supports any IoT data architecture, and works extremely well on limited edge compute devices, or gateway/fog platforms, as well as cloud/cluster implementations.\n\nMachbase solves the problem of storing and processing massive amounts of sensor data that could not be solved by existing current big data solutions, while safeguarding the future emergence of new sensor data requirements through various capabilities and functions.\n\nThe Emergence of New Data\n\nThrough the recent development of applying big data and real-time analytics to the internet of Things (IoT) domain and with the promise of Machine Learning (ML), various and vast amounts of data are being captured and accumulated.  With the future of 5G systems entering the market, machine to machine data traffic will continue to explode.\n\nThe Emergence of New Data\nTypes of Sensor Data\nSensor Data Structure in PLC\nTherefore, by analyzing these data in real-time, new applications, better efficiencies, greater reliability and fewer failures will be achieved.\nIn particular, the number of source devices, such as edge devices or sensors, have dramatically increased in the last few years, and as a result the generated data from these devices have also increased exponentially.\nHowever, traditional, and even big data processing software are not providing adequate solutions to meet the volume and performance needs.\nThe reasons why the current, conventional solutions are not suitable for the current data processing are summarized as follows:\n\nFirst, the rate of data generation is increasing exponentially.\n\nAs the number of data sources grows and the types of information that need to be processed increases, the rate at which the server can receive and store the data is entirely different from before.\n\nThere is no software solution optimized for storing tens of thousands to hundreds of thousands of data per second aside from saving it as a plain text file in the file system.\n\nSecond, the demand for real - time data analysis is increasing in proportion to the rate of data generation.\n\nUsing big data technics to help decision making is very important; however, existing solutions are not technically advanced enough to index hundreds of thousands of data per second and deliver results to the user, through search and analysis, in real-time.\n\nFinally, as previously mentioned, the characteristics of “sensor data” is completely different.\n\nConventional databases have architectures that are inadequate for processing sensor-type machine data, and to address this, a new technological approach is needed.\n\nFor this reason, Machbase has been newly developed with the best architecture to handle new “sensor-type machine data” and is the only database solution that can store, process, and analyze real-time data.\n\nTypes of Sensor Data\n\n\n  \n    \n      ID\n      Time\n      Data\n    \n  \n  \n    \n      Temperature01\n      2020-01-01 00:00:00\n      20\n    \n    \n      Pressure01\n      2020-01-01 00:00:00\n      0.98\n    \n  \n\n\nID\n\nThis value represents a symbol and a number indicating the uniqueness of the device (source code) where the corresponding machine data is generated. It is the serial number of the machine or sensor and is represented as a 32-bit or 64-bit integer.\n\nTIME\n\nThis value represents the time when the corresponding machine data occurred. This time stamp has the tendency to increase continuously and is usually in units of a second but can also be as fine as in nanoseconds.\n\nDATA\n\nThis value is binary data, mainly in the form of integer type, real number type, or IP address value. Typically, this domain includes numeric values ​​such as temperature, acceleration, and brightness from a specific sensor, or fixed data of 4-byte or 16-byte similar to IPv4 or IPv6.  In certain applications the value can be in binary format as from camera or audio devices.\n\nBy providing a Tag Table, Machbase offers optimized architecture that can receive hundreds of thousands of sensor data per second.\n\nSensor Data Structure in PLC\n\ndata from PLC\nTime          SN01M  SN02M  SN03M  SN04M  SN05M  SN06M  SN07M  SN08M  SN09M  SN10M\n04:01:56.005    11.1      1      0      0      0      1      0      0      0      0\n04:01:56.057    11.3      1      0      0      0      1      0      0      0      1\n04:01:56.109    11.1      1      0      1      0      1      0      1      1      0\n04:01:56.161    12.3      1      0      0      0      1      0      0      0      1\n04:01:56.213     9.1      1      1      0      0      1      0      0      0      0\n04:01:56.266     0.9      1      0      0      1      1      0      0      0      0\n04:01:56.319     8.9      1      0      1      0      1      0      1      0      1\n04:01:56.370     1.3      1      0      0      0      1      0      0      0      0\n04:01:56.422    33.1      1      0      0      0      1      0      0      0      0\n04:01:56.474     3.3      1      0      0      0      1      0      0      0      0\n04:01:56.526     5.6      1      0      0      1      1      0      0      0      1\n04:01:56.578     5.5      1      0      0      0      1      0      0      1      0\n04:01:56.630     4.5      1      0      0      0      1      0      0      0      0\n04:01:56.682     5.3      1      0      0      0      1      0      0      0      0\n04:01:56.733     1.2      1      0      0      0      1      0      0      0      1\n04:01:56.785     3.4      1      0      0      1      1      0      0      0      0\n04:01:56.838    11.3      1      0      1      0      1      0      1      0      0\n04:01:56.890    11.2      1      0      0      0      1      0      0      0      0\n04:01:56.942     9.9      1      0      0      0      1      0      0      0      1\n04:01:56.994     8.4      1      0      0      0      1      0      0      0      0\n04:01:57.046     8.4      1      0      1      0      1      0      0      0      0\n04:01:57.097     1.2      1      0      0      1      1      0      1      1      1\n04:01:57.149     1.3      1      0      0      0      1      0      0      0      0\n04:01:57.200    11.2      1      0      0      0      1      0      0      0      0\n04:01:57.252     3.1      1      0      0      0      1      0      0      0      0\n\n\nMachbase provides a Log Table, which also provides a structure to store the above PLC data.\n\nThis table also provides the ability to enter and analyze tens of thousands of data per second."
					}
					
				
		
				
					,
					
					"sdk-jdbc-html": {
						"id": "sdk-jdbc-html",
						"title": "JDBC",
						"version": "all",
						"categories": "",
						"url": " /sdk/jdbc.html",
						"content": "Index\n\n  JDBC Overview\n  Standard JDBC Functions\n  Extension JDBC Functions\n    \n      setIPv4\n      setIPv6\n      executeAppendOpen\n      executeAppendData\n      executeAppendDataByTime\n      executeAppendClose\n      executeSetAppendErrorCallback\n      getAppendSuccessCount\n      getAppendFailCount\n    \n  \n  Application Development\n    \n      JDBC Library Installation Check\n      Makefile Creation Guide\n      Compile and Link\n      JDBC Sample\n      Connection Example\n      Data Input and Output Example (1) Direct I/O\n      Data Input and Output Example (2) PreparedStatement Input Used\n      Extension Function Append Example\n    \n  \n\n\nJDBC Overview\n\nThe set of database manipulation interfaces created in the Java programming language is called JDBC (Java DataBase Connectivity). A set of APIs that provide a consistent interface for a variety of relational databases, defining a set of object-oriented classes of classes that the programmer will use to build SQL requests. That is, if you use a JDBC driver, no matter which database you use, there is an advantage that you can apply it directly without modifying the code.\n\nStandard JDBC Functions\n\nStandard Function Specs 4.0\n\nExtension JDBC Functions\n\nsetIPv4\n\nvoid setIpv4(int ind, String ipString)\n\n\nThis is a function to input IPv4 address type in PrepareStatement.\n\nReceives column index and IPv4 string as arguments.\n\nsetIPv6\n\nvoid setIpv6(int ind, String ipString)\n\n\nThis is a function to input IPv6 address type in PrepareStatement.\n\nReceives column index and IPv6 string as arguments.\n\nexecuteAppendOpen\n\nResultSet executeAppendOpen(String aTableName, int aErrorCheckCount)\n\n\nOpens the protocol to write the Append protocol in the Statement.\n\nThe table name and error checking interval are received as arguments. Returns a ResultSet with the result value.\n\nexecuteAppendData\n\nint executeAppendData(ResultSetMetaData rsmd, ArrayList aData)\n\n\nEnters the actual data for the Append protocol in the statement.\n\nReceives the metadata of the ResultSet, which is the result value of executeAppendOpen, and the data to input. When the result value is stored in the transfer buffer, 1 is returned. If the transfer buffer is transferred to Machbase, 2 is returned. Therefore, if 1 or 2 is returned, it is judged as success.\n\nexecuteAppendDataByTime\n\nint executeAppendDataByTime(ResultSetMetaData rsmd, long aTime, ArrayList aData)\n\n\nEnters the actual data for the Append protocol on a time basis in the statement.\n\nReceives the metadata of the ResultSet which is the result value of executeAppendOpen, the time value of the specific time zone to be set, and the data to input as arguments. If the result value is stored in the transmission buffer, 1 is returned.\n\nexecuteAppendClose\n\nint executeAppendClose()\n\n\nTerminates the statement for the Append protocol in the statement.\n\nIf the result is a success, it returns 1.\n\nexecuteSetAppendErrorCallback\n\nint executeSetAppendErrorCallback(MachAppendCallback aCallback)\n\n\nSets a callback function that outputs an error if an error occurs during Append execution.\n\nIt takes a callback function that outputs an error log as an argument. If the result is successful, 1 is returned.\n\ngetAppendSuccessCount\n\nlong getAppendSuccessCount()\n\n\nReturns the number of successes for the Append protocol in the Statement.\n\nReturns the number of successful results.\n\ngetAppendFailCount\n\nlong getAppendFailCount()\n\n\nReturns the number of failures for the Append protocol in the Statement.\n\nReturns the number of failures as a result.\n\nApplication Development\n\nJDBC Library Installation Check\nVerifies that the machbase.jar file exists in the $MACHBASE_HOME/lib directory.\n\n[mach@localhost ~]$ cd $MACHBASE_HOME/lib\n[mach@localhost lib]$ ls -l machbase.jar\n-rw-rw-r-- 1 mach mach 78599 Jun 18 10:00 machbase.jar\n[mach@localhost lib]$\n\n\nMakefile Creation Guide\n\n$(MACHBASE_HOME)/lib/machbase.jar must be specified in the classpath. The following is an example of a Makefile.\n\nCLASSPATH=\".:$(MACHBASE_HOME)/lib/machbase.jar\"\n \nSAMPLE_SRC = Sample1Connect.java Sample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n \nall: build\n \nbuild:\n    -@rm -rf *.class\n    javac -classpath $(CLASSPATH) -d . $(SAMPLE_SRC)\n \ncreate_table:\n    machsql -s localhost -u sys -p manager -f createTable.sql\n \nselect_table:\n    machsql -s localhost -u sys -p manager -f selectTable.sql\n \nrun_sample1:\n    java -classpath $(CLASSPATH) Sample1Connect\n \nrun_sample2:\n    java -classpath $(CLASSPATH) Sample2Insert\n \nrun_sample3:\n    java -classpath $(CLASSPATH) Sample3PrepareStmt\n \nrun_sample4:\n    java -classpath $(CLASSPATH) Sample4Append\n \nclean:\n    rm -rf *.class\n\n\nCompile and Link\nRun the make command to compile and link as follows:\n\n[mach@localhost jdbc]$ make\njavac -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" -d . Sample1Connect.java Sample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n[mach@localhost jdbc]$\n\n\nJDBC Sample\n\nConnection Example\n\nLet’s write an example program that connects to a Machbase server using a Machbase JDBC driver. Name the source file Sample1Connect.java.\n\n\n   The _arrival_time column is not displayed by default. Therefore, to display the _arrival_time column, add show_hidden_cols = 1 to the connection string.\n     You can modify the connection string in the following example source as follows:\n     String sURL = “jdbc:machbase://localhost:5656/mhdb?show_hidden_cols=1”;\n\n\nimport java.util.*;\nimport java.sql.*;\nimport com.machbase.jdbc.*;\n \npublic class Sample1Connect\n{\n    public static Connection connect()\n    {\n        Connection conn = null;\n        try\n        {\n            String sURL = \"jdbc:machbase://localhost:5656/mhdb\";\n \n            Properties sProps = new Properties();\n            sProps.put(\"user\", \"sys\");\n            sProps.put(\"password\", \"manager\");\n \n            Class.forName(\"com.machbase.jdbc.driver\");\n            conn = DriverManager.getConnection(sURL, sProps);\n        }\n        catch ( ClassNotFoundException ex )\n        {\n            System.err.println(\"Exception : unable to load mach jdbc driver class\");\n        }\n        catch ( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        return conn;\n    }\n \n    public static void main(String[] args) throws Exception\n    {\n        Connection conn = null;\n \n        try\n        {\n            conn = connect();\n            if( conn != null )\n            {\n                System.out.println(\"mach JDBC connected.\");\n            }\n        }\n        catch( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        finally\n        {\n            if( conn != null )\n            {\n                conn.close();\n                conn = null;\n            }\n        }\n    }\n}\n\n\nNow compile and run the source code. Use the Makefile you have already created.\n\n[mach@localhost jdbc]$ make\njavac -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" -d . Sample1Connect.java Sample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n[mach@localhost jdbc]$ make run_sample1\njava -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" Sample1Connect\nmach JDBC connected.\n\n\nData Input and Output Example (1) Direct I/O\n\nCreate and display an example that uses the Machbase JDBC driver to input and output data.\n\nThe name of the source file is called Sample2Insert.java.\nFirst, you need to create the necessary tables using the machsql program. \nIn the example, we used the sample code to create a table called sample_table in advance.\n\n[mach@localhost jdbc]$ machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version 3.5.0.826b8f2.official\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase server address (Default:127.0.0.1):\nMachbase rser ID  (Default:SYS)\nMachbase user password: MANAGER\nMACHBASE_CONNECT_MODE=INET, PORT=5656\nmach&gt; create table sample_table(d1 short, d2 integer, d3 long, f1 float, f2 double, name varchar(20), text text, bin binary, v4 ipv4, v6 ipv6, dt datetime);\nCreated successfully.\nmach&gt; exit\n[mach@localhost jdbc]$\n\n\nimport java.util.*;\nimport java.sql.*;\nimport com.machbase.jdbc.*;\n \npublic class Sample2Insert\n{\n    public static Connection connect()\n    {\n        Connection conn = null;\n        try\n        {\n \n            String sURL = \"jdbc:machbase://localhost:5656/mhdb\";\n \n            Properties sProps = new Properties();\n            sProps.put(\"user\", \"sys\");\n            sProps.put(\"password\", \"manager\");\n \n            Class.forName(\"com.machbase.jdbc.driver\");\n \n            conn = DriverManager.getConnection(sURL, sProps);\n \n        }\n        catch ( ClassNotFoundException ex )\n        {\n            System.err.println(\"Exception : unable to load mach jdbc driver class\");\n        }\n        catch ( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n \n        return conn;\n    }\n \n \n    public static void main(String[] args) throws Exception\n    {\n        Connection conn = null;\n        Statement stmt = null;\n        String sql;\n \n        try\n        {\n            conn = connect();\n            if( conn != null )\n            {\n                System.out.println(\"mach JDBC connected.\");\n \n                stmt = conn.createStatement();\n \n                for(int i=1; i&lt;10; i++)\n                {\n                    sql = \"INSERT INTO SAMPLE_TABLE VALUES (\";\n                    sql += (i - 5) * 6552;//short\n                    sql += \",\"+ ((i - 5) * 429496728);//integer\n                    sql += \",\"+ ((i - 5) * 922337203685477580L);//long\n                    sql += \",\"+ 1.23456789+\"e\"+((i&lt;=5)?\"\":\"+\")+((i-5)*7);//float\n                    sql += \",\"+ 1.23456789+\"e\"+((i&lt;=5)?\"\":\"+\")+((i-5)*61);//double\n                    sql += \",'id-\"+i+\"'\";//varchar\n                    sql += \",'name-\"+i+\"'\";//text\n                    sql += \",'aabbccddeeff'\";//binary\n                    sql += \",'192.168.0.\"+i+\"'\";//ipv4\n                    sql += \",'::192.168.0.\"+i+\"'\";\n                    sql += \",TO_DATE('2014-08-0\"+i+\"','YYYY-MM-DD')\";//dt\n                    sql += \")\";\n \n                    stmt.execute(sql);\n \n                    System.out.println( i+\" record inserted.\");\n                }\n \n                String query = \"SELECT d1, d2, d3, f1, f2, name, text, bin, to_hex(bin), v4, v6, to_char(dt,'YYYY-MM-DD') as dt from SAMPLE_TABLE\";\n                ResultSet rs = stmt.executeQuery(query);\n                while( rs.next () )\n                {\n                    short d1 = rs.getShort(\"d1\");\n                    int d2 = rs.getInt(\"d2\");\n                    long d3 = rs.getLong(\"d3\");\n                    float f1 = rs.getFloat(\"f1\");\n                    double f2 = rs.getDouble(\"f2\");\n                    String name = rs.getString(\"name\");\n                    String text = rs.getString(\"text\");\n                    String bin = rs.getString(\"bin\");\n                    String hexbin = rs.getString(\"to_hex(bin)\");\n                    String v4 = rs.getString(\"v4\");\n                    String v6 = rs.getString(\"v6\");\n                    String dt = rs.getString(\"dt\");\n \n                    System.out.print(\"d1: \" + d1);\n                    System.out.print(\", d2: \" + d2);\n                    System.out.print(\", d3: \" + d3);\n                    System.out.print(\", f1: \" + f1);\n                    System.out.print(\", f2: \" + f2);\n                    System.out.print(\", name: \" + name);\n                    System.out.print(\", text: \" + text);\n                    System.out.print(\", bin: \" + bin);\n                    System.out.print(\", hexbin: \"+hexbin);\n                    System.out.print(\", v4: \" + v4);\n                    System.out.print(\", v6: \" + v6);\n                    System.out.println(\", dt: \" + dt);\n \n                }\n                rs.close();\n            }\n        }\n        catch( SQLException se )\n        {\n            System.err.println(\"SQLException : \" + se.getMessage());\n        }\n        catch( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        finally\n        {\n            if( stmt != null )\n            {\n                stmt.close();\n                stmt = null;\n            }\n            if( conn != null )\n            {\n                conn.close();\n                conn = null;\n            }\n        }\n    }\n}\n\n\nNow compile and run the source code. Use the Makefile you have already created.\n\n[mach@localhost jdbc]$ make\njavac -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" -d . Sample1Connect.java Sample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n[mach@localhost jdbc]$ make run_sample2\nmake run_sample2\njava -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" Sample2Insert\nmach JDBC connected.\n1 record inserted.\n2 record inserted.\n3 record inserted.\n4 record inserted.\n5 record inserted.\n6 record inserted.\n7 record inserted.\n8 record inserted.\n9 record inserted.\nd1: 26208, d2: 1717986912, d3: 3689348814741910320, f1: 1.2345679E28, f2: 1.23456789E244, name: id-9, text: name-9, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.9, v6: 0:0:0:0:0:0:c0a8:9, dt: 2014-08-09\nd1: 19656, d2: 1288490184, d3: 2767011611056432740, f1: 1.2345678E21, f2: 1.23456789E183, name: id-8, text: name-8, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.8, v6: 0:0:0:0:0:0:c0a8:8, dt: 2014-08-08\nd1: 13104, d2: 858993456, d3: 1844674407370955160, f1: 1.23456788E14, f2: 1.23456789E122, name: id-7, text: name-7, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.7, v6: 0:0:0:0:0:0:c0a8:7, dt: 2014-08-07\nd1: 6552, d2: 429496728, d3: 922337203685477580, f1: 1.2345679E7, f2: 1.23456789E61, name: id-6, text: name-6, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.6, v6: 0:0:0:0:0:0:c0a8:6, dt: 2014-08-06\nd1: 0, d2: 0, d3: 0, f1: 1.2345679, f2: 1.23456789, name: id-5, text: name-5, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.5, v6: 0:0:0:0:0:0:c0a8:5, dt: 2014-08-05\nd1: -6552, d2: -429496728, d3: -922337203685477580, f1: 1.2345679E-7, f2: 1.23456789E-61, name: id-4, text: name-4, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.4, v6: 0:0:0:0:0:0:c0a8:4, dt: 2014-08-04\nd1: -13104, d2: -858993456, d3: -1844674407370955160, f1: 1.2345679E-14, f2: 1.23456789E-122, name: id-3, text: name-3, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.3, v6: 0:0:0:0:0:0:c0a8:3, dt: 2014-08-03\nd1: -19656, d2: -1288490184, d3: -2767011611056432740, f1: 1.2345679E-21, f2: 1.23456789E-183, name: id-2, text: name-2, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.2, v6: 0:0:0:0:0:0:c0a8:2, dt: 2014-08-02\nd1: -26208, d2: -1717986912, d3: -3689348814741910320, f1: 1.2345679E-28, f2: 1.23456789E-244, name: id-1, text: name-1, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.1, v6: 0:0:0:0:0:0:c0a8:1, dt: 2014-08-01\n\n\nData Input and Output Example (2) PreparedStatement Input Used\n\nCreate and view an example that uses a PreparedStatement to input and output data.\n\nThe name of the source file is Sample3PrepareStmt.java.\n\nimport java.util.*;\nimport java.sql.*;\nimport java.text.SimpleDateFormat;\nimport com.machbase.jdbc.*;\n \npublic class Sample3PrepareStmt\n{\n    public static Connection connect()\n    {\n        Connection conn = null;\n        try\n        {\n            String sURL = \"jdbc:machbase://localhost:5656/mhdb\";\n \n            Properties sProps = new Properties();\n            sProps.put(\"user\", \"sys\");\n            sProps.put(\"password\", \"manager\");\n \n            Class.forName(\"com.machbase.jdbc.driver\");\n \n            conn = DriverManager.getConnection(sURL, sProps);\n \n        }\n        catch ( ClassNotFoundException ex )\n        {\n            System.err.println(\"Exception : unable to load mach jdbc driver class\");\n        }\n        catch ( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        return conn;\n    }\n \n    public static void main(String[] args) throws Exception\n    {\n        Connection conn = null;\n        Statement stmt = null;\n        machPreparedStatement preStmt = null;\n        SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss SSS\");\n \n        try\n        {\n            conn = connect();\n            if( conn != null )\n            {\n                System.out.println(\"mach JDBC connected.\");\n \n                stmt = conn.createStatement();\n                preStmt = (machPreparedStatement)conn.prepareStatement(\"INSERT INTO SAMPLE_TABLE VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\");\n \n                String ipStr = null;\n                String dateStr = null;\n                for(int i=1; i&lt;10; i++)\n                {\n                    ipStr = String.format(\"172.16.0.%d\",i);\n                    dateStr = String.format(\"2014-08-09 12:23:34 %03d\", i);\n                    byte[] bin = new byte[20];\n                    for(int j=0;j&lt;20;j++){\n                        bin[j]=(byte)(Math.random()*255);\n                    }\n                    java.util.Date day = sdf.parse(dateStr);\n                    java.sql.Date sqlDate = new java.sql.Date(day.getTime());\n \n                    preStmt.setShort(1, (i-5) * 3276 );\n                    preStmt.setInt(2, (i-5) * 214748364 );\n                    preStmt.setLong(3, (i-5) * 922337203685477580L );\n                    preStmt.setFloat(4, 1.23456789101112131415*Math.pow(10,i));\n                    preStmt.setDouble(5, 1.23456789101112131415*Math.pow(10,i*10));\n                    preStmt.setString(6, String.format(\"varchar-%d\",i));\n                    preStmt.setString(7, String.format(\"text-%d\",i));\n                    preStmt.setBytes(8, bin);\n                    preStmt.setIpv4(9, ipStr);\n                    preStmt.setIpv6(10, \"::\"+ipStr);\n                    preStmt.setDate(11, sqlDate);\n                    preStmt.executeUpdate();\n \n                    System.out.println( i+\" record inserted.\");\n                }\n \n                //date type format : YYYY-MM-DD HH24:MI:SS mmm:uuu:nnnn\n                String query = \"SELECT d1, d2, d3, f1, f2, name, text, bin, to_hex(bin), v4, v6, to_char(dt,'YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn') as dt from SAMPLE_TABLE\";\n                ResultSet rs = stmt.executeQuery(query);\n                while( rs.next () )\n                {\n                    short d1 = rs.getShort(\"d1\");\n                    int d2 = rs.getInt(\"d2\");\n                    long d3 = rs.getLong(\"d3\");\n                    float f1 = rs.getFloat(\"f1\");\n                    double f2 = rs.getDouble(\"f2\");\n                    String name = rs.getString(\"name\");\n                    String text = rs.getString(\"text\");\n                    String bin = rs.getString(\"bin\");\n                    String hexbin = rs.getString(\"to_hex(bin)\");\n                    String v4 = rs.getString(\"v4\");\n                    String v6 = rs.getString(\"v6\");\n                    String dt = rs.getString(\"dt\");\n \n                    System.out.print(\"d1: \" + d1);\n                    System.out.print(\", d2: \" + d2);\n                    System.out.print(\", d3: \" + d3);\n                    System.out.print(\", f1: \" + f1);\n                    System.out.print(\", f2: \" + f2);\n                    System.out.print(\", name: \" + name);\n                    System.out.print(\", text: \" + text);\n                    System.out.print(\", bin: \" + bin);\n                    System.out.print(\", hexbin: \"+hexbin);\n                    System.out.print(\", v4: \" + v4);\n                    System.out.print(\", v6: \" + v6);\n                    System.out.println(\", dt: \" + dt);\n                }\n                rs.close();\n            }\n        }\n        catch( SQLException se )\n        {\n            System.err.println(\"SQLException : \" + se.getMessage());\n        }\n        catch( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        finally\n        {\n            if( stmt != null )\n            {\n                stmt.close();\n                stmt = null;\n            }\n            if( conn != null )\n            {\n                conn.close();\n                conn = null;\n            }\n        }\n    }\n}\n\n\nNow compile and run the source code. Use the Makefile you have already created.\n\nIt should be noted that the data entered in Sample2Insert.java is output together.\n\n[mach@localhost jdbc]$ make\njavac -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" -d . Sample1Connect.java\nSample2Insert.java Sample3PrepareStmt.java Sample4Append.java\n[mach@localhost jdbc]$ make run_sample3\nmake run_sample3\njava -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" Sample3PrepareStmt\nMach JDBC connected.\n1 record inserted.\n2 record inserted.\n3 record inserted.\n4 record inserted.\n5 record inserted.\n6 record inserted.\n7 record inserted.\n8 record inserted.\n9 record inserted.\nd1: 13104, d2: 858993456, d3: 3689348814741910320, f1: 754454.6, f2: 453821.380752063, name:\nvarchar-9, text: text-9, bin: ?+,??r?J?????S)n?, hexbin:\nA4C9A8D491D6728B4AACB39EE5FC5300296EFA9F, v4: 172.16.0.9, v6: 0:0:0:0:0:0:ac10:9, dt:\n2014-08-09 12:23:34 009:000:000\n?h???a?, hexbin: 6C20F09329ABBA3E7DE501C30DA368D6EFC961EF, v4: 172.16.0.8, v6:\n0:0:0:0:0:0:ac10:8, dt: 2014-08-09 12:23:34 008:000:000\nd1: 6552, d2: 429496728, d3: 1844674407370955160, f1: 2664182.0, f2: 1357910.1926900472, name:\nvarchar-7, text: text-7, bin: ????Uls?q?H?I?&amp;(?, hexbin:\nB5A0A2EFA185556C73BF719448BD49C92628F8C6, v4: 172.16.0.7, v6: 0:0:0:0:0:0:ac10:7, dt:\n2014-08-09 12:23:34 007:000:000\nd1: 3276, d2: 214748364, d3: 922337203685477580, f1: 443847.1, f2: 9342855.256576871, name:\nvarchar-6, text: text-6, bin: ??&gt;x??Eu?? ?Iw??+n, hexbin:\nBC973E78F5B44575D6CC15F94977DAE62B6E1D0E, v4: 172.16.0.6, v6: 0:0:0:0:0:0:ac10:6, dt:\n2014-08-09 12:23:34 006:000:000\nd1: 0, d2: 0, d3: 0, f1: 1283723.1, f2: 1771261.2019240903, name: varchar-5, text: text-5,\nbin: &amp;== j?j3?? T??y?\n??, hexbin: 263D3D1C6AF56A33F79D0C54A5C479A4030AFE8B, v4: 172.16.0.5, v6: 0:0:0:0:0:0:ac10:5,\ndt: 2014-08-09 12:23:34 005:000:000\nd1: -3276, d2: -214748364, d3: -922337203685477580, f1: 9447498.0, f2: 7529392.937964935,\nname: varchar-4, text: text-4, bin: ?Sw ??)? ?h2?E??/?, hexbin:\nC653771DD2DF29CDB30ED96832E745D3D7A52FD2, v4: 172.16.0.4, v6: 0:0:0:0:0:0:ac10:4, dt:\n2014-08-09 12:23:34 004:000:000\nd1: -6552, d2: -429496728, d3: -1844674407370955160, f1: 9589634.0, f2: 5994172.201347323,\nname: varchar-3, text: text-3, bin: 9aB,.????L/?=3,?`?f, hexbin:\n3961422C2EA39BE6F2964C2FCD3D332C8960A466, v4: 172.16.0.3, v6: 0:0:0:0:0:0:ac10:3, dt:\n2014-08-09 12:23:34 003:000:000\nd1: -9828, d2: -644245092, d3: -2767011611056432740, f1: 7409537.5, f2: 2313739.6613546023,\nname: varchar-2, text: text-2, bin: _? N?3 ?? ??~H ??= 8, hexbin:\n5F84144EF63320F3C718B0FD7E4809A4CB3D1838, v4: 172.16.0.2, v6: 0:0:0:0:0:0:ac10:2, dt:\n2014-08-09 12:23:34 002:000:000\nd1: -13104, d2: -858993456, d3: -3689348814741910320, f1: 596626.75, f2: 2649492.1936065694,\nname: varchar-1, text: text-1, bin: ???d??Wu$v? 7m?-, hexbin:\nE8D0C564B4EB57E59B08752476FC07376DBF2D14, v4: 172.16.0.1, v6: 0:0:0:0:0:0:ac10:1, dt:\n2014-08-09 12:23:34 001:000:000\nd1: 26208, d2: 1717986912, d3: 3689348814741910320, f1: 1.2345679E28, f2: 1.23456789E244,\nname: id-9, text: name-9, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.9, v6: 0:0:0:0:0:0:c0a8:9, dt: 2014-08-09 00:00:00 000:000:000\nd1: 19656, d2: 1288490184, d3: 2767011611056432740, f1: 1.2345678E21, f2: 1.23456789E183,\nname: id-8, text: name-8, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.8, v6: 0:0:0:0:0:0:c0a8:8, dt: 2014-08-08 00:00:00 000:000:000\nd1: 13104, d2: 858993456, d3: 1844674407370955160, f1: 1.23456788E14, f2: 1.23456789E122,\nname: id-7, text: name-7, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.7, v6: 0:0:0:0:0:0:c0a8:7, dt: 2014-08-07 00:00:00 000:000:000\nd1: 6552, d2: 429496728, d3: 922337203685477580, f1: 1.2345679E7, f2: 1.23456789E61, name:\nid-6, text: name-6, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.6, v6:\n0:0:0:0:0:0:c0a8:6, dt: 2014-08-06 00:00:00 000:000:000\nd1: 0, d2: 0, d3: 0, f1: 1.2345679, f2: 1.23456789, name: id-5, text: name-5, bin:\naabbccddeeff, hexbin: 616162626363646465656666, v4: 192.168.0.5, v6: 0:0:0:0:0:0:c0a8:5, dt:\n2014-08-05 00:00:00 000:000:000\nd1: -6552, d2: -429496728, d3: -922337203685477580, f1: 1.2345679E-7, f2: 1.23456789E-61,\nname: id-4, text: name-4, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.4, v6: 0:0:0:0:0:0:c0a8:4, dt: 2014-08-04 00:00:00 000:000:000\nd1: -13104, d2: -858993456, d3: -1844674407370955160, f1: 1.2345679E-14, f2: 1.23456789E-122,\nname: id-3, text: name-3, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.3, v6: 0:0:0:0:0:0:c0a8:3, dt: 2014-08-03 00:00:00 000:000:000\nd1: -19656, d2: -1288490184, d3: -2767011611056432740, f1: 1.2345679E-21, f2: 1.23456789E-183,\nname: id-2, text: name-2, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.2, v6: 0:0:0:0:0:0:c0a8:2, dt: 2014-08-02 00:00:00 000:000:000\nd1: -26208, d2: -1717986912, d3: -3689348814741910320, f1: 1.2345679E-28, f2: 1.23456789E-244,\nname: id-1, text: name-1, bin: aabbccddeeff, hexbin: 616162626363646465656666, v4:\n192.168.0.1, v6: 0:0:0:0:0:0:c0a8:1, dt: 2014-08-01 00:00:00 000:000:000\n\n\nExtension Function Append Example\n\nThe Machbase JDBC driver supports the Append protocol to quickly upload large numbers of data.\n\nThe following is an example of using the Append protocol. \nUse the sample_table used in the previous example.\nThe name of the source file is called Sample4Append.java. \nEnter the contents of data.txt into sample_table. \nCopy the data.txt file used in the CLI append example.\n\nimport java.util.*;\nimport java.sql.*;\nimport java.io.*;\nimport java.text.SimpleDateFormat;\nimport java.math.BigDecimal;\nimport com.machbase.jdbc.*;\n \n \npublic class Sample4Append\n{\n    protected static final String sTableName = \"sample_table\";\n    protected static final int sErrorCheckCount = 100;\n \n    public static Connection connect()\n    {\n        Connection conn = null;\n        try\n        {\n            String sURL = \"jdbc:machbase://localhost:5656/mhdb\";\n \n            Properties sProps = new Properties();\n            sProps.put(\"user\", \"sys\");\n            sProps.put(\"password\", \"manager\");\n \n            Class.forName(\"com.machbase.jdbc.driver\");\n \n            conn = DriverManager.getConnection(sURL, sProps);\n \n        }\n        catch ( ClassNotFoundException ex )\n        {\n            System.err.println(\"Exception : unable to load mach jdbc driver class\");\n        }\n        catch ( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        return conn;\n    }\n \n    public static void main(String[] args) throws Exception\n    {\n        Connection conn = null;\n        MachStatement stmt = null;\n        SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n        Calendar cal = Calendar.getInstance();\n        String filename = \"data.txt\";\n \n        try\n        {\n            conn = connect();\n            if( conn != null )\n            {\n                System.out.println(\"Mach JDBC connected.\");\n \n                stmt = (MachStatement)conn.createStatement();\n \n                ResultSet rs = stmt.executeAppendOpen(sTableName, sErrorCheckCount);\n                ResultSetMetaData rsmd = rs.getMetaData();\n \n                System.out.println(\"append open ok\");\n \n                MachAppendCallback cb = new MachAppendCallback() {\n                        @Override\n                        public void onAppendError(long aErrNo, String aErrMsg, String aRowMsg) {\n                             System.out.format(\"Append Error : [%05d - %s]\\n%s\\n\", aErrNo, aErrMsg, aRowMsg);\n                        }\n                    };\n \n                stmt.executeSetAppendErrorCallback(cb);\n \n                System.out.println(\"append data start\");\n                BufferedReader in = new BufferedReader(new FileReader(filename));\n                String buf = null;\n                int cnt = 0;\n                long dt;\n \n                long startTime = System.nanoTime();\n \n                while( (buf = in.readLine()) != null )\n                {\n                    ArrayList&lt;Object&gt; sBuf = new ArrayList&lt;Object&gt;();\n                    StringTokenizer st = new StringTokenizer(buf,\",\");\n                    for(int i=0; st.hasMoreTokens() ;i++ )\n                    {\n                        switch(i){\n                            case 7://binary case\n                                sBuf.add(new ByteArrayInputStream(st.nextToken().getBytes())); break;\n                            case 10://date case\n                                java.util.Date day = sdf.parse(st.nextToken());\n                                cal.setTime(day);\n                                dt = cal.getTimeInMillis()*1000000; //make nanotime\n                                sBuf.add(dt);\n                                break;\n                            default:\n                                sBuf.add(st.nextToken()); break;\n                        }\n                    }\n \n                    if( stmt.executeAppendData(rsmd, sBuf) != 1 )\n                    {\n                        System.err.println(\"Error : AppendData error\");\n                        break;\n                    }\n \n                    if( (cnt++%10000) == 0 )\n                    {\n                        System.out.print(\".\");\n                    }\n                    sBuf = null;\n \n                }\n                System.out.println(\"\\nappend data end\");\n \n                long endTime = System.nanoTime();\n                stmt.executeAppendClose();\n                System.out.println(\"append close ok\");\n                System.out.println(\"Append Result : success = \"+stmt.getAppendSuccessCount()+\", failure = \"+stmt.getAppendFailureCount());\n                System.out.println(\"timegap \" + ((endTime - startTime)/1000) + \" in microseconds, \" + cnt + \" records\" );\n \n                try {\n                    BigDecimal records = new BigDecimal( cnt );\n                    BigDecimal gap = new BigDecimal( (double)(endTime - startTime)/1000000000 );\n                    BigDecimal rps = records.divide(gap, 2, BigDecimal.ROUND_UP );\n \n                    System.out.println( rps + \" records/second\" );\n                } catch(ArithmeticException ae) {\n                    System.out.println( cnt + \" records/second\");\n                }\n \n                rs.close();\n            }\n        }\n        catch( SQLException se )\n        {\n            System.err.println(\"SQLException : \" + se.getMessage());\n        }\n        catch( Exception e )\n        {\n            System.err.println(\"Exception : \" + e.getMessage());\n        }\n        finally\n        {\n            if( stmt != null )\n            {\n                stmt.close();\n                stmt = null;\n            }\n            if( conn != null )\n            {\n                conn.close();\n                conn = null;\n            }\n        }\n    }\n}\n\n\nWhen appending, date type data must be converted to long type nanosecond time.\n\n[mach@localhost jdbc]$ make run_sample4\nmake run_sample4\njava -classpath \".:/home/machbase/machbase_home/lib/machbase.jar\" Sample4Append;\nMach JDBC connected.\nappend open ok\nappend data start\n......\nappend data end\nappend close ok\nAppend Result : success = 60000, failure = 0\ntimegap 6905594 in microseconds, 60000 records\n8688.61 records/second\n\n\nDisplays the dot (.) every 10,000, and can know the input time.\n\n# Use machsql to check number actually entered.\n# Confirm that 60018 are entered including those from Sample2Insert and Sample3PrepareStmt.\n \n \n[mach@localhost jdbc]$ machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version 3.0.0\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase server address (Default:127.0.0.1):\nMachbase user ID  (Default:SYS)\nMachbase user password: MANAGER\nMACH_CONNECT_MODE=INET, PORT=5656\nmach&gt; select count(*) from sample_table;\ncount(*)            \n-----------------------\n60018               \n[1] row(s) selected."
					}
					
				
		
				
					,
					
					"install-license-html": {
						"id": "install-license-html",
						"title": "License Installation",
						"version": "all",
						"categories": "",
						"url": " /install/license.html",
						"content": "Index\n\n\n  License File Structure\n  No License File\n  License Installation\n  Verifying License Installation\n    \n      License Installed\n      License Not Installed\n    \n  \n\n\nLicense key installation is usually performed after MACHBASE installation is finished. If you do not install a specific license after installation, you can use MACHBASE with some restrictions. This section describes MACHBASE’s license policy, structure, and installation method.\n\nLicense File Structure\n\nThe MACHBASE license is managed in the license.dat file. Licenses purchased for the product or for testing are displayed in a text file.\n\nmach@localhost:~$ cat license.dat \n \n\\#Company\\#ID-ProjectName: test\\#0-Machbase \n \\#License Policy: SIZE4DAY \n \\#License Type \\(Version 2\\): OFFICIAL \n \\#Issue DATE: 20160216 \n \\#Expiry DATE: 20160319 \n \\#Tag Count Limit: 0\n VBz5h4TC-d3+Bf3Efkpdp/Tx873PpZA-78LRSdrxbPY-xhGf4355/iXaY5/jfnn+Jdpjn+N+ef4l2mOf4355/iXaY5/jfnn+Jdpjn+N+ef4l2mOf4355/iXaY5/jfnn+Jdpjn+N+ef4l2mOf4355/iXaY5/jfnn+Jdpjn+\n\n\nNo License File\n\nThe server will run even if there is no license, but there are some limitations. The server can only be used for evaluation purposes, so if you intend to use it formally, you must obtain the license in a legitimate procedure.\n\nIf there is no license file, the following functional limitations will exist.\n\n1. If you enter more than 100 million records through the Append protocol in one session, a warning message is displayed. Append input is then stopped. The input limit state is released only when the server is restarted.\n\n2. When creating a tablespace, you can not create more than two disk directories. If you use more than one disk, the following warning indicating that the parallel I / O function for high performance data input can not be used will be displayed. \n\n\nCREATE TABLESPACE tbs1 DATADISK disk1 (disk_path=\"tbs1_disk1\"), disk2 (disk_path=\"tbs1_disk2\"), disk3 (disk_path=\"tbs1_disk3\");\n[ERR-00867 : Error in adding disk to tablespace. You cannot use multiple disks for tablespace without valid license.]\n\n\nLicense Installation\n\nThe MACHBASE license must be installed in $MACHBASE_HOME/conf, and the default name is license.dat .\n\nCopy the License File to $ MACHBASE_HOME / conf\n\nAt this time, the name of the license file issued must be changed to license.dat and copied. Then, when the server is started, it will determine if the license is appropriate and begin the installation.\n\nLaunch machadmin -t ‘licensefile_path’\n\nThe advantage of this method is that it can be easily installed with commands without having to adjust the license file name or location.\n\nInstalling as a query: This is a way to install a license using a query statement while the server is running.\n\nVerifying License Installation\n\nLicense Installed\n\nIf the license file is installed, the following is displayed in machbase.trc after the server is started.\n\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] LICENSE [License Type (Version 2)][OFFICIAL]\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] LICENSE [License Policy] [CORE]\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] LICENSE [Host ID] [FFFFFFFFFFFFFFF]\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] LICENSE [Expiry DATE] [25300318]\n[2016-02-17 14:51:00 P-20913 T-140709874054912][INFO] Machbase Logs Signature! : OFFICIAL:CORE:FFFFFFFFFFFFFFF:25300318-3.5.0.826b8f2.official-LINUX-X86-64-release\n\n\nYou can also use the machadmin -f command.\n\nLicense Not Installed\n\nIf the license file is not installed or if an abnormal file is used, the following output is displayed.\n\n[2016-02-17 14:49:54 P-6620 T-140539052701440][INFO] LICENSE [License Type(Version 2)][Only for evaluation (No license)]\n[2016-02-17 14:49:54 P-6620 T-140539052701440][INFO] LICENSE [License Policy] [None]\n[2016-02-17 14:49:54 P-6620 T-140539052701440][INFO] LICENSE [Host ID] [Unknown]\n[2016-02-17 14:49:54 P-6620 T-140539052701440][INFO] LICENSE [Expiry DATE] [N/A]"
					}
					
				
		
				
					,
					
					"install-linux-linux-env-html": {
						"id": "install-linux-linux-env-html",
						"title": "Preparing Linux Environment for Installation",
						"version": "all",
						"categories": "",
						"url": " /install/linux/linux-env.html",
						"content": "index\n\n  Check and Set Maximum Number of Files\n  Check and Set Server Time\n  Setting Port\n\n\nCheck and Set Maximum Number of Files\n\n  Check the maximum number of Linux files with the following command.\n\n\n[machbase@localhost ~] ulimit -Sn\n1024\n\n\n\n  If the result is less than 65535, modify the files below and reboot the server.\n\n\n[machbase@localhost ~] sudo vi /etc/security/limits.conf\n \n \n#&lt;domain&gt;      &lt;type&gt;  &lt;item&gt;         &lt;value&gt;\n#\n \n*               hard    nofile          65535\n*               soft    nofile          65535\n \n \n[machbase@localhost ~] sudo vi /etc/systemd/user.conf\n \nDefaultLimitNOFILE=65535\n\n\n\n  Reboot the server and check the value again.\n\n\n[machbase@localhost ~] ulimit -Sn\n65535\n\n\nCheck and Set Server Time\n\nBecause Machbase is a database that deals with time series data, you need to set the time value correctly on the server where Machbase will be installed.\n\nSetting Time Zone\n\nSince Machbase uses all the data in the local time where the server is located, you need to make sure that the timezone matches the time of the current server.\nMake sure it matches the timezone where you are located with the following command: If different, select the correct region from /usr/share/zoneinfo and link.\n\n\n[machbase@localhost ~] ls -l /etc/localtime\nlrwxrwxrwx 1 root root 32 Sep 27 14:08 /etc/localtime -&gt; ../usr/share/zoneinfo/Asia/Seoul\n \n \n# You can check the timezone set through the date command.\n[machbase@localhost ~] date\nWed Jan  2 11:12:44 KST 2019\n\n\nSetting Time\n\nIf the current local time is not correct, reset the time using the following command.\n\n[machbase@localhost ~] sudo date -s '2018/12/25 12:34:56'\n\n\nSetting Port\n\nThe operating system port to be used by Machbase must be reserved so that it cannot be used by other programs.\n\nIf you set a reserved port with the command below, the operating system does not allocate the port to other programs, avoiding port conflicts.\n\n[machbase@localhost ~] [machbase@localhost ~] sudo echo reserved port range~reserved port range &gt; /proc/sys/net/ipv4/ip_local_reserved_ports\n\n\nThe above method is a temporary method. To set it permanently, you need to edit the /etc/sysctl.conf file.\n\n[machbase@localhost ~] sudo vim /etc/sysctl.conf\n\n# add text below\nnet.ipv4.ip_local_reserved_ports = reserved port range-reserved port range"
					}
					
				
		
				
					,
					
					"install-linux-html": {
						"id": "install-linux-html",
						"title": "Linux Installation",
						"version": "all",
						"categories": "",
						"url": " /install/linux.html",
						"content": "Preparing Linux Environment for Installation\n  Tarball Installation\n  Docker Installation"
					}
					
				
		
				
					,
					
					"feature-tables-log-input-load-html": {
						"id": "feature-tables-log-input-load-html",
						"title": "Load by SQL",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/input/load.html",
						"content": "Loading Data\n  Confirm Data Loading\n  Sample Example\n\n\nThe ‘Load Data’ statement puts the data in the csv file into Machbase.\n\nFirst, create a table to store the data, using the first line of the csv file to create the columns.\n\n\n  The data type of the generated columns is VARCHAR (32768).\n  The data file path is a relative path based on $MACHBASE_HOME. It can also be set to an absolute path.\n\n\nTo save the table data as csv file, use the SAVE DATA statement.\n\nIf you already know the data type for each field in the csv file, you can create the table in advance and enter the data.\n\nIf you enter the file ‘load_sample.csv’ into the LOAD DATA statement, the table ‘load_sample’ is automatically created.\n\nLoading Data\n\nLOAD DATA INFILE 'sample/quickstart/load_sample.csv' INTO TABLE load_sample AUTO HEADUSE;\n\n\nConfirm Data Loading\n\nSELECT * FROM load_sample;\n\n\nSample Example\n\nUsing the sample file, you can do the following.\n\n[mach@localhost ~]$ cd $MACHBASE_HOME/sample/quickstart\n[mach@localhost ~]$ ls -l load_sample.csv\n-rw-r\n--r--- 1 root root 2827 2017-02-23 15:01 load_sample.csv\n \n[mach@localhost ~]$ machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version x.x.x.official\n     Copyright 2014, Machbase Inc. or its subsidiaries.\n     All Rights Reserved\n=================================================================\nMachbase server address (Default:127.0.0.1) :\nMachbase user ID  (Default:SYS)\nMachbase User Password :\nMACH_CONNECT_MODE=INET, PORT=5656\n \nMach&gt; LOAD DATA INFILE 'sample/quickstart/load_sample.csv' INTO TABLE load_sample AUTO HEADUSE;\n50 row(s) loaded. Failed to load 0 row(s).\nMach&gt; DESC load_sample;\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH\n----------------------------------------------------------------\nSENSOR_ID                     varchar             32767\nEPOCH_TIME                    varchar             32767\nE_YEAR                        varchar             32767\nE_MONTH                       varchar             32767\nE_DAY                         varchar             32767\nE_HOUR                        varchar             32767\nE_MINUTE                      varchar             32767\nE_SECOND                      varchar             32767\nVALUE                         varchar             32767\nMach&gt; SELECT COUNT(*) FROM load_sample;\nCOUNT(*)\n-----------------------\n50\n[1] row(s) selected.\nMach&gt;"
					}
					
				
		
				
					,
					
					"feature-tables-log-html": {
						"id": "feature-tables-log-html",
						"title": "Log Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log.html",
						"content": "Concept\n\nThe log table is a table capable of storing machine log data in which input data is time series data.\n\nData is infinitely entered into this table, and the value of each field has a unique meaning. \nYou can also retrieve data through text fields (varchar or text) and perform fast statistical calculations.\n\nGenerally, a Machbase table refers to the Log Table.\n\nThe characteristics of the log table are as follows.\n\nExisting Hidden Time Columns\n\nEvery log table has a hidden column called _arrival_time. \nThis column stores the time at which the record was generated, and supports nanosecond precision.\n\nReverse Time Search\n\nThe general database is output regardless of the input order when retrieving data. \nHowever, Machbase’s log tables always have the most up-to-date data as long as they do not have a sort option through a separate ORDER BY. \nThis can also be seen in the _arrival_time column.\n\nThis is because the importance of recent data in machine log data is much higher than that of previous data.\n\nView-Only Post Input\n\nMachbase’s log tables do not allow Update operations. In other words, once user log data is stored in Machbase,\n\nBy disallowing changes to the data, it is possible to support the stability of the data and the integrity of the log data itself at the engine level.\n\nLimited Deletion Allowed\n\nMachbase permits the deletion of necessary data in special situations, even if the data can not be changed.\nHowever, you can not delete arbitrary data as in a conventional database, and it is only possible to delete the oldest data sequentially.\nWith this function, it is possible to conveniently manage data by deleting data periodically from embedded devices that are limited in storage space or devices that are not easy to manage.\n\nSupports Text Search Function\n\nMachbase goes one step further in the way it handles strings in generic databases, and provides word-based searching.\nThis function is best suited for the purpose of machine log data, and searching for log data stored at a specific time is a major function frequently used in real business situations.\n\nTo do this, Machbase provides a real-time reverse index, which enables real-time text search at the same time as insertion of data.\n\nSpecial Data Type Support\n\nMachbase supports IPv4 and IPv6. This is a special type of Internet address, and many machine log data are represented by this type of address system.\nThis data type makes it easy to search and extract specific addresses.\n\nIt also provides the ability to search or extract some address ranges of a particular address scheme, using extended syntax such as select * from t1 where ipaddr = ‘192.168.0.*’.\nYou can also use the netmask operator to easily determine whether a particular Internet address is included in a particular address range.\n\nLarge Object (LOB) Data Support\n\nThe log table provides text and binary types that can store up to 64MB of bytes.\n\nIf the data is required to be retrieved as a text document type, it can be stored as a text type and retrieved.\nIf the data is a binary data type such as picture or music, it can be saved as a binary type.\n\nTime-based Partitioning\n\nA log table is a sequence of partition files that maintains a certain number of records and indexes relative to the time axis.\nIn other words, as the data continues to be entered, a new partition file is created, and if a certain number of records in that partition are full, the next partition will be created.\n\nPartition management mainly reflects the characteristics of log data in which search is performed based on time, and has a great advantage in data input performance.\nIt is also an easy structure for high-speed data access for statistical analysis.\n\nOperations\n\n\n  Creating and Managing Log Table\n  Log Data Input\n  Log Data Extraction\n  Deletion of Log Data\n  Index for log table\n  Example of Log Table"
					}
					
				
		
				
					,
					
					"feature-tables-lookup-html": {
						"id": "feature-tables-lookup-html",
						"title": "Lookup Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/lookup.html",
						"content": "Concept\n\nLike the volatile table, the lookup table resides in the memory, so that it can perform fast query processing.\n\nIn addition, data input and change are reflected on disk to ensure data permanence. Compared with the volatile table, the query processing performance is the same, but the data input and change performance is somewhat lower.\n\nThe characteristics of this table are as follows.\n\nLow Input / Update Performance\n\nUnlike volatile tables, the performance of input and update due to the maintenance of disk data images is low, making them unsuitable for tables for real-time data representation such as dashboards.\n\nSchema Preservation\n\nAgain, unlike volatile tables, the structure (schema) information in the lookup table is retained even after the server is restarted. You must explicitly use DROP TABLE  to drop the table .\n\nData Preservation\n\nUnlike the volatile table, the lookup table is restored to its original state when the server is restarted.Providing Index\n\nIndex\n\nLike the volatile table, it provides a RED-BLACK index. Therefore, it can be efficiently used in the search process or the join process with the log table.\n\nHow to use Lookup table\n\n\n  Creating and Managing Lookup Table\n  Lookup Data Insert\n  Lookup Data Extraction\n  Lookup Data Deletion\n  Creating and Managing Lookup Index\n  Lookup Table Utilization Example"
					}
					
				
		
				
					,
					
					"tools-utils-machadmin-html": {
						"id": "tools-utils-machadmin-html",
						"title": "machadmin",
						"version": "all",
						"categories": "",
						"url": " /tools/utils/machadmin.html",
						"content": "Index\n\n\n  Option and Features\n  Recovery Mode\n  Server Normal Shutdown\n  Create Database\n  Delete Database\n  Delete Database\n  Force to abort Server\n  Run Silent Mode\n  Database Recovery\n  Check server is running\n  Install License File\n\n\nmachadmin is used to start up or shut down the Machbase server and to check the creation, deletion, and execution status.\n\nOption and Features\n\nThe options for machadmin are as follows. The functions described in the previous installation section are omitted.\n\nmach@localhost:~$ machadmin -h\n\n\n\n  \n    \n      Options\n      Describe\n       \n    \n  \n  \n    \n      -u, –startup/ –recovery[=simple,complex,reset]\n      Machbase server startup/recovery mode (default: simple)\n       \n    \n    \n      -s, –shutdown\n       \n      Machbase server shuts down  normally\n    \n    \n      -c, –createdb\n      Creates Machbase database\n       \n    \n    \n      -d, –destroydb\n      Deletes Machbase database\n       \n    \n    \n      -k, –kill\n      Force quits Machbase server\n       \n    \n    \n      -i, –silence\n      Runs without output\n       \n    \n    \n      -r, –restore\n      Recovers database from backup\n       \n    \n    \n      -x, –extract\n      Converts backup files to backup directory\n       \n    \n    \n      -e, –check\n      Checks Machbase server run status\n       \n    \n    \n      -t, –licinstall\n      Installs license file\n       \n    \n    \n      -f, –licinfo\n      Outputs installed license information\n       \n    \n  \n\n\nRecovery Mode\n\nSyntax\n\nmachadmin -u --recovery=[simple | complex | reset]\n\n\nThe recovery mode is as follows:\n\n\n  simple: If there is no power loss when the server is running, simple recovery mode is run by default.\n  complex: The complex recovery mode takes longer to execute than the simple mode. It is executed by default when restarting after the power is turned off.\n  reset: When recovery is not performed in simple or complex mode, all data in all tables are checked to recover the database. In this case, some loss of data may occur.\n\n\nServer Normal Shutdown\n\nExample:\n\nmach@localhost:~$ machadmin -s\n \n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for the server shut down...\nServer shut down successfully.\n\n\nCreate Database\n\nExample:\n\nmach@localhost:~$ machadmin -c\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDatabase created successfully.\n\n\nDelete Database\n\nExample:\n\nmach@localhost:~$ machadmin -d\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDestroy Machbase database- Are you sure?(y/N) y\nDatabase destroyed successfully.\n\n\nForce to abort Server\n\nSyntax:\n\nmachadmin -k\n\n\nExample:\n\nmach@localhost:~$ machadmin -k\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for Machbase terminated...\nServer terminated successfully.\n\n\nRun Silent Mode\n\nRemoves the message that is output when ‘machadmin’  runs.\n\nSyntax:\n\nmachadmin -i\n\n\nDatabase Recovery\n\nSyntax:\n\nmachadmin -r backup_database_path\n\n\nExample:\n\nmach@localhost:~$ machadmin -r 'backup'\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nBacked up database restored successfully.\n\n\nCheck server is running\n\nSyntax:\n\nmachadmin -e\n\n\nExample when server is not running:\n\nmach@localhost:~$ machadmin -e\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\n[ERR] Server is not running.\n\n\nExample when server is running:\n\nmach@localhost:~$ machadmin -e\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nMachbase server is already running with PID (14098).\n\n\nInstall License File\n\nSyntax:\n\nmachadmin -t license_file\n\n\nExample:\n\nmach@localhost:~$ machadmin -t license.dat\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nLicense installed successfully.\n\n\nCheck License\n\nExample:\n\nmach@localhost:~$ machadmin -f\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - 5.1.9.community\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\n                   INFORMATION\nInstall Date                      : 2018-12-20 11:34:43\nCompany#ID-ProjectName            : machbase\nLicense Policy                    : CORE\nLicense Type(Version 2)           : OFFICIAL\nHost ID                           : FFFFFFFFFFFFFFF\nIssue Date                        : 2013-03-25\nExpiry Date                       : 2037-03-18\nMax Data Size For a Day(GB)       : 0\nPercentage Of Data Addendum(%)    : 0\nOverflow Action                   : 0\nOverflow Count to Stop Per Month  : 0\nStop Action                       : 0\nReset Flag                        : 0\n-----------------------------------------------------------------\n                   STATUS\nUsage Of Data(GB)                 : 0.000000\nPrevious Checked Date             : 2018-12-22\nViolation Count                   : 0\nStop Enabled                      : 0\n-----------------------------------------------------------------\nLicense information displayed successfully."
					}
					
				
		
				
					,
					
					"tools-machcoordinatoradmin-html": {
						"id": "tools-machcoordinatoradmin-html",
						"title": "machcoordinatoradmin",
						"version": "all",
						"categories": "",
						"url": " /tools/machcoordinatoradmin.html",
						"content": "Coordinator is a cluster-wide management tool.\n\nOnly exits in Cluster Edition Package.\n\nOptions and Features\n\nThe options for machcoordinatoradmin are as follows. The functions described in the previous section are omitted.\n\nmach@localhost:~$ machcoordinatoradmin -h\n\n\n\n  \n    \n      Options\n      Description\n    \n  \n  \n    \n      -u, –startup\n      Runs the Coordinator process\n    \n    \n      -s, –shutdown\n      Terminates Coordinator process\n    \n    \n      -k, –kill\n      Stops Coordinator process\n    \n    \n      -c, –createdb\n      Creates Coordinator meta\n    \n    \n      -d, –destroydb\n      Removes Coordinator meta, Deletes the package files in $MACHBASE_COORDINATOR_HOME/package\n    \n    \n      -e, –check\n      Checks that the Coordinator process is running\n    \n    \n      -i, –silence\n      Runs without output\n    \n    \n      –configuration[=name]\n      Outputs keys and values in configuration settings (only certain keys can be output)\n    \n    \n      –activate\n      Switches Cluster status to Service\n    \n    \n      –deactivate\n      Switches Cluster status to Deactivate\n    \n    \n      –list-package[=package]\n      Lists information of registered packages (only specific packages can be output)\n    \n    \n      –add-package=package\n      Adds package\n    \n    \n      –remove-package=package\n      Deletes package\n    \n    \n      –list-node[=node]\n      Lists information of nodes (only specific nodes can be output)\n    \n    \n      –add-node=node\n      Adds node\n    \n    \n      –remove-node=node\n      Deletes node\n    \n    \n      –upgrade-node=node\n      Upgrades node\n    \n    \n      –startup-node=node\n      Runs node\n    \n    \n      –shutdown-node=node\n      Terminates node\n    \n    \n      –kill-node=node\n      Stops node\n    \n    \n      –cluster-status\n      Outputs each node status of the cluster\n    \n    \n      –cluster-status-full\n      Outputs of each node status of cluster in detail\n    \n    \n      –cluster-node\n      Outputs information of cluster\n    \n    \n      –set-group-state=[normal | readonly]\n      Changes the status of a specific warehouse group\n    \n    \n      –get-host-resource\n      Outputs host resource information where each node is located\n    \n    \n      –host-resource-enable\n      Starts collecting Host resource information of each node\n    \n    \n      –host-resource-disable\n      Stops collecting Host resource information for each node\n    \n  \n\n\n\n  \n    \n      Additional Options\n      Description\n      Required Options\n    \n  \n  \n    \n      –file-name=filename\n      File name\n      –add-package\n    \n    \n      –port-no=portno\n      Port number\n      –add-node\n    \n    \n      –deployer=node\n      Deployer node name\n      –add-node\n    \n    \n      –package-name=packagename\n      Package name to be the installation source\n      –add-package\n    \n    \n      –home-path=path\n      Based on Deployer server, installation path of current Node\n      –add-node\n    \n    \n      –node-type=[broker | warehouse]\n      Node type to install (choose between broker/warehouse)\n      –add-node\n    \n    \n      –group=groupname\n      Group name of the node to install\n      –add-node\n    \n    \n      –replication=host:port\n      host: port to exchange replication\n      –add-node\n    \n    \n      –no-replicate\n      Does not use Replication on the node to install\n      –add-node\n    \n    \n      –primary=host:port\n      Specifies the node name of the Primary Coordinator when installing the Secondary Coordinator\n      -u, –startup\n    \n    \n      –host=host\n      Specifies specific host to output Host resource information\n      –get-host-resource\n    \n    \n      –metric=[cpu|memory|disk|network]\n      Specifies specific metric to output Host resource information\n      –get-host-resource\n    \n  \n\n\nCheck Running Status\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin -e\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nMachbase Coordinator is running with pid(29245)!\n\n\nCreate / Delete Meta\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin -c\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nCoordinator metadata created successfully.\n  \nmach@localhost:~$ machcoordinatoradmin -d\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nCoordinator metadata destroyed successfully.\n\n\nOutput Configuration\n\nSyntax:\n\nmachcoordinatoradmin --configuration[=name]\n\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin --configuration\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nName  : CLUSTER\nValue : 3\n \nName  : DECISION\nValue : ON\n \nName  : HOST-RESOURCE\nValue : OFF\n  \nmach@localhost:~$ machcoordinatoradmin --configuration=decision\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : DECISION\n             Value : ON\n            Format : text/plain\n\n\nChange Cluster Status\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin --activate\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : CLUSTER\n             Value : 3\n            Format : text/plain\n \n \nmach@localhost:~$ machcoordinatoradmin --deactivate\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : CLUSTER\n             Value : 0\n            Format : text/plain \n\n\nList Package Information\n\nSyntax:\n\nmachcoordinatoradmin --list-package[=package]\n\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin --list-package\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nPackage Name : machbase\nFile Name    : machbase-cluster-6bab497c9.develop-LINUX-X86-64-release-lightweight.tgz\nFile Size    : 64630670 bytes\n \nPackage Name : machbase2\nFile Name    : machbase-cluster-e3c0717.develop-LINUX-X86-64-release-lightweight.tgz\nFile Size    : 64677030 bytes\n \n \nmach@localhost:~$ machcoordinatoradmin --list-package=machbase\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nPackage Name : machbase\nFile Name    : machbase-cluster-6bab497c9.develop-LINUX-X86-64-release-lightweight.tgz\nFile Size    : 64630670 bytes\n\n\nList Node Information\n\nSyntax:\n\nmachcoordinatoradmin --list-node[=node]\n\n\nExample:\n\nmach@localhost:~$  machcoordinatoradmin --list-node\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nNode Name             : 192.168.0.32:5101\nNode Type             : coordinator\nHTTP Admin Port       : 5102\nGroup Name            : Coordinator\nDesired State         : primary\nActual State          : primary\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 497590\nLast Modify Time      : 421020408\nLast Response Elapsed : 1006148\n \nNode Name             : 192.168.0.32:5201\nNode Type             : deployer\nGroup Name            : Deployer\nDesired State         : normal\nActual State          : normal\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 497594\nLast Modify Time      : 404915419\nLast Response Elapsed : 1006128\n \nNode Name             : 192.168.0.32:5301\nNode Type             : broker\nPort Number           : 5757\nDeployer              : 192.168.0.32:5201\nPackage Name          : machbase\nHome Path             : /home/machbase/broker1\nGroup Name            : Broker\nDesired State         : leader\nActual State          : leader\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 497544\nLast Modify Time      : 353606480\nLast Response Elapsed : 1006157\n \nNode Name             : 192.168.0.32:5401\nNode Type             : warehouse\nPort Number           : 5400\nDeployer              : 192.168.0.32:5201\nPackage Name          : machbase\nHome Path             : /home/machbase/warehouse_a1\nGroup Name            : Group1\nDesired State         : normal\nActual State          : normal\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 497556\nLast Modify Time      : 332480933\nLast Response Elapsed : 1006160\n  \nmach@localhost:~$  machcoordinatoradmin --list-node=192.168.0.32:5401\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nNode Name             : 192.168.0.32:5401\nNode Type             : warehouse\nPort Number           : 5400\nDeployer              : 192.168.0.32:5201\nPackage Name          : machbase\nHome Path             : /home/cumulus/warehouse_a1\nGroup Name            : Group1\nDesired State         : normal\nActual State          : normal\nCoordinator Host      : 192.168.0.32:5101\nLast Response Time    : 648879\nLast Modify Time      : 419153148\nLast Response Elapsed : 1005962\n\n\nOutput Cluster Node Status\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin --cluster-status\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n+-------------+-------------------+-------------------+-------------------+--------------+\n|  Node Type  |     Node Name     |    Group Name     |    Group State    |     State    |\n+-------------+-------------------+-------------------+-------------------+--------------+\n| coordinator | 192.168.0.32:5101 | Coordinator       | normal            | primary      |\n| deployer    | 192.168.0.32:5201 | Deployer          | normal            | normal       |\n| broker      | 192.168.0.32:5301 | Broker            | normal            | leader       |\n| warehouse   | 192.168.0.32:5401 | Group1            | normal            | normal       |\n+-------------+-------------------+-------------------+-------------------+--------------+\n \nmach@localhost:~$ machcoordinatoradmin --cluster-status-full\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n+-------------+-------------------+-------------------+-------------------+-------------------------------+-------------+\n|  Node Type  |     Node Name     |    Group Name     |    Group State    |    Desired &amp; Actual State     |  RP State   |\n+-------------+-------------------+-------------------+-------------------+-------------------------------+-------------+\n| coordinator | 192.168.0.32:5101 | Coordinator       | normal            | primary       | primary       | ----------- |\n| deployer    | 192.168.0.32:5201 | Deployer          | normal            | normal        | normal        | ----------- |\n| broker      | 192.168.0.32:5301 | Broker            | normal            | leader        | leader        | ----------- |\n| warehouse   | 192.168.0.32:5401 | Group1            | normal            | normal        | normal        | ----------- |\n+-------------+-------------------+-------------------+-------------------+-------------------------------\n\n\nOutput Cluster Information\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin --cluster-node\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nToken Pid      : 29245\nToken Time     : 1553153902646178\nModify Time    : 1553154010296715\nModify Count   : 8\nCluster Status : Service\nBroker         : 192.168.0.32:5301\nWarehouse      : 192.168.0.32:5401\n\n\nChange Group State\n\nSyntax:\n\nmachcoordinatoradmin --set-group-state=[ normal | readonly ] --group=group\n\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin --set-group-state=readonly --group=Group1\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nGroup Name: Group1\nFlag      : 1\n  \nmach@localhost:~$ machcoordinatoradmin --cluster-status\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n+-------------+-------------------+-------------------+-------------------+--------------+\n|  Node Type  |     Node Name     |    Group Name     |    Group State    |     State    |\n+-------------+-------------------+-------------------+-------------------+--------------+\n| coordinator | 192.168.0.32:5101 | Coordinator       | normal            | primary      |\n| deployer    | 192.168.0.32:5201 | Deployer          | normal            | normal       |\n| broker      | 192.168.0.32:5301 | Broker            | normal            | leader       |\n| warehouse   | 192.168.0.32:5401 | Group1            | readonly          | normal       |\n+-------------+-------------------+-------------------+-------------------+--------------+\n\n\nOutput Host Resource\n\nSyntax:\n\nmachcoordinatoradmin --host-resource-enable [--metric=metric] [host=host]\n\n\nExample:\n\nmach@localhost:~$ machcoordinatoradmin --host-resource-enable\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : HOST-RESOURCE\n             Value : ON\n            Format : text/plain\n  \nmach@localhost:~$ machcoordinatoradmin --get-host-resource\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nHost Name : 192.168.0.32\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 14.0%\n      CPU IOWait Ratio    : 0.0%\n   Memory Info :\n      Physical Memory Utilization : 99.1%\n      Virtual Memory Utilization  : 98.6%\n   Network Info :\n      Receive Bytes(per second)    : 42809\n      Receive Packets(per second)  : 337\n      Transmit Bytes(per second)   : 42885\n      Transmit Packets(per second) : 332\n   Disk Info :\n      /dev/sda1 : 87.4%\n         |-&gt; 192.168.0.32:5101   /home/cumulus/coordinator1\n         |-&gt; 192.168.0.32:5301   /home/cumulus/broker1\n         |-&gt; 192.168.0.32:5401   /home/cumulus/warehouse_a1\nHost Name : 192.168.0.33\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 2.0%\n      CPU IOWait Ratio    : 0.0%\n   Memory Info :\n      Physical Memory Utilization : 46.9%\n      Virtual Memory Utilization  : 22.8%\n   Network Info :\n      Receive Bytes(per second)    : 12336\n      Receive Packets(per second)  : 103\n      Transmit Bytes(per second)   : 13500\n      Transmit Packets(per second) : 103\n   Disk Info :\n      /dev/sda1 : 64.2%\n         |-&gt; 192.168.0.33:5101   /home/cumulus/coordinator2\n         |-&gt; 192.168.0.33:5401   /home/cumulus/warehouse_a2\n  \nmach@localhost:~$ machcoordinatoradmin --get-host-resource --metric=cpu\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nHost Name : 192.168.0.32\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 13.9%\n      CPU IOWait Ratio    : 0.0%\nHost Name : 192.168.0.33\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 1.9%\n      CPU IOWait Ratio    : 0.0%\n  \nmach@localhost:~$ machcoordinatoradmin --get-host-resource --host=192.168.0.33\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nHost Name : 192.168.0.33\n   CPU Info :\n      Model Name          : Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz\n      Number of CPUs      : 8\n      Number of CPU Cores : 4\n      CPU Utilization     : 2.0%\n      CPU IOWait Ratio    : 0.0%\n   Memory Info :\n      Physical Memory Utilization : 46.9%\n      Virtual Memory Utilization  : 22.8%\n   Network Info :\n      Receive Bytes(per second)    : 12588\n      Receive Packets(per second)  : 106\n      Transmit Bytes(per second)   : 13330\n      Transmit Packets(per second) : 100\n   Disk Info :\n      /dev/sda1 : 64.2%\n         |-&gt; 192.168.0.33:5101   /home/cumulus/coordinator2\n         |-&gt; 192.168.0.33:5401   /home/cumulus/warehouse_a2\n  \nmach@localhost:~$ machcoordinatoradmin --host-resource-disable\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\n              Name : HOST-RESOURCE\n             Value : OFF\n            Format : text/plain"
					}
					
				
		
				
					,
					
					"tools-machdeployeradmin-html": {
						"id": "tools-machdeployeradmin-html",
						"title": "machdeployeradmin",
						"version": "all",
						"categories": "",
						"url": " /tools/machdeployeradmin.html",
						"content": "You can check the status of the Deployer, or directly issue the Deployer’s startup/shutdown/stop commands.\n\nNormally the fastest way to issue the commands is through machcoordinatoradmin, but if not possible, you must do the following.\n\nOnly exits in Cluster Edition Package.\n\nOptions and Features\n\nThe options for machdeployeradmin are as follows. The functions described in the previous section are omitted.\n\nmach@localhost:~$ machdeployeradmin -h\n\n\n\n  \n    \n      Options\n      Description\n    \n  \n  \n    \n      -u, –startup\n      Runs Deployer process\n    \n    \n      -s, –shutdown\n      Terminates Deployer process\n    \n    \n      -k, –kill\n      Stops Deployer process\n    \n    \n      -c, –createdb\n      Creates Deployer meta\n    \n    \n      -d, –destroydb\n      Deletes Deployer meta\n    \n    \n      -i, –silence\n      Runs without output\n    \n    \n      -e, –check\n      Checks to see if Deployer process is running\n    \n  \n\n\nChecking Running Status\n\nExample:\n\nmach@localhost:~$ machdeployeradmin -e\n-------------------------------------------------------------------------\n     Machbase Deployer Administration Tool\n     Release Version - e3c0717.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nMachbase Deployer is running with pid(29373)!"
					}
					
				
		
				
					,
					
					"tools-utils-machloader-html": {
						"id": "tools-utils-machloader-html",
						"title": "machadmin",
						"version": "all",
						"categories": "",
						"url": " /tools/utils/machloader.html",
						"content": "machloader is used to import/export text file data to the Machbase server. It works with CSV files by default, but it also supports other formats.\n\nThe features of machloader are as follows.\n\n\n  machloader can specify a datetime type in the schema file. The datetime type specified must be of the type supported by the Machbase server. One datetime type can be applied to all fields, and each field can have a different format.\n  To delete and input the input target table data, use the “-m replace” option.\n  machloader does not verify the schema and data file consistency. The user must check that the schema, tables, and data files meet the consistency.\n  machloader supports APPEND mode by default.\n  machloader does not use the _ARRIVAL_TIME column by default. You must use the “-a” option to import/export the corresponding column data.\n\n\nThe options for machloader can be seen with the following command:\n\nIndex\n\n\n  Basic Usage\n  CSV File Import\n  CSV File Export\n  Use CSV File Header\n  Automatic Table Creation\n  Files Not CSV Format\n  Specify Input Mode\n  Specify Connection Information\n  Create Log File\n  Create Schema File\n  Set datetime Format in Schema File\n  IGNORE\n\n\n[mach@localhost]$ machloader -h\n\n\n\n  \n    \n      Option\n      Description\n    \n  \n  \n    \n      -s, –server=SERVER\n      Enters Machbase server IP address (default: 127.0.0.1)\n    \n    \n      -u, –user=USER\n      Enters connecting user name (default: SYS)\n    \n    \n      -p, –password=PASSWORD\n      Connecting user password (default: MANAGER)\n    \n    \n      -P, –port=PORT\n      Machbase server port number (default: 5656)\n    \n    \n      -i, –import\n      Data import command option\n    \n    \n      -o, –export\n      Data export command option\n    \n    \n      -c, –schema\n      Command option to create schema file using the database table information\n    \n    \n      -t, –table=TABLE_NAME\n      Sets table name that is creating a schema file\n    \n    \n      -f, –form=SCHEMA_FORM_FILE\n      Specifies schema filename\n    \n    \n      -d, –data=DATA_FILE\n      Specifies a data file name\n    \n    \n      -l, –log=LOG_FILE\n      Specifies a machloader execution log file\n    \n    \n      -b, –bad=BAD_FILE\n      Records the data in which the input error occurred and specifies the file name that records the error description when executing -i option.\n    \n    \n      -m, –mode=MODE\n      Indicates import method when executing -i option. The append or replace option is available. Append enters the data after the existing data and replace deletes the existing data and enters the data.\n    \n    \n      -D, –delimiter=DELIMITER\n      Sets each field delimiter. The default value is ‘,’.\n    \n    \n      -n, –newline=NEWLINE\n      Sets each record separator. The default is ‘\\n’.\n    \n    \n      -e, –enclosure=ENCLOSURE\n      Sets the enclosing delimiter for each field.\n    \n    \n      -r, –format=FORMAT\n      Specifies the format for file input/output. (default: csv)\n    \n    \n      -a, –atime\n      Determines whether to use the built-in column _ARRIVAL_TIME. The default value is to not use the column.\n    \n    \n      -z, –timezone\n      Set timezone ex) +0900 -1230\n    \n    \n      -I, –silent\n      Does not display copyright-related output and import/export status information.\n    \n    \n      -h, –help\n      Displays a list of options.\n    \n    \n      -F, –dateformat=DATEFORMAT\n      Sets the column dateformat. (_arrival_time YYYY-MM-DD HH24:MI:SS) If you set ‘unixtimestamp’ instead of dateformat, the input value is regarded as the unix timestamp value. (“time_column unixtimestamp”) If you set ‘nanotimestamp’ instead of dateformat, the input value is regarded as a timestamp value in nanoseconds. (“time_column nanotimestamp”)\n    \n    \n      -E, –encoding=CHARACTER_SET\n      Sets the encoding of input/output files. Supported encodings are UTF8 (default), ASCII, MS949, KSC5601, EUCJP, SHIFTJIS, BIG5, GB231280, and UTF16.\n    \n    \n      -C, –create\n      Creates a table if one does not exist upon import.\n    \n    \n      -H, –header\n      Sets whether header information is present upon import/export. The default value is unset.\n    \n    \n      -S, –slash\n      Specifies the backslash delimiter.\n    \n  \n\n\nBasic Usage\n\nThe table must be created first before running the usage below.\n\nCSV File Import\n\nImports CSV file to Machbase server.\n\nOption:\n\n-i: import specification options\n-d: data file naming options\n-t: table name specification option\n\n\nExample:\n\nmachloader -i -d data.csv -t table_name\n\n\nCSV File Export\n\nWrites data to a CSV file.\n\nOption:\n\n-o: export specification options\n-d: data file naming options\n-t: table name specification option\n\n\nExample:\n\nmachloader -o -d data.csv -t table_name\n\n\nUse CSV File Header\n\nThe header-related setting of the CSV file.\n\nOption:\n\n-i -H: Upon import, the first line of the csv file is recognized as a header. Therefore, the first line is excluded from input.\n-o -H: Upon export, generates the csv header as the column name of the table.e\n\n\nExample:\n\nmachloader -i -d data.csv -t table_name -H\nmachloader -o -d data.csv -t table_name -H\n\n\nAutomatic Table Creation\n\nRegards automatic table creation.\n\nOption:\n\n-C: Automatically generates the table when importing. The column names are automatically generated as c0, c1, .... The generated column is varchar (32767) type.\n-H: Generates column names with csv header name when importing.\n\n\nExample:\n\nmachloader -i -d data.csv -t table_name -C\nmachloader -i -d data.csv -t table_name -C -H\n\n\nFiles Not CSV Format\n\nSets delimiter for files that are not in CSV format.\n\nOption:\n\n-D: Delimiter option for each field\n-n: Specifies each record delimiter option\n-e: Specifies the enclosing character for each field.\n\n\nExample:\n\nmachloader -i -d data.txt -t table_name -D '^' -n '\\n' -e '\"'\nmachloader -o -d data.txt -t table_name -D '^' -n '\\n' -e '\"'\n\n\nSpecify Input Mode\n\nWhen importing (with -i option), there are two modes, REPLACE and APPEND. APPEND is the default. Use REPLACE mode with caution because it deletes existing data.\n\nOption:\n\n-m: Specifies import mode\n\n\nExample:\n\nmachloader -i -d data.csv -t table_name -m replace\n\n\nSpecify Connection Information\n\nSpecifies server IP, user, and password separately.\n\nOption:\n\n-s: Specifies server IP address (default: 127.0.0.1)\n-P: Specifies server port number (default: 5656)\n-u: Specifies the connecting user name (default: SYS)\n-p: Specifies the password of the connecting user (default: MANAGER)\n\n\nExample:\n\nmachloader -i -s 192.168.0.10 -P 5656 -u mach -p machbase -d data.csv -t table_name\n\n\nCreate Log File\n\nCreates the execution log file for machloader.\n\nOption:\n\n-b: Sets the name of the log file to generate the data that is not input when importing.\n-l: Sets the name of the log file to generate the data and error message that were not input when importing.\n\n\nExample:\n\nmachloader -i -d data.csv -t table_name -b table_name.bad -l table_name.log\n\n\nCreate Schema File\n\nThe machloader schema file can be created. Import/export is possible even if the data type format is changed using a schema file or the number of columns in the table and data file is different.\n\nOption:\n\n-c: schema file creation options\n-t: table name specification option\n-f: created schema file name specification option\n\n\nExample:\n\nmachloader -c -t table_name -f table_name.fmt\nmachloader -c -t table_name -f table_name.fmt -a\n\n\nSet datetime Format in Schema File\n\nThe date format can be set to preference with the DATEFORMAT option.\n\nSyntax:\n\n# Set for all datetime columns.\nDATEFORMAT &lt;dateformat&gt;\n\nSet for individual datetime column.\n\nDATEFORMAT &lt;column_name&gt; &lt;format&gt;\n\n\nExample:\n\n-- Set dateformat for each field in datetest.csv file in the schema file (datetest.fmt).\ndatetest.fmt\ntable datetest\n{\nINS_DT datetime;\nUPT_DT datetime;\n}\nDATEFORMAT ins_dt \"YYYY/MM/DD HH12:MI:SS\"\nDATEFORMAT upt_dt \"YYYY DD MM HH12:MI:SS\"\n \ndatetest.csv\n2017/02/20 11:05:23,2017 20 02 11:05:23\n2017/02/20 11:06:34,2017 20 02 11:06:34\n \n-- Import datetest.csv file and check input data.\nmachloader -i -f datetest.fmt -d datetest.csv\n-----------------------------------------------------------------\nMachbase Data Import/Export Utility.\nRelease Version 5.1.9.community\nCopyright 2014, MACHBASE Corporation or its subsidiaries.\nAll Rights Reserved.\n-----------------------------------------------------------------\nImport time : 0 hour 0 min 0.39 sec\nLoad success count : 2\nLoad fail count : 0\n \nmach&gt; SELECT * FROM datetest;\nINS_DT UPT_DT\n-------------------------------------------------------------------\n2017-02-20 11:06:34 000:000:000 2017-02-20 11:06:34 000:000:000\n2017-02-20 11:05:23 000:000:000 2017-02-20 11:05:23 000:000:000\n[2] row(s) selected.\nElapsed time: 0.000\n\n\nIGNORE\n\nWhen you do not want to enter a specific field in the CSV file, you can set the IGNORE option in the fmt file.\nThe ignoretest.csv file has three fields, but if the last field is not needed, specify IGNORE in the column that is not needed in the fmt file.\n\nExample:\n\n-- Set ignore option for last field in ignoretest.fmt file.\nignoretest.fmt\ntable ignoretest\n{\nID integer;\nMSG varchar(40);\nSUB_ID integer IGNORE;\n}\n \nignoretest.csv\n1, \"msg1\", 3\n2, \"msg2\", 4\n \n \n-- Import ignoretest.csv file and check input data.\nmachloader -i -f ignoretest.fmt -d ignoretest.csv\n-----------------------------------------------------------------\nMachbase Data Import/Export Utility.\nRelease Version 5.1.9.community\nCopyright 2014, MACHBASE Corporation or its subsidiaries.\nAll Rights Reserved.\n-----------------------------------------------------------------\nNLS : US7ASCII EXECUTE MODE : IMPORT\nSCHMEA FILE : ignoretest.fmt DATA FILE : ignoretest.csv\nIMPORT_MODE : APPEND FILED TERM : ,\nROW TERM : \\n ENCLOSURE : \"\nARRIVAL_TIME : FALSE ENCODING : NONE\nHEADER : FALSE CREATE TABLE : FALSE\n \nProgress bar Imported records Error records\n2 0\n \nImport time : 0 hour 0 min 0.39 sec\nLoad success count : 2\nLoad fail count : 0\n \n \nmach&gt; SELECT * FROM ignoretest;\nID MSG\n---------------------------------------------------------\n2 msg2\n1 msg1\n[2] row(s) selected.\nElapsed time: 0.000\n\n\nIf Number of Columns Is More Than Number of Fields\n\nIf the number of columns in the table is greater than the number of fields in the data file, only the columns specified in the schema file are entered, and the other columns are entered as NULL.\n\nIf Number of Columns Is Less Than Number of Fields\n\nIf the number of columns in the table is less than the number of fields in the data file, fields not in the table must be excluded with the IGNORE option\n\nExample:\n\n-- Import ignoretest.csv file and exclude input data by setting ignore option for last field.\nloader_test.fmt\ntable loader_test\n{\nID integer;\nMSG varchar (40);\nSUB_ID integer IGNORE;\n}"
					}
					
				
		
				
					,
					
					"tools-utils-machsql-html": {
						"id": "tools-utils-machsql-html",
						"title": "machsql",
						"version": "all",
						"categories": "",
						"url": " /tools/utils/machsql.html",
						"content": "machsql is an interactive tool that performs SQL queries through the terminal screen.\n\nRun Option Description\n\n[mach@localhost]$ machsql -h\n\n\n\n  \n    \n      Short Option\n      Full Option\n      Description\n    \n  \n  \n    \n      -s\n      –server\n      Connecting server IP address (default: 127.0.0.1)\n    \n    \n      -u\n      –user\n      User name (default: SYS)\n    \n    \n      -p\n      –password\n      User password (default: MANAGER)\n    \n    \n      -P\n      –port\n      Server port number (default: 5656)\n    \n    \n      -n\n      –nls\n      NLS configuration\n    \n    \n      -f\n      –script\n      SQL script file to run\n    \n    \n      -z\n      –timezone=+-HHMM\n      Set Timezone ex) +0900   -1230\n    \n    \n      -o\n      –output\n      Filename to save query results\n    \n    \n      -i\n      –silent\n      Runs without the copyright notice\n    \n    \n      -v\n      –verbose\n      Detailed output\n    \n    \n      -r\n      –format\n      Specifies output file format (default: csv)\n    \n    \n      -h\n      –help\n      Displays options\n    \n    \n      -c\n      N/A\n      Add Connection parameter(Supported from version 6.1 or later)\n    \n  \n\n\nExample:\n\nmachsql -s localhost -u sys -p manager\nmachsql --server=localhost --user=sys --password=manager\nmachsql -s localhost -u sys -p manager -f script.sql\n# Supported from version 6.1 or later\nmachsql -s 127.0.0.1 -u sys -p manager -P 8888 -c ALTERNATIVE_SERVERS=192.168.0.147:9209;CONNECTION_TIMEOUT=10\n\n\nEnvironment Variable MACHBASE_CONNECTION_STRING\n\nSpecifies basic connection parameters. For example, to add CONNECTION_TIMEOUT, ALTERNATIVE_SERVERS, you may use environment variable setting below.\n\nexport MACHBASE_CONNECTION_STRING=ALTERNATIVE_SERVERS=192.168.0.148:8888;CONNECTION_TIMEOUT=3\n\n\nSetting connection parameter with -c option, it takes precedence over environment variables. This option is supported from version 6.1 or later\n\nSHOW Command\n\nDisplays information such as tables, tablespaces, and indexes.\n\nSHOW command list:\n\n\n  SHOW INDEX\n  SHOW INDEXES\n  SHOW INDEXGAP\n  SHOW LSM\n  SHOW LICENSE\n  SHOW STATEMENTS\n  SHOW STORAGE\n  SHOW TABLE\n  SHOW TABLES\n  SHOW TABLESPACE\n  SHOW TABLESPACES\n  SHOW USERS\n\n\nSHOW INDEX\nDisplays index information.\n\nSyntax:\n\nSHOW INDEX index_name\n\n\nExample:\n\nMach&gt; CREATE TABLE t1 (c1 INTEGER, c2 VARCHAR(10));\nCreated successfully.\nMach&gt; CREATE VOLATILE TABLE t2 (c1 INTEGER, c2 VARCHAR(10));\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx1 ON t1(c1) INDEX_TYPE LSM;\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx2 ON t1(c1) INDEX_TYPE BITMAP;\nCreated successfully.\nMach&gt; CREATE INDEX t2_idx1 ON t2(c1) INDEX_TYPE REDBLACK;\nCreated successfully.\nMach&gt; CREATE INDEX t2_idx2 ON t2(c2) INDEX_TYPE REDBLACK;\nCreated successfully.\n \nMach&gt; SHOW INDEX t1_idx2;\nTABLE_NAME                                COLUMN_NAME                               INDEX_NAME                      \n----------------------------------------------------------------------------------------------------------------------------------\nINDEX_TYPE   BLOOM_FILTER  KEY_COMPRESS  MAX_LEVEL   PART_VALUE_COUNT BITMAP_ENCODE\n--------------------------------------------------------------------------------------------\nT1                                        C1                                        T1_IDX2                         \nLSM          ENABLE   COMPRESSED    2           100000      EQUAL\n[1] row(s) selected.\n\n\nSHOW INDEXGAP\n\nDisplays index building GAP information.\n\nExample:\n\nMach&gt; SHOW INDEXGAP\nTABLE_NAME                                INDEX_NAME                                GAP\n-------------------------------------------------------------------------------------------------------------\nINDEX_TABLE                               T1_IDX1                                   0\nINDEX_TABLE                               T1_IDX2                                   0\n\n\nSHOW LSM\n\nDisplays LSM index building information.\n\nExample:\n\nMach&gt; SHOW LSM;\nTABLE_NAME                                INDEX_NAME                                LEVEL       COUNT\n--------------------------------------------------------------------------------------------------------------------------\nT1                                        IDX1                                      0           0\nT1                                        IDX1                                      1           100000\nT1                                        IDX1                                      2           0\nT1                                        IDX1                                      3           0\nT1                                        IDX2                                      0           100000\nT1                                        IDX2                                      1           0\n[6] row(s) selected.\n\n\nSHOW LICENSE\n\nDisplays license information.\n\nExample:\n\nMach&gt; SHOW LICENSE\nINSTALL_DATE          ISSUE_DATE            EXPIRY_DATE  TYPE        POLICY    \n---------------------------------------------------------------------------------------\n2016-07-01 10:24:37   20160325              20170325    2           0         \n[1] row(s) selected.\n\n\nSHOW STATEMENTS\n\nDisplays all query statements (Prepare, Execute, Fetch) registered in the server.\n\nExample:\n\nMach&gt; SHOW STATEMENTS\nUSER_ID     SESSION_ID  QUERY                                                                           \n--------------------------------------------------------------------------------------------------------------\n0           2           SELECT ID USER_ID, SESS_ID SESSION_ID, QUERY FROM V$STMT                        \n[1] row(s) selected.\n\n\nSHOW STORAGE\n\nDisplays the disk usage for each table created by the user.\n\nSyntax:\n\nSHOW STORAGE\n\n\nExample:\n\nMach&gt; CREATE TAGDATA TABLE TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\nCreated successfully.\n  \nMach&gt; SHOW STORAGE\nTABLE_NAME                                          DATA_SIZE            INDEX_SIZE           TOTAL_SIZE         \n------------------------------------------------------------------------------------------------------------------------ \n_TAG_DATA_0                                         50335744             0                    50335744           \n_TAG_DATA_1                                         50335744             0                    50335744           \n_TAG_DATA_2                                         50335744             0                    50335744           \n_TAG_DATA_3                                         50335744             0                    50335744           \n_TAG_META                                           0                    0                    0\n\n\nSHOW TABLE\n\nDisplays information about the table created by the user.\n\nSyntax:\n\nSHOW TABLE table_name\n\n\nExample:\n\nMach&gt; CREATE TABLE t1 (c1 INTEGER, c2 VARCHAR(10));\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx1 ON t1(c1) INDEX_TYPE LSM;\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx2 ON t1(c1) INDEX_TYPE BITMAP;\nCreated successfully.\n \nMach&gt; SHOW TABLE T1\n[ COLUMN ]\n----------------------------------------------------------------\nNAME                          TYPE                LENGTH\n----------------------------------------------------------------\nC1                            integer             11\nC2                            varchar             10\n \n[ INDEX ]\n----------------------------------------------------------------\nNAME                          TYPE                COLUMN\n----------------------------------------------------------------\nT1_IDX1                       LSM                 C1\nT1_IDX2                       LSM                 C1\n\n\nSHOW TABLES\n\nDisplays a list of all tables created by the user.\n\nExample:\n\nMach&gt; SHOW TABLES\nNAME                                    \n--------------------------------------------\nBONUS                                   \nDEPT                                    \nEMP                                     \nSALGRADE                                \n[4] row(s) selected.\n\n\nSHOW TABLESPACE\n\nDisplays tablespace information.\n\nExample:\n\nMach&gt; CREATE TABLE t1 (id integer);\nCreated successfully.\nMach&gt; CREATE INDEX t1_idx_id ON t1(id);\nCreated successfully.\n \nMach&gt; SHOW TABLESPACE SYSTEM_TABLESPACE;\n[TABLE]\nNAME                                      TYPE\n-------------------------------------------------------\nT1                                        LOG\n[1] row(s) selected.\n \n[INDEX]\nTABLE_NAME                                COLUMN_NAME                               INDEX_NAME                      \n----------------------------------------------------------------------------------------------------------------------------------\nT1                                        ID                                        T1_IDX_ID                   \n[1] row(s) selected.\n\n\nSHOW TABLESPACES\n\nDisplays a complete list of tablespaces.\n\nExample:\n\nMach&gt; CREATE TABLESPACE tbs1 DATADISK disk1 (DISK_PATH=\"tbs1_disk1\"), disk2 (DISK_PATH=\"tbs1_disk2\"), disk3 (DISK_PATH=\"tbs1_disk3\");\nCreated successfully.\n \n-- Insert data here\n...\n...\n \n \nMach&gt; SHOW TABLESPACES;\nNAME                                                                              DISK_COUNT  USAGE\n-----------------------------------------------------------------------------------------------------------------------\nSYSTEM_TABLESPACE                                                                 1           0\nTBS1                                                                              3           25824256\n[2] row(s) selected.\n\n\nSHOW USERS\n\nDisplays a list of users.\n\nExample:\n\nMach&gt; CREATE USER testuser IDENTIFIED BY 'test1234';\nCreated successfully.\n \nMach&gt; SHOW USERS;\nUSER_NAME                               \n--------------------------------------------\nSYS                                     \nTESTUSER\n[2] row(s) selected."
					}
					
				
		
				
		
				
					,
					
					"assets-js-main-js": {
						"id": "assets-js-main-js",
						"title": "",
						"version": "all",
						"categories": "",
						"url": " /assets/js/main.js",
						"content": "(function($) {\n    'use strict';\n    $(function() {\n        $('[data-toggle=\"tooltip\"]').tooltip();\n        $('[data-toggle=\"popover\"]').popover();\n        $('.popover-dismiss').popover({\n            trigger: 'focus'\n        })\n    });\n\n    function bottomPos(element) {\n        return element.offset().top + element.outerHeight();\n    }\n    $(function() {\n        var promo = $(\".js-td-cover\");\n        if (!promo.length) {\n            return\n        }\n        var promoOffset = bottomPos(promo);\n        var navbarOffset = $('.js-navbar-scroll').offset().top;\n        var threshold = Math.ceil($('.js-navbar-scroll').outerHeight());\n        if ((promoOffset - navbarOffset) < threshold) {\n            $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n        }\n        $(window).on('scroll', function() {\n            var navtop = $('.js-navbar-scroll').offset().top - $(window).scrollTop();\n            var promoOffset = bottomPos($('.js-td-cover'));\n            var navbarOffset = $('.js-navbar-scroll').offset().top;\n            if ((promoOffset - navbarOffset) < threshold) {\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n            } else {\n                $('.js-navbar-scroll').removeClass('navbar-bg-onscroll');\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll--fade');\n            }\n        });\n    });\n}(jQuery));\n(function($) {\n    'use strict';\n    var Search = {\n        init: function() {\n            $(document).ready(function() {\n                $(document).on('keypress', '.td-search-input', function(e) {\n                    if (e.keyCode !== 13) {\n                        return\n                    }\n                    var query = $(this).val();\n                    var searchPage = \"/en/search/?q=\" + query;\n                    document.location = searchPage;\n                    return false;\n                });\n            });\n        },\n    };\n    Search.init();\n}(jQuery));"
					}
					
				
		
				
					,
					
					"feature-tables-tag-managing-html": {
						"id": "feature-tables-tag-managing-html",
						"title": "Managing tag meta (tag name)",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/managing.html",
						"content": "Index\n\n\n  Concept of tag meta\n  Tag meta with tag name only\n    \n      Creation of tag meta\n      Input of tag meta\n      Output of tag meta\n      Modifying tag meta\n      Deleting tag meta\n    \n  \n  Tag meta with additional information\n    \n      Creation of tag meta\n      Input of tag meta\n      Modifying tag meta\n    \n  \n  Tag meta lookup via RESTful API\n    \n      Get all tag lists\n      Get the time range of a specific tag\n        \n          Syntax\n        \n      \n    \n  \n  Getting time range of all tags\n  Getting Time range of a specific tag\n\n\nConcept of tag meta\n\nThe tag meta represents the name and additional information of an arbitrary tag stored in the Machbase.\n\nThat is, if there are three tags existing in a specific apparatus, an arbitrary name representing the tag and related additional information are required, which are all referred to as meta information of the tag.\n\nThis tag meta should specify at least a name and, if necessary, specify various data types for the device.\n\nTag meta with tag name only\n\nCreation of tag meta\n\nBelow is the command to create the TAG table where the most basic tag meta is created.\n\ncreate tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized);\nMach&gt; desc tag;\n[ COLUMN ]                             \n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nNAME                          varchar             20                 \nTIME                          datetime            31             \nVALUE                         double              17\n\n\nThe above is the basic TAG table creation, and there is no separate information about the tag meta.\n\nIn this case, the tag meta has only basic information of VARCHAR (20).\n\nInput of tag meta\n\nNow, let’s insert one piece of tag information named TAG1.\n\nMach&gt; insert into tag metadata values ('TAG_0001');\n1 row(s) inserted.\n\nThrough the above query, we created one tag named TAG_0001.\n\nOutput of tag meta\n\nMachbase provides a special table _tag_meta for identifying the information of the input tag meta.\n\nTherefore, the user can confirm the information of all the tags input in the Machbase through the following query.\n\nMach&gt; select * from _tag_meta;\nID                   NAME                 \n----------------------------------------------\n1                    TAG_0001             \n[1] row(s) selected.\n\n\nThe ID is automatically assigned as an internally managed value.\n\nModifying tag meta\n\nMachbase allows you to modify the input tag meta information. The name can be modified as follows.\n\nMach&gt; update tag metadata set name = 'NEW_0001' where NAME = 'TAG_0001';\n1 row(s) updated.\n \nMach&gt; select * from _tag_meta;\nID                   NAME                 \n----------------------------------------------\n1                    NEW_0001             \n[1] row(s) selected.\n\n\nYou can see that the name has been changed from TAG_0001 to NEW_0001 as above.\n\nDeleting tag meta\n\nYou can delete the actual tag meta information as shown below.\n\nMach&gt; delete from tag metadata where name = 'NEW_0001';\n1 row(s) deleted.\n \nMach&gt; select * from _tag_meta;\nID                   NAME                 \n----------------------------------------------\n[0] row(s) selected.\n\n\nHowever, it should be noted that tag meta can be deleted when the actual data of the tag dosen’t refer to the corresponding tag meta.\n\nTag meta with additional information\n\nCreation of tag meta\n\nBelow, we will add 16 bit integer, time, and IPv4 information to the tag meta information.\n\nNote that once you have created a tag meta, you can modify the value except the structure.\n\ncreate tag table TAG (name varchar(20) primary key, time datetime basetime, value double summarized)\nmetadata (type short, create_date datetime, srcip ipv4) ;\n \nMach&gt; desc tag;\n[ COLUMN ]                             \n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nNAME                          varchar             20                 \nTIME                          datetime            31             \nVALUE                         double              17                 \n[ META-COLUMN ]                             \n----------------------------------------------------------------\nNAME                          TYPE                LENGTH       \n----------------------------------------------------------------\nTYPE                          short               6              \nCREATE_DATE                   datetime            31             \nSRCIP                         ipv4                15                 \n\n\nInput of tag meta\n\nYou can check the information by typing the following with the additional information other than the name.\n\nMach&gt; insert into tag metadata(name) values ('TAG_0001');\n1 row(s) inserted.\n \nMach&gt; select * from _tag_meta;\nID                   NAME                  TYPE        CREATE_DATE                     SRCIP          \n-------------------------------------------------------------------------------------------------------------\n1                    TAG_0001              NULL        NULL                            NULL           \n[1] row(s) selected.\n\n\nLike above, NULL is inserted except NAME column.\n\nNow let’s add more information as shown below.\n\nMach&gt; insert into tag metadata values ('TAG_0002', 99, '2010-01-01', '1.1.1.1');\n1 row(s) inserted.\n \nMach&gt; select * from _tag_meta;\nID                   NAME                  TYPE        CREATE_DATE                     SRCIP          \n-------------------------------------------------------------------------------------------------------------\n1                    TAG_0001              NULL        NULL                            NULL           \n2                    TAG_0002              99          2010-01-01 00:00:00 000:000:000 1.1.1.1        \n[2] row(s) selected.\n\n\nAdditional Information made each tag meta can be given a wealth of information.\n\nModifying tag meta\n\nNow let’s change the type of TAG_0001 from NULL to 11.\n\nMach&gt; update tag metadata set type = 11 where name = 'TAG_0001';\n1 row(s) updated.\n \nMach&gt; select * from _tag_meta;\nID                   NAME                  TYPE        CREATE_DATE                     SRCIP          \n-------------------------------------------------------------------------------------------------------------\n2                    TAG_0002              99          2010-01-01 00:00:00 000:000:000 1.1.1.1        \n1                    TAG_0001              11          NULL                            NULL           \n[2] row(s) selected.\n\n\nYou can modify the values of all fields through the UPDATE statement.\n\nHowever, it is a common constraint that NAME must be specified in the WHERE clause.\n\nTag meta lookup via RESTful API\n\nGet all tag lists\n\nBelow is an example of getting a list of all the tags in Machbase.\n\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/list\"\n{\"ErrorCode\": 0,\n \"ErrorMessage\": \"\",\n \"Data\": [{\"NAME\": \"TAG_0001\"},\n          {\"NAME\": \"TAG_0002\"}]}\nHost:~$\n\n\nGet the time range of a specific tag\n\nBelow is an example of obtaining the minimum and maximum time range of the data that the desired tag has.\n\nThis feature is very useful when charting specific tags.\n\nSyntax\n\n{MWA URL}/machiot-rest-api/tags/range/  # Time Range of whole DB\n{MWA URL}/machiot-rest-api/tags/range/{TagName}  # Time Range of a specific Tag\n\n\nGetting time range of all tags\n\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/range/\"\n{\"ErrorCode\": 0,\n \"ErrorMessage\": \"\",\n \"Data\": [{\"MAX\": \"2018-02-10 10:00:00 000:000:000\", \"MIN\": \"2018-01-01 01:00:00 000:000:000\"}]}\nGetting Time range of a specific tag\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/range/TAG_0001\"\n{\"ErrorCode\": 0, \"ErrorMessage\": \"\", \"Data\": [{\"MAX\": \"2018-01-10 10:00:00 000:000:000\", \"MIN\": \"2018-01-01 01:00:00 000:000:000\"}]}\nHost:~$\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/range/TAG_0002\"\n{\"ErrorCode\": 0, \"ErrorMessage\": \"\", \"Data\": [{\"MAX\": \"2018-02-10 10:00:00 000:000:000\", \"MIN\": \"2018-02-01 01:00:00 000:000:000\"}]}\n\n\nGetting Time range of a specific tag\n\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/range/TAG_0001\"\n{\"ErrorCode\": 0, \"ErrorMessage\": \"\", \"Data\": [{\"MAX\": \"2018-01-10 10:00:00 000:000:000\", \"MIN\": \"2018-01-01 01:00:00 000:000:000\"}]}\nHost:~$\nHost:~$ curl  -G  \"http://192.168.0.148:5001/machiot-rest-api/tags/range/TAG_0002\"\n{\"ErrorCode\": 0, \"ErrorMessage\": \"\", \"Data\": [{\"MAX\": \"2018-02-10 10:00:00 000:000:000\", \"MIN\": \"2018-02-01 01:00:00 000:000:000\"}]}"
					}
					
				
		
				
					,
					
					"feature-tables-tag-manipulate-html": {
						"id": "feature-tables-tag-manipulate-html",
						"title": "Manipulating tag data",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag/manipulate.html",
						"content": "Manipulating tag data\n\n\n  Input tag data\n  Extract tag data\n  Delete tag data"
					}
					
				
		
				
					,
					
					"faq-memory-error-html": {
						"id": "faq-memory-error-html",
						"title": "Insufficient Memory",
						"version": "all",
						"categories": "",
						"url": " /faq/memory-error.html",
						"content": "This section describes how to fix properties when an insufficient memory error occurs after executing a query.\n\nAn error occurred due to insufficient memory when executing the query\n\nThe memory required to execute the query is limited for the following reasons.\n\nIf a specific query uses too much memory, other queries running at the same time may not be executed due to insufficient memory.\n\nTo prevent this, the error can be resolved by increasing the property value of the maximum size of memory that can be used by one query.\n\nThe MAX_QPX_MEM Property manages the maximum available memory that can be used in one SQL.\n\nRefer to the SET MAX_QPX_MEM page for how to set during execution, as well as error messages and TRC messages that occur due to insufficient memory.\n\nIf the property value is set with the SET command, the set value is not applied when the machbase is restarted, so the machbase.conf file must also be modified as follows.\n\nStandard Edition\n\nModify MAX_QPX_MEM in machbase.conf to a larger value.\n\nCluster Edition\n\nSame as Standard edtion. However, machbase.conf of all cluster nodes must be modified."
					}
					
				
		
				
					,
					
					"config-monitor-metatable-html": {
						"id": "config-monitor-metatable-html",
						"title": "Meta Table",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/metatable.html",
						"content": "Index\n\n\n  Index\n  User Objects\n    \n      M$SYS_TABLES\n      M$SYS_TABLE_PROPERTY\n      M$SYS_COLUMNS\n      M$SYS_INDEXES\n      M$SYS_INDEX_COLUMNS\n      M$SYS_TABLESPACES\n      M$SYS_TABLESPACE_DISKS\n      M$SYS_USERS\n    \n  \n  Others\n    \n      M$TABLES\n      M$COLUMNS\n      M%RETENTION\n    \n  \n\n\nThe Meta Tables are tables that present the schema information of Machbase. The table names begin with “M$”.\n\nThese tables hold the table name, column information,  and index information, and reflect the creation, modification and deletion information resulting from the DDL statement.\nThe Meta Tables can not be added, deleted, or changed by the user.\n\nUser Objects\n\nM$SYS_TABLES\n\nDisplays the table created by the user.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      NAME\n      Table name\n    \n    \n      TYPE\n      Table type - 0: Log - 1: Fixed - 3: Volatile - 4: Lookup - 5: Key Value - 6: Tag\n    \n    \n      DATABASE_ID\n      Database identifier\n    \n    \n      ID\n      Table identifier\n    \n    \n      USER ID\n      User of created table\n    \n    \n      COLCOUNT\n      Number of columns\n    \n    \n      FLAG\n      classification Table Type - 1 : Tag Data Table - 2 : Rollup Table - 4 : Tag Meta Table - 8 : Tag Stat Table\n    \n  \n\n\nM$SYS_TABLE_PROPERTY\n\nDisplays table property information applied to each table.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      ID\n      Table identifier\n    \n    \n      NAME\n      Property Name\n    \n    \n      VALUE\n      Property Value\n    \n  \n\n\nM$SYS_COLUMNS\n\nDisplays the column information of the user table displayed in M$SYS_TABLES.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      NAME\n      Column name\n    \n    \n      TYPE\n      Column type\n    \n    \n      DATABASE_ID\n      Database identifier\n    \n    \n      ID\n      Column identifier\n    \n    \n      LENGTH\n      Column length\n    \n    \n      TABLE_ID\n      Table identifier of column\n    \n    \n      FLAG\n      (Information for internal use of the server)\n    \n    \n      PART_PAGE_COUNT\n      Pages per partition\n    \n    \n      PAGE_VALUE_COUNT\n      Number of data per page\n    \n    \n      MINMAX_CACHE_SIZE\n      Size of MIN-MAX cache\n    \n    \n      MAX_CACHE_PART_COUNT\n      Maximum number of partition caches\n    \n  \n\n\nM$SYS_INDEXES\n\nDisplays the index information generated by the user.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      NAME\n      Index name\n    \n    \n      TYPE\n      Index type\n    \n    \n      DATABASE_ID\n      Database identifier\n    \n    \n      ID\n      Index identifier\n    \n    \n      TABLE_ID\n      Table of index identifier\n    \n    \n      COLCOUNT\n      Number of columns of created index\n    \n    \n      PART_VALUE_COUNT\n      Number of data per partition of index table\n    \n    \n      BLOOM_FILTER\n      Availability of Bloom Filter\n    \n    \n      KEY_COMPRESS\n      Compression status of key values\n    \n    \n      MAX_LEVEL\n      Maximum level of index (LSM only)\n    \n    \n      PAGE_SIZE\n      Page size\n    \n    \n      MAX_KEYWORD_SIZE\n      Maximum keyword length (keyword only)\n    \n    \n      BITMAP_ENCODE\n      Bitmap encoding type (RANGE / EQUAL)\n    \n  \n\n\nM$SYS_INDEX_COLUMNS\n\nDisplays the column information of the user index shown in M$SYS_INDEXES.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      INDEX_ID\n      Index identifier\n    \n    \n      INDEX_TYPE\n      Index type\n    \n    \n      NAME\n      Column name\n    \n    \n      COL_ID\n      Column identifier\n    \n    \n      DATABASE_ID\n      Database identifier\n    \n    \n      TABLE_ID\n      Table identifier\n    \n    \n      TYPE\n      Data type of column\n    \n  \n\n\nM$SYS_TABLESPACES\n\nDisplays the table space information created by the user.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      NAME\n      Tablespace name\n    \n    \n      ID\n      Tablespace identifier\n    \n    \n      DISK_COUNT\n      Number of disks in tablespace\n    \n  \n\n\nM$SYS_TABLESPACE_DISKS\n\nMaintains the disk information used by the tablespace.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      NAME\n      Disk name\n    \n    \n      ID\n      Disk identifier\n    \n    \n      TABLESPACE_ID\n      Disk tablespace identifier\n    \n    \n      PATH\n      Disk path\n    \n    \n      IO_THREAD_COUNT\n      Number of IO threads allocated to this disk\n    \n    \n      VIRTUAL_DISK_COUNT\n      Number of Virtual Disk units assigned to this disk\n    \n  \n\n\nM$SYS_USERS\n\nMaintain user information registered in Machbase.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      USER_ID\n      User identifier\n    \n    \n      NAME\n      User name\n    \n  \n\n\nOthers\n\nM$TABLES\n\nDisplay all meta tables beginning with M$.\n\n\n  \n    \n      Column Name\n      Description\n    \n  \n  \n    \n      NAME\n      Meta table name\n    \n    \n      TYPE\n      Table type\n    \n    \n      DATABASE_ID\n      Database identifier\n    \n    \n      ID\n      Meta table identifier\n    \n    \n      USER ID\n      Table user (in this case, SYS)\n    \n    \n      COLCOUNT\n      Number of columns\n    \n  \n\n\nM$COLUMNS\n\nDisplays the column information of the meta table displayed in M​$TABLES.\n\n|Column Name|Description|\n|–|–|\n|NAME|Column name|\n|TYPE|Column type|\n|DATABASE_ID|Database identifier|\n|ID|Column identifier|\n|LENGTH|Column length|\n|TABLE_ID|Column table identifier|\n|FLAG|(Information for internal use of the server)|\n|PART_PAGE_COUNT|Pages per partition|\n|PAGE_VALUE_COUNT|Number of data per page|\n|MINMAX_CACHE_SIZE|Size of MIN-MAX cache|\n|MAX_CACHE_PART_COUNT|Maximum number of partition caches|\n«««&lt; HEAD\n\nM%RETENTION\n\nDisplays the RETENTION POLICY information.\n\n|Column Name|Description|\n|————-|—————-|\n| USER_ID     | User ID      |\n| POLICY_NAME | policy name    |\n| DURATION    | retention period(sec) |\n| INTERVAL    | update cycle(sec) |\n=======\n\n  \n    \n      \n        \n          \n            \n              5218c02 (en)"
					}
					
				
		
				
					,
					
					"install-windows-msi-install-html": {
						"id": "install-windows-msi-install-html",
						"title": "MSI Installation",
						"version": "all",
						"categories": "",
						"url": " /install/windows/msi-install.html",
						"content": "Installation\n\n\n  When the installation start screen is displayed, click the Next button.\n\n\n\n\n\n  On the screen to select the directory folder, the default is usually “C:\\Machbase-5.1\". \nIf you want to install to a different directory, you can change the path.\n\n\nAfter choosing the directory, click the Next button.\n\n\n\n\n  The installation progress screen is displayed. When the installation is complete, the Next button is activated. Click this button.\n\n\n\n\n\n  The installation completion screen is displayed. Click the Close button.\n\n\n\n\nLaunch Machbase\n\n\n  When the Machbase installation is completed, the Machbase shortcut icon is displayed on the desktop.\n Double-click to run the Machbase server.\n\n\n\n\n\n  This is the window interface screen for managing the Machbase server. \n You can control the Machbase server and the MWA Web server by clicking the menu."
					}
					
				
		
				
					,
					
					"feature-tables-log-extract-network-html": {
						"id": "feature-tables-log-extract-network-html",
						"title": "Network Data Type / Operator",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/extract/network.html",
						"content": "Index\n\n  IPv4\n  IPv6\n  Network Mask\n    \n      Mask Representation Type\n      Mask Operator\n      Example of Mask Usage\n    \n  \n\n\nMachbase supports network data types and supports functions available in SELECT statements.\n\n\n  IPv4 format: 4 byte address type\n  IPv6 format: 16 byte address type\n  Network mask: Network mask specification format (/ number of bits) for IPv4 or IPv6\n\n\nIPv4\n\nINSERT\n\nINSERT INTO table_name VALUES (value1,value2,value3,...);\n\n\nCREATE TABLE addrtable (addr IPV4);\nINSERT  INTO addrtable VALUES ('127.0.0.1');\nINSERT  INTO addrtable VALUES ('127.0' || '.0.2');\nINSERT  INTO addrtable VALUES ('127.0.0.3');\nINSERT  INTO addrtable VALUES ('127.0.0.4');\nINSERT  INTO addrtable VALUES ('127.0.0.5');\nINSERT  INTO addrtable VALUES ('255.255.255.255');\n\n\nSELECT\n\nSELECT  column_name,column_name FROM    table_name;\n\n\nMach&gt; SELECT addr FROM addrtable WHERE addr = '127.0.0.3' or addr = '127.0.0.5';\naddr           \n------------------\n127.0.0.5      \n127.0.0.3      \n[2] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable WHERE addr &gt; '127.0.0.3' AND addr &lt; '127.0.0.5';\naddr           \n------------------\n127.0.0.4      \n[1] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable WHERE addr &lt;&gt; '127.0.0.3';\naddr           \n------------------\n255.255.255.255\n127.0.0.5      \n127.0.0.4      \n127.0.0.2      \n127.0.0.1      \n[5] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable WHERE addr = '127.0.0.*';\naddr           \n------------------\n127.0.0.5      \n127.0.0.4      \n127.0.0.3      \n127.0.0.2      \n127.0.0.1      \n[5] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable WHERE addr = '*.0.0.*';\naddr           \n------------------\n127.0.0.5      \n127.0.0.4      \n127.0.0.3      \n127.0.0.2      \n127.0.0.1      \n[5] row(s) selected.\n\n\nIPv6\n\nINSERT\n\nINSERT  INTO    table_name  VALUES  (value1,value2,value3,...);\n\n\nCREATE TABLE addrtable6 (addr ipv6);\nINSERT INTO addrtable6 VALUES ('::0.0.0.0');\nINSERT INTO addrtable6 VALUES ('::127.0' || '.0.1');\nINSERT INTO addrtable6 VALUES ('::127.0.0.3');\nINSERT INTO addrtable6 VALUES ('::127.0.0.4');\nINSERT INTO addrtable6 VALUES ('21DA:D3:0:2F3B:2AA:FF:FE28:9C5A');\nINSERT INTO addrtable6 VALUES ('::FFFF:255.255.255.255');\n\n\nSELECT\n\nSELECT  column_name,column_name FROM    table_name;\n\n\nMach&gt; SELECT addr FROM addrtable6 WHERE addr = '::127.0.0.3' or addr = '::127.0.0.5';\naddr                                                        \n---------------------------------------------------------------\n::127.0.0.3                  \n[1] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable6 WHERE addr &gt; '::127.0.0.3' and addr &lt; '::127.0.0.5';\naddr                                                        \n---------------------------------------------------------------\n::127.0.0.4                     \n[1] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable6 WHERE addr &lt;&gt; '::127.0.0.3';\naddr                                                        \n---------------------------------------------------------------\n::ffff:255-255.255.255\n21da:d3::2f3b:2aa:ff:fe28:9c5a\n::127.0.0.4\n::127.0.0.1\n::                     \n[5] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable6 WHERE addr &gt;= '21DA::';\naddr                                                        \n---------------------------------------------------------------\n21da:d3::2f3b:2aa:ff:fe28:9c5a                   \n[1] row(s) selected.\n \nMach&gt; SELECT addr FROM addrtable6 order by addr desc;\naddr                                                        \n---------------------------------------------------------------\n21da:d3::2f3b:2aa:ff:fe28:9c5a\n::ffff:255.255.255.255\n::127.0.0.4\n::127.0.0.3\n::127.0.0.1\n::                   \n[6] row(s) selected.\n\n\nNetwork Mask\n\nThe network mask is an expression format that specifies whether a particular address is included in a particular network. Machbase supports network mask types and related operators.\n\nMask Representation Type\n\nLike the normal network representation, the network address is represented by the / symbol and the number of bits at the end.\n\n'192.128.0.0/16'\n'FFFF::192.128.99.0/32'\n\n\nMask Operator\n\nCONTAINS\n\nThis operator should have a network mask on the left and a network address data type on the right. In other words, it checks whether the input address is included in a given network mask. The NOT operator can be used together.\n\nSELECT addr FROM addrtable  WHERE '192.0.0.0/16'    CONTAINS    addr;\nSELECT addr FROM addrtable  WHERE '192.128.99.0/32' NOT CONTAINS    addr;\n\n\nCONTAINED\n\nOppositely to CONTAINS, the network address is left and the network mask is right. It checks whether the left address is part of the right mask.\n\nSELECT addr FROM addrtable  WHERE addr CONTAINED '192.0.0.0/16';\nSELECT addr FROM addrtable  WHERE addr NOT CONTAINED '192.128.99.0/32';\n\n\nExample of Mask Usage\n\nAn example of a search using the network mask type is as follows.\n\nCREATE TABLE ip_table (addr4 IPV4, addr6 IPV6);\n \nINSERT INTO ip_table VALUES ('192.0.0.1','FFFF::192.0.0.1');\nINSERT INTO ip_table VALUES ('192.0.10.1','FFFF::192.0.10.1');\nINSERT INTO ip_table VALUES ('192.128.0.1','FFFF::192.128.0.1');\nINSERT INTO ip_table VALUES ('192.128.99.128','FFFF::192.128.99.128');\nINSERT INTO ip_table VALUES ('192.128.99.64','FFFF::192.128.99.64');\nINSERT INTO ip_table VALUES ('192.128.99.32','FFFF::192.128.99.32');\nINSERT INTO ip_table VALUES ('192.128.99.16','FFFF::192.128.99.16');\nINSERT INTO ip_table VALUES ('192.128.99.8','FFFF::192.128.99.8');\nINSERT INTO ip_table VALUES ('192.128.99.4','FFFF::192.128.99.4');\nINSERT INTO ip_table VALUES ('192.128.99.2','FFFF::192.128.99.2');\nINSERT INTO ip_table VALUES ('192.128.99.1','FFFF::192.128.99.1');\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.0.0.0/16' CONTAINS addr4;\naddr4\n-----------\n192.0.10.1\n192.0.0.1\n[2] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.128.0.0/16' CONTAINS addr4;\naddr4\n-----------\n192.128.99.1\n192.128.99.2\n192.128.99.4\n192.128.99.8\n192.128.99.16\n192.128.99.32\n192.128.99.64\n192.128.99.128\n192.128.0.1\n[9] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.0.10.0/24' CONTAINS addr4;\naddr4\n--------------------------------------------------------------------\n192.0.10.1\n[1] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.128.99.0/31' CONTAINS addr4;\naddr4\n-------------------------------------------------------\n192.128.99.1\n[1] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE '192.128.99.0/32' NOT CONTAINS addr4;\naddr4\n-----------\n192.128.99.1\n192.128.99.2\n192.128.99.4\n192.128.99.8\n192.128.99.16\n192.128.99.32\n192.128.99.64\n192.128.99.128\n192.128.0.1\n192.0.10.1\n192.0.0.1\n[11] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE addr4 CONTAINED '192.0.0.0/16';\naddr4\n-------------------------------------\n192.0.10.1\n192.0.0.1\n[2] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE addr4 CONTAINED '192.128.0.0/16';\naddr4\n-------------------------------------\n192.128.99.1\n192.128.99.2\n192.128.99.4\n192.128.99.8\n192.128.99.16\n192.128.99.32\n192.128.99.64\n192.128.99.128\n192.128.0.1\n[9] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE addr4 CONTAINED '192.0.10.0/24';\naddr4\n----------------------------\n192.0.10.1\n[1] row(s) selected.\n \nMach&gt; SELECT addr4 FROM ip_table WHERE addr4 not CONTAINED '192.128.99.0/32';\naddr4\n-------------------------------------------------\n192.128.99.1\n192.128.99.2\n192.128.99.4\n192.128.99.8\n192.128.99.16\n192.128.99.32\n192.128.99.64\n192.128.99.128\n192.128.0.1\n192.0.10.1\n192.0.0.1\n[11] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.0.0.0/104' CONTAINS addr6;\naddr6\n-------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\nffff::c000:a01\nffff::c000:1\n[11] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.128.0.0/112' CONTAINS addr6;\naddr6\n------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\n[9] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.0.10.0/120' CONTAINS addr6;\naddr6\n------------------------------------------------\nffff::c000:a01\n[1] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.128.99.0/31' CONTAINS addr6;\naddr6\n---------------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\nffff::c000:a01\nffff::c000:1\n[11] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE 'FFFF::192.128.99.0/32' not CONTAINS addr6;\naddr6\n-------------------------------------\n[0] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE addr6 CONTAINED 'FFFF::192.0.0.0/104';\naddr6\n-------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\nffff::c000:a01\nffff::c000:1\n[11] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE addr6 CONTAINED 'FFFF::192.128.0.0/112';\naddr6\n-------------------------------------\nffff::c080:6301\nffff::c080:6302\nffff::c080:6304\nffff::c080:6308\nffff::c080:6310\nffff::c080:6320\nffff::c080:6340\nffff::c080:6380\nffff::c080:1\n[9] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE addr6 CONTAINED 'FFFF::192.0.10.0/120';\naddr6\n-------------------------------------\nffff::c000:a01\n[1] row(s) selected.\n \nMach&gt; SELECT addr6 FROM ip_table WHERE addr6 not CONTAINED 'FFFF::192.128.99.0/32';\naddr6\n-------------------------------------\n[0] row(s) selected."
					}
					
				
		
				
					,
					
					"feature-tables-backup-mount-overview-html": {
						"id": "feature-tables-backup-mount-overview-html",
						"title": "Backup Overview",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/backup-mount/overview.html",
						"content": "BACKUP/MOUNT\n\nTo ensure the permanence of the database, the data stored in memory is stored on the disk as soon as possible. In case of a general failure such as Process Failure, Restart Recovery makes the database consistent. However, in case of power failure or hardware damage caused by fire, database recovery is impossible. In order to solve this problem, the database backup and recovery function saves data to another disk or hardware periodically in another area and recovers the data using the corresponding data in case of an emergency.\n\nDatabase backups are divided into two types depending on when they are performed.\n\n  Offline Backup\n  Online Backup\n\n\nFirst, the Offline Backup function is called Cold Backup as it shuts down the DBMS and copies the database. It is very simple, but it has the disadvantage of the user’s service being interrupted. Therefore, it is rarely used during operation and tends to be used only for initial testing or data construction.\n\nSecond, Online Backup is called Hot Backup as a function to backup the database when DBMS is running. This function can be performed without interrupting the service, increasing the user’s service availability. Most DBMS Backup refers to Online Backup. Unlike other database backups, Machbase, a time-series database, provides Duration Backup. This allows you to specify the time of the database to be backed up at the time of backup so that only the data at the desired time will be backed up.\n\n\n\nbackup database into disk = 'backup';\nbackup database from to_date('2015-07-14 00:00:00','YYYY-MM-DD HH24:MI:SS') to to_date('2015-07-14 23:59:59 999:999:999','YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn')\n                into disk = 'backup_20150714';\n\n\nThe backed-up database can be used as an existing database through a recovery process. This recovery method is called Restore. This Restore function deletes the damaged database and restores the backed up database image to the Primary Database. Therefore, when recovering, delete the existing database and restore it using machadmin -r.\n\nmachadmin -r 'backup'\n\n\nThe mount / unmount function is an online function that attaches a backed up database to the currently running database.\n\nmount database 'backup' to mountName;\numount database mountName;\n\n\nDatabase Backup\n\nMachbase offers two options for backing up your data. It provides DATABASE backup function which backs up information of running DB and TABLE backup function which can select only necessary tables to back up.\nThe backup command provided by DB is as follows.\n\nBACKUP [ DATABASE | TABLE table_name ]  [ time_duration ] INTO DISK = 'path/backup_name';\ntime_duration = FROM start_time TO end_time\npath = 'absolute_path' or  'relative_path'\n# Directory backup\n       BACKUP DATABASE INTO DISK = 'backup_dir_name';\n# Set backup duration\n      - Directory backup\n       BACKUP DATABASE FROM TO_DATE('2015-07-14 00:00:00','YYYY-MM-DD HH24:MI:SS')\n                         TO TO_DATE('2015-07-14 23:59:59','YYYY-MM-DD HH24:MI:SS')\n                         INTO DISK = '/home/machbase/backup_20150714'\n\n\nWhen performing a DB backup, the backup type, time duration, and path must be entered as options. When backing up DATABASE entirely, type DATABASE for the backup type. To backup only a specific table, enter TABLE, and then enter the name of the table to be backed up. The TIME_DURATION statement can be set to back up only the data for the required period. In the FROM field, enter the start time of the date you want to back up, and enter the time of the last date in the TO field. In Example 3, the TIME_DURATION is set to FROM, “July 14, 2014, 0, 0, 0”, and TO “July 14, 2015, 23:59:59” meaning only 14 days of data are set to be backed up. If information is not entered for the DURATION item, the FROM item is set to ‘January 1, 1970, 9:00:00,’ and the TO item is automatically set to the time to execute the command. Time range backup using the DURATION clause cannot be used in the database including the TAG table and the TAG table, and the INCREMENTAL BACKUP function, which is a function to back up incremental data, must be used.\n\nFinally, a storage medium to store needs to be configured to store the results of the backup. If you want to create a backup in a single file, set the creation type to IBFILE, or enter DISK to create it in directory units. Note that you can specify the PATH information to store the product. If you enter a relative path, it will be created in the path specified in the DB_PATH item of the current DB configuration. If you want to store it somewhere other than DB_PATH, you must enter an absolute path starting with ‘/’.\n\nIncremental Backup\n\nIncremental backup is a function that backs up only data entered after the previous backup. The target for incremental backup is only data in the log and tag tables, and the lookup table always backs up all data. \nIn order to perform an incremental backup, the previously performed incremental backup directory or the entire backup directory is required. \nIncremental backup is performed as follows.\n\nMach&gt; BACKUP DATABASE INTO DISK = 'backup1'; /* run full backup */\nExecuted successfully.\nMach&gt; ...\n  \nMach&gt; BACKUP DATABASE AFTER 'backup1' INTO DISK = 'backup2'; /* Running incremental backup on the data inserted after backup1 */\nExecuted successfully.\nMach&gt; ...\n\n\nIncremental backup is available for the entire database (at this time, the lookup table becomes a full backup), log table, and tag table. If you want to backup by RESTORE function, you need backup data which is saved before incremental backup.\nIf you do not want to delete the current data and return to the previous state, you can use the MOUNT function described below.\n\nWarning of Incremental Backup\n\nAs above, if backup2 is created as an incremental backup based on backup1, if backup1 is lost (due to disk failure, etc.), it cannot be restored using backup2.\n\nFor the same reason, if a previous backup is lost after an incremental backup, it cannot be restored using a later backup.\n\nIf the backup is performed 3 times as shown below, the previous backup of backup3 becomes backup2 and the previous backup of backup2 becomes backup1.\n\nTherefore, if backup1 is lost, both backup2 and backup3 cannot be used, and if backup2 is lost, it cannot be recovered using backup3.\n\nMach&gt; BACKUP DATABASE INTO DISK = 'backup1'; /* run full backup */\nExecuted successfully.\nMach&gt; ...\n  \nMach&gt; BACKUP DATABASE AFTER 'backup1' INTO DISK = 'backup2'; /* Running incremental backup on the data inserted after backup1 */\nExecuted successfully.\nMach&gt; ...\n \nMach&gt; BACKUP DATABASE AFTER 'backup2' INTO DISK = 'backup3'; /* Running incremental backup on the data inserted after backup2 */\nExecuted successfully.\nMach&gt; ...\n\n\nDatabase Restore\n\nThe Database Restore feature is not provided as a syntax, and can be recovered offline using machadmin -r. You must check the following before restoration.\n\n\n  Has Machbase been shutdown?\n  Has the previously created DB been deleted?\n\n\nmachadmin -r backup_database_path;\n\n\nbackup database into disk = '/home/machbase/backup';\n\n\nmachadmin -k\nmachadmin -d\nmachadmin -r /home/machbase/backup;\n\n\nDatabase Mount\n\nThe following problems arise when periodically backing up a large number of databases and adding data continuously in preparation for a system failure.\n\n\n  Increased disk cost to store data\n  Limitations of the physical disk space of the running machine\n\n\nIn order to solve this problem, periodical deletion is performed by leaving only data necessary for the current service. However, if you need to refer to the past data, you need to restore the backed up database. In case of a very large backup image, recovery time is long and additional equipment is needed. This is because the Restore function can only be performed by deleting the currently running database. To solve this problem, Machbase provides the Database Mount function.\n\nThe Database Mount function is an online function that attaches a backed up database to the currently running database. By attaching multiple backup databases to the primary database, the user can refer to multiple backup databases as if they were one database. The mounted database is read-only.\n\nThe Mount DATABASE command is a function that prepares the database or table DATA created by Backup in a state that it can be viewed from the currently running database. So, Mounted DATABASE can query the data using the same DB command.\n\nThe current Database Mount function restrictions are as follows.\n\n\n  The backup information must be compatible with the database to be mounted, the DB major number, and the Meta major number.\n  When mounting backup data, it is read-only and does not support index creation, data insertion or deletion.\n  Information about the currently mounted DATABASE can be found by querying V$STORAGE_MOUNT_DATABASES.\n  When incremental backup data is mounted, only the incremental data recorded in the backup data is searched, and it does not mount by following the previously performed incremental data.\n\n\nMount\n\nTo execute the mount command, Backup_database_path information and DatabaseName are required. Backup_database_path is the location information of the DB created by Backup command. DatabaseName is the name that can be distinguished when mounting to Database. Backup_database_path is searched based on the directory specified in the DB_PATH set in the environment variable of the DB when the relative path is entered in the same way as when performing the backup.\n\nMOUNT DATABASE 'backup_database_path' TO mount_name;\nMOUNT DATABASE '/home/machbase/backup' TO mountdb;\n\n\nUnmount\n\nIf the mounted database will no longer be used, it can be removed using the unmount command.\n\nUNMOUNT DATABASE mount_name;\nUNMOUNT DATABASE mountdb;\n\n\nMOUNT DB Data Retrieval\n\nWhen querying DATA of Backup DB, it can be retrieved by using the same SQL statement when querying the DATA of the DB in operation.\n\nThe mounted DB can retrieve data only by the SYS admin user of the DB in operation. To retrieve the data, you must put MountDBName and UserName in front of the TableName to be queried, and use ‘.’ for each delimiter. MountDBName is used to refer to a specific DB among currently mounted DBs, and UserName refers to the information of the user that owns the mounted DB table.\n\nSELECT column_name FROM mount_name.user_name.table_name;\n\n\nSELECT * FROM mountdb.sys.backuptable;"
					}
					
				
		
				
					,
					
					"install-package-html": {
						"id": "install-package-html",
						"title": "Package Overview",
						"version": "all",
						"categories": "",
						"url": " /install/package.html",
						"content": "Package Type\n\nMACHBASE provides manual installation and package installation files.\n\n\n  \n    \n      Installation type\n      Description\n      Note\n    \n  \n  \n    \n      manual installation\n      Has a compressed file format and the extension tgz for Unix.The user decompresses using tar and GNU gzip to proceed with the installation.\n      Can be installed only in console environment\n    \n    \n      package installation\n      Provides an installation package for each operating system environment. - Windows: msi  -  Linux: tgz\n      Can be installed only in console environment\n    \n  \n\n\nPackage File Name Structure\n\nThe package file name is configured as follows.\n\n_machbase-EDITION_VERSION-OS-CPU-BIT-MODE-OPTIONAL.EXT_\n\n\n\n  \n    \n      item\n      Description\n    \n  \n  \n    \n      EDITION\n      Indicates the edition of the package. - standard: Standard Edition - cluster: Cluster Edition\n    \n    \n      VERSION\n      Indicates the version of the package.In detail, it is classified as MajorVersion.MinorVersion.FixVersion.AUX by numbers and characters.- Major Version: Product main version - number- Minior Version: A version with relatively large features added in the same main version. DB file / protocol compatibility is not guaranteed. -number- Fix Version: A bug / minor feature added in the same main version. DB file / protocol compatibility  is guaranteed. - number- AUX: Indicates the package classification -number – official: general package – community: community edition package\n    \n    \n      OS\n      Indicates the operating system name. (Example) LINUX, WINDOWS\n    \n    \n      CPU\n      Indicates the type of CPU installed in the operating system. (Example) X86, IA64\n    \n    \n      BIT\n      Indicates whether  the compiled binary  is 32-bit or 64-bit. (Example) 32, 64\n    \n    \n      MODE\n      Indicates the release mode of the binary once compiled. (Example) release, debug, prerelease\n    \n    \n      OPTIONAL\n      Only displayed in Enterprise Edition.lightweight: Indicates a lightweight package to be added to the Coordinator.\n    \n    \n      EXT\n      The package file extension. Depending on the package, it is available as tgz, rpm, deb, and msi."
					}
					
				
		
				
		
				
					,
					
					"config-monitor-property-cluster-html": {
						"id": "config-monitor-property-cluster-html",
						"title": "Property (Cluster)",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/property-cluster.html",
						"content": "Index\n\n\n  CLUSTER_LINK_ACCEPT_TIMEOUT\n  CLUSTER_LINK_BUFFER_SIZE\n  CLUSTER_LINK_CHECK_INTERVAL\n  CLUSTER_LINK_CONNECT_RETRY_TIMEOUT\n  CLUSTER_LINK_CONNECT_TIMEOUT\n  CLUSTER_LINK_ERROR_ADD_ORIGIN_HOST\n  CLUSTER_LINK_HANDSHAKE_TIMEOUT\n  CLUSTER_LINK_BUFFER_SIZE\n  CLUSTER_LINK_SEND_RETRY_COUNT\n  CLUSTER_LINK_HOST\n  CLUSTER_LINK_LONG_TERM_CALLBACK_INTERVAL\n  CLUSTER_LINK_LONG_WAIT_INTERVAL\n  CLUSTER_LINK_MAX_LISTEN\n  CLUSTER_LINK_MAX_POLL\n  CLUSTER_LINK_PORT_NO\n  CLUSTER_LINK_RECEIVE_TIMEOUT\n  CLUSTER_LINK_REQUEST_TIMEOUT\n  CLUSTER_LINK_SEND_RETRY_COUNT\n  CLUSTER_LINK_SEND_TIMEOUT\n  CLUSTER_LINK_SESSION_TIMEOUT\n  CLUSTER_LINK_THREAD_COUNT\n  CLUSTER_QUERY_STAT_LOG_ENABLE\n  CLUSTER_REPLICATION_BLOCK_SIZE\n  CLUSTER_WAREHOUSE_DIRECT_DML_ENABLE\n  COORDINATOR_DBS_PATH\n  COORDINATOR_DDL_REQUEST_TIMEOUT\n  COORDINATOR_DDL_TIMEOUT\n  COORDINATOR_DECISION_DELAY\n  COORDINATOR_DECISION_INTERVAL\n  COORDINATOR_HOST_RESOURCE_ENABLE\n  COORDINATOR_HOST_RESOURCE_COLLECT_INTERVAL\n  COORDINATOR_HOST_RESOURCE_INTERVAL\n  COORDINATOR_HOST_RESOURCE_REQUEST_TIMEOUT\n  COORDINATOR_NODE_REQUEST_TIMEOUT\n  COORDINATOR_NODE_TIMEOUT\n  COORDINATOR_STARTUP_DELAY\n  COORDINATOR_STATUS_NODE_INTERVAL\n  COORDINATOR_STATUS_NODE_REQUEST_TIMEOUT\n  COORDINATOR_DISK_FULL_UPPER_BOUND_RATIO\n  COORDINATOR_DISK_FULL_LOWER_BOUND_RATIO\n  DEPLOYER_DBS_PATH\n  EXECUTION_STAGE_MEMORY_MAX\n  HTTP_ADMIN_PORT\n  HTTP_CONNECT_TIMEOUT\n  HTTP_RECEIVE_TIMEOUT\n  HTTP_SEND_TIMEOUT\n  INSERT_BULK_DATA_MAX_SIZE\n  INSERT_RECORD_COUNT_PER_NODE\n  LOOKUPNODE_COMMAND_RETRY_MAX_COUNT\n  STAGE_RESULT_BLOCK_SIZE\n\n\nSeparate from Property, Property (Cluster) organizes the Property only available in Cluster Edition.\n\nCLUSTER_LINK_ACCEPT_TIMEOUT\nTimeout until receiving Handshake message after Accept when connecting to a specific Node.\n\nFailure to receive within the timeout will cause the connection to fail.\n\nThe default value is 5 seconds.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      5000000\n    \n  \n\n\nCLUSTER_LINK_BUFFER_SIZE\nIf this size is insufficient, it will try again until the buffer is empty during transmission.\n\nThe size of the request/receive buffer.\n\nDefault is 32M.\n\n\n  \n    (byte)\n    Value\n  \n  \n    \n      Minimum\n      1024768\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      33554432 (32M)\n    \n  \n\n\nCLUSTER_LINK_CHECK_INTERVAL\nCheck interval of the Timeout Thread that checks the Sockets connected to a specific Node.\n\nThere is a Timeout Thread that checks RECEIVE_TIMEOUT and SESSION_TIMEOUT.\n\nThe shorter the cycle is, the more frequently it is checked but the Timeout determination is made according to the following values.\n\nThe default value is 1 second.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      1000000\n    \n  \n\n\nCLUSTER_LINK_CONNECT_RETRY_TIMEOUT\nTimeout to repeat reconnect attempt after connection failure with a specific Node.\n\nIf it is not connected within the timeout, it is determined to be completely disconnected.\n\nThe default value is 1 minute.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      60000000\n    \n  \n\n\nCLUSTER_LINK_CONNECT_TIMEOUT\nTime to wait when trying to connect to a specific Node.\n\nIf it does not connect within the Timeout, it will try to reconnect until CLUSTER_LINK_CONNECT_RETRY_TIMEOUT has passed.\n\nThe default value is 5 seconds.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      5000000\n    \n  \n\n\nCLUSTER_LINK_ERROR_ADD_ORIGIN_HOST\nYou can choose whether to add an errored host name to error messages that occur during communication between the Cluster.\n\nIf you want to display a detailed error message, set the property to 1.\n\nThe default value is 0, which means the host name is not displayed.\n\n\n  \n    (boolean)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nCLUSTER_LINK_HANDSHAKE_TIMEOUT\nTimeout until receiving a Handshake message while connected to a specific Node and Cluster Socket.\n\nTwo Nodes that have just finished connecting exchange small size Handshake messages to check the connection status.\n\nThe Accept Node sends the Handshake message first, and the time to wait for the response is set here.\n\nThe default value is 5 seconds.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      5000000\n    \n  \n\n\nCLUSTER_LINK_BUFFER_SIZE\nThe size of the send / receive buffer.\n\nIf this size is insufficient, the transmission will retry until the buffer is empty.\n\nThe default value is 32M.\n\n\n  \n    (byte)\n    Value\n  \n  \n    \n      Minimum\n      1024768\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      33554432 (32M)\n    \n  \n\n\nCLUSTER_LINK_SEND_RETRY_COUNT\nNumber of times to retry sending until the send buffer is empty.\n\nEvery retry will take 1ms off. If you retry beyond this number, you will be disconnected.\n\nThe default value is 5000 (msec).\n\n\n  \n    (count)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      5000\n    \n  \n\n\nCLUSTER_LINK_HOST\nHost name of the current Node to connect to a specific Node and Cluster Socket\n\n\n  \n    (string)\n    Value\n  \n  \n    \n      Default\n      localhost\n    \n  \n\n\nCLUSTER_LINK_LONG_TERM_CALLBACK_INTERVAL\nIf the execution time of Receive Callback to process a message received on Cluster Socket exceeds the set value, it is recognized as Long-Term Callback.\n\nSince the number of receive Threads is limited, Receive Callback should not process messages for a long time.\n\nIf Receive Callback processes the message after this time, it recognizes it as Long-Term Callback and records it in Trace Log.\n\nThe default value is 1 second.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      1000000\n    \n  \n\n\nCLUSTER_LINK_LONG_WAIT_INTERVAL\nIf the time until the arrival of a message received on Cluster Socket exceeds the set value, it is recognized as Long-Wait Message.\n\nIf the time from receiving start to receiving end is long, it can be regarded as a problem of the network environment.\n\nIf the received message does not arrive after this time, it is recognized as a Long-Wait Message and recorded in the Trace Log.\n\nThe default value is 1 second.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      1000000\n    \n  \n\n\nCLUSTER_LINK_MAX_LISTEN\nThe maximum number of Socket’s Accept Queue when connecting to a specific Node.\n\n\n  \n    (count)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      512\n    \n  \n\n\nCLUSTER_LINK_MAX_POLL\nThe maximum number of Events that can be retrieved at a time by Poll when communicating with a specific node.\n\n\n  \n    (count)\n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      4096\n    \n  \n\n\nCLUSTER_LINK_PORT_NO\nThe port number of the current Node for connecting the specific Node to the Cluster Socket\n\n\n  \n    (port)\n    Value\n  \n  \n    \n      Minimum\n      1024\n    \n    \n      Maximum\n      65535\n    \n    \n      Default\n      3868\n    \n  \n\n\nCLUSTER_LINK_RECEIVE_TIMEOUT\nTimeout until the Timeout Thread determines that the connection has been disconnected since the last reception.\n\nConnections that exist in the ‘Linked List’ should be continuously receiving because the connection between Cluster Nodes is terminated when the reception is complete.\n\nIf the last received time is not updated after the set time has elapsed, the Timeout Thread records its contents in the Trace Log and closes the Socket.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      30000000\n    \n  \n\n\nCLUSTER_LINK_REQUEST_TIMEOUT\nTimeout from when a request message is sent from the Cluster Socket to when a response to the request is received.\n\nFor specific messages, specify the time to wait for a response after the request.\n\nIf the response message does not arrive at this time, write log to the Trace Log and close the Socket.\n\nThe default value is 60 seconds, Timeout is long enough because it is not known what kind of message and receive processing will happen.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      60000000\n    \n  \n\n\nCLUSTER_LINK_SEND_RETRY_COUNT\nNumber of times to retry transmission until the transmit buffer is empty.\n\nEach retry will result in 1 ms of rest.\n\nIf you try again beyond this number, disconnect.\n\nThe default value is 5000.\n\n\n  \n    (count)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      5000\n    \n  \n\n\nCLUSTER_LINK_SEND_TIMEOUT\nTimeout to set when sending messages through Cluster Socket.\n\nSet the corresponding timeout when transmitting.\n\nIf transmission is not completed until Timeout, it is recorded in the Trace Log.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      30000000\n    \n  \n\n\nCLUSTER_LINK_SESSION_TIMEOUT\nTimeout until the Timeout thread determines that the connection has been disconnected since the last receive in a specific session.\n\nCluster connection manages the session of all messages internally, which is a necessary property in case the session can suddenly not be fixed.\n\nIf the last receive time for the session is not updated after this time, the Timeout Thread writes to the Trace Log and closes the session.\n\nThe default value is 1 hour.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      3600000000\n    \n  \n\n\nCLUSTER_LINK_THREAD_COUNT\nThe number of Threads to process the received messages when communicating with a specific Node.\n\nIf the size of the Cluster grows or the number of operations to be processed increases, you can increase the number of receive threads.\n\n\n  \n    (count)\n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      4096\n    \n    \n      Default\n      8\n    \n  \n\n\nCLUSTER_QUERY_STAT_LOG_ENABLE\nOutputs statistical information about the executed query to the trace log.\n\n\n  \n    (boolean)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nCLUSTER_REPLICATION_BLOCK_SIZE\nThe size of the data to be sent at once when the Replication for adding Node is performed in the Cluster Edition.\n\nThe Property must be applied directly to the warehouse (=Transmitting Warehouse) that becomes the Replication Active.\n\nThe default value is 640 KB.\n\n\n  \n    (size)\n    Value\n  \n  \n    \n      Minimum\n      64 * 1024\n    \n    \n      Maximum\n      100 * 1024 * 1024\n    \n    \n      Default\n      640 * 1024 (655360)\n    \n  \n\n\nCLUSTER_WAREHOUSE_DIRECT_DML_ENABLE\nIt is made possible to connect directly to the Warehouse to perform DML in Cluster Edition.\n\n\n  1: Executable\n  2: Not executable. An error is returned.\n\n\nWhen directly performing the DML in Warehouse, there are performance advantages over Brokers but there is an issue where the DML is not propagated to the same Group.\n\nTherefore, it is used only for emergency recovery due to data discrepancies, or if the data discrepancies of the Group can be taken into account.\n\nYou must apply Properties directly to the specific Warehouse you want.\n\nThe default value is 0.\n\n\n    The Coordinator does not check for data discrepancies, even if there is a data difference between the Warehouses in the Group with the corresponding Property turned on.\n\n\n\n  \n    (boolean)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nCOORDINATOR_DBS_PATH\nSpecifies the directory where the Coordinator data file will be created.\n\nThe default value is set to ?/dbs, and ? is replaced with the $ MACHBASE_COORDINATOR_HOME environment variable.\n\nThis is an environment variable $MACHBASE_COORDINATOR_HOME/dbs directory.\n\nIt must be applied to the Coordinator, and it has no effect on other Nodes.\n\n\n  \n    (path)\n    Value\n  \n  \n    \n      Default\n      ?/dbs\n    \n  \n\n\nCOORDINATOR_DDL_REQUEST_TIMEOUT\nTimeout until the Coordinator waits after requesting the Node to execute DDL.\n\nThis value refers to the time the Coordinator waits after requesting each Node to perform DDL.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      3600000000\n    \n  \n\n\nCOORDINATOR_DECISION_DELAY\nTimeout until the Coordinator requests the status change and effectively reflects it.\n\nIf the status does not actually change over this time, disable the cluster status.\n\nIf the status of the Warehouse Active is not changed but the connected Standby exists, the Fail-Over operation starts.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      1000000\n    \n  \n\n\nCOORDINATOR_DECISION_INTERVAL\nTime to determine how often the Coordinator changes status.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      1000000\n    \n  \n\n\nCOORDINATOR_HOST_RESOURCE_ENABLE\nWhether the Coordinator collects Host Resources for Cluster Nodes.\n\n\n  \n    (boolean)\n    Value\n  \n  \n    \n      Minimum\n      0 (false)\n    \n    \n      Maximum\n      1 (true)\n    \n    \n      Default\n      0 (false)\n    \n  \n\n\nCOORDINATOR_HOST_RESOURCE_COLLECT_INTERVAL\nInterval at which Cluster Nodes collect Host Resources.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      1000000\n    \n  \n\n\nCOORDINATOR_HOST_RESOURCE_INTERVAL\nInterval at which the Coordinator exchanges Host Resources with Nodes.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      1000000\n    \n  \n\n\nCOORDINATOR_HOST_RESOURCE_REQUEST_TIMEOUT\nTime that the Coordinator waits after requesting the Host Resource information from the Nodes.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      10000000\n    \n  \n\n\nCOORDINATOR_NODE_REQUEST_TIMEOUT\nTimeout until the Coordinator waits after requesting the Node to execute the command.\n\nBecause the Add/Remove-node and Add/Remove-Package includes the Node command execution, if it is caught in a short time, the command processing may not be completed.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      600000000\n    \n  \n\n\nCOORDINATOR_NODE_TIMEOUT\nTime the Coordinator waits before determining that the Node has failed.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      30000000\n    \n  \n\n\nCOORDINATOR_STARTUP_DELAY\nGrace time until activating the Decision Thread immediately after Coordinator startup.\n\nIf it takes a long time to run the entire Cluster, you can start the Node control of Coordinator later by setting a larger value.\n\nIf the Decision Thread runs before the entire drive, there is a high likelihood that the Coordinator will be misplaced.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      3000000\n    \n  \n\n\nCOORDINATOR_STATUS_NODE_INTERVAL\nInterval in which the Coordinator exchanges status inquiry messages with the Nodes.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      1000000\n    \n  \n\n\nCOORDINATOR_STATUS_NODE_REQUEST_TIMEOUT\nTime the Coordinator waits after requesting status inquiries from Nodes.\n\nIf there is no status inquiry response during that time, the Coordinator proceeds without updating the status of the corresponding Node.\n\nIf the network situation is not good and you need to update the state, you could consider increasing the value.\n\nInstead, if there is no status query response, the Coordinator will wait for as much as the value was increased.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      5000000\n    \n  \n\n\nCOORDINATOR_DISK_FULL_UPPER_BOUND_RATIO\nIf the disk usage of some servers configured in the cluster exceeds the property value, the group to which the warehouse belongs will enter the DISKFULL state.\n\nInput is restricted for the group in the DISKFULL state, and only inquiry and deletion are possible.\n\nIf the property value is 0, the function is disabled.\n\n\n  \n    (percent)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      99\n    \n    \n      Default\n      0\n    \n  \n\n\nCOORDINATOR_DISK_FULL_LOWER_BOUND_RATIO\nIf the disk usage of the server operating in the DISKFULL state falls below the property value, the group state transitions to the normal.\n\nIf the property value is 0, the function is disabled.\n\n\n  \n    (percent)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      99\n    \n    \n      Default\n      0\n    \n  \n\n\nDEPLOYER_DBS_PATH\nSpecifies the directory where the Deployer’s data files will be created.\n\nThe default value is set to?/dbs, and ? is replaced with the $ MACHBASE_DEPLOYER_HOME environment variable.\n\nThis is an environment variable $MACHBASE_DEPLOYER_HOME /dbs directory.\n\nIt must be applied to Deployer, and it has no effect on other Nodes.\n\n\n  \n    (path)\n    Value\n  \n  \n    \n      Default\n      ?/dbs\n    \n  \n\n\nEXECUTION_STAGE_MEMORY_MAX\nThe maximum amount of Memory used by the Stage Thread performing the SELECT query in Cluster Edition.\n\nBecause it is the maximum size of each Stage, the complexity of the SELECT query with an increase in the number of Stages can lead to a larger memory requirement.\n\nIf there is a Stage that exceeds the maximum size, the Stage is canceled and the Query is canceled with an error.\n\nYou must apply Properties directly to the specific Warehouse you want.\n\nThe default value is 1GB.\n\n\n  \n    (size)\n    Value\n  \n  \n    \n      Minimum\n      1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      1024 *1024 * 1024\n    \n  \n\n\nHTTP_ADMIN_PORT\nPort number to receive requests from MWA or machcoordinatoradmin.\n\n\n  \n    (port)\n    Value\n  \n  \n    \n      Minimum\n      1024\n    \n    \n      Maximum\n      65535\n    \n    \n      Default\n      5779\n    \n  \n\n\nHTTP_CONNECT_TIMEOUT\nTimeout used when connecting to machcoordinatoradmin.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      30000000\n    \n  \n\n\nHTTP_RECEIVE_TIMEOUT\nTimeout used when communicating with machcoordinatoradmin.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      3600000000\n    \n  \n\n\nHTTP_SEND_TIMEOUT\nTimeout used when communicating with machcoordinatoradmin.\n\n\n  \n    (usec)\n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      60000000\n    \n  \n\n\nINSERT_BULK_DATA_MAX_SIZE\nMaximum size of input data block when executing Append or INSERT-SELECT.\n\n\n  \n    (size)\n    Value\n  \n  \n    \n      Minimum\n      1024\n    \n    \n      Maximum\n      10 * 1024 * 1024\n    \n    \n      Default\n      1024 * 1024\n    \n  \n\n\nINSERT_RECORD_COUNT_PER_NODE\nNumber of data inputs that lead to the warehouse group conversion when performing the input.\n\n\n  \n    (count)\n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      10000\n    \n  \n\n\nLOOKUPNODE_COMMAND_RETRY_MAX_COUNT\nNumber of retry when command and connection to Lookup node fails\n\n\n  \n    (count)\n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      3600\n    \n    \n      Default\n      30\n    \n  \n\n\nSTAGE_RESULT_BLOCK_SIZE\nMaximum block size created in one stage.\n\n\n  \n    (size)\n    Value\n  \n  \n    \n      Minimum\n      1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      1024 * 1024"
					}
					
				
		
				
					,
					
					"config-monitor-property-html": {
						"id": "config-monitor-property-html",
						"title": "Property",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/property.html",
						"content": "Index\n\n\n  CPU_AFFINITY_BEGIN_ID\n  CPU_AFFINITY_COUNT\n  CPU_COUNT\n  CPU_PARALLEL\n  DBS_PATH\n  DEFAULT_LSM_MAX_LEVEL\n  DISK_BUFFER_COUNT\n  DISK_COLUMNAR_INDEX_CHECKPOINT_INTERVAL_SEC\n  DISK_COLUMNAR_INDEX_FDCACHE_COUNT\n  DISK_COLUMNAR_INDEX_SHUTDOWN_BUILD_FINISH\n  DISK_COLUMNAR_PAGE_CACHE_MAX_SIZE\n  DISK_COLUMNAR_TABLE_CHECKPOINT_INTERVAL_SEC\n  DISK_COLUMNAR_TABLE_COLUMN_FDCACHE_COUNT\n  DISK_COLUMNAR_TABLE_COLUMN_MINMAX_CACHE_SIZE\n  DISK_COLUMNAR_TABLE_COLUMN_PART_FLUSH_MODE\n  DISK_COLUMNAR_TABLE_COLUMN_PART_IO_INTERVAL_MIN_SEC\n  DISK_COLUMNAR_TABLE_COLUMN_PARTITION_PRECREATE_COUNT\n  DISK_COLUMNAR_TABLE_TIME_INVERSION_MODE\n  DISK_COLUMNAR_TABLESPACE_DWFILE_EXT_SIZE\n  DISK_COLUMNAR_TABLESPACE_DWFILE_INT_SIZE\n  DISK_COLUMNAR_TABLESPACE_MEMORY_EXT_SIZE\n  DISK_COLUMNAR_TABLESPACE_MEMORY_MAX_SIZE\n  DISK_COLUMNAR_TABLESPACE_MEMORY_MIN_SIZE\n  DISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_HIGH_LIMIT_PCT\n  DISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_MSEC\n  DISK_IO_THREAD_COUNT\n  DISK_TABLESPACE_DIRECT_IO_FSYNC\n  DISK_TABLESPACE_DIRECT_IO_READ\n  DISK_TABLESPACE_DIRECT_IO_WRITE\n  DISK_TAG_AUTO_RECLAIM\n  DUMP_APPEND_ERROR\n  DUMP_TRACE_INFO\n  DURATION_BEGIN\n  DURATION_GAP\n  FEEDBACK_APPEND_ERROR\n  GRANT_REMOTE_ACCESS\n  HTTP_THREAD_COUNT\n  INDEX_BUILD_MAX_ROW_COUNT_PER_THREAD\n  INDEX_BUILD_THREAD_COUNT\n  INDEX_FLUSH_MAX_REQUEST_COUNT_PER_INDEX\n  INDEX_LEVEL_PARTITION_AGER_THREAD_COUNT\n  INDEX_LEVEL_PARTITION_BUILD_MEMORY_HIGH_LIMIT_PCT\n  INDEX_LEVEL_PARTITION_BUILD_THREAD_COUNT\n  LOOKUP_APPEND_UPDATE_ON_DUPKEY\n  MAX_QPX_MEM\n  MEMORY_ROW_TEMP_TABLE_PAGESIZE\n  PID_PATH\n  PORT_NO\n  PROCESS_MAX_SIZE\n  QUERY_PARALLEL_FACTOR\n  ROLLUP_FETCH_COUNT_LIMIT\n  RS_CACHE_APPROXIMATE_RESULT_ENABLE\n  RS_CACHE_ENABLE\n  RS_CACHE_MAX_MEMORY_PER_QUERY\n  RS_CACHE_MAX_MEMORY_SIZE\n  RS_CACHE_MAX_RECORD_PER_QUERY\n  RS_CACHE_TIME_BOUND_MSEC\n  SHOW_HIDDEN_COLS\n  TABLE_SCAN_DIRECTION\n  TAGDATA_AUTO_META_INSERT\n  TAG_TABLE_META_MAX_SIZE\n  TAG_PARTITION_COUNT\n  TAG_DATA_PART_SIZE\n  TRACE_LOGFILE_COUNT\n  TRACE_LOGFILE_PATH\n  TRACE_LOGFILE_SIZE\n  UNIX_PATH\n  VOLATILE_TABLESPACE_MEMORY_MAX_SIZE\n\n\nThe properties are the settings used by the Machbase server and stored as key-value pairs in the $MACHBASE_HOME/conf/machbase.conf file. \nThese values ​​are set when the Machbase server starts and are used continuously during runtime. To change this value for performance tuning, you must understand the meaning of these values ​​and set them carefully.\n\nCPU_AFFINITY_BEGIN_ID\n\nThis is the start number of the CPU used by the Machbase server. It is used to control the CPU usage of the Machbase server.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2 ^ 32 - 1\n    \n    \n      Default\n      0\n    \n  \n\n\nCPU_AFFINITY_COUNT\n\nThis is the number of CPUs that the Machbase server will use. If set to 0, the Machbase server uses all CPUs.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2 ^ 32 - 1\n    \n    \n      Default\n      0\n    \n  \n\n\nCPU_COUNT\n\nSpecifies the number of CPUs set in the system. Based on this value, the Machbase Thread determines the number. If set to 0, all CPUs in the system are used.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0(auto detect the physically installed count of CPU on  the system)\n    \n    \n      Maximum\n      2 ^ 32 - 1\n    \n    \n      Default\n      0\n    \n  \n\n\nCPU_PARALLEL\n\nSpecifies the number of threads to spawn per CPU. If this value is 2 and the number of CPUs is 2, then two parallel threads are created per CPU, so the number of parallel processing threads is four. If this value is too large, memory can be consumed quickly.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2 ^ 32 - 1\n    \n    \n      Default\n      1\n    \n  \n\n\nDBS_PATH\n\nSpecifies the path where the basic data of the Machbase server will be stored. The default is “? Dbs”,  which means $MACHBASE_HOME/dbs.\n\n\n  \n     \n    Value\n  \n  \n    \n      Default\n      ?/dbs\n    \n  \n\n\nDEFAULT_LSM_MAX_LEVEL\n\nSets the base level of the LSM index. If you do not enter a MAX_LEVEL value when creating an index, this value applies.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      3\n    \n    \n      Default\n      2\n    \n  \n\n\nDISK_BUFFER_COUNT\n\nSpecifies the number of buffers for disk I/O.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      4G (4 * 1024 * 1024 * 1024)\n    \n    \n      Default\n      16\n    \n  \n\n\nDISK_COLUMNAR_INDEX_CHECKPOINT_INTERVAL_SEC\n\nSets the checkpoint interval for the index. If set too long, errors may occur during index creation.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1 (sec)\n    \n    \n      Maximum\n      2^32 -1 (sec)\n    \n    \n      Default\n      120 (sec)\n    \n  \n\n\nDISK_COLUMNAR_INDEX_FDCACHE_COUNT\n\nSpecifies the number of opened index partition file descriptors.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2 ^ 32 - 1\n    \n    \n      Default\n      0\n    \n  \n\n\nDISK_COLUMNAR_INDEX_SHUTDOWN_BUILD_FINISH\n\nSets whether or not to reflect index information on the disk when the Machbase server is shutdown. If this value is set to ‘1’, all index information is reflected on the disk and ends, so waiting times may be long.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (false)\n    \n    \n      Maximum\n      1 (True)\n    \n    \n      Default\n      0 (False)\n    \n  \n\n\nDISK_COLUMNAR_PAGE_CACHE_MAX_SIZE\n\nSets the maximum size of the page cache.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      2 * 1024 * 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLE_CHECKPOINT_INTERVAL_SEC\n\nSets checkpoint period of table data. If this value is too large, the recovery time will be longer at restart. If this value is too small, I/O will frequently occur and the overall performance may be degraded.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1 (sec)\n    \n    \n      Maximum\n      2 ^ 32 - 1 (sec)\n    \n    \n      Default\n      120 (sec)\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_FDCACHE_COUNT\n\nSpecifies the maximum number of open file descriptors for column data in the table.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2 ^ 32 - 1\n    \n    \n      Default\n      0\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_MINMAX_CACHE_SIZE\nSets the size of the default MINMAX cache set in the _ARRIVAL_TIME column.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2 ^ 64 - 1\n    \n    \n      Default\n      100 *1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_PART_FLUSH_MODE\n\nSets the automatic flush interval for column partition files.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (sec)\n    \n    \n      Maximum\n      2^32-1 (sec)\n    \n    \n      Default\n      60 (sec)\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_PART_IO_INTERVAL_MIN_SEC\nSets the frequency with which the partition file is reflected on the disk. When more data is input than the number of partitions set, it is reflected on the disk regardless of this period.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (sec)\n    \n    \n      Maximum\n      2^32-1 (sec)\n    \n    \n      Default\n      3 (sec)\n    \n  \n\n\nDISK_COLUMNAR_TABLE_COLUMN_PARTITION_PRECREATE_COUNT\n\nDefines the number of pre-generated column partition objects to be used for the table.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      3\n    \n  \n\n\nDISK_COLUMNAR_TABLE_TIME_INVERSION_MODE\n\nIf set to 1, the input is allowed even if the value of the _ARRIVAL_TIME column is reduced. If it is 0, a value smaller than the Maximum of the _ARRIVAL_TIME column value is entered as an error.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (False)\n    \n    \n      Maximum\n      1 (True)\n    \n    \n      Default\n      1 (True)\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_DWFILE_EXT_SIZE\n\nSpecifies the size at which the double write file used for recovery at startup increases at one time.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024 * 1024\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_DWFILE_INT_SIZE\nSpecifies the amount of space secured by the double write file when the file is created.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024 * 1024\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      2 * 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_EXT_SIZE\n\nSpecifies the block size of the memory to reserve for the column partition.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024 * 1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      2 * 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_MAX_SIZE\n\nSpecifies the maximum amount of memory allocated by the log table. If the server allocates more than this amount of memory, the memory allocation will wait until the memory usage drops below this value. It is recommended to set this value to 50 ~ 80% of physical memory.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      256 * 1024 * 1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      8 * 1024 * 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_MIN_SIZE\n\nWhen the Machbase server starts, it pre-allocates memory by this value to prevent performance degradation due to memory allocation. Since this memory is used only as a data input buffer, it is recommended to use it only when memory is sufficient.\n\nTable 24. Range of values\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024 * 1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      100 * 1024 * 1024\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_HIGH_LIMIT_PCT\n\nLimits the performance when the memory usage exceeds the set value when data is input to the log table.\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_MAX_SIZE * (DISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_HIGH_LIMIT_PCT / 100)\n\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      100\n    \n    \n      Default\n      80\n    \n  \n\n\nDISK_COLUMNAR_TABLESPACE_MEMORY_SLOWDOWN_MSEC\n\nSets the next wait time for each record entry if the memory usage for the column data file exceeds the criterion.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (msec)\n    \n    \n      Maximum\n      2^32 - 1 (msec)\n    \n    \n      Default\n      1 (msec)\n    \n  \n\n\nDISK_IO_THREAD_COUNT\n\nSets the number of I/O threads that write data to disk.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      3\n    \n  \n\n\nDISK_TABLESPACE_DIRECT_IO_FSYNC\n\nWhen running Direct I/O, fsync is unnecessary for data files. Disable fsync when using Direct I/O to improve data I/O performance (Set to 0). \nAlthough fsync is unncessary, fsync must be set to perform in case of failure situations such as a power outage because in a normal situation there is no data loss,\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nDISK_TABLESPACE_DIRECT_IO_READ\n\nSets whether to use DIRECT I/O for data read operation.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nDISK_TABLESPACE_DIRECT_IO_WRITE\n\nSets whether to use DIRECT I/O for data write operation.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      1\n    \n  \n\n\nDISK_TAG_AUTO_RECLAIM\n\nDetermines whether to automatically secure unused space for TAG data.\n\nIf it is 1, which is the default value, the automatic space securing function works, and if it is 0, it does not work, and the user must directly execute the function using the ALTER TABLE statement.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      1\n    \n  \n\n\nDUMP_APPEND_ERROR\nIf this value is set to 1, the $MACHBASE_HOME/trc/machbase.trc file will record the error if the Append API fails.\nIn this situation, the append performance is very low, so it is recommended to use for testing purposes only.\n\nIf you want to check for errors in the user application,  it is helpful to use the SQLAppendSetErrorCallback API.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nDUMP_TRACE_INFO\n\nThe server periodically records the DBMS system status information in the machbase.trc file at regular intervals, and sets this period. \nIf it is set to 0, it is not recorded.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (sec)\n    \n    \n      Maximum\n      2^32 - 1 (sec)\n    \n    \n      Default\n      60 (sec)\n    \n  \n\n\nDURATION_BEGIN\n\nSets the start time of the duration value that sets the default for the SELECT statements that do not specify the DURATION clause.\nIf set to 60, data will be retrieved 60 seconds before the current time.\n\nThe default is 0 to retrieve all data.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      0\n    \n  \n\n\nDURATION_GAP\nSets the start time of the duration value that sets the default for the SELECT statements that do not specify the DURATION clause.\n\n\n  If set to 60, data will be retrieved for 60 seconds from the current time.\n  If the DURATION_BEGIN value is 60, the data is retrieved from 60 seconds before to 60 seconds from the current time.\n\n\nThe default is 0 to retrieve all data.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      Non-zero\n    \n    \n      Default\n      0\n    \n  \n\n\nFEEDBACK_APPEND_ERROR\n\nSets whether to send error data to the client when an Append API error occurs. If 0, no error data is sent to the client. If it is 1, error information is sent to the client.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      1\n    \n  \n\n\nGRANT_REMOTE_ACCESS\n\nDetermines whether the database can be accessed remotely. If 0, the remote connection is blocked.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (False)\n    \n    \n      Maximum\n      1 (True)\n    \n    \n      Default\n      1 (True)\n    \n  \n\n\nHTTP_THREAD_COUNT\n\nSet the number of threads to be used by the Machbase web server.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      0\n    \n    \n      최대값\n      1024\n    \n    \n      기본값\n      32\n    \n  \n\n\nINDEX_BUILD_MAX_ROW_COUNT_PER_THREAD\nIf the number of records not indexed is greater than this value, the index build thread begins to add indexes.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      100000\n    \n  \n\n\nINDEX_BUILD_THREAD_COUNT\nSpecifies the number of index creation threads. If set to 0, no index is created.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      3\n    \n  \n\n\nINDEX_FLUSH_MAX_REQUEST_COUNT_PER_INDEX\nSpecifies the maximum number of flush requests per index.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      3\n    \n  \n\n\nINDEX_LEVEL_PARTITION_AGER_THREAD_COUNT\nSpecifies the number of threads to delete index files that are not needed when creating LSM indexes.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1024\n    \n    \n      Default\n      1\n    \n  \n\n\nINDEX_LEVEL_PARTITION_BUILD_MEMORY_HIGH_LIMIT_PCT\nSets the maximum memory usage for LSM index creation as a percent. This percent is set based on the maximum memory usage used by Machbase. If the memory usage exceeds the limit, the LSM partition merge is stopped.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      100\n    \n    \n      Default\n      70\n    \n  \n\n\nINDEX_LEVEL_PARTITION_BUILD_THREAD_COUNT\nDetermines the number of threads performing the merge operation for the creation of the LSM index.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1024\n    \n    \n      Default\n      3\n    \n  \n\n\nLOOKUP_APPEND_UPDATE_ON_DUPKEY\nWhen appending to the lookup table, it specifies how to handle duplicate primary keys.\n\n\n  0 : Append fail\n  1 : Update Row for the corresponding Primary Key.\n\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nMAX_QPX_MEM\nSets the maximum amount of memory used by the query processor to perform the GROUP BY, DISTINCT, and ORDER BY clauses. \nIf one query uses memory with a larger value, the query is canceled. At this time, an error message is sent to the client, and the relevant content is recorded in the machbase.trc file.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024 * 1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      500 * 1024 * 1024\n    \n  \n\n\nMEMORY_ROW_TEMP_TABLE_PAGESIZE\nSets the page size of the temporary tablespace for volatile tables and lookup tables. Because this page stores volatile tables and lookup table records, it should be larger than the maximum record size for volatile tables.\nIf you want to enter N records into the page, you should set this value to the maximum record size * N.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      8 * 1024\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      32 * 1024\n    \n  \n\n\nPID_PATH\nSpecifies the location where the PID file of the Machbase server process is to be written. The default is “?/Conf”, which means $MACHBASE_HOME/conf.\n\n\n  \n     \n    Value\n  \n  \n    \n      Default\n      ?/conf\n    \n  \n\n\n\n  \n    PID_PATH Value\n    PID File Location Path\n  \n  \n    \n      Not Specified\n      $MACHBASE_HOME/conf/machbase.pid\n    \n    \n      ?/test\n      $MACHBASE_HOME/test/machbase.pid\n    \n    \n      /tmp\n      /tmp/machbase.pid\n    \n  \n\n\nPORT_NO\nSpecifies the TCP/IP port for the Machbase server process to communicate with the client. The Default is 5656.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024\n    \n    \n      Maximum\n      65535\n    \n    \n      Default\n      5656\n    \n  \n\n\nPROCESS_MAX_SIZE\nSpecifies the maximum memory size used by machbased programs that are Machbase server processes. If you try to use more memory than the set limit, the server operates as follows to reduce the memory usage.\n\n\n  Stops data insert or treats it as an error\n  Decreased index creation speed\n\n\nIn this case, the performance is greatly degraded, so the cause of overuse of the memory must be found and solved.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024 * 1024 * 1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      8 * 1024 * 1024 * 1024\n    \n  \n\n\nQUERY_PARALLEL_FACTOR\nSpecifies the number of execution threads of the parallel query executor.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      100\n    \n    \n      Default\n      8\n    \n  \n\n\nROLLUP_FETCH_COUNT_LIMIT\nLimits the amount of data the rollup thread can fetch at one time.\n\nIf set to 0, there is no limit.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^32 - 1\n    \n    \n      Default\n      3000000\n    \n  \n\n\nRS_CACHE_APPROXIMATE_RESULT_ENABLE\nDetermines whether to use the approximate result mode of the result cache. If this value is 1, the speculative value is obtained (very fast but the data may be inaccurate) when using the result cache, and if it is 0, the correct value is obtained.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (false)\n    \n    \n      Maximum\n      1 (True)\n    \n    \n      Default\n      0 (False)\n    \n  \n\n\nRS_CACHE_ENABLE\nDetermines whether to use the result cache.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0 (false)\n    \n    \n      Maximum\n      1 (True)\n    \n    \n      Default\n      1 (True)\n    \n  \n\n\nRS_CACHE_MAX_MEMORY_PER_QUERY\nSets the amount of memory the result cache will use. If the memory usage of a particular query result exceeds this value, the result of the query is not stored in the result cache.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      16 * 1024 * 1024\n    \n  \n\n\nRS_CACHE_MAX_MEMORY_SIZE\nSpecifies the maximum memory usage of the result cache.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      32 * 1024\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      512 * 1024 * 1024\n    \n  \n\n\nRS_CACHE_MAX_RECORD_PER_QUERY\nThe maximum number of records to be stored in the result cache. If the number of records resulting from the query is greater than this value, the query result is not stored in the cache.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^64 - 1\n    \n    \n      Default\n      10000\n    \n  \n\n\nRS_CACHE_TIME_BOUND_MSEC\nIf a particular query is executed very quickly, it is better not to store it in the result cache because it can reduce memory usage.\nThis value determines how fast the query executed should not be stored in the cache. When set to 0, all query results are stored in the result cache.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1 (msec)\n    \n    \n      Maximum\n      2^64 - 1 (msec)\n    \n    \n      Default\n      1000 (msec)\n    \n  \n\n\nSHOW_HIDDEN_COLS\nIf set to the Default of 0, the _ARRIVAL_TIME column is not displayed by the SELECT * FROM query. If this value is set to 1, the corresponding column is displayed.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nTABLE_SCAN_DIRECTION\nYou can set the scan direction of the tag table. The property value is one of -1, 0, and 1, and the default value is 0.\n\n\n  -1 : Reverse scan\n  0  : Tag Table(Forward scan), Log Table(Reverse scan)\n  1  : Forward scan\n\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      -1\n    \n    \n      Maximum\n      1\n    \n    \n      Default\n      0\n    \n  \n\n\nTAGDATA_AUTO_META_INSERT\n\n    In 5.5 version, this property name was TAGDATA_AUTO_NAME_INSERT and supports only 0 or 1. \n     Below 5.7 version, default value is 1.\n\n\nWhen entering data through APPEND / INSERT into the TAGDATA table, specify how to handle it if there is no matching TAG_NAME.\n\n\n  0: Input fails.\n  1: Input TAG_NAME value to input. If there are additional metadata columns, the values of all columns are entered as NULL.\n  2: Enter the additional metadata column value along with the TAG_NAME value you want to enter.\n    \n      This setting is valid only in APPEND. INSERT works like 1 because you cannot enter additional metadata column values.\n      After this setting, the APPEND parameter must include the metadata column value in APPEND.\n    \n  \n\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2\n    \n    \n      Default\n      1\n    \n  \n\n\nTAG_TABLE_META_MAX_SIZE\nWhen creating the TAGDATA table, set the maximum size of memory to store the metadata area.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1024*1024\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      100*1024*1024\n    \n  \n\n\nTAG_PARTITION_COUNT\n\nSpecify the number of Key Value tables that consist the tag table.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1\n    \n    \n      최대값\n      4\n    \n    \n      기본값\n      1024\n    \n  \n\n\nTAG_DATA_PART_SIZE\n\nDetermines the partition size in tag data storage.\n\n\n  \n    \n       \n      Value\n    \n  \n  \n    \n      최소값\n      1048576 (1MB)\n    \n    \n      최대값\n      1073741824 (1GB)\n    \n    \n      기본값\n      16777216 (16MB)\n    \n  \n\n\nTRACE_LOGFILE_COUNT\nSpecifies the maximum number of log trace files generated in TRACE_LOGFILE_PATH. To save disk space, delete the oldest log file if more than the maximum number of log files are created.\n\nIf more than the maximum number of log trace files is created and the oldest file is deleted, the name of the deleted file is saved as the newest log file.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      1\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      5\n    \n  \n\n\nTRACE_LOGFILE_PATH\nSet the path of the log trace files (machbase.trc, machadmin.trc, machsql.trc). \nThese files continuously record internal information at the start, end, and run of Machbase. The default ?/trc  means $MACHBASE_HOME/trc.\n\n\n  \n     \n    Value\n  \n  \n    \n      Default\n      ?/conf\n    \n  \n\n\n\n  \n    TRACE_LOGFILE_PATH \n    trc direction location\n  \n  \n    \n      Not Specified\n      $MACHBASE_HOME/trc/\n    \n    \n      ?/test\n      $MACHBASE_HOME/test/\n    \n    \n      /tmp\n      /tmp/\n    \n  \n\n\nTRACE_LOGFILE_SIZE\nSets the maximum size of the log trace file. If it is necessary to record more data than the size, a new log file is created.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      10 * 1024 * 1024\n    \n    \n      Maximum\n      2^32-1\n    \n    \n      Default\n      10 * 1024 * 1024\n    \n  \n\n\nUNIX_PATH\nSets the path to the Unix domain socket file. The Default when not set by user is ?/conf/machbase-unix.\n\n\n  \n     \n    Value\n  \n  \n    \n      Default\n      ?/conf/machbase-unix\n    \n  \n\n\nVOLATILE_TABLESPACE_MEMORY_MAX_SIZE\nSets the total amount of memory usage for all volatile and lookup tables in the system.\n\n\n  \n     \n    Value\n  \n  \n    \n      Minimum\n      0\n    \n    \n      Maximum\n      2^64-1\n    \n    \n      Default\n      2 * 1024 * 1024 * 1024"
					}
					
				
		
				
					,
					
					"sdk-python-html": {
						"id": "sdk-python-html",
						"title": "Python",
						"version": "all",
						"categories": "",
						"url": " /sdk/python.html",
						"content": "Index\n\n\n  Python Module Usage Overview\n  Preferences\n  Creating Class\n  Connection and Disconnection\n    \n      machbase.open(aHost, aUser, aPw, aPort)\n      machbase.close()\n      machbase.isConnected()\n    \n  \n  Executing Commands and User Convenience Functions\n    \n      machbase.execute(aSql)\n      machbase.append(aTableName, aTypes, aValues, aFormat)\n      machbase.tables()\n      machbase.columns(aTableName)\n    \n  \n  Checking Results\n    \n      machbase.result()\n    \n  \n  Examples\n    \n      Connect\n      Simple\n      Append\n    \n  \n\n\nPython Module Usage Overview\n\nMachbase supports Python modules. By installing the module, it provides a class that can exchange values ​​with the Machbase server and the CLI method. You can use this to easily enter and delete values ​​in the query base in Python, create and delete tables, and so on.\n\nPreferences\n\nLinux Platform\n\nSimple configuration and library files are required to use this. First, make sure that $MAC_LIBRARY_PATH is registered in the $MACHBASE_HOME/lib directory. The libmachbasecli_dll.so file must exist in the library folder because you are accessing Machbase using the CLI. Then unzip machbaseAPI-1.0.tar.gz in the $MACHBASE_HOME/3rd-party/python3-module folder and install the module into the Python you want to use via the python setup.py install command.\n\nWindows Platform\n\nFirst, unzip machbaseAPI-1.0.zip in the %MACHBASE_HOME%/3rd-party/python3-module directory, and execute “python setup.py install” command to install the module into the Python.\n\nCreating Class\nTo use the Machbase Python module, you need to declare the class.\n\nThe class name is machbase.\n\nfrom machbaseAPI import machbase\n\n\nConnection and Disconnection\n\nmachbase.open(aHost, aUser, aPw, aPort)\n\nThis is a function to connect to Machbase. When the appropriate parameter value is input, it returns whether connection to the DB is successful or failed. Returns 1 on success and 0 on failure.\n\nmachbase.close()\n\nThis is a function to close the Machbase connection. Returns 1 on success and 0 on failure.\n\nmachbase.isConnected()\n\nThis is a function that determines whether the declared class is connected to the server. Returns 1 when connected and 0 when not connected.\n\nExecuting Commands and User Convenience Functions\n\nmachbase.execute(aSql)\n\nThis is a command to send a query to the server when it is connected to the server. It returns 1 when it is normally executed, and 0 when it fails or an error occurs.\n\nYou can use any command except UPDATE which is not supported by Machbase.\n\nmachbase.append(aTableName, aTypes, aValues, aFormat)\nThis is a function that can use Append protocol supported by Machbase.\n\nAppend can be executed by inputting the table name, the dictionary of the type of each column, and the values ​​to input in JSON format and specifying dateformat.\nReturns 1 if executed normally, 0 otherwise.\n\n\n  \n    \n      Type Name\n      Value\n    \n  \n  \n    \n      short\n      4\n    \n    \n      ushort\n      104\n    \n    \n      integer\n      8\n    \n    \n      uinteger\n      108\n    \n    \n      long\n      12\n    \n    \n      ulong\n      112\n    \n    \n      float\n      16\n    \n    \n      double\n      20\n    \n    \n      datetime\n      6\n    \n    \n      varchar\n      5\n    \n    \n      ipv4\n      32\n    \n    \n      ipv6\n      36\n    \n    \n      text\n      49\n    \n    \n      binary\n      97\n    \n  \n\n\nmachbase.tables()\n\nReturns information about all tables in the connected server. Returns 1 if successful; 0 if unsuccessful.\n\nmachbase.columns(aTableName)\n\nReturns information about the columns in the corresponding table on the connected server. Returns 1 if successful; 0 if unsuccessful.\n\nChecking Results\n\nIn the Machbase Python module, all result values ​​are returned in JSON.\n\nIt is adopted to return the result in a form that is easy to use in various environments.\n\nmachbase.result()\n\nThe functions described above do not represent the execution results as the return value of the function, but only return success or failure. The result of a function can be obtained only by the return value of this function.\n\nExamples\n\nLet’s see how to use the Machbase Python module with simple examples.\n\nYou can check using $MACHBASE_HOME/sample/python files. The directory has a Makefile that makes it easy to test and a MakeData.py file that creates the data. Make sure that the value of PYPATH in the Makefile is set to Python with the Machbase Python module installed. The default is specified in Python installed in the Machbase package. Also, you need to make init.py file to execute the module in Python independently, so make sure that the file exists in that directory.\n\n[mach@localhost]$ cd $MACHBASE_HOME/sample/python\n[mach@localhost python]$ ls -l\ntotal 20\n-rw-rw-r-- 1 mach mach    0 Oct  7 14:37 __init__.py\n-rw-rw-r-- 1 mach mach  764 Oct  7 14:37 MakeData.py\n-rw-rw-r-- 1 mach mach  593 Oct  7 14:58 Makefile\n-rw-rw-r-- 1 mach mach  664 Oct  7 14:37 Sample1Connect.py\n-rw-rw-r-- 1 mach mach 2401 Oct  7 14:37 Sample2Simple.py\n-rw-rw-r-- 1 mach mach 1997 Oct  7 14:37 Sample3Append.py\n\n\nConnect\n\nThe following example is a simple function that connects to the server, executes the query, and returns the result. If each function returns a failure value (0), it returns an error result. If successful, the number of values ​​in the m $ tables table is returned.\n\nThe file name is Sample1Connect.py.\n\nfrom machbaseAPI import machbase\ndef connect():\n    db = machbase()\n    if db.open('127.0.0.1','SYS','MANAGER',5656) is 0 :\n        return db.result()\n    if db.execute('select count(*) from m$tables') is 0 :\n        return db.result()\n    result = db.result()\n    if db.close() is 0 :\n        return db.result()\n    return result\nif __name__==\"__main__\":\n    print connect()\n\n\n[mach@localhost python]$ make run_sample1\n/home/machbase/machbase_home/webadmin/flask/Python/bin/python Sample1Connect.py\n{\"count(*)\":\"13\"}\n\n\nSimple\n\nUsing the example below, we simply create a table using Python in Machbase, input the value, and extract the input value to check. The file name is Sample2Simple.py.\n\nimport re\nimport json\nfrom machbaseAPI import machbase\ndef insert():\n    db = machbase()\n    if db.open('127.0.0.1','SYS','MANAGER',5656) is 0 :\n        return db.result()\n    db.execute('drop table sample_table')\n    db.result()\n    if db.execute('create table sample_table(d1 short, d2 integer, d3 long, f1 float, f2 double, name varchar(20), text text, bin binary, v4 ipv4, v6 ipv6, dt datetime)') is 0:\n        return db.result()\n    db.result()\n    for i in range(1,10):\n        sql = \"INSERT INTO SAMPLE_TABLE VALUES (\"\n        sql += str((i - 5) * 6552) #short\n        sql += \",\"+ str((i - 5) * 42949672) #integer\n        sql += \",\"+ str((i - 5) * 92233720368547758L) #long\n        sql += \",\"+ \"1.234\"+str((i-5)*7) #float\n        sql += \",\"+ \"1.234\"+str((i-5)*61) #double\n        sql += \",'id-\"+str(i)+\"'\" #varchar\n        sql += \",'name-\"+str(i)+\"'\" #text\n        sql += \",'aabbccddeeff'\" #binary\n        sql += \",'192.168.0.\"+str(i)+\"'\" #ipv4\n        sql += \",'::192.168.0.\"+str(i)+\"'\" #ipv6\n        sql += \",TO_DATE('2015-08-0\"+str(i)+\"','YYYY-MM-DD')\" #date\n        sql += \")\";\n        if db.execute(sql) is 0 :\n            return db.result()\n        else:\n            print db.result()\n        print str(i)+\" record inserted.\"\n    query = \"SELECT d1, d2, d3, f1, f2, name, text, bin, to_hex(bin), v4, v6, to_char(dt,'YYYY-MM-DD') as dt from SAMPLE_TABLE\";\n    if db.execute(query) is 0 :\n        return db.result()\n    result = db.result()\n    for item in re.findall('{[^}]+}',result):\n        res = json.loads(item)\n        print \"d1 : \"+res.get('d1')\n        print \"d2 : \"+res.get('d2')\n        print \"d3 : \"+res.get('d3')\n        print \"f1 : \"+res.get('f1')\n        print \"f2 : \"+res.get('f2')\n        print \"name : \"+res.get('name')\n        print \"text : \"+res.get('text')\n        print \"bin : \"+res.get('bin')\n        print \"to_hex(bin) : \"+res.get('to_hex(bin)')\n        print \"v4 : \"+res.get('v4')\n        print \"v6 : \"+res.get('v6')\n        print \"dt : \"+res.get('dt')\n    if db.close() is 0 :\n        return db.result()\n    return result\nif __name__==\"__main__\":\n    print insert()\n\n\n[mach@loclahost python]$ make run_sample2\n/home/machbase/machbase_home/webadmin/flask/Python/bin/python Sample2Simple.py\n{\"EXECUTE RESULT\":\"Execute Success\"}\n1 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n2 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n3 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n4 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n5 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n6 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n7 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n8 record inserted.\n{\"EXECUTE RESULT\":\"Execute Success\"}\n9 record inserted.\nd1 : 26208\nd2 : 171798688\nd3 : 368934881474191032\nf1 : 1.23428\nf2 : 1.23424\nname : id-9\ntext : name-9\nbin : 616162626363646465656666\nto_hex(bin) : 616162626363646465656666\nv4 : 192.168.0.9\nv6 : ::192.168.0.9\n...\n\n\nAppend\n\nAppend method to input data at high speed in Machbase can also be used by using Python module. The following example shows how to input data at high speed. We used a method of declaring a connection class db2 for the column information and initialization, and a connection class db2 for the Append, and using each function. The file name is Sample3Append.py.\n\nimport re\nimport json\nfrom machbaseAPI import machbase\ndef append():\n#init,columns start\n    db = machbase()\n    if db.open('127.0.0.1','SYS','MANAGER',5656) is 0 :\n        return db.result()\n    db.execute('drop table sample_table')\n    db.result()\n    if db.execute('create table sample_table(d1 short, d2 integer, d3 long, f1 float, f2 double, name varchar(20), text text, bin binary, v4 ipv4, v6 ipv6, dt datetime)') is 0:\n        return db.result()\n    db.result()\n    tableName = 'sample_table'\n    db.columns(tableName)\n    result = db.result()\n    if db.close() is 0 :\n        return db.result()\n#init, colums end\n#append start\n    db2 = machbase()\n    if db2.open('127.0.0.1','SYS','MANAGER',5656) is 0 :\n        return db2.result()\n    types = []\n    for item in re.findall('{[^}]+}',result):\n        types.append(json.loads(item).get('type'))\n    values = []\n    with open('data.txt','r') as f:\n        for line in f.readlines():\n            v = []\n            i = 0\n            for l in line[:-1].split(','):\n                t = int(types[i])\n                if t == 4 or t == 8 or t == 12 or t == 104 or t == 108 or t == 112:\n                    #short   integer    long       ushort      uinteger     ulong\n                    v.append(int(l))\n                elif t == 16 or t == 20:\n                    #float      double\n                    v.append(float(l))\n                else:\n                    v.append(l)\n                i+=1\n            values.append(v)\n    db2.append(tableName, types, values, 'YYYY-MM-DD HH24:MI:SS')\n    result = db2.result()\n    if db2.close() is 0 :\n        return db2.result()\n#append end\n    return result\nif __name__==\"__main__\":\n    print append()\n\n\n[mach@localhost python]$ make run_sample3\n/home/machbase/machbase_home/webadmin/flask/Python/bin/python Sample3Append.py\n{\"EXECUTE RESULT\":\"Append success\"}"
					}
					
				
		
				
					,
					
					"sdk-restful-html": {
						"id": "sdk-restful-html",
						"title": "RESTful API",
						"version": "all",
						"categories": "",
						"url": " /sdk/restful.html",
						"content": "Index\n\n\n  RESTful API Overview\n  Machbase RestAPI\n  RestAPI for Tag Table Usage\n    \n      Raw data processing function\n      Statistical data processing function\n      Tag metadata processing function\n      Other function\n    \n  \n\n\nRESTful API Overview\n\nRepresentational State Transfer (REST) is a type of software architecture style that consists of guidelines and best practices for interfaces provided by scalable Web services.\n\nThe four methods defined in the HTTP protocol define the CRUD for the resource.\n\n\n  \n    \n      HTTP Method\n      Meaning\n    \n  \n  \n    \n      POST\n      Create\n    \n    \n      GET\n      Select\n    \n    \n      PUT\n      Update\n    \n    \n      DELETE\n      Delete\n    \n  \n\n\nMachbase is not a standard RESTful API, but rather a RESTful API that handles CRUD using only POST and GET methods.\n\nThat is, the POST method is used for data input and the rest is transmitted as a GET Method parameter to the SQL query so that all the operations can be performed.\n\nMachbase RestAPI\n\nMachBase supports convenient and fast Rest API functions through the web server built into the server. \nMachbase Edition support embedded web server\nAll type of machbase editions are supported. (Standard / Cluster)\n\nLocation of version-specific .conf files\n\nStandard edition\n\n$MACHBASE_HOME/conf/machbase.conf $MACHBASE_HOME/http/conf/http.conf\n\nCluster edition\n\n$EACH_BROKER_HOME/conf/machbase.conf (Modify by Broker) $EACH_BROKER_HOME/http/conf/http.conf (modify all per Broker)\n\nAdded properties for embedded web server\nmachbase.conf (set as PROPERTY = VALUE)\n\n\n  \n    \n      Property\n      Description\n    \n  \n  \n    \n      HTTP_ENABLE\n      Whether to run the embedded web server 0: not driven, 1: driven\n    \n    \n      HTTP_PORT_NO\n      Embedded web server connection port number Port range: 0 ~ 65535 Default : 5657\n    \n    \n      HTTP_MAX_MEM\n      Maximum memory used by one Web Session Min: 1048576 (1MB) Default : 536870912 (512MB)\n    \n    \n      HTTP_AUTH\n      Whether to use Basic authentication when using the Embedded Web Server 0: Authentication not used, 1: Authentication enabled\n    \n  \n\n\nhttp.conf (set in JSON format)\n\n\n  \n    \n      Property\n      Description\n    \n  \n  \n    \n      document_root\n      html file location based on $MACHBASE_HOME Default : http/html ($MACHBASE_HOME/http/html)\n    \n    \n      max_request_size\n      Limit the maximum request byte size for one request\n    \n    \n      request_timeout_ms\n      Maximum response latency for one request (millisecond)\n    \n    \n      enable_auth_domain_check\n      Whether to enable domain authentication Set to “yes” or “no” value Default: “no”\n    \n    \n      reverse_proxy\n      change request url to specific url\n    \n  \n\n\nsample conf files\n\nmachbase.conf\n\n#################################################################################\n# Rest-API port\n#################################################################################\nHTTP_PORT_NO = 5657\n   \n#################################################################################\n# Maximum memory per web session.\n# Default Value: 536870912 (512MB)\n#################################################################################\nHTTP_MAX_MEM = 536870912\n   \n#################################################################################\n# Min Value:     0\n# Max Value:     1\n# Default Value: 0\n#\n# Enable REST-API service.\n#################################################################################\nHTTP_ENABLE = 0\n   \n#################################################################################\n# Min Value:     0\n# Max Value:     1\n# Default Value: 0\n#\n# Enable Basic Authentication for Rest-API service\n#################################################################################\nHTTP_AUTH = 0\n\n\nhttp.conf\n\n{\n    \"document_root\":\"http/html/\",\n    \"max_request_size\": \"100000\",\n    \"request_timeout_ms\": \"10000\",\n    \"enable_auth_domain_check\": \"no\",\n    \"reverse_proxy\" : [[\"/machbase/tables\", \"http://127.0.0.1:55657/machbase\"],\n        [\"/self_machbase_proxy\", \"http://127.0.0.1:55657/machbase\"],\n        [\"/dead_proxy\", \"http://127.0.0.0/machbase\"]]\n}\n\n\nDDL / DML / Append REST API Usage\n\nBasic request format\n  \nhttp://addr:port/machbase?q=query&amp;f=dateformat\n  \nResponse DDL / Append / DML (except Select)\n{\"error_code\":0, \"error_message\" :\"Message\", \"data\":[]}\n  \nResponse DML (Select)\n{\"error_code\":0, \"error_message\" :\"Message\", \"columns\":[Columns], \"data\":[Data]}\n\n\nDDL Sample\n\n## Request of creating a table\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=create table test_table (name varchar(20), time datetime, value double)'\n   \n## Normal response\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[]}\n   \n## Request of dropping a table\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=drop table test_table'\n   \n## Normal response\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[]}\n\n\nDML Sample\n\n## Request Log table data insert\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=insert into test_table values (\"test\", \"1999-01-01 00:00:00\", 0)'\n   \n## Response\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[]}\n   \n## Request Log table select\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from test_table'\n   \n## Response\n{\"error_code\":0, \"error_message\": \"\", \"columns\" : [{\"name\":\"NAME\", \"type\":5, \"length\":20},{\"name\":\"TIME\", \"type\":6, \"length\":8},{\"name\":\"VALUE\", \"type\":20, \"length\":8}],\"data\" :[{\"NAME\":\"test\", \"TIME\":\"1999-01-01 00:00:00 000:000:000\", \"VALUE\":0.000}]}\n\n\nAppend Sample\n\n## Append some data to log table\ncurl -X POST -H \"Content-Type: application/json\" \"http://127.0.0.1:5657/machbase\" -d '{\"name\":\"test_table\", \"date_format\":\"YYYY-MM-DD\",\"values\":[[\"test\", \"1999-01-01 00:00:01\", 1], [\"test\", \"1999-01-01 00:00:02\", 2], [\"test\", \"1999-01-01 00:00:03\", 3]]}'\n   \n## Response\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[], \"append_success\":3, \"append_failure\":0}\n\n\nIn the case of Binary Append, if binary data is encoded in Base64 and transmitted, the server will decode it and store it. When outputting, binary data is returned after being encoded in Base64.\nInput : Binary Data » Base64 Encoding » HTTP(POST) » Base64 Decoding » Append(BLOB Binary)\n\nOutput : BLOB Binary » Base64 Encoding » HTTP (GET) » Base64 Decoding » Save or View Binary\n\nBinary Append Sample\n\n## Example of sending binary data. data should be encoded by Base64.\n   \n## Request append to log table\ncurl  -X POST -H \"Content-Type: application/json\" \"http://127.0.0.1:5657/machbase\" -d '{\"name\":\"test_table\", \"date_format\":\"YYYY-MM-DD\",\"values\":[[\"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWprbG1ub3BxcnN0dXZ3eHl6e3x9fn+AgYKDhIWGh4iJiouMjY6PkJGSk5SVlpeYmZqbnJ2en6ChoqOkpaanqKmqq6ytrq+wsbKztLW2t7i5uru8vb6/wMHCw8TFxsfIycrLzM3Oz9DR0tPU1dbX2Nna29zd3t/g4eLj5OXm5+jp6uvs7e7v8PHy8/T19vf4+fr7/P3+/w==\"]]}'\n   \n## Result\n{\"error_code\":0, \"error_message\" :\"No Error\", \"data\":[], \"append_success\":1, \"append_failure\":0}\n   \n## Get data from log table\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from test_table';\n   \n## The Base64 encoded data are displaied\n{\"error_code\" :0, \"error_message\": \"No Error\", \"columns\" : [{\"name\":\"V1\", \"type\":57, \"length\":67108864}],\"data\" :[{\"V1\":\"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWprbG1ub3BxcnN0dXZ3eHl6e3x9fn+AgYKDhIWGh4iJiouMjY6PkJGSk5SVlpeYmZqbnJ2en6ChoqOkpaanqKmqq6ytrq+wsbKztLW2t7i5uru8vb6/wMHCw8TFxsfIycrLzM3Oz9DR0tPU1dbX2Nna29zd3t/g4eLj5OXm5+jp6uvs7e7v8PHy8/T19vf4+fr7/P3+/w==\"}]}\n   \n## Can check data using machsql\nselect to_hex(v1) from test_table;\nto_hex(v1)                                                                      \n------------------------------------------------------------------------------------\n000102030405060708090A0B0C0D0E0F101112131415161718191A1B1C1D1E1F2021222324252627\n28292A2B2C2D2E2F303132333435363738393A3B3C3D3E3F404142434445464748494A4B4C4D4E4F\n505152535455565758595A5B5C5D5E5F606162636465666768696A6B6C6D6E6F7071727374757677\n78797A7B7C7D7E7F808182838485868788898A8B8C8D8E8F909192939495969798999A9B9C9D9E9F\nA0A1A2A3A4A5A6A7A8A9AAABACADAEAFB0B1B2B3B4B5B6B7B8B9BABBBCBDBEBFC0C1C2C3C4C5C6C7\nC8C9CACBCCCDCECFD0D1D2D3D4D5D6D7D8D9DADBDCDDDEDFE0E1E2E3E4E5E6E7E8E9EAEBECEDEEEF\nF0F1F2F3F4F5F6F7F8F9FAFBFCFDFEFF                                                \n[1] row(s) selected.\n\n\nUsing HTTP Auth Property\n\nThis is an option to set authentication as a normal user by including the string ‘Authorization: Basic Base64String’ in the Request Header. Base64 string is written in ID@Host:Password structure. (However, the host name does not need to be correct. ID and password must be entered in Machbase user information.)\n\nHow to create a Basic Base64String for authorize\n\n## In case of ID: sys, Password: manager , creating a Base64String\necho -n \"sys@localhost:manager\" | base64\n   \n## Result\nc3lzQGxvY2FsaG9zdDptYW5hZ2Vy\n\n\nUsing Auth Sample (HTTP_AUTH = 1)\n\n## Request of result without authorization clause\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from v$stmt'\n   \n## Error occurred\n{\"error_code\":3118, \"error_message\" :\"There is No Authorization Header.\", \"data\":[]}\n   \n## Adding 'Authorization:Base64String' at the request header\ncurl -H \"Authorization: Basic c3lzQGxvY2FsaG9zdDptYW5hZ2Vy\"  -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from v$stmt'\n   \n## Normal response\n{\"error_code\":0, \"error_message\": \"No Error\", \"columns\" : [{\"name\":\"ID\", \"type\":8, \"length\":4},{\"name\":\"SESS_ID\", \"type\":8, \"length\":4},{\"name\":\"STATE\", \"type\":5, \"length\":64},{\"name\":\"RECORD_SIZE\", \"type\":8, \"length\":4},{\"name\":\"QUERY\", \"type\":5, \"length\":32767}],\"data\" :[{\"ID\":0, \"SESS_ID\":52, \"STATE\":\"Fetch prepared\", \"RECORD_SIZE\":0, \"QUERY\":\"select * from v$stmt\"}]}\n\n\nChanging floating point precision with s option\n\nSpecify how many decimal places of response data to output Set to a value from 0 to 9 (If it is not a range value, it operates as 3)\n\nSample (s=5)\n\n## display result 5 decimal places\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from test_table' --data-urlencode 's=5';\n   \n## Normal response\n{\"error_code\" :0, \"error_message\": \"\", \"columns\" : [{\"name\":\"C1\", \"type\":16, \"length\":4},{\"name\":\"C2\", \"type\":20, \"length\":8}],\"data\" :[{\"C1\":12345.00000, \"C2\":1234.01235}]}\n\n\nChanging data Fetch mode (m option)\n\nDefault Fetch mode Sample (m=0)\n\n## Request fetch mode (m=0)\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from tag limit 2' --data-urlencode 'm=0';\n   \n## Normal response (Conatains Column Name in result data)\n{\"error_code\" :0, \"error_message\": \"\", \"columns\" : [{\"name\":\"NAME\", \"type\":5, \"length\":20},{\"name\":\"TIME\", \"type\":6, \"length\":8},{\"name\":\"VALUE\", \"type\":20, \"length\":8}],\n\"data\" :[{\"NAME\":\"tag1\", \"TIME\":\"2001-09-09 10:46:40 000:000:000\", \"VALUE\":1000000000.000}, {\"NAME\":\"tag1\", \"TIME\":\"2001-09-09 10:46:41 000:000:000\", \"VALUE\":1000000001.000}]}\n\n\nAdvanced Fetch mode Sample (m=1)\n\n## Request fetch mode (m=1)\ncurl -G \"http://127.0.0.1:5657/machbase\" --data-urlencode 'q=select * from tag limit 2' --data-urlencode 'm=1';\n   \n## Normal response (Column Names are not included in results)\n{\"error_code\" :0, \"error_message\": \"\", \"columns\" : [{\"name\":\"NAME\", \"type\":5, \"length\":20},{\"name\":\"TIME\", \"type\":6, \"length\":8},{\"name\":\"VALUE\", \"type\":20, \"length\":8}],\n\"data\" :[[\"tag1\", \"2001-09-09 10:46:40 000:000:000\", 1000000000.000], [\"tag1\", \"2001-09-09 10:46:41 000:000:000\", 1000000001.000]]}\n\n\nHandling of NULL values\n\nWhen inserting or appending during DML processing, a NULL value can be entered as null as it is.\n\nJSON Sample for appending null values\n\n[[\"data1\", \"data2\", \"data3\"],[\"data11\", \"data12\", \"data13\"],[\"data21\", \"data22\", \"data23\"],[null,null,null]]\n\n\nSample result containing null values from SELECT\n\n[{\"C1\":null, \"C2\":null, \"C3\":null, \"C4\":null, \"C5\":null, \"C6\":null, \"C7\":null, \"C8\":null, \"C9\":null, \"C10\":null, \"C11\":null, \"C12\":null}]\n\n\nRestAPI for Tag Table Usage\n\nMachbase provides Historian-like RestAPI access to Tag Table.\n\nThe default request format is http://ipaddr:port/machiot/ or http://ipaddr:port/machiot-rest-api/\n\nIn addition, the following parameters can be handed over to the URL.\n\n\n  \n    \n      Parameter\n      Description\n      Sample\n    \n  \n  \n    \n      f or DateFormat\n      set date format\n      XXX?f=YYYY/MM/DD XXX?DateFormat=YYYY/MM/DD\n    \n    \n      m or FetchMode\n      set fetch mode\n      XXX?m=1 XXX?FetchMode=1\n    \n    \n      s or Scale\n      set scale\n      XXX?s=12 XXX?Scale=12\n    \n  \n\n\nRaw data processing function\n\nRaw Value Append API\n\nThis API is a function of appending a large amount of data into a given table.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/raw/{Table}\nhttp://ipaddr:port/machiot/v1/datapoints/raw/{Table}\n\n\n  HTTP method : POST\n  Table : Target tag table to be input\n\n\nUsage\n\nRequest\ncurl  -X POST -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot-rest-api/datapoints/raw/tag\"\n-d '{\"date_format\":\"YYYY-MM-DD HH24:MI:SS\",\n     \"values\":\n       [\n          [\"tag1\", \"1999-01-01 00:00:00\", 0],\n          [\"tag1\", \"1999-01-01 00:00:01\", 1],\n          [\"tag1\", \"1999-01-01 00:00:02\", 2]\n        ]\n     }';\nResponse\n{\n   \"error_code\":0,\n   \"error_message\" :\"No Error\",\n   \"timezone\":\"+0900\",\n   \"data\":[],\n   \"append_success\":3,\n   \"append_failure\":0\n}\n\n\nRaw Value Select API\n\nThis API is a function of obtaining data from a given table.\n\nIt supports the method of directly specifying all URLs, and also supports the method of passing them to the factor of the GET method.\n\nEach directory name may be designated as a factor in the URL below.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/raw/{Table}/{TagNames}/{Start}/{End}/{Direction}/{Count}/{Offset}\nhttp://ipaddr:port/machiot/v1/datapoints/raw/{Table}/{TagNames}/{Start}/{End}/{Direction}/{Count}/{Offset}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be selected\n  TagNames : Target tag name to be selected\n    \n      This tag name can be divided into a comma and multiple tag results can be obtained in one series.\n    \n  \n  Start : Indicates the starting time value of the data to be selected.\n  End :  Indicates the last time value of the data to be selected.\n    \n      Time format: Both space-free and space-support forms are supported as shown below. (If tested with curl, additional forms can be used)\n        \n          Basic form (space supported, up to nanoseconds)\n            \n              YYYY-MM-DD HH:MI:SS, millis\n              YYYY-MM-DD HH:MI:SS milliSec:microSec:nanoSec\n            \n          \n          Additional Form (space not supported, Use uppercase T instead of space and support up to milliseconds)\n            \n              YYYY-MM-DDTHH:MI:SS,millis\n            \n          \n        \n      \n      Example\n        \n          “2020-12-12”\n          “2020-12-12 03:22:22”\n          “2020-12-12 03:22:22 222:333:444”\n          “2020-12-12T03:22:22”\n          “2020-12-12T03:22:22,234”\n        \n      \n    \n  \n  Direction (Omitable)\n    \n      0 (Default): Output in the order entered\n      1 : Output in the direction of time decrease\n      2 : Output in the direction of time increase\n    \n  \n  Count (Omitable)\n    \n      0 (Default): Output all data\n      Else : Output a given number of records\n    \n  \n  Offset (Omitable)\n    \n      0 (Default) : Do not skip over\n      Else : Skip over a given value\n    \n  \n\n\nUsage\n\nRequest (Directly specifying all URL)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw/tag/tag-1/2001-09-09T00:00:00,000/2001-09-09T01:20:00,000/0/5/0\"\nRequest\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:00:01 000:000:000\",\n      \"VALUE\": 8001\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:01:41 000:000:000\",\n      \"VALUE\": 8101\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:03:21 000:000:000\",\n      \"VALUE\": 8201\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:05:01 000:000:000\",\n      \"VALUE\": 8301\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:06:41 000:000:000\",\n      \"VALUE\": 8401\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:08:21 000:000:000\",\n      \"VALUE\": 8501\n    }\n  ]\n}\n \nRequest (Passing factors)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw?Table=tag&amp;amp;TagNames=tag-1&amp;amp;Start=2001-09-09T00:00:00,000&amp;amp;End=2001-09-09T01:20:00,000&amp;amp;Direction=0&amp;amp;Count=5&amp;amp;Offset=0\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:00:01 000:000:000\",\n      \"VALUE\": 8001\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:01:41 000:000:000\",\n      \"VALUE\": 8101\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:03:21 000:000:000\",\n      \"VALUE\": 8201\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:05:01 000:000:000\",\n      \"VALUE\": 8301\n    },\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:06:41 000:000:000\",\n      \"VALUE\": 8401\n    }\n  ]\n}\n  \nRequest (When specifying multiple tags (tag-1, tag-2, tag-3))\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw/tag/tag-1,tag-2,tag-3/2001-09-09T00:00:00,000/2001-09-09T01:20:00,000/0/0/0\";\nResponse\n[\n  {\n    \"NAME\": \"tag-1\",\n    \"TIME\": \"2001-09-09 01:00:01 000:000:000\",\n    \"VALUE\": 8001\n  },\n  {\n    \"NAME\": \"tag-1\",\n    \"TIME\": \"2001-09-09 01:01:41 000:000:000\",\n    \"VALUE\": 8101\n  },\n  {\n    \"NAME\": \"tag-2\",\n    \"TIME\": \"2001-09-09 01:00:02 000:000:000\",\n    \"VALUE\": 8002\n  },\n  {\n    \"NAME\": \"tag-2\",\n    \"TIME\": \"2001-09-09 01:01:42 000:000:000\",\n    \"VALUE\": 8102\n  },\n  {\n    \"NAME\": \"tag-3\",\n    \"TIME\": \"2001-09-09 01:00:03 000:000:000\",\n    \"VALUE\": 8003\n  },\n  {\n    \"NAME\": \"tag-3\",\n    \"TIME\": \"2001-09-09 01:01:43 000:000:000\",\n    \"VALUE\": 8103\n  },\n  {\n    \"NAME\": \"tag-3\",\n    \"TIME\": \"2001-09-09 01:03:23 000:000:000\",\n    \"VALUE\": 8203\n  }\n]\n\n\nAll Tag-based Raw Value Delete API\n\nThis API deletes all data prior to a specific time for all entered tags.\n\nThis function can be usefully used to remove data that is no longer needed after disk capacity is insufficient or backup is completed.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/raw/{Table}/{BeforeTime}\nhttp://ipaddr:port/machiot/v1/datapoints/raw/{Table}/{BeforeTime}\n\n\n\n  HTTP method : DELETE\n  Table : Target tag table to be deleted\n  BeforeTime : Indicates time value which all data before it will deleted\n\n\nUsage\n\nRequest\ncurl -X DELETE  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw/tag/2001-09-09T01:20:00,000\";\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"No Error\",\n  \"timezone\": \"+0900\",\n  \"effect_rows\": \"1201\",\n  \"data\": []\n}\n\n\nSpecific Tag-based Raw Value deletion API\n\nThis API deletes all data prior to a specific time for a specific tag.\n\nThis function can be usefully used to remove data that is no longer needed after disk capacity is insufficient or backup is completed.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/raw/{Table}/{TagNames}/{BeforeTime}\nhttp://ipaddr:port/machiot/v1/datapoints/raw/{Table}/{TagNames}/{BeforeTime}\n\n\n\n  HTTP method : DELETE\n  Table : Target tag table to be deleted\n  TagNames : Target tag names to be deleted. You can specify multiple tags separated by a comma.\n  BeforeTime : Indicates time value which all data before it will deleted\n\n\nUsage\n\nRequest\ncurl -X DELETE  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/raw/tag/tag-2,tag-3/2001-09-09T01:20:00,000\";\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"No Error\",\n  \"timezone\": \"+0900\",\n  \"effect_rows\": \"32\",\n  \"data\": []\n}\n\n\nStatistical data processing function\n\nStatistical Data Select API\n\nThis API is a function that quickly obtains statistical results for stored data.\n\nURL\n\nhttp://ipaddr:port/machiot/datapoints/calculated/{Table}/{TagNames}/{Start}/{End}/{CalculationMode}/{Count}/{IntervalType}/{IntervalValue}\nhttp://ipaddr:port/machiot/v1/datapoints/calculated/{Table}/{TagNames}/{Start}/{End}/{CalculationMode}/{Count}/{IntervalType}/{IntervalValue}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be selected\n    \n      TagNames : Target tag names to be selected. You can specify multiple tags separated by a comma.\n    \n  \n  If multiple tags are specified, the total operation result for those tags is output. (If you want to get statistical results for each tag, you have to call it tag by tag)\n  Start, End : Specify the time range in which data is to be obtained (Lookup Raw Value Select API)\n  Count : Number of result\n  CalculationMode : Target statistical function to be obtained. You can specify multiple statistical functions separated by a comma. (The function name must be the same as below)\n    \n      min : Minimum value\n      max : Maximum value\n      sum : Sum of values\n      count : Count of values\n      avg : Average of values\n    \n  \n  IntervalType : Desired time unit type of values\n    \n      sec : Per second\n      min : Per minute\n      hour : Per hour\n    \n  \n  IntervalValue : Desired time unit of IntervalType\n    \n      A value greater than 0 is specified as a minor number of 60.\n      Mainly 5, 10, 15, and 30 are designated.\n    \n  \n\n\nUsage\n\nRequest (single statistical function)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/calculated/tag/tag-1/2001-09-09T00:00:00,000/2001-09-09T01:20:00,000/sum/5/min/5\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"time\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"sum\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"time\": \"2001-09-09 01:00:00 000:000:000\",\n      \"sum\": 24303\n    },\n    {\n      \"time\": \"2001-09-09 01:05:00 000:000:000\",\n      \"sum\": 25203\n    },\n    {\n      \"time\": \"2001-09-09 01:10:00 000:000:000\",\n      \"sum\": 26103\n    },\n    {\n      \"time\": \"2001-09-09 01:15:00 000:000:000\",\n      \"sum\": 27003\n    },\n    {\n      \"time\": \"2001-09-09 01:20:00 000:000:000\",\n      \"sum\": 9201\n    }\n  ]\n}\nRequest (multiple statistical function)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/v1/datapoints/calculated/tag/tag-1/2001-09-09T00:00:00,000/2001-09-09T01:20:00,000/sum,min,max/5/min/5\"\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"time\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"sum\",\n      \"type\": 20,\n      \"length\": 17\n    },\n    {\n      \"name\": \"min\",\n      \"type\": 20,\n      \"length\": 17\n    },\n    {\n      \"name\": \"max\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"time\": \"2001-09-09 01:00:00 000:000:000\",\n      \"sum\": 24303,\n      \"min\": 8001,\n      \"max\": 8201\n    },\n    {\n      \"time\": \"2001-09-09 01:05:00 000:000:000\",\n      \"sum\": 25203,\n      \"min\": 8301,\n      \"max\": 8501\n    },\n    {\n      \"time\": \"2001-09-09 01:10:00 000:000:000\",\n      \"sum\": 26103,\n      \"min\": 8601,\n      \"max\": 8801\n    },\n    {\n      \"time\": \"2001-09-09 01:15:00 000:000:000\",\n      \"sum\": 27003,\n      \"min\": 8901,\n      \"max\": 9101\n    },\n    {\n      \"time\": \"2001-09-09 01:20:00 000:000:000\",\n      \"sum\": 9201,\n      \"min\": 9201,\n      \"max\": 9201\n    }\n  ]\n}\n\n\nTag metadata processing function\n\nThe schema of the table used in this section was created as follows.\n\ncurl -X GET \"http://127.0.0.1:${ITF_HTTP_PORT}/machbase\" --data-urlencode 'q=create tagdata table TAG (name_multi varchar(20) primary key, time_multi datetime basetime, value_multi double summarized, value2_multi short, value3_multi varchar(10)) metadata (myshortmeta short, baseip ipv4);';\n\n\nTag Information INSERT API\n\nThis API is used to register tags to be used.\n\nEnter data as many as the number of metadata columns added when creating a table with the Tag\n\nhttp://ipaddr:port/machiot/tags/list/{TableName}\nhttp://ipaddr:port/machiot/v1/tags/list/{TableName}\n\n\n\n  HTTP method : POST\n  TableName : Target tag table to be input.\n\n\nUsage\n\nRequest\ncurl -X POST -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag\" -d\n       '{\n        \"values\":[\n            [\"tag3\", 0, \"127.0.0.0\"],\n            [\"tag4\", 1, \"127.0.0.1\"],\n            [\"tag4\", 1, \"127.0.0.1\"],\n            [\"tag5\", 2, \"127.0.0.2\"]\n         ]\n        }';\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"No Error\",\n  \"timezone\": \"+0900\",\n  \"data\": [],\n  \"append_success\": 3,\n  \"append_failure\": 1\n}\n#tag4의 중복 입력으로 에러 1건, 나머지 3건 성공\n\n\nTag Information SELECT API\n\nURL\n\nhttp://ipaddr:port/machiot/tags/list/{Table}/{TagName}\nhttp://ipaddr:port/machiot/v1/tags/list/{Table}/{TagName}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be selected\n    \n      Print a list of all tag names if only table name is specified\n    \n  \n  TagName : Target tag name to be selected\n    \n      Print the tag details\n    \n  \n\n\nUsage\n\nRequest (select all tag names)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name_multi\",\n      \"type\": 5,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name_multi\": \"tag0\"\n    },\n    {\n      \"name_multi\": \"tag1\"\n    },\n    {\n      \"name_multi\": \"tag3\"\n    },\n    {\n      \"name_multi\": \"tag4\"\n    },\n    {\n      \"name_multi\": \"tag5\"\n    }\n  ]\n}\n\n\nTag Information UPDATE API\n\nThis API supports modification of additional tag information.\n\nBoth PUT and PATCH are supported, and the value of the JSON format used at the time of input must be the same as the column name of the table.\n\nSince multiple column names are supported, values of two or more columns can be changed at once.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/list/{Table}/{TagName}\nhttp://ipaddr:port/machiot/v1/tags/list/{Table}/{TagName}\n\n\n\n  HTTP method : PUT / PATCH\n  Table : Target tag table to be updated\n  TagName : Target tag name to be updated\n\n\nUsage\n\nRequest (PUT)\ncurl -X PUT -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag/tag0\" -d  '{\"baseip\":\"192.168.0.1\"}';\nResponse\n{\n   \"error_code\":0,\n   \"error_message\" :\"No Error\",\n   \"timezone\":\"+0900\",\n   \"effect_rows\":\"1\",\n   \"data\":[]\n}\n  \nRequest (PATCH)\ncurl -X PATCH -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag/tag3\" -d  '{\"baseip\":\"255.255.255.0\", \"myshortmeta\":9999 }';\nResponse\n{\n   \"error_code\":0,\n   \"error_message\" :\"No Error\",\n   \"timezone\":\"+0900\",\n   \"effect_rows\":\"1\",\n   \"data\":[]\n}\n\n\nTag Information DELETE API\n\nThis API deletes the specified tag. However, if raw data is present in the tag, deletion fails.\n\nIn order to delete a tag in which raw data exists, this function must be called after performing the deletion of raw data for that tag.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/list/{Table}/{TagNames}\nhttp://ipaddr:port/machiot/v1/tags/list/{Table}/{TagNames}\n\n\n\n  HTTP method : DELETE\n  Table: Target tag table to be deleted\n  TagName: Target tag name to be deleted\n\n\nUsage\n\nRequest (Try to delete tag with raw data)\ncurl -X DELETE -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag/tag0\";\nResponse (Error)\n{\n   \"error_code\":2324,\n   \"error_message\" :\"Cannot delete tagmeta. there exist data with deleted_tag key.\",\n   \"timezone\":\"+0900\",\n   \"data\":[]\n}\n \n \nRequest (Try to delete tag without raw data)\ncurl -X DELETE -H \"Content-Type: application/json\" \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/list/tag/tag4\";\nResponse (Success)\n{\n   \"error_code\":0,\n   \"error_message\" :\"No Error\",\n   \"timezone\":\"+0900\",\n   \"effect_rows\":\"1\",\n   \"data\":[]\n}\n\n\nOther functions\n\nTime range lookup API\n\nThis API obtains an overall time range (minimum and maximum) for data of the specified table and tag.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/range/{Table}/{TagName}\nhttp://ipaddr:port/machiot/v1/tags/range/{Table}/{TagName}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be selected\n  TagName : Target tag name to be selected\n    \n      If not specified, return the whole tag time range (the tag name will be returned to ALL).\n    \n  \n\n\nUsage\n\nRequest (Whole tag range)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/range/tag\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 3\n    },\n    {\n      \"name\": \"min\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"max\",\n      \"type\": 6,\n      \"length\": 31\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"ALL\",\n      \"min\": \"2001-09-09 01:00:00 000:000:000\",\n      \"max\": \"2032-09-09 10:46:49 000:000:000\"\n    }\n  ]\n}\n  \nRequest (Time range of each specified tag)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/range/tag/tag-1,tag-2\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"min\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"max\",\n      \"type\": 6,\n      \"length\": 31\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"min\": \"2001-09-09 01:00:01 000:000:000\",\n      \"max\": \"2001-09-21 12:31:41 000:000:000\"\n    },\n    {\n      \"name\": \"tag-2\",\n      \"min\": \"2001-09-09 01:00:02 000:000:000\",\n      \"max\": \"2001-09-21 12:31:42 000:000:000\"\n    }\n  ]\n}\n\n\nMinimum value API\n\nThis API gets the minimum Value existing in the specified table or tag.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/min/{Table}/{TagName}\nhttp://ipaddr:port/machiot/v1/tags/min/{Table}/{TagName}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be selected\n  TagName : Target tag name to be selected\n    \n      If not specified, return the minimum value of the table\n    \n  \n\n\nUsage\n\nRequest (Minimum value of whole table)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/min/tag\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"min\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"min\": 0.0\n    }\n  ]\n}\n \n \nRequest (Minimum value of each specified tag)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/min/tag/tag-1,tag-2\";\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 100\n    },\n    {\n      \"name\": \"time\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"min\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"time\": \"2001-09-09 10:46:42 000:000:000\",\n      \"min\": 10001.0\n    },\n    {\n      \"name\": \"tag-2\",\n      \"time\": \"2001-09-09 10:46:43 000:000:000\",\n      \"min\": 10002.0\n    }\n  ]\n}\n\n\nMaximum value API\n\nThis API gets the maximum Value existing in the specified table or tag.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/max/{Table}/{TagName}\nhttp://ipaddr:port/machiot/v1/tags/max/{Table}/{TagName}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be selected\n  TagName : Target tag name to be selected\n    \n      If not specified, return the maximum value of the table\n    \n  \n\n\nUsage\n\nRequest (Maximum value of the table)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/max/tag\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"max\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"max\": 10000000000.0\n    }\n  ]\n}\n \n \nRequest (Maximum value of each specified tag)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/max/tag/tag-1,tag-2\";\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 100\n    },\n    {\n      \"name\": \"time\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"max\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"time\": \"2001-09-09 13:12:12 000:000:000\",\n      \"max\": 9999999991.0\n    },\n    {\n      \"name\": \"tag-2\",\n      \"time\": \"2001-09-09 13:12:13 000:000:000\",\n      \"max\": 9999999992.0\n    }\n  ]\n}\n\n\nFirst row API\n\nThis API gets the row with the smallest time value that exists in the specified table or tag.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/first/{Table}/{TagName}\nhttp://ipaddr:port/machiot/v1/tags/first/{Table}/{TagName}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be selected\n  TagName : Target tag name to be selected\n    \n      If not specified, return the row with the smallest time value of the table\n    \n  \n\n\nUsage\n\nRequest (Earliest row data of the table)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/first/tag\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-0\",\n      \"TIME\": \"2001-09-09 01:00:00 000:000:000\",\n      \"VALUE\": 8000.0\n    }\n  ]\n}\n \n \nRequest (Earliest row data of each specified tag)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/first/tag/tag-1,tag-2\";\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-09 01:00:01 000:000:000\",\n      \"VALUE\": 8001.0\n    },\n    {\n      \"NAME\": \"tag-2\",\n      \"TIME\": \"2001-09-09 01:00:02 000:000:000\",\n      \"VALUE\": 8002.0\n    }\n  ]\n}\n\n\nLast row API\n\nThis API gets the row with the largest time value that exists in the specified table or tag.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/last/{Table}/{TagName}\nhttp://ipaddr:port/machiot/v1/tags/last/{Table}/{TagName}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be selected\n  TagName : Target tag name to be selected\n    \n      If not specified, return the row with the largest time value of the table\n    \n  \n\n\nUsage\n\nRequest (Latest row data of the table)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/last/tag\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"dummy\",\n      \"TIME\": \"2032-09-09 10:46:49 000:000:000\",\n      \"VALUE\": 1000000009.0\n    }\n  ]\n}\n \n \nRequest (Latest row data of each specified tag)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/last/tag/tag-1,tag-2\";\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"NAME\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"TIME\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"VALUE\",\n      \"type\": 20,\n      \"length\": 17\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"NAME\": \"tag-1\",\n      \"TIME\": \"2001-09-21 12:31:41 000:000:000\",\n      \"VALUE\": 999901.0\n    },\n    {\n      \"NAME\": \"tag-2\",\n      \"TIME\": \"2001-09-21 12:31:42 000:000:000\",\n      \"VALUE\": 999902.0\n    }\n  ]\n}\n\n\nRecord count API\n\nThis API obtains the number of records present in the specified table or tag.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/count/{Table}/{TagNames}\n\n\nhttp://ipaddr:port/machiot/tags/cnt/{Table}/{TagNames}\nhttp://ipaddr:port/machiot/v1/tags/count/{Table}/{TagNames}\n\n\nhttp://ipaddr:port/machiot/v1/tags/cnt/{Table}/{TagNames}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be counted\n  TagName : Target tag name to be counted\n    \n      If not specified, output the total number of records in the table.\n    \n  \n\n\nUsage\n\nRequest (Total number of records in the table)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/count/tag\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"count\",\n      \"type\": 12,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"count\": 1000001\n    }\n  ]\n}\n \n \nRequest (Records count of each specified tag)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/count/tag/tag-1,tag-2\";\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 100\n    },\n    {\n      \"name\": \"count\",\n      \"type\": 12,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"count\": 10000\n    },\n    {\n      \"name\": \"tag-2\",\n      \"count\": 10000\n    }\n  ]\n}\n\n\nDisk usage API\n\nThis API approximates the disk usage in use by the specified table or tag.\n\nURL\n\nhttp://ipaddr:port/machiot/tags/disksize/{Table}/{TagNames}\nhttp://ipaddr:port/machiot/v1/tags/disksize/{Table}/{TagNames}\n\n\n\n  HTTP method : GET\n  Table : Target tag table to be measured\n  TagName : Target tag name to be measured\n    \n      If not specified, output the total disk usage for that table.\n    \n  \n\n\nUsage\n\nRequest (Total disk usage for table)\ncurl -X GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/disksize/tag/\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"disksize\",\n      \"type\": 12,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"disksize\": 276904448\n    }\n  ]\n}\n \nRequest (Disk usage of each specified tag)\ncurl -X  GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/disksize/tag/tag-1,tag-2\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"name\",\n      \"type\": 5,\n      \"length\": 100\n    },\n    {\n      \"name\": \"disksize\",\n      \"type\": 12,\n      \"length\": 20\n    }\n  ],\n  \"timezone\": \"+0900\",\n  \"data\": [\n    {\n      \"name\": \"tag-1\",\n      \"disksize\": 240000\n    },\n    {\n      \"name\": \"tag-2\",\n      \"disksize\": 240000\n    }\n  ]\n}\n\n\nRollup request API\n\nThis API requests compulsory execution for a specific rollup table. Through this, it is forced to calculate statistical values that have not yet been calculated.\n\nIf you call this API, you can wait for seconds to minutes depending on the situation, so you should use it carefully.\n\nURL\n\nhttp://ipaddr:port/machiot//rollup/{Table}\nhttp://ipaddr:port/machiot/v1//rollup/{Table}\n\n\n\n  HTTP method : GET\n  Table : Target table to be execute rollup\n\n\nUsage\n\nRequest\ncurl -X HTTP GET  \"http://127.0.0.1:${ITF_HTTP_PORT}/machiot/tags/rollup/tag\"\nResponse\n{\n  \"error_code\": 0,\n  \"error_message\": \"No Error\",\n  \"timezone\": \"+0900\",\n  \"data\": []\n}"
					}
					
				
		
				
					,
					
					"feature-tables-retention-html": {
						"id": "feature-tables-retention-html",
						"title": "Data Auto Delete",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/retention.html",
						"content": "This function automatically deletes the data after the designated data retention period.\n\nYou can create a retention policy that specifies the retention period and deletion cycle, and apply/release it to the table through the ALTER statement.\n\nCreate Retention Policy\n\nCreate a RETENTION POLICY by specifying the retention period and deletion cycle.\n\nThe retention period can be specified in units of months and days, and the deletion cycle can be specified in units of days and hours.\n\nPOLICY information can be checked by querying the M$RETENTION table.\n\nSyntax:\nCREATE RETENTION policy_name DURATION duration {MONTH|DAY} INTERVAL interval {DAY|HOUR}\n\n\n\n  policy_name : Policy name to create\n  duration : Retention period of data to be deleted (based on system time)\n  interval : Retention period checking cycle\n\n\nExample:\n-- Data older than one day is deleted, and the update cycle is set to one hour.\nMach&gt; CREATE RETENTION policy_1d_1h DURATION 1 DAY INTERVAL 1 HOUR;\nExecuted successfully.\n\n-- Data older than one month is deleted, and the renewal cycle is set to three days.\nMach&gt; CREATE RETENTION policy_1m_3d DURATION 1 MONTH INTERVAL 3 DAY;\nExecuted successfully.\n\nMach&gt; SELECT * FROM M$RETENTION;\nUSER_ID     POLICY_NAME                               DURATION             INTERVAL             \n-----------------------------------------------------------------------------------------------------\n1           POLICY_1D_1H                              86400                3600                 \n1           POLICY_1M_3D                              2592000              259200               \n[2] row(s) selected.\n\n\nApply Retention Policy\n\nApply the previously created RETENTION POLICY to the table.\n\nAfter application, the retention period is checked and deleted every deletion cycle.\n\nTable information to which the RETENTION POLICY is applied can be checked by querying the V$RETENTION_JOB table.\n\nSyntax:\nALTER TABLE table_name ADD RETENTION policy_name\n\n\n\n  table_name : table name to apply\n  policy_name : policy name to apply\n\n\nExample:\nMach&gt; CREATE TAG TABLE tag (name VARCHAR(20) PRIMARY KEY, time DATETIME BASETIME, value DOUBLE SUMMARIZED);\nExecuted successfully.\n\nMach&gt; ALTER TABLE tag ADD RETENTION policy_1d_1h;\nAltered successfully.\n\nMach&gt; SELECT * FROM V$RETENTION_JOB;\nUSER_NAME                                                                         TABLE_NAME                                                                        \n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nPOLICY_NAME                                                                       STATE                                                                             LAST_DELETED_TIME               \n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSYS                                                                               TAG                                                                               \nPOLICY_1D_1H                                                                      WAITING                                                                           NULL                            \n[1] row(s) selected.\n\n\n\nRelease Retention Policy\n\nRelease the RETENTION POLICY applied to the table.\n\nAfter release, the data is not deleted and is permanently preserved.\n\nSyntax:\nALTER TABLE table_name DROP RETENTION;\n\n\n\n  table_name : table name to release\n\n\nExample:\nMach&gt; ALTER TABLE tag DROP RETENTION;\nAltered successfully.\n\n\nDrop Retention Policy\n\nIf a table to which the RETENTION POLICY is being applied exists, it cannot be dropped.\n\nYou must release the RETENTION of the table being applied and delete it.\n\nSyntax:\nDROP RETENTION policy_name\n\n\n\n  policy_name : policy name to remove\n\n\nExample:\nMach&gt; ALTER TABLE tag ADD RETENTION policy_1d_1h;\nAltered successfully.\n\n-- ERROR\nMach&gt; DROP RETENTION policy_1d_1h;\n[ERR-02702: Policy (POLICY_1D_1H) is in use.]\n\nMach&gt; ALTER TABLE tag DROP RETENTION;\nAltered successfully.\n\n-- SUCCESS\nMach&gt; DROP RETENTION policy_1d_1h;\nDropped successfully."
					}
					
				
		
				
		
				
		
				
					,
					
					"sql-ref-select-hint-html": {
						"id": "sql-ref-select-hint-html",
						"title": "SELECT Hint",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/select-hint.html",
						"content": "# Index\n\n* [Introduction](#introduction)\n* [PARALLEL](#parallel)\n* [NOPARALLEL](#noparallel)\n* [FULL](#full)\n* [NO_INDEX](#no_index)\n* [RID_RANGE](#rid_range)\n* [SCAN_FORWARD, SCAN_BACKWARD](#scan_forward-scan_backward)\n\n\n# Introduction\n\nHints that can be used in a SELECT queries are described.\n\n\n# PARALLEL\n\nSpecifies parallel factor for parallel query execution.\n\n```sql\nSELECT /*+ PARALLEL(table_name, parallel_factor) */ ...\n```\n\n```sql\nMach> CREATE TABLE log_parallel_test (sensor VARCHAR(32), frequency DOUBLE, value DOUBLE, ts DATETIME);\nMach> CREATE INDEX idx_ts ON log_parallel_test (ts);\n \nMach> EXPLAIN SELECT /*+ PARALLEL(log_parallel_test, 8) */ sensor, frequency, avg(value)\n      FROM log_parallel_test\n      WHERE ts >= TO_DATE('2007-07-01', 'YYYY-MM-DD') and ts = TO_DATE('2007-07-01', 'YYYY-MM-DD')                                \n     * ts  CREATE TABLE log_parallel_test (sensor VARCHAR(32), frequency DOUBLE, value DOUBLE, ts DATETIME);\nMach> CREATE INDEX idx_ts ON log_parallel_test (ts);\n \nMach> EXPLAIN SELECT /*+ NOPARALLEL(log_parallel_test) */ sensor, frequency, avg(value)\n      FROM log_parallel_test\n      WHERE ts >= TO_DATE('2007-07-01', 'YYYY-MM-DD') and ts = TO_DATE('2007-07-01', 'YYYY-MM-DD')                                 \n     * ts  CREATE TABLE log_full_test (sensor VARCHAR(32), I1 INTEGER);\nMach> CREATE INDEX idx_I1 ON log_full_test (I1);\n \nMach> EXPLAIN SELECT * FROM log_full_test WHERE I1 = 1;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   *BITMAP RANGE (table id:14, column id:2, index id:15)                         \n   [KEY RANGE]                                                                   \n    * I1 = 1                                                                     \n[5] row(s) selected.\n \nMach> EXPLAIN SELECT /*+ FULL(log_full_test) */ * FROM log_full_test WHERE I1 = 1;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  FULL SCAN                                                                      \n[2] row(s) selected.\n```\n\n\n# NO_INDEX\n\nDoes not use the corresponding INDEX.\n\n```sql\nSELECT /*+ NO_INDEX(table_name,index_name) */ ...\n```\n\n```sql\nMach> CREATE TABLE log_no_index_test (sensor VARCHAR(32), I1 INTEGER, I2 INTEGER);\nMach> CREATE INDEX idx_I1 ON log_no_index_test (I1);\nMach> CREATE INDEX idx_I2 ON log_no_index_test (I2);\n \nMach> EXPLAIN SELECT * FROM TEST WHERE I1 = 1;\nPLAN\n------------------------------------------------------------------------------------\n PROJECT\n  INDEX SCAN\n   *BITMAP RANGE (t:7, c:1, i:8) with BLOOMFILTER\n   [KEY RANGE]                                                                  \n    * I1 = 1                                                                    \n[5] row(s) selected.\n \nMach> EXPLAIN SELECT /*+ NO_INDEX(TEST,TEST_IDX) */ * FROM TEST WHERE I1 = 1;\nPLAN\n------------------------------------------------------------------------------------\n PROJECT\n  FULL SCAN\n[2] row(s) selected.\n \nMach> EXPLAIN SELECT * FROM log_no_index_test WHERE I1 = 1 or I2 = 2;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   INDEX (OR)                                                                    \n    *BITMAP RANGE (table id:21, column id:2, index id:22)                        \n    *BITMAP RANGE (table id:21, column id:3, index id:23)                        \n   [KEY RANGE]                                                                   \n    * I1 = 1 or I2 = 2                                                           \n[7] row(s) selected.\n \nMach> EXPLAIN SELECT /*+ NO_INDEX(log_no_index_test, idx_I1) */ * FROM log_no_index_test WHERE I1 = 1 or I2 = 2;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  FULL SCAN                                                                      \n[2] row(s) selected.\n \nMach> EXPLAIN SELECT * FROM log_no_index_test WHERE I1 = 1 and I2 = 2;\nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   *BITMAP RANGE (table id:21, column id:2, index id:22)                         \n   *BITMAP RANGE (table id:21, column id:3, index id:23)                         \n   [KEY RANGE]                                                                   \n    * I1 = 1                                                                     \n    * I2 = 2                                                                     \n[7] row(s) selected.\n \nMach> EXPLAIN SELECT /*+ NO_INDEX(log_no_index_test, idx_I1) */ * FROM log_no_index_test WHERE I1 = 1 and I2 = 2;\n \nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   *BITMAP RANGE (table id:21, column id:3, index id:23)                         \n   [KEY RANGE]                                                                   \n    * I2 = 2                                                                     \n   [FILTER]                                                                      \n    * I1 = 1                                                                     \n[7] row(s) selected.\nElapsed time: 0.001\nMach>\nMach>\nMach> EXPLAIN SELECT /*+ NO_INDEX(log_no_index_test, idx_I2) */ * FROM log_no_index_test WHERE I1 = 1 and I2 = 2;\n \nPLAN                                                                             \n------------------------------------------------------------------------------------\n PROJECT                                                                         \n  INDEX SCAN                                                                     \n   *BITMAP RANGE (table id:21, column id:2, index id:22)                         \n   [KEY RANGE]                                                                   \n    * I1 = 1                                                                     \n   [FILTER]                                                                      \n    * I2 = 2                                                                     \n[7] row(s) selected.\n```\n\n\n# RID_RANGE\n\nRuns within RID range.\n\n```sql\nSELECT /*+ RID_RANGE(table_name,number,number) */ ...\n```\n\n```sql\nMach> SELECT /*+ RID_RANGE(TEST,45,50) */ _RID, * FROM TEST;\n_RID                 I1\n------------------------------------\n49                   1\n48                   1\n47                   1\n46                   1\n45                   1\n[5] row(s) selected.\n```\n\n\n# SCAN_FORWARD, SCAN_BACKWARD\n\nIt specifies the direction of scanning for LOG table. With SCAN_FORWARD, the oldest record input is retrived first, whereas with SCAN_BACKWARD, the newest record input is retrieved first.\n\nIt affects LOG tables in Fog edition only.\n\n```sql\nSELECT /*+ SCAN_FORWARD(table_name) */ ...\nSELECT /*+ SCAN_BACKWARD(table_name) */ ...\n```\n\n```sql\nMach> SELECT /*+ SCAN_FORWARD(mytbl) */  _ARRIVAL_TIME, VALUE FROM mytbl LIMIT 10;\n_ARRIVAL_TIME                   VALUE                   \n----------------------------------------------------------------\n2017-01-01 00:00:49 500:000:000 0                         \n2017-01-01 00:01:39 500:000:000 1                         \n2017-01-01 00:02:29 500:000:000 2                         \n2017-01-01 00:03:19 500:000:000 3                         \n2017-01-01 00:04:09 500:000:000 4                         \n2017-01-01 00:04:59 500:000:000 5                         \n2017-01-01 00:05:49 500:000:000 6                         \n2017-01-01 00:06:39 500:000:000 7                         \n2017-01-01 00:07:29 500:000:000 8                         \n2017-01-01 00:08:19 500:000:000 9                         \n[10] row(s) selected.\n \nMach> SELECT /*+ SCAN_BACKWARD(mytbl) */ _ARRIVAL_TIME, VALUE FROM mytbl LIMIT 10;\n_ARRIVAL_TIME                   VALUE                   \n----------------------------------------------------------------\n2017-02-27 20:53:19 500:000:000 9                         \n2017-02-27 20:52:29 500:000:000 8                         \n2017-02-27 20:51:39 500:000:000 7                         \n2017-02-27 20:50:49 500:000:000 6                         \n2017-02-27 20:49:59 500:000:000 5                         \n2017-02-27 20:49:09 500:000:000 4                         \n2017-02-27 20:48:19 500:000:000 3                         \n2017-02-27 20:47:29 500:000:000 2                         \n2017-02-27 20:46:39 500:000:000 1                         \n2017-02-27 20:45:49 500:000:000 0                         \n[10] row(s) selected.\n```"
					}
					
				
		
				
					,
					
					"sql-ref-select-html": {
						"id": "sql-ref-select-html",
						"title": "SELECT",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/select.html",
						"content": "# Index\n\n* [SELECT Syntax](#select-syntax)\n* [SET OPERATOR](#set-operator)\n* [TARGET LIST](#target-list)\n    * [CASE statement](#case-statement)\n* [FROM](#from)\n    * [SUBQUERY(INLINE VIEW)](#subqueryinline-view)\n    * [JOIN(INNER JOIN)](#joininner-join)\n    * [INNER JOIN and OUTER JOIN](#inner-join-and-outer-join)\n    * [PIVOT](#pivot)\n* [WHERE](#where)\n    * [Use of SUBQUERY](#use-of-subquery)\n    * [SEARCH Statement](#search-statement)\n    * [ESEARCH Statement](#esearch-statement)\n    * [NOT SEARCH Statement](#not-search-statement)\n    * [REGEXP Statement](#regexp-statement)\n    * [IN Statement](#in-statement)\n    * [Use In Statement and SUBQUERY](#use-in-statement-and-subquery)\n    * [BETWEEN Statement](#between-statement)\n    * [RANGE Statement](#range-statement)\n* [GROUP BY / HAVING](#group-by--having)\n* [ORDER BY](#order-by)\n* [SERIES BY](#series-by)\n* [LIMIT](#limit)\n* [DURATION](#duration)\n* [SAVE DATA](#save-data)\n\n\n\nSELECT is a syntax used to find, filter, and manipulate data from various tables in Machbase.\n\n\n# SELECT Syntax\n\n```sql\nselect_stmt UNION ALL select_stmt\n```\n\n```sql\nSELECT target_list FROM TableList WHERE Condition GROUP BY Expr ORDER BY Expr [Desc] HAVING Expr SERIES BY Expr LIMIT N[,N] DURATION TimeExpr;\n```\n\n\n# SET OPERATOR\n\nUsed when receiving the results of multiple Select queries as a single query result. Machbase supports only the UNION ALL set operator. The set operator can be executed only if the left and right Select statements are (1) the same or compatible types, (2) the number of query results is the same, and if any of the two conditions does not match, they are treated as errors.\n\nData type conversion and compatibility verification are performed based on the following criteria.\n* Signed integer types and unsigned integer types are not compatible.\n* The integer type is compatible with the real type, and the query result is converted to the real type and returned.\n* Character types are compatible with different lengths.\n* IPv6 type and IPv4 type are not compatible.\n* Of the two SELECT statements, the column name of the left query is always used.\n\nExamples\n\n```sql\nSELECT i1, i2 FROM table_1\nUNION ALL\nSELECT c1, c2 FROM table_2\n```\n\n\n# TARGET LIST\n\nThis is a **list of columns or subqueries** targeted by the Select statement .\n\nThe subquery used in the target list is treated as an error if it has two or more values ​​or two or more result columns, such as a subquery used in the WHERE clause.\n\n```sql\nSELECT i1, i2 ...\nSELECT i1 (Select avg(c1) FROM t1), i2 ...\n```\n\n## CASE statement\n\n```sql\nCASE  [else_clause] END\n \nsimple_case_expression ::=\n    expr WHEN comparison_expr THEN return_expr\n        [WHEN comparison_expr THEN return_expr ...]\n \nsearched_case_expression ::=\n    WHEN condtion_expr THEN return EXPR [WHEN condtion_expr THEN return EXPR ...]\n \nelse_clause ::=\n    ELSE else_value_expr\n```\n\nThis is an expression that supports the IF ... THEN ... ELSE block of a typical programming language. simple_case_expression is executed in the form of return_expr when one column or expression is equal to the value of comparison_expr followed by when, and this when ... then clause can be repeated as many times as desired.\n\nsearched_case_expression does not specify an expression after CASE but describes a conditional clause that includes a comparison operator in the when clause. If the result of each comparison operation is true, then the value of the then clause is returned. The else clause returns else_value if the value of the when clause is not satisfied (even if the expression is NULL).\n\n```sql\nselect * from t1;\nI1          I2         \n---------------------------\n2           2          \n1           1          \n[2] row(s) selected.\n \nselect case i1 when 1 then 100 end from t1;\ncase i1 when 1 then 100 end\n------------------------------\nNULL       \n100        \n[2] row(s) selected.\n```\n\nIn the simple_case_expression example, if the value of the i1 column is 2, NULL is returned.\n\n```\nselect case when i1 > 0 then 100 when i1 > 1 then 200 end from t1;\ncase when i1 > 0 then 100 when i1 > 1 then 200 end\n------------------------------------------\n100        \n100        \n[2] row(s) selected.\n```\n\nSince searched_case_expression returns the first condition that satisfies the condition, 100 is returned, and the second condition is not executed.\n\n\n# FROM\n\nYou can specify a table name or an Inline view in the FROM clause. To perform a join between tables, lists the table or Inline view separated by a comma (,).\n\n```sql\nFROM table_name\n```\n\nRetrieves data in the table specified by table_name.\n\n## SUBQUERY(INLINE VIEW)\n\n```sql\nFROM (Select statement)\n```\n\nRetrievse data for the contents of the subquery enclosed in parentheses.\n\n* Machbase server does not support correlated subqueries, so you can not reference columns in a subquery in an outer query.\n\n## JOIN(INNER JOIN)\n\n```sql\nJOIN(INNER JOIN)\n```\n\nJoins two tables, table_1 and table_2. An INNER JOIN can be used when three or more tables are listed, and both the search condition and the conditional clause are described in the WHERE clause.\n\n```sql\nSELECT t1.i1, t2.i1 FROM t1, t2 WHERE t1.i1 = t2.i1 AND t1.i1 > 1 AND t2.i2 = 3;\n```\n\n## INNER JOIN and OUTER JOIN\n\nSupports ANSI style INNER JOIN, LEFT OUTER JOIN, and RIGHT OUTER JOIN. FULL OUTER JOIN is not supported.\n\n```sql\nFROM TABLE_1 [INNER|LEFT OUTER|RIGHT OUTER] JOIN TABLE_2 ON expression\n```\n\nThe ON clause of the ANSI-style JOIN clause uses the conditional clause that is performed by the JOIN. If the WHERE clause in the OUTER JOIN query has a clause for an inner table (a table that is filled with NULL if the condition of the ON clause is not satisfied), the query is converted to an INNER JOIN.\n\n```sql\nSELECT t1.i1 t2.i1 FROM t1 LEFT OUTER JOIN t2 ON (t1.i1 = t2.i1) WHERE t2.i2 = 1;\n```\n\nThe above query is converted to an INNER JOIN by the condition t2.i2 = 1 in the WHERE clause.\n\n## PIVOT\n\n* The PIVOT syntax is supported from Machbase version 5.6.\n\n![pivot_clause](/en/sql-ref/select_image/pivot_clause.png)\n\npivot_clause:\n\nThe PIVOT statement shows the aggregated results of GROUP BY output as ROW, rearranged into columns.\n\nIt is used in conjunction with the Inline view and is performed as follows.\n* Performs GROUP BY on columns that are not used in the PIVOT clause of the inline view, and then performs aggregate functions on the values ​​listed in the PIVOT IN clause.\n* The resulting grouping column and the aggregation result are rotated and displayed as columns.\n\nFor example, aggregate the value of each device from the data collected from various sensors.\nThe query that should be performed through the CASE statement can be expressed simply through the PIVOT statement.\n\n```sql\n-- w/o PIVOT\nSELECT * FROM (\n    SELECT\n             regtime,\n             SUM(CASE WHEN tagid = 'FRONT_AXIS_TORQUE' THEN dvalue ELSE 0 END)  AS front_axis_torque,\n             SUM(CASE WHEN tagid = 'REAR_AXIS_TORQUE' THEN dvalue ELSE 0 END)  AS rear_axis_torque,\n             SUM(CASE WHEN tagid = 'HOIST_AXIS_TORQUE' THEN dvalue ELSE 0 END)  AS hoist_axis_torque,\n             SUM(CASE WHEN tagid = 'SLIDE_AXIS_TORQUE' THEN dvalue ELSE 0 END)  AS slide_axis_torque\n    FROM     result_d\n    WHERE    regtime BETWEEN TO_DATE('2018-12-07 00:00:00') AND TO_DATE('2018-12-08 05:00:00')\n    GROUP BY regtime\n) WHERE front_axis_torque >= 40 AND rear_axis_torque >= 20;\n  \n-- w/ PIVOT\nSELECT * FROM (\n    SELECT regtime, tagid, dvalue FROM result_d\n    WHERE  regtime BETWEEN TO_DATE('2018-12-07 00:00:00') AND TO_DATE('2018-12-08 05:00:00')\n) PIVOT (SUM(dvalue) FOR tagid IN ('FRONT_AXIS_TORQUE', 'REAR_AXIS_TORQUE', 'HOIST_AXIS_TORQUE', 'SLIDE_AXIS_TORQUE'))\nWHERE front_axis_torque >= 40 AND rear_axis_torque >= 20;\n \n-- Result\nregtime                         'FRONT_AXIS_TORQUE'         'REAR_AXIS_TORQUE'          'HOIST_AXIS_TORQUE'         'SLIDE_AXIS_TORQUE'       \n------------------------------------------------------------------------------------------------------------------------------------------------------\n2018-12-07 16:42:29 840:000:000 12158                       7244                        NULL                        NULL                      \n2018-12-07 14:56:26 220:000:000 3308                        663                         NULL                        NULL                      \n2018-12-07 12:20:13 844:000:000 3804                        113                         NULL                        NULL                      \n2018-12-07 11:10:01 957:000:000 8729                        5384                        NULL                        NULL                      \n2018-12-07 17:46:57 812:000:000 7500                        4559                        NULL                        NULL                      \n2018-12-07 14:30:06 138:000:000 5080                        6817                        NULL                        -429                      \n2018-12-07 13:09:20 464:000:000 5233                        1869                        -7253                       NULL                      \n2018-12-07 15:43:03 539:000:000 7491                        4453                        NULL                        NULL\n...\n```\n\n\n# WHERE\n\n## Use of SUBQUERY\n\nSubquery can be used for conditional statements. If the subquery returns more than one record in a clause except the IN clause, or if there is more than one result column in the subquery, it is not supported.\n\n```sql\nWHERE i1 = (SELECT MAX(c2) FROM T1)\n```\n\nUses subquery by surrounding parentheses to the right of the conditional operator.\n\n* Machbase server does not support correlated subqueries, so you can not reference columns in a subquery in an outer query.\n\n## SEARCH Statement\n\nThe syntax is the same as for a regular database. However, a keyword index must be registered, and an additional search operation is possible by adding \"SEARCH\" as an operator keyword for text search.\n\n```sql\n-- drop table realdual;\ncreate table realdual (id1 integer, id2 varchar(20), id3 varchar(20));\n \ncreate keyword index idx1 on realdual (id2);\ncreate keyword index idx2 on realdual (id3);\n \ninsert into realdual values(1, 'time time2', 'series series2');\n \nselect * from realdual;\n \nselect * from realdual where id2 search 'time';\nselect * from realdual where id3 search 'series' ;\nselect * from realdual where id2 search 'time' and id3 search 'series';\n```\n\n-- drop table realdual;\ncreate table realdual (id1 integer, id2 varchar(20), id3 varchar(20));\n \ncreate keyword index idx1 on realdual (id2);\ncreate keyword index idx2 on realdual (id3);\n \ninsert into realdual values(1, 'time time2', 'series series2');\n \nselect * from realdual;\n \nselect * from realdual where id2 search 'time';\nselect * from realdual where id3 search 'series' ;\nselect * from realdual where id2 search 'time' and id3 search 'series';\n\n```sql\nMach> create table realdual (id1 integer, id2 varchar(20), id3 varchar(20));\nCreated successfully.\n \nMach> create keyword index idx1 on realdual (id2);\nCreated successfully.\n \nMach> create keyword index idx2 on realdual (id3);\nCreated successfully.\n \nMach> insert into realdual values(1, 'time time2', 'series series2');\n1 row(s) inserted.\n \nMach> select * from realdual;\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time time2            series series2 \n[1] row(s) selected.\n \nMach> select * from realdual where id2 search 'time';\nID1         ID2                   ID3\n------------------------------------------------------------\n1           time time2            series series2\n[1] row(s) selected.\n \nMach> select * from realdual where id3 search 'series';\nID1         ID2                   ID3\n------------------------------------------------------------\n1           time time2            series series2\n[1] row(s) selected.\n \nMach> select * from realdual where id2 search 'time' and id3 search 'series';\nID1         ID2                   ID3\n------------------------------------------------------------\n1           time time2            series series2\n[1] row(s) selected.\n```\n\n## ESEARCH Statement\n\nThe ESEARCH statement is a search keyword that enables extended searches on ASCII text. For this extension, search for the desired pattern is performed using the % character. In this Like operation, if all the records are checked before the %, the advantage of ESEARCH is that the words can be found quickly even in this case. This feature can be very useful when looking for a part of an English string (an error string or code).\n\n```sql\n-- Example\n \nselect id2 from realdual where id2 esearch 'bbb%';\nid2\n--------------------------------------------\nbbb ccc1\naaa bbb1\n \n[2] row(s) selected.\n \n-- Search pattern 'bbb%' also includes bbb1 in search results.\n \n \nselect id3 from realdual where id3 esearch '%cd%';\nid3\n--------------------------------------------\ncdf def1\nbcd/cdf1ad\nabc, bcd1\n[3] row(s) selected.\n \n-- % character works in middle of search pattern as well as beginning and end.\n \nselect id3 from realdual where id3 esearch '%cd%';\nid3\n--------------------------------------------\ncdf def1\nbcd/cdf1ad\nabc, bcd1\n[3] row(s) selected.\n```\n\n## NOT SEARCH Statement\n\nNOT SEARCH is a statement that returns true for records other than those found in the SEARCH statement.\n\nNOT ESEARCH can not be used.\n\n```sql\ncreate table t1 (id integer, i2 varchar(10));\ncreate keyword index t1_i2 on t1(i2);\ninsert into t1 values (1, 'aaaa');\ninsert into t1 values (2, 'bbbb');\n \nselect id from t1 where i2 not search 'aaaa';\n \nid\n--------------------------------------------\n2\n[1] row(s) selected.\n```\n\n## REGEXP Statement\n\nThe REGEXP statement is used to perform searches on data using regular expressions. In general, patterns of a particular column are filtered using regular expressions.\n\nOne thing to keep in mind is that you can not use indexes when using the REGEXP clause, so you must lower the overall search cost by putting index conditions on other columns in order to reduce the overall search scope.\nIf you want to check a specific pattern, use index by SEARCH or ESEARCH, and then use REGEXP again in a state where the total number of data is small, it helps to improve the efficiency of the whole system.\n\n```sql\nMach>\ncreate table realdual (id1 integer, id2 varchar(20), id3 varchar(20));\ncreate table dual (id integer);\ninsert into dual values(1);\ninsert into realdual values(1, 'time1', 'series1 series21');\ninsert into realdual values(1, 'time2', 'series2 series22');\ninsert into realdual values(1, 'time3', 'series3 series32');\n \n \nMach> select * from realdual where id2 REGEXP 'time' ;\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time3                 series3 series32\n1           time2                 series2 series22\n1           time1                 series1 series21\n[3] row(s) selected.\n \nMach> select * from realdual where id2 REGEXP 'time[12]' ;\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time2                 series2 series22\n1           time1                 series1 series21\n[2] row(s) selected.\n \nMach> select * from realdual where id2 REGEXP 'time[13]' ;\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time3                 series3 series32\n1           time1                 series1 series21\n[2] row(s) selected.\n \nMach> select * from realdual where id2 regexp 'time[13]' and id3 regexp 'series[12]';\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time1                 series1 series21 \n[1] row(s) selected.\n \nMach> select * from realdual where id2 NOT REGEXP 'time[12]';\nID1         ID2                   ID3                  \n------------------------------------------------------------\n1           time3                 series3 series32\n[1] row(s) selected.\n \nMach> SELECT 'abcde' REGEXP 'a[bcd]{1,10}e' from dual;\n'abcde' REGEXP 'a[bcd]{1,10}e'\n---------------------------------\n1          \n[1] row(s) selected.\n```\n\n## IN Statement\n\n```sql\ncolumn_name IN (value1, value2,...)\n```\n\nThe IN statement returns TRUE if it is satisfied in the value list. It is the same as the syntax linked by OR.\n\n## Use In Statement and SUBQUERY\n\nYou can use a subquery to the right of the IN statement in the conditional statement. However, if you specify more than one column on the left side of the IN condition, it treats it as an error and checks whether the result set returned from the right subquery exists in the left column value.\n\n```sql\nWHERE i1 IN (Select c1 from ...)\n```\n\n* Machbase server does not support correlated subqueries, so you can not reference columns in a subquery in an outer query.\n\n## BETWEEN Statement\n\n```sql\ncolumn_name BETWEEN value1 AND value2\n```\n\nThe BETWEEN statement returns TRUE if the value of column is in the range of value1 and value2.\n\n## RANGE Statement\n\n```sql\ncolumn_name RANGE duration_spec;\n \n-- duration_spec : integer (YEAR | WEEK | HOUR | MINUTE | SECOND);\n```\n\nProvides a Range operator that allows you to easily specify a time condition for a given column. The Range operator specifies the time range from the current time as the target of the operation, rather than specifying a specific time (as specified by the BEFORE keyword). With this operator, you can easily retrieve result records within a desired time range.\n\n```sql\nselect * from test where id  ]\n \nselect id1, avg(id2) from exptab where id2 group by id1 order by id1;\nObtain average value of id2 based on id1 column.\n```\n\n\n# ORDER BY\n\nThe ORDER BY clause sorts the query results in ascending or descending order. If no sorting options such as ASC or DESC are specified, the ORDER BY clause sorts by default in ascending order. If the ORDER BY clause is not specified, the order of the records to be queried depends on the query.\n\n```sql\nSELECT ...\nORDER BY {col_name | expr} [ASC | DESC]\n \nselect id1, avg(id2) from exptab where id2 group by id1 order by id1;\nObtain average value of id2 based on id1 column.\n```\n\n\n# SERIES BY\n\nThe SERIES BY clause extracts the sorted result set as successive result values ​​satisfying the SERIES BY condition. If the ORDER BY clause is not specified, it generates the sorted result using the _ARRIVAL_TIME column value. Therefore, if you use the GROUP BY clause or the query for a volatile table or lookup table that does not have the _ARRIVAL_TIME column, you must use the ORDER BY clause do.\n\nThe result values ​​that satisfy the conditional clause will have the return value of the same SERIESNUM () function.\n\n```sql\nFor example, for the following data\n \nCREATE TABLE T1 (C1 INTEGER, C2 INTEGER);\nINSERT INTO T1 VALUES (0, 1);\n \nINSERT INTO T1 VALUES (1, 2);\n \nINSERT INTO T1 VALUES (2, 3);\n \nINSERT INTO T1 VALUES (3, 2);\n \nINSERT INTO T1 VALUES (4, 1);\n \nINSERT INTO T1 VALUES (5, 2);\n \nINSERT INTO T1 VALUES (6, 3);\n \nINSERT INTO T1 VALUES (7, 1);\n \n \nThe following query produces the following output:\n \nSELECT C1,C2 FROM T1 ORDER BY C1 SERIES BY C2>1;\nC1          C2         \n---------------------------\n1           2          \n2           3          \n3           2          \n5           2          \n6           3   \n \nIf you want to know the RANGE value of C1 where the value of the C2 column is larger than 1, you can determine the range by outputting to which group each record is included with the SERIESNUM function.\n```\n\n\n# LIMIT\n\nThe LIMIT clause is used to limit the number of records to be output. You can specify an integer to output from the first row to the last row of the result set\n\n```sql\nLIMIT [offset,] row_count\n \nselect id1, avg(id2) from exptab where id2 group by id1 order by id1 LIMIT 10;\n```\n\n\n# DURATION\n\nDURATION is a keyword that allows you to easily determine the data retrieval scope based on _arrival_time. Used with the BEFORE statement to set a specific range of data at a specific point in time. By using this DURATION, search performance can be dramatically increased and the system load can be dramatically reduced. For more detailed usage, please refer to the following.\n\n```sql\nDURATION Number TimeSpec [BEFORE/AFTER Number TimeSpec]\nTimeSpec : YEAR | MONTH | WEEK |  DAY | HOUR | MINUTE | SECOND\n```\n\n```sql\ncreate table t8(i1 integer);\ninsert into t8 values(1);\ninsert into t8 values(2);\n \nselect i1 from t8;\n \n# Without BEFORE clause\nselect i1 from t8 duration 2 second;\nselect i1 from t8 duration 1 minute;\nselect i1 from t8 duration 1 hour;\nselect i1 from t8 duration 1 day;\nselect i1 from t8 duration 1 week;\nselect i1 from t8 duration 1 month;\nselect i1 from t8 duration 1 year;\n \n# Using full DURATION statement\nselect i1 from t8 duration 1 second before 1 day;\nselect i1 from t8 duration 1 minute before 1 day;\nselect i1 from t8 duration 1 hour before 1 day;\nselect i1 from t8 duration 1 day before 1 day;\nselect i1 from t8 duration 1 week before 1 day;\nselect i1 from t8 duration 1 month before 1 day;\nselect i1 from t8 duration 1 year before 1 day;\n```\n\nThe results are as follows.\n\n```sql\nMach> create table t8(i1 integer);\nCreated successfully.\n \nMach> insert into t8 values(1);\n1 row(s) inserted.\n \nMach> insert into t8 values(2);\n1 row(s) inserted.\n \nMach> select i1 from t8;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \n# BEFORE 절 없이\nMach> select i1 from t8 duration 2 second;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 minute;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 hour;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 day;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 week;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 month;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \nMach> select i1 from t8 duration 1 year;\ni1         \n--------------\n2          \n1          \n[2] row(s) selected.\n \n# Using full DURATION statement\nMach> select i1 from t8 duration 1 second before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 minute before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 hour before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 day before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 week before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 month before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n \nMach> select i1 from t8 duration 1 year before 1 day;\ni1         \n--------------\n[0] row(s) selected.\n```\n\n\n# SAVE DATA\n\nSaves the results of the query directly into the CSV data file.\n\n```sql\nSAVE DATA INTO 'file_name.csv' [HEADER ON|OFF] [(FIELDS | COLUMNS) [TERMINATED BY 'char'] [ENCLOSED BY 'char']] [ENCODED BY coding_name] AS select query;\n```\n\nThe options are described below.\n\n|Options|Description|\n|--|--|\n|HEADER (ON\\|OFF)|Specifies the column delimiter and escape delimiter of the csv file to be created.|\n|(FIELDS\\|COLUMNS) TERMINATED BY 'term_char'ENCLOSED BY 'escape_char'|Decides whether to enter the column name on the first line of the csv file to be created. The default is OFF.|\n|ENCODED BY coding_namecoding_name = ( UTF8, MS949, KSC5601, EUCJP, SHIFTJIS, BIG5, GB231280 )|Specifies the encoding format of the output data file. The default value is UTF8.|\n\n```sql\nSAVE DATA INTO '/tmp/aaa.csv' AS select * from t1;\n-- Execute select statement and write result to '/tmp/aaa.csv' file in csv format.\n  \nSAVE DATA INTO '/tmp/ccc.csv' HEADER ON FIELDS TERMINATED BY ';' ENCLOSED BY '\\'' ENCODED BY MS949 AS select * from t1 where i1 > 100;\n-- Execute select statement and write result to /tmp/ccc.csv file. Specify field separator and escape separator, respectively, and set encoding of stored data to MS949.\n```"
					}
					
				
		
				
					,
					
					"feature-tables-log-extract-simple-html": {
						"id": "feature-tables-log-extract-simple-html",
						"title": "Simple Join",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/extract/simple.html",
						"content": "# Index \n\n* [Simple Join](#simple-join)\n* [Join Using Alias](#join-using-alias)\n* [GROUP BY/ORDER BY](#group-by-order-by)\n* [Join without JOIN clause](#join-without-join-clause)\n* [Inner Join / Outer Join](#inner-join-outer-join)\n\n\nLog tables, volatile tables, lookup tables and meta tables can be searched by Join.\n\n\n# Simple Join\n\n```sql\nMach> CREATE TABLE logtable (code INT,value INT);\nCreated successfully.\n \nMach> INSERT INTO logtable VALUES(1,20 );\n1 row(s) inserted.\n \nMach> INSERT INTO logtable VALUES(2,10 );\n1 row(s) inserted.\n \nMach> INSERT INTO logtable VALUES(3,15 );\n1 row(s) inserted.\n \nMach> INSERT INTO logtable VALUES(4,20 );\n1 row(s) inserted.\n \nMach> INSERT INTO logtable VALUES(5,10 );\n1 row(s) inserted.\n \nMach> CREATE VOLATILE table VTABLE (code INT,name VARCHAR(32));\nCreated successfully.\n \nMach> INSERT INTO vtable VALUES(1, 'Sam');\n1 row(s) inserted.\n \nMach> INSERT INTO vtable VALUES(3, 'Thomas');\n1 row(s) inserted.\n \nMach> INSERT INTO vtable VALUES(5, 'Micheal');\n1 row(s) inserted.\n \nMach> INSERT INTO vtable VALUES(7, 'Jessica');\n1 row(s) inserted.\n \nMach> SELECT name,value FROM logtable, vtable WHERE logtable.code=vtable.code;\nname                              value\n-------------------------------------------------\nMicheal                           10\nThomas                            15\nSam                               20\n[3] row(s) selected.\n```\n\n\n# Join Using Alias\n\nWhen using Join, an alias can be used for the join target table.\n\n```sql\nSELECT c.name FROM m$sys_tables t, m$sys_columns c WHERE t.id = c.table_id AND t.name = 'T1'\nAND c.id NOT IN(0, 65534) ORDER BY c.name;\n \nc.name                                  \n--------------------------------------------\nADDR\nISTYPE\nSRCIP                        \n[3] row(s) selected.\n```\n\n\n# GROUP BY/ORDER BY \n\nGROUP BY, ORDER BY, and aggregate functions are also available.\n\n```sql\nMach> SELECT t.name, COUNT(c.name) FROM m$sys_columns c, m$sys_tables t WHERE t.id = c.table_id GROUP BY t.name ORDER BY t.name;\nt.name                                    count(c.name)\n------------------------------------------------------------------\nCOMMON_TABLE                              5\nDURATIONT                                 3\n[2] row(s) selected.\n```\n\n\n# Join without JOIN clause \n\nA join query without a JOIN clause causes an error. Because there is so much data in the log table, the speed of queries without join conditionality is unpredictably slow.\n\nAlso, two log table joins can be very slow. So, when designing a database, it is better to design so that join does not occur considering denormalization.\n\n```sql\nMach> CREATE TABLE log_table1(i1 INTEGER);\nCreated successfully.\nMach> INSERT INTO log_table1 VALUES(1);\n1 row(s) inserted.\nMach> INSERT INTO log_table1 VALUES(20);\n1 row(s) inserted.\nMach> INSERT INTO log_table1 VALUES(30);\n1 row(s) inserted.\n \n \nMach>CREATE TABLE log_table2(i1 INTEGER);\nCreated successfully.\nMach> INSERT INTO log_table2 VALUES(1);\n1 row(s) inserted.\nMach> INSERT INTO log_table2 VALUES(30);\n1 row(s) inserted.\nMach> INSERT INTO log_table2 VALUES(50);\n1 row(s) inserted.\n \nMach> SELECT log_table1.i1 FROM log_table1, log_table2;\n[ERR-02101 : Error in joining tables. Cannot join without join predicate.]\n \nMach> SELECT log_table1.i1 FROM log_table1, log_table2 where log_table1.i1 = 1;\n[ERR-02101 : Error in joining tables. Cannot join without join predicate.]\n \nMach> SELECT log_table1.i1 from log_table1, log_table2 WHERE log_table1.i1 = log_table2.i1;\ni1\n--------------\n30\n1\n[2] row(s) selected.\n```\n\n\n# Inner Join / Outer Join\n\nANSI type INNER, LEFT OUTER, or RIGHT OUTER join can be used, but FULL OUTER JOIN can not be used.\n\n```sql\nFROM    TABLE_1 [INNER|LEFT OUTER|RIGHT OUTER]  JOIN    TABLE_2 ON  expression\n```\n\n```sql\nSELECT t1.i1 t2.i1 FROM t1 LEFT OUTER JOIN t2 ON (t1.i1 = t2.i1) WHERE t2.i2 = 1;\n```\n\nThe above query is changed to Inner Join by t2.i2 = 1 condition in the where clause."
					}
					
				
		
				
					,
					
					"sitemap-xml": {
						"id": "sitemap-xml",
						"title": "",
						"version": "all",
						"categories": "",
						"url": " /sitemap.xml",
						"content": "/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% for section in site.data.toc %}\n     {{ site.baseurl }}{{ section.url }}/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% endfor %}"
					}
					
				
		
				
					,
					
					"intro-edition-standard-html": {
						"id": "intro-edition-standard-html",
						"title": "Standard Edition",
						"version": "all",
						"categories": "",
						"url": " /intro/edition/standard.html",
						"content": "# Necessity\n\nStandard Edition is a product used to store and analyze large amounts of data transmitted by the Edge compute device.\n\nStandard Edition is designed for maximum performance on a single hardware device, so it can be installed, managed and operated very quickly and easily, without the need for complex installation and configuration.\n\nIn particular, it is built in a single appliance and installed and used as factory and building equipment, and plays a primary role of collecting client's data.\n\n\n# Supported Hardware and Operating Systems\n\nStandard Edition supports 64-bit Linux and Windows 2000 or later operating system based on Intel CPU."
					}
					
				
		
				
					,
					
					"feature-tables-stream-html": {
						"id": "feature-tables-stream-html",
						"title": "STREAM",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/stream.html",
						"content": "# Concept\n\nSince Machbase 5, STREAM is a newly supported real-time data processing function based on Continuous Query Language (CQL).\nIn other words, it is possible to extract the data satisfying a condition of incremental data inputted to the log table and to load it in real-time into another table.\nIf a conditional search is performed on all the data without using the stream function, the retrieval of the accumulated data is not only slow but also may be a heavy burden imposed on the system.\nStream can be used to retrieve conditions for specific log data entered in real time and respond quickly to events.\n\n\n# Restrictions\n\n* Currently, Machbase (version 5.5) supports STREAM only in Edge Edition and Fog Edition.\n    * The Cluster Edition will be support STREAM in a future versions. \n* The data input source is only available with the log table.\n    * The ability to use the tag table as a source will be supported in future versions.\n* The data output destination is available for log and tag tables."
					}
					
				
		
				
					,
					
					"sql-ref-sys-session-manage-html": {
						"id": "sql-ref-sys-session-manage-html",
						"title": "System/Session Management",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/sys-session-manage.html",
						"content": "# Index\n\n* [ALTER SYSTEM](#alter-system)\n    * [KILL SESSION](#kill-session)\n    * [CANCEL SESSION](#cancel-session)\n    * [CHECK DISK_USAGE](#check-disk_usage)\n    * [INSTALL LICENSE](#install-license)\n    * [INSTALL LICENSE (PATH)](#install-license-path)\n    * [SET](#set)\n* [ALTER SESSION](#alter-session)\n    * [SET SQL_LOGGING](#set-sql_logging)\n    * [SET DEFAULT_DATE_FORMAT](#set-default_date_format)\n    * [SET SHOW_HIDDEN_COLS](#set-show_hidden_cols)\n    * [SET FEEDBACK_APPEND_ERROR](#set-feedback_append_error)\n    * [SET MAX_QPX_MEM](#set-max_qpx_mem)\n    * [SET SESSION_IDLE_TIMEOUT_SEC](#set-session_idle_timeout_sec)\n    * [SET QUERY_TIMEOUT](#set-query_timeout)\n\n\n# ALTER SYSTEM\n\nThis statement is the syntax for managing system-wide resources or changing settings.\n\n## KILL SESSION\n\n**alter_system_kill_session_stmt:**\n\n![alter_system_kill_session_stmt](/en/sql-ref/sys_image/alter_system_kill_session_stmt.png)\n\n```sql\nalter_system_kill_session_stmt: 'ALTER SYSTEM KILL SESSION' number\n```\n\nTerminates a specific session with a SessionID.\n\nHowever, only the SYS user can execute this statement and can not KILL their own session\n\n## CANCEL SESSION\n\n**alter_system_cancel_session_stmt:**\n\n![alter_system_cancel_session_stmt](/en/sql-ref/sys_image/alter_system_cancel_session_stmt.png)\n\n```sql\nalter_system_cancel_session_stmt ::= 'ALTER SYSTEM CANCEL SESSION' number\n```\n\nCancels a specific session with a SessionID.\n\nRather than disconnecting the connection, it cancels the action being performed and returns an error code to the user that the action was aborted. However, like KILL, you can not cancel your own connected sessions.\n\n## CHECK DISK_USAGE\n\n**alter_system_check_disk_stmt:**\n\n![alter_system_check_disk_stmt](/en/sql-ref/sys_image/alter_system_check_disk_stmt.png)\n\n```sql\nalter_system_check_disk_stmt ::= 'ALTER SYSTEM CHECK DISK_USAGE'\n```\n\nCorrects the value of DC_TABLE_FILE_SIZE, which indicates the disk usage of the log table in V$STORAGE.\n\nDisk usage may be inaccurate when process failures or power failures occur. This command reads the correct value from the file system. However, it should be avoided because it can put a considerable load on the file system.\n\n## INSTALL LICENSE\n\n**alter_system_install_license_stmt:**\n\n![alter_system_install_license_stmt](/en/sql-ref/sys_image/alter_system_install_license_stmt.png)\n\n```sql\nalter_system_install_license_stmt ::= 'ALTER SYSTEM INSTALL LICENSE'\n```\n\nInstalls the license file in the default location of the license file ($MACHBASE_HOME/conf/license.dat).\n\nIt is installed after determining whether the license is suitable for installation.\n\n## INSTALL LICENSE (PATH)\n\n**alter_system_install_license_path_stmt:**\n\n![alter_system_install_license_path_stmt](/en/sql-ref/sys_image/alter_system_install_license_path_stmt.png)\n\n```sql\nalter_system_install_license_path_stmt: ::= 'ALTER SYSTEM INSTALL LICENSE' '=' \"'\" path \"'\"\n```\n\nInstalls the license file in a specific location.\n\nAn error occurs when you enter a license file that does not exist at that location or is incorrect. The path must be an absolute path. It is installed after determining whether the license is suitable for installation.\n\n## SET\n\n**alter_system_set_stmt:**\n\n![alter_system_set_stmt](/en/sql-ref/sys_image/alter_system_set_stmt.png)\n\n```sql\nalter_system_set_stmt ::= 'ALTER SYSTEM SET' prop_name '=' value\n```\n\nThe list of properties that can be modified is as follows.\n* QUERY_PARALLEL_FACTOR\n* DEFAULT_DATE_FORMAT\n* TRACE_LOG_LEVEL\n* PAGE_CACHE_MAX_SIZE\n* MAX_SESSION_COUNT\n* SESSION_IDLE_TIMEOUT_SEC\n* PROCESS_MAX_SIZE\n* TAG_CACHE_MAX_MEMORY_SIZE\n\n\n# ALTER SESSION\n\nThis is the syntax for managing resources or changing settings on a per-session basis.\n\n## SET SQL_LOGGING\n\n**alter_session_sql_logging_stmt:**\n\n![alter_session_sql_logging_stmt](/en/sql-ref/sys_image/alter_session_sql_logging_stmt.png)\n\n```sql\nalter_session_sql_logging_stmt ::= 'ALTER SESSION SET SQL_LOGGING' '=' flag\n```\n\nDetermines whether to leave a message in the Trace Log of the session.\n\nYou can use this message as a Bit Flag with the following values:\n* 0x1: Parsing, Validation, Optimization.\n* 0x2: It leaves the result of performing DDL.\n\nThat is, when the value of the corresponding flag is 2, only the DDL is logged, and when the flag is 3, the error and DDL are logged together.\nBelow is an example of changing the logging flag of the session and leaving error logging.\n\n```sql\nMach> alter session set SQL_LOGGING=1;\nAltered successfully.\nMach> exit\n```\n\n## SET DEFAULT_DATE_FORMAT\n\n**alter_session_set_defalut_dateformat_stmt:**\n\n![alter_session_set_defalut_dateformat_stmt](/en/sql-ref/sys_image/alter_session_set_defalut_dateformat_stmt.png)\n\n```sql\nalter_session_set_defalut_dateformat_stmt ::= 'ALTER SESSION SET DEFAULT_DATE_FORMAT' '=' date_format\n```\nSets the default format for Datetime data types for this session.\n\nWhen the server is started, the property **DEFAULT_DATE_FORMAT** is set to the session attribute. \nIf the property of the property has not changed, the value of the session will also be \"YYYY-MM-DD HH24: MI: SS mmm: uuu: nnn\". \nUse this command to modify the default format of a datetime datatype for a specific user, regardless of the system.\nV$session has a default date format set for each session and can be checked. Below is an example of checking and changing the value of the session.\n\n```sql\nMach> CREATE TABLE time_table (time datetime);\nCreated successfully.\n \nMach> SELECT DEFAULT_DATE_FORMAT from v$session;\ndefault_date_format                                                              \n-----------------------------------------------\nYYYY-MM-DD HH24:MI:SS mmm:uuu:nnn                                                \n[1] row(s) selected.\n \nMach> INSERT INTO time_table VALUES(TO_DATE('2016-11-11'));\n[ERR-00300 : Invalid date format or input string.([2016-11-11]:[%Y-%m-%d %H:%M:%S %0:%1:%2])]\n \nMach> ALTER SESSION SET DEFAULT_DATE_FORMAT='YYYY-MM-DD';\nAltered successfully.\n \nMach> SELECT DEFAULT_DATE_FORMAT from v$session;\n \ndefault_date_format                                                              \n----------------------------------------------\nYYYY-MM-DD                                                                       \n[1] row(s) selected.\n \nMach> INSERT INTO time_table VALUES(TO_DATE('2016-11-11'));\n1 row(s) inserted.\n \nMach> SELECT * FROM time_table;\n \nTIME                              \n----------------------------------\n2016-11-11\n \n[1] row(s) selected.\n```\n\n## SET SHOW_HIDDEN_COLS\n\n**alter_session_set_hidden_column_stmt:**\n\n![alter_session_set_hidden_column_stmt](/en/sql-ref/sys_image/alter_session_set_hidden_column_stmt.png)\n\n```sql\nalter_session_set_hidden_column_stmt ::= 'ALTER SESSION SET SHOW_HIDDEN_COLS' '=' ( '0' | '1' )\n```\n\nDecides whether to output the hidden column (_arrival_time) in the column represented by * when executing the select of the session.\n\nWhen the server is started, the value of the global property SHOW_HIDDEN_COLS is set to 0 for the session attribute. \nIf you want to change the default behavior of your session, you can set this value to 1.\nV$session has a SHOW_HIDDEN_COLS value set for each session.\n\n\n```sql\nMach> SELECT * FROM  v$session;\nID                   CLOSED      USER_ID     LOGIN_TIME                      SQL_LOGGING SHOW_HIDDEN_COLS\n-----------------------------------------------------------------------------------------------------------------\nDEFAULT_DATE_FORMAT                                                               HASH_BUCKET_SIZE\n------------------------------------------------------------------------------------------------------\n1                    0           1           2015-04-29 17:23:56 248:263:000 3           0\nYYYY-MM-DD HH24:MI:SS mmm:uuu:nnn                                                 20011\n[1] row(s) selected.                            \nMach> ALTER SESSION SET SHOW_HIDDEN_COLS=1;\nAltered successfully.\nMach> SELECT * FROM v$session;\n_ARRIVAL_TIME                   ID                   CLOSED      USER_ID     LOGIN_TIME                      SQL_LOGGING\n--------------------------------------------------------------------------------------------------------------------------------\nSHOW_HIDDEN_COLS DEFAULT_DATE_FORMAT                                                               HASH_BUCKET_SIZE\n------------------------------------------------------------------------------------------------------------------------\n1970-01-01 09:00:00 000:000:000 1                    0           1           2015-04-29 17:23:56 248:263:000 3\n1           YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn                                                 20011\n[1] row(s) selected.\n```\n\n## SET FEEDBACK_APPEND_ERROR\n\n**alter_session_set_feedback_append_err_stmt:**\n\n![alter_session_set_feedback_append_err_stmt](/en/sql-ref/sys_image/alter_session_set_feedback_append_err_stmt.png)\n\n```sql\nalter_session_set_feedback_append_err_stmt ::= 'ALTER SESSION SET FEEDBACK_APPEND_ERROR' '=' ( '0' | '1' )\n```\n\nSets whether to send the session's Append error message to the client program.\n\nUse the following values ​​for the error message.\n* 0 = Do not send an error message.\n* 1 = Send an error message.\nBelow is an example of use.\n\n```sql\nmach> ALTER SESSION SET FEEDBACK_APPEND_ERROR=0;\nAltered successfully.\n```\n\n## SET MAX_QPX_MEM\n\n**alter_session_set_max_qpx_mem_stmt:**\n\n![alter_session_set_max_qpx_mem_stmt](/en/sql-ref/sys_image/alter_session_set_max_qpx_mem_stmt.png)\n\n```sql\nalter_session_set_max_qpx_mem_stmt ::= 'ALTER SESSION SET MAX_QPX_MEM' '=' value\n```\n\nSpecifies the maximum amount of memory that a single SQL statement in the session will use when performing GROUP BY, DISTINCT, ORDER BY operations.\n\nIf you try to allocate more memory than the maximum memory, the system cancels the execution of the SQL statement and treats it as an error. \nIn case of error, record the error code and error message in machbase.trc including the query.\n\n```sql\nMach> ALTER SESSION SET MAX_QPX_MEM=1073741824;\nAltered successfully.\n \nMach> SELECT * FROM v$session;\nID                   CLOSED      USER_ID     LOGIN_TIME                      CLIENT_TYPE                                                                      \n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUSER_NAME                                                                         USER_IP                                                                           SQL_LOGGING SHOW_HIDDEN_COLS\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nFEEDBACK_APPEND_ERROR DEFAULT_DATE_FORMAT                                                               HASH_BUCKET_SIZE MAX_QPX_MEM          RS_CACHE_ENABLE      RS_CACHE_TIME_BOUND_MSEC\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nRS_CACHE_MAX_MEMORY_PER_QUERY RS_CACHE_MAX_RECORD_PER_QUERY RS_CACHE_APPROXIMATE_RESULT_ENABLE IDLE_TIMEOUT         QUERY_TIMEOUT       \n-----------------------------------------------------------------------------------------------------------------------------------------------\n14                   0           1           2021-03-08 16:33:01 503:181:809 CLI                                                                              \nNULL                                                                              192.168.0.194                                                                     11          0               \n1                     YYYY-MM-DD HH24:MI:SS mmm:uuu:nnn                                                 20011            1073741824           1                    1000                    \n16777216                      50000                         0                                  0                    0                   \n[1] row(s) selected.\nElapsed time: 0.001\n```\n\n- trc error when using more than the maximum memory size in an SQL statement\n\n```sql\n[2021-03-08 16:36:32 P-69000 T-140515328653056][INFO] DML FAILURE (2E10000084:Memory allocation error (alloc'd: 1048595, max: 1048576).)\n```\n\n- machsql error message when using more than the maximum memory size in an SQL statement\n\n```sql\nMach> select * from tag order by value DESC, time ASC;\nNAME                  TIME                            VALUE                      \n--------------------------------------------------------------------------------------\n[ERR-00132: Memory allocation error (alloc'd: 1048595, max: 1048576).]\n[0] row(s) selected.\nElapsed time: 0.447\n```\n\n## SET SESSION_IDLE_TIMEOUT_SEC\n\n**alter_session_set_session_idle_timeout_sec_stmt:**\n\n![alter_session_set_session_idle_timeout_sec_stmt](/en/sql-ref/sys_image/alter_session_set_session_idle_timeout_sec_stmt.png)\n\n```sql\nalter_session_set_session_idle_timeout_sec_stmt ::= 'ALTER SESSION SET SESSION_IDLE_TIMEOUT_SEC' '=' value\n```\n\nSpecifies the duration of the connection when the session is idle.\nIt is specified in seconds, and the session is terminated when the set time in the idle state elapses.\nYou can inquire the idle timeout time set in the session in v$session.\n\n```sql\nMach> ALTER SESSION SET SESSION_IDLE_TIMEOUT_SEC=200;\nAltered successfully.\n \n \nMach> SELECT IDLE_TIMEOUT FROM V$SESSION;\nIDLE_TIMEOUT        \n-----------------------\n200                                     \n[1] row(s) selected.\n```\n\n## SET QUERY_TIMEOUT\n\n**alter_session_set_query_timeout_stmt:**\n\n![alter_session_set_query_timeout_stmt](/en/sql-ref/sys_image/alter_session_set_query_timeout_stmt.png)\n\n```sql\nalter_session_set_query_timeout_stmt ::= 'ALTER SESSION SET QUERY_TIMEOUT' '=' value\n```\n\nThis is the time to wait for a response from the server when performing query in the session.\nIt is specified in seconds, and when the response from the server exceeds the specified time after executing the query, the query is terminated.\nYou can inquire the QUERY_TIME in the session in v$session\n\n```sql\nMach> ALTER SESSION SET QUERY_TIMEOUT=200;\nAltered successfully.\n \nMach> SELECT QUERY_TIMEOUT FROM V$SESSION;\nQUERY_TIMEOUT        \n-----------------------\n200                                     \n[1] row(s) selected.\n```"
					}
					
				
		
				
					,
					
					"feature-tables-tag-html": {
						"id": "feature-tables-tag-html",
						"title": "Tag Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/tag.html",
						"content": "# Concept\n\n![tag](/en/feature-tables/tag.png)\n\nThe TAG table is responsible for data storage and related additional information management for sensor data processing.\n\nThe TAG table provides three conceptual data processing spaces as described below.\n\n## Sensor Partition\n\nThis is an internal sensor data table based on the schema defined by the user when TAG table is created.\n\nThis data can be extracted through a SELECT query on the TAG table.\n\nThis table has very strong data management capabilities as listed below.\n1. Tens of thousands to Hundreds of thousands of sensor data can be loaded at high speed.\n2. Tens of thousands of sensor data can be retrieved at high speed given the time range condition.\n3. Real-time compression allows long-term storage of sensor data.\n4. Chronological deletion of sensor data beginning with the oldest is possible.\n\n\nThe user sensor data to be stored is, in basic, a time series data and is a specific data type with the corresponding tag of the name, time, and 64-bit real value.\n\n|TagName(User defined length string)|Time(64bit)|Real value(64bit)|(User defined Extended Columns...)|\n|--|--|--|--|\n\n## ROLLUP Partition\n\nThis is an internal table that automatically generates statistical data based on the sensor data stored in the sensor storage.\n\nThis was developed in order to obtain statistical data over a long period of time within a few seconds in real time analysis.\n\nROLLUP table supports three types of tables: hour, minute, and second; and five types of statistics functions: MIN, MAX, AVG, SUM, COUNT.\n\nTo get this ROLLUP result value, you must execute a SELECT query with HINT in the TAG table.\n\n## META Table\n\nThis is another table that stores the name and additional meta information for TAG data.\n\nUsers can generate this meta information with the INSERT clause and manipulate it in several ways through SELECT, UPDATE, and DELETE\n\n## Duplication removal\n\nThis is a function that automatically removes duplicated data.\nIf the TAG name and time of newly inserted data matches those of existing data within a predefined duration (with a maxmum duration of 30 days) those redundant data will be automatically deleted.\n\n# How to use Tag table\n\n* [Creating and Dropping Tag table](/en/feature-tables/tag/create-drop.html)\n* [Managing tag meta (tag name)](/en/feature-tables/tag/managing.html)\n* [Manipulating tag data](/en/feature-tables/tag/manipulate.html)\n* [Creating and Selecting in Rollup Table](/en/feature-tables/tag/create-select.html)\n* [An example of tag table](/en/feature-tables/tag/ex.html)\n* [Index for tag table](/en/feature-tables/tag/)\n* [Duplication removal](/en/feature-tables/tag/duplication-removal.html)"
					}
					
				
		
				
					,
					
					"pages-tags-html": {
						"id": "pages-tags-html",
						"title": "Tags Index",
						"version": "all",
						"categories": "",
						"url": " /pages/tags.html",
						"content": "Tags Index\n{% capture site_tags %}{% for tag in site.tags %}{% if tag %}{{ tag | first }}{% unless forloop.last %},{% endunless %}{% endif %}{% endfor %}{% endcapture %}{% assign docs_tags = \"\" %}{% for doc in site.docs %}{% assign ttags = doc.tags | join:',' | append:',' %}{% assign docs_tags = docs_tags | append:ttags %}{% endfor %}\n{% assign all_tags = docs_tags | append:site_tags %}{% assign tags_list = all_tags | split:',' | uniq | sort %}\n\n{% for tag in tags_list %}{% if tag %}{{ tag }}\n\n    {% for post in site.tags[tag] %}\n    {{- post.title -}}\n     {{- post.date | date: \"%B %d, %Y\" -}}\n{% endfor %}\n{% for doc in site.docs %}{% if doc.tags contains tag %}\n\n    {{ doc.title }}\n         {{- doc.date | date: \"%B %d, %Y\" -}}\n    {% endif %}{% endfor %}\n{% endif %}{% endfor %}"
					}
					
				
		
				
					,
					
					"feature-tables-log-extract-text-search-html": {
						"id": "feature-tables-log-extract-text-search-html",
						"title": "Text Search",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/extract/text-search.html",
						"content": "# Index \n\n* [SEARCH](#search)\n* [Multilingual Search](#multilingual-search)\n* [ESEARCH](#esearch)\n* [REGEXP](#regexp)\n* [LIKE](#like)\n\nThis document deals with text search using keyword indexes.\n\nText search is faster than comparable DBMS LIKE search because it searches a special kind of index called \"reverse index\" to search the desired string pattern. Keyword indexes can only be created for varchar and text type columns, which are variable-length character columns. However, the search target string must match exactly. Machbase does not perform keywords based on special characters or morphological analysis.\n\n\n# SEARCH\n\n```sql\nSELECT  column_name(s)\nFROM    table_name\nWHERE   column_name\nSEARCH  pattern;\n```\n\n```sql\nMach> CREATE TABLE search_table (id INTEGER, name VARCHAR(20));\nCreated successfully.\n \nMach> CREATE INDEX idx_SEARCH ON SEARCH_table (name) INDEX_TYPE KEYWORD;\nCreated successfully.\n \nMach> INSERT INTO search_table VALUES(1, 'time flys');\n1 row(s) inserted.\n \nMach> INSERT INTO search_table VALUES(1, 'time runs');\n1 row(s) inserted.\n \nMach> SELECT * FROM search_table WHERE name SEARCH 'time' OR name SEARCH 'runs2' ;\nID          NAME\n-------------------------------------\n1           time runs\n1           time flys\n[2] row(s) selected.\n \nMach> SELECT * FROM search_table WHERE name SEARCH 'time' AND name SEARCH 'runs2' ;\nID          NAME\n-------------------------------------\n[0] row(s) selected.\n \nMach> SELECT * FROM search_table WHERE name SEARCH 'flys' OR name SEARCH 'runs2' ;\nID          NAME\n-------------------------------------\n1           time flys\n[1] row(s) selected.\n```\n\n\n# Multilingual Search\n\nMachbase can search variable-length strings of various kinds of languages ​​stored in ASCII and UTF-8. In order to search only part of a sentence in a language such as Korean or Japanese, a 2-gram technique is used.\n\n```sql\nSELECT  column_name(s)\nFROM    table_name\nWHERE   column_name\nSEARCH  pattern;\n```\n\n```sql\nMach> CREATE TABLE multi_table (message varchar(100));\nCreated successfully.\n \nMach> CREATE INDEX idx_multi ON multi_table(message)INDEX_TYPE KEYWORD;\nCreated successfully.\n \nMach> INSERT INTO multi_table VALUES(\"Machbase is the combination of ideal solutions\");\n1 row(s) inserted.\n \nMach> INSERT INTO multi_table VALUES(\"Machbase is a columnar DBMS\");\n1 row(s) inserted.\n \nMach> INSERT INTO multi_table VALUES(\"Machbaseは理想的なソリューションの組み合わせです\");\n1 row(s) inserted.\n \nMach> INSERT INTO multi_table VALUES(\"Machbaseは円柱状のDBMSです\");\n1 row(s) inserted.\n \nMach>  SELECT * from multi_table WHERE message SEARCH 'Machbase DBMS';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbaseは円柱状のDBMSです\nMachbase is a columnar DBMS\n[2] row(s) selected.\n \nMach> SELECT * from multi_table WHERE message SEARCH 'DBMS is';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbase is a columnar DBMS\n[1] row(s) selected.\n \nMach> SELECT * from multi_table WHERE message SEARCH 'DBMS' OR message SEARCH 'ideal';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbaseは円柱状のDBMSです\nMachbase is a columnar DBMS\nMachbase is the combination of ideal solutions\n[3] row(s) selected.\n \nMach> SELECT * from multi_table WHERE message SEARCH '組み合わせ';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbaseは理想的なソリューションの組み合わせです\n[1] row(s) selected.\nElapsed time: 0.001\nMach> SELECT * from multi_table WHERE message SEARCH '円柱';\nMESSAGE\n------------------------------------------------------------------------------------\nMachbaseは円柱状のDBMSです\n[1] row(s) selected.\n```\n\nWhen the input data is \"대한민국\", three words of \"대한,\" \"한민,\" and \"민국\" are recorded in the index. Therefore, you can search for \"대한민국\" with the keywords \"대한\" or \"민국\".\n\nBasically, the keywords entered in the search statement are searched by the AND condition, so even if you enter only three words, the result is displayed very accurately. For example, if the search target keyword is a \"computer utilization guide\", the three words \"computer\", \"utilization\", and \"guide\" are set as AND conditions.\n\n# ESEARCH\n\nThe ESEARCH operator is used to expand the search target keyword. The search target keyword must be ASCII. Search keywords can be set using the % character. Using a keyword that begins with the % character, such as the LIKE conditional, searches all records, but searches for this condition on words in the keyword index, which makes searching faster than LIKE. This feature is useful for quickly searching for alphabet strings (such as error statements or code).\n\n```sql\nSELECT  column_name(s)\nFROM    table_name\nWHERE   column_name\nESEARCH pattern;\n```\n\n```sql\nMach> CREATE TABLE esearch_table(id INTEGER, name VARCHAR(20), data VARCHAR(40));\nCreated successfully.\n \nMach> CREATE INDEX idx1 ON esearch_table(name) INDEX_TYPE KEYWORD;\nCreated successfully.\n \nMach> CREATE INDEX idx2 ON esearch_table(data) INDEX_TYPE KEYWORD;\nCreated successfully.\n \nMach> INSERT INTO esearch_table VALUES(1, 'machbase', 'Real-time search technology');\n1 row(s) inserted.\n \nMach> INSERT INTO esearch_table VALUES(2, 'mach2flux', 'Real-time data compression');\n1 row(s) inserted.\n \nMach> INSERT INTO esearch_table VALUES(3, 'DB MS', 'Memory cache technology');\n1 row(s) inserted.\n \nMach> INSERT INTO esearch_table VALUES(4, 'ファ ッションアドバイザー、', 'errors');\n1 row(s) inserted.\n \nMach> INSERT INTO esearch_table VALUES(5, '인피 니 플럭스', 'socket232');\n1 row(s) inserted.\n \nMach> SELECT * FROM esearch_table where name ESEARCH '%mach';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n1           machbase            Real-time search technology\n[1] row(s) selected.\nElapsed time: 0.001\nMach> SELECT * FROM esearch_table where data ESEARCH '%echn%';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n3           DB MS                 Memory cache technology\n1           machbase            Real-time search technology\n[2] row(s) selected.\n \nMach> SELECT * FROM esearch_table where name ESEARCH '%피니%럭스';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n[0] row(s) selected.\n \nMach> SELECT * FROM esearch_table where data ESEARCH '%232';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n5           인피 니 플럭스  socket232\n[1] row(s) selected.\n```\n\n\n# REGEXP\n\nThe REGEXP operator is used to perform a text search on data through a regular expression. The REGEXP operator is executed by performing a regular expression on the target column, and because the index is not available, the search performance may be degraded. Therefore, it is a good idea to add another search condition that can use the index as an AND operator to improve the search speed.\n\nApplying a SEARCH or ESEARCH operator that can use an index before searching for a particular regular expression pattern is a good way to improve search performance by first reducing the result set and then using REGEXP.\n\n```sql\nMach> CREATE TABLE regexp_table(id INTEGER, name VARCHAR(20), data VARCHAR(40));\nCreated successfully.\n \nMach> INSERT INTO regexp_table VALUES(1, 'machbase', 'Real-time search technology');\n1 row(s) inserted.\n \nMach> INSERT INTO regexp_table VALUES(2, 'mach2base', 'Real-time data compression');\n1 row(s) inserted.\n \nMach> INSERT INTO regexp_table VALUES(3, 'DBMS', 'Memory cache technology');\n1 row(s) inserted.\n \nMach> INSERT INTO regexp_table VALUES(4, 'ファ ッショ', 'errors');\n1 row(s) inserted.\n \nMach> INSERT INTO regexp_table VALUES(5, '인피니플럭스', 'socket232');\n1 row(s) inserted.\n \nMach> SELECT * FROM regexp_table WHERE name REGEXP 'mach';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n2           mach2base           Real-time data compression\n1           machbase            Real-time search technology\n[2] row(s) selected.\n \nMach> SELECT * FROM regexp_table WHERE data REGEXP 'mach[1]';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n[0] row(s) selected.\n \nMach> SELECT * FROM regexp_table WHERE data REGEXP '[A-Za-z]';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n5           인피니플럭스  socket232\n4           ファ ッショ      errors\n3           DBMS                  Memory cache technology\n2           mach2base           Real-time data compression\n1           machbase            Real-time search technology\n[5] row(s) selected.\n```\n\n\n# LIKE\n\nMachbase also supports the SQL standard LIKE operator. The LIKE operator is available in Korean, Japanese, and Chinese.\n\n```sql\nSELECT  column_name(s)\nFROM    table_name\nWHERE   column_name\nLIKE    pattern;\n```\n\nExample:\n\n```sql\nMach> CREATE TABLE like_table (id INTEGER, name VARCHAR(20), data VARCHAR(40));\nCreated successfully.\n \nMach> INSERT INTO like_table VALUES(1, 'machbase', 'Real-time search technology');\n1 row(s) inserted.\n \nMach> INSERT INTO like_table VALUES(2, 'mach2base', 'Real-time data compression');\n1 row(s) inserted.\n \nMach> INSERT INTO like_table VALUES(3, 'DBMS', 'Memory cache technology');\n1 row(s) inserted.\n \nMach> INSERT INTO like_table VALUES(4, 'ファ ッションアドバイザー、', 'errors');\n1 row(s) inserted.\n \nMach> INSERT INTO like_table VALUES(5, '인피 니 플럭스', 'socket232');\n1 row(s) inserted.\n \nMach> SELECT * FROM like_table WHERE name LIKE 'mach%';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n2           mach2base           Real-time data compression\n1           machbase            Real-time search technology\n[2] row(s) selected.\n \nMach> SELECT * FROM like_table WHERE name LIKE '%니%';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n5           인피 니 플럭스  socket232\n[1] row(s) selected.\n \nMach> SELECT * FROM like_table WHERE data LIKE '%technology';\nID          NAME                  DATA\n--------------------------------------------------------------------------------\n3           DBMS                  Memory cache technology\n1           machbase            Real-time search technology\n[2] row(s) selected.\n```"
					}
					
				
		
				
					,
					
					"install-linux-tgz-install-html": {
						"id": "install-linux-tgz-install-html",
						"title": "Tarball Installation",
						"version": "all",
						"categories": "",
						"url": " /install/linux/tgz-install.html",
						"content": "# Create User\n\nCreate a Linux user 'machbase' for installing and using  Machbase.\n\n```bash\nsudo useradd machbase\n```\n\nAfter setting the password,  log in as 'machbase' account.\n\n\n# Package Installation\n\nCreate a directory called 'macbase_home' and download and install the package from the Machbase download site.\n\n```bash\n[machbase@localhost ~]$ wget http://www.machbase.com/dist/machbase-fog-x.x.x.official-LINUX-X86-64-release.tgz\n[machbase@localhost ~]$ mkdir machbase_home\n[machbase@localhost ~]$ mv machbase-fog-x.x.x.official-LINUX-X86-64-release.tgz machbase_home/\n[machbase@localhost ~]$ cd machbase_home/\n[machbase@localhost machbase_home]$ tar zxf machbase-fog-x.x.x.official-LINUX-X86-64-release.tgz\n \n[machbase@loclahost machbase_home]$ ls -l\ndrwxrwxr-x  5 machbase machbase        64 Oct 30 16:10 3rd-party\ndrwxrwxr-x  2 machbase machbase      4096 Oct 30 16:10 bin\ndrwxrwxr-x  2 machbase machbase       306 Jan  2 11:36 conf\ndrwxrwxr-x  2 machbase machbase       136 Jan  2 11:37 dbs\ndrwxrwxr-x  3 machbase machbase        22 Oct 30 16:10 doc\ndrwxrwxr-x  2 machbase machbase        96 Oct 30 16:10 include\ndrwxrwxr-x  2 machbase machbase        29 Oct 30 16:10 install\ndrwxrwxr-x  2 machbase machbase       283 Oct 30 16:10 lib\n-rw-rw-r--  1 machbase machbase 139888377 Dec 20 11:33 machbase-fog-x.x.x.official-LINUX-X86-64-release.tgz\ndrwxrwxr-x  2 machbase machbase        22 Dec 21 15:43 msg\n \ndrwxrwxr-x  2 machbase machbase         6 Oct 30 16:10 package\ndrwxrwxr-x 12 machbase machbase       140 Oct 30 16:10 sample\ndrwxrwxr-x  2 machbase machbase      4096 Jan  2 09:37 trc\ndrwxrwxr-x 10 machbase machbase       160 Oct 30 16:10 tutorials\ndrwxrwxr-x  3 machbase machbase        19 Oct 30 16:10 webadmin\n \n[machbase@loclahost machbase_home]$\n```\n\nThe directory descriptions installed are as follows.\n\n|Direcotry|Description|\n|--|--|\n|bin|Executable files|\n|conf|Configuration files|\n|dbs|Data storage space|\n|doc|License files|\n|include|Various header files for the CLI program|\n|install|mk files for Makefile|\n|lib|Various libraries|\n|msg|Machbase server error messages|\n|package|(Cluster edition) Path to save the added package|\n|sample|Various example files|\n|trc|Machbase server logs and trace contents|\n|webadmin|MWA Web Server Files|\n|3rd-party|Grafana plug-in files|\n\n\n# Set Environment Variable\n\nAdd Machbase-related environment variables to your .bashrc file.\n\n```bash\n\nexport MACHBASE_HOME=/home/machbase/machbase_home\nexport PATH=$MACHBASE_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$MACHBASE_HOME/lib:$LD_LIBRARY_PATH\n \n# Apply the changes with the following command.\nsource .bashrc\n```\n\n\n# Set Machbase Property\n\nThe $MACHBASE_HOME/conf directory contains the file macbase.conf.sample.\n\n```bash\n\n[machbase@localhost ~]$ cd $MACHBASE_HOME/conf\n[machbase@localhost conf]$ ls -l\n-rw-rw-r-- 1 machbase machbase   106 Oct 30 16:10 machtag.sql.sample\n-rw-rw-r-- 1 machbase machbase 17556 Oct 30 16:10 machbase.conf.sample\n \n[machbase@localhost conf]$\n```\n\nYou can also change the Machbase connection port number using the Linux environment variable. Below is an example of switching to a different port number (7878)  other than the default value (5656).\n\n```bash\nexport MACHBASE_PORT_NO=7878\n```\n\n\n# Machbase Simple Usage\n\n## Create Database\n\nTo create the database, use the machadmin utility. You can see the command with the --help option.\n\n```bash\n[machbase@localhost machbase_home]$ machadmin --help\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\n>\n  -u, --startup                         Startup Machbase server.\n      --recovery[=simple,complex,reset] Recovery mode. (default: simple)\n  -s, --shutdown                        Shutdown Machbase server.\n  -c, --createdb                        Create Machbase database.\n  -d, --destroydb                       Destroy Machbase database.\n  -k, --kill                            Terminate Machbase server.\n  -i, --silence                         Produce less output.\n  -r, --restore                         Restore Machbase database.\n  -x, --extract                         Extract BackupFile to BackupDirectory.\n  -w, --viewimage                       Display information of BackupImageFile.\n  -e, --check                           Check whether Machbase Server is running.\n  -t, --licinstall                      Install the license file.\n  -f, --licinfo                         Display information of installed license file.\n \n[machbase@localhost machbase_home]$\n```\n\nCreate a database with the -c option.\n\n```bash\n\n[machbase@localhost machbase_home]$ machadmin -c\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDatabase created successfully.\n[machbase@localhost machbase_home]$\n```\n\n## Launch Machbase Server\n\nRun the Machbase server with the -u option.\n\n```bash\n\n[machbase@localhost machbase_home]$ machadmin -u\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for Machbase server start.\nMachbase server started successfully.\n[machbase@localhost machbase_home]$\n```\n\nYou can see that the server daemon, machbased, is running through the ps command as shown below.\n\n```bash\n[machbase@localhost machbase_home]$  ps -ef |grep machbased\nmachbase 11178     1  2 11:25 ?        00:00:01 /home/machbase/machbase_home/bin/machbased -s --recovery=simple\nmachbase 11276  9867  0 11:26 pts/1    00:00:00 grep --color=auto machbased\n[machbase@localhost machbase_home]$\n```\n\n## Machbase Server Connection\n\nConnect to the Machbase server using an access utility called machsql.\n\nThe administrator account SYS is ready and the password is set to MANAGER.\n\n```bash\n[machbase@localhost machbase_home]$  machsql\n=================================================================\n     Machbase Client Query Utility\n     Release Version x.x.x.official\n     Copyright 2014 MACHBASE Corporation or its subsidiaries.\n     All Rights Reserved.\n=================================================================\nMachbase server address (Default:127.0.0.1) :\nMachbase user ID  (Default:SYS)\nMachbase User Password :\nMACHBASE_CONNECT_MODE=INET, PORT=5656\nType 'help' to display a list of available commands.\nMach>\n```\n\nLet's create a simple table and input / output data.\n\n```sql\ncreate table hello( id integer );\ninsert into hello values( 1 );\ninsert into hello values( 2 );\nselect * from hello;\nselect _arrival_time, * from hello;\n```\n\n```sql\nMach> create table hello( id integer );\nCreated successfully.\nElapsed time: 0.054\nMach> insert into hello values( 1 );\n1 row(s) inserted.\nElapsed time: 0.000\nMach> insert into hello values( 2 );\n1 row(s) inserted.\nElapsed time: 0.000\nMach> select * from hello;\nID\n--------------\n2\n1\n[2] row(s) selected.\nElapsed time: 0.000\nMach> select _arrival_time, * from hello;\n_arrival_time                   ID\n-----------------------------------------------\n2019-01-02 11:33:00 122:806:804 2\n2019-01-02 11:32:57 383:848:361 1\n[2] row(s) selected.\nElapsed time: 0.000\nMach>\n```\n\nThe above SELECT results show that the most recently input data is displayed first.\n\nAlso, it can be seen through the _arrival_time column that the input time of the record is set to the nanosecond.\n\n## Stop Machbase Server\n\nShut down the Machbase server with the -s option.\n\n```bash\n[machbase@localhost machbase_home]$ machadmin -s\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nWaiting for Machbase server shut down...\nMachbase server shut down successfully.\n[machbase@localhost machbase_home]$\n```\n\n## Delete Database\n\nDelete the database with the -d option.\n**Be very careful because all data will be deleted.**\n\n```bash\n[machbase@localhost machbase_home]$ machadmin -d\n-----------------------------------------------------------------\n     Machbase Administration Tool\n     Release Version - x.x.x.official\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-----------------------------------------------------------------\nDestroy Machbase database. Are you sure?(y/N) y\nDatabase destoryed successfully.\n[machbase@localhost machbase_home]$\n```"
					}
					
				
		
				
					,
					
					"feature-tables-log-extract-time-series-html": {
						"id": "feature-tables-log-extract-time-series-html",
						"title": "Time Series Data Retrieval",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/log/extract/time-series.html",
						"content": "# Index \n\n* [DURATION...BEFORE](#durationbefore)\n    * [Search Based On Absolute Time Value](#search-based-on-absolute-time-value)\n    * [Search Based On Relative Time Value](#search-based-on-relative-time-value)\n* [DURATION...AFTER](#durationafter)\n* [DURATION...FROM/TO](#durationfromto)\n\nThe DURATION clause of the SELECT statement defines the time condition to be searched. The main reason for using the DURATION clause is to improve the performance even when retrieving a large amount of data by reducing the search target.\n\nSince Machbase divides and stores the data based on the input time, the data is easily searched based on the time condition. The input time is stored in the auto generated column named '_ARRIVAL_TIME', not the user defined column. Therefore, in order to use Machbase most efficiently, it is better to use the built-in '_ARRIVAL_TIME' column without specifying an additional time column.\n\nMachbase outputs data in the reverse order of the input order. In other words, the newest  data is outputted first, and the oldest data is outputted later. Generally, when retrieving time series data, the most recent data is more important and often needs to be obtained first. Also, the data output by all DURATION conditionals is outputted from the latest to the last. If you want to output the reverse, from the past to the latest, you should use the AFTER clause. The syntax is as follows.\n\n\n# Syntax\n\n```sql\nDURATION    time_expression [BEFORE time_expression | TO_DATE(time) ];\nDURATION    time_expression [AFTER TO_DATE(time)]; \ntime_expression\n -  ALL\n -  n   year\n -  n   month\n -  n   week\n -  n   day\n -  n   hour   \n -  n   minute \n -  n   second\n```\n\n\n# DURATION...BEFORE\n\nAs mentioned earlier, explicit or undefined use of BEFORE (automatically applying BEFORE) outputs data in the order of the most recent to the oldest.\n\nYou can query data by absolute time value or relative time value.\n\n## Search Based On Absolute Time Value\n\n```sql\nMach> CREATE TABLE time_table (id INTEGER);\nCreated successfully.\n \nMach> INSERT INTO time_table(_arrival_time, id) VALUES(TO_DATE('2014-6-12 10:00:00', 'YYYY-MM-DD HH24:MI:SS'), 1);\n1 row(s) inserted.\n \nMach> INSERT INTO time_table(_arrival_time, id) VALUES(TO_DATE('2014-6-12 11:00:00', 'YYYY-MM-DD HH24:MI:SS'), 2);\n1 row(s) inserted.\n \nMach> INSERT INTO time_table(_arrival_time, id) VALUES(TO_DATE('2014-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS'), 3);\n1 row(s) inserted.\n \nMach> INSERT INTO time_table(_arrival_time, id) VALUES(TO_DATE('2014-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS'), 4);\n1 row(s) inserted.\n \nMach> INSERT INTO time_table VALUES(5);\n1 row(s) inserted.\n \nMach> SELECT _arrival_time, * FROM time_table DURATION 1 MINUTE;\n_arrival_time                   ID\n-----------------------------------------------\n2017-02-16 12:17:01 880:937:028 5\n[1] row(s) selected.\n \nMach> SELECT _arrival_time, * FROM time_table DURATION 1 DAY BEFORE TO_DATE('2014-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2014-06-12 12:00:00 000:000:000 3\n2014-06-12 11:00:00 000:000:000 2\n2014-06-12 10:00:00 000:000:000 1\n[3] row(s) selected.\n```\n\n## Search Based On Relative Time Value\n\nA search based on relative time values can be viewed as a search based on the current time.\n\n```sql\nMach> CREATE TABLE relative_table(id INTEGER);\nCreated successfully.\n \nMach> INSERT INTO relative_table values(1);\n1 row(s) inserted.\n \n------ WAIT for 30 SECONDS before the second value ------\n \nMach> INSERT INTO relative_table values(2);\n1 row(s) inserted.\n \nMach> SELECT _arrival_time, * FROM relative_table;\n_arrival_time                   ID\n-----------------------------------------------\n2017-02-16 12:35:34 476:055:014 2\n2017-02-16 12:35:04 430:802:356 1\n[2] row(s) selected.\n \nMach> SELECT id FROM relative_table DURATION 30 second ;\nid\n--------------\n2\n[1] row(s) selected.\n \nMach> SELECT id FROM relative_table DURATION 60 second ;\nid\n--------------\n2\n1\n[2] row(s) selected.\n \nMach> SELECT id FROM relative_table DURATION 30 second BEFORE 30 second;\nid\n--------------\n1\n[1] row(s) selected.\n```\n\n# DURATION...AFTER\n\nWhen AFTER is applied, the data is outputted from the past to the latest.\n\nThe BEFORE command automatically outputs the data in reverse order based on the input time as compared to the past output.\n\n```sql\nMach> CREATE TABLE after_table (id INTEGER);\nCreated successfully.\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 10:00:00', 'YYYY-MM-DD HH24:MI:SS'), 1);\n1 row(s) inserted.\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 11:00:00', 'YYYY-MM-DD HH24:MI:SS'), 2);\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS'), 3);\n1 row(s) inserted.\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS'), 4);\n1 row(s) inserted.\n \nMach> INSERT INTO after_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 14:00:00', 'YYYY-MM-DD HH24:MI:SS'), 5);\n1 row(s) inserted.\n \nMach> select _arrival_time, * from after_table duration ALL after TO_DATE('2016-6-12 11:00:00', 'YYYY-MM-DD HH24:MI:SS');\n \n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 11:00:00 000:000:000 2\n2016-06-12 12:00:00 000:000:000 3\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 14:00:00 000:000:000 5\n[4] row(s) selected.\n \nMach> select _arrival_time, * from after_table duration ALL before TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 12:00:00 000:000:000 3\n2016-06-12 11:00:00 000:000:000 2\n2016-06-12 10:00:00 000:000:000 1\n[4] row(s) selected.\n```\n\n\n# DURATION...FROM/TO\n\nWhen the user tries to retrieve data based on two absolute times, a conditional form of the form \"DURATION FROM A TO B\" is used.\n\nA and B are absolute times and are expressed using the TO_DATE function. A and B can be set differently according to the user's intention. E.g,\n\n* When A comes after B, the search direction outputs the data in order of latest to oldest just as it is used for BEFORE.\n* When B comes before A, the search direction outputs the data in order of oldest to latest just as it is used for AFTER.\n\nThe following example shows how the data is output.\n\n```sql\nMach> CREATE TABLE from_table (id INTEGER);\nCreated successfully.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 10:00:00', 'YYYY-MM-DD HH24:MI:SS'), 1);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 11:00:00', 'YYYY-MM-DD HH24:MI:SS'), 2);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS'), 3);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS'), 4);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 14:00:00', 'YYYY-MM-DD HH24:MI:SS'), 5);\n1 row(s) inserted.\n \nMach> INSERT INTO from_table(_arrival_time, id) VALUES(TO_DATE('2016-6-12 15:00:00', 'YYYY-MM-DD HH24:MI:SS'), 6);\n1 row(s) inserted.\n \nMach> SELECT _arrival_time, * FROM from_table DURATION FROM TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 14:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 12:00:00 000:000:000 3\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 14:00:00 000:000:000 5\n[3] row(s) selected.\n \nMach> SELECT _arrival_time, * FROM from_table limit 2 DURATION FROM TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 15:00:00',\n'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 12:00:00 000:000:000 3\n2016-06-12 13:00:00 000:000:000 4\n[2] row(s) selected.\n \nMach> SELECT _arrival_time, * FROM from_table DURATION FROM TO_DATE('2016-6-12 15:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 12:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 15:00:00 000:000:000 6\n2016-06-12 14:00:00 000:000:000 5\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 12:00:00 000:000:000 3\n[4] row(s) selected.\n \nMach> SELECT _arrival_time, * FROM from_table LIMIT 2 duration FROM TO_DATE('2016-6-12 15:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 12:00:00',\n'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 15:00:00 000:000:000 6\n2016-06-12 14:00:00 000:000:000 5\n[2] row(s) selected.\n \nMach> SELECT _arrival_time, * from from_table duration FROM TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 13:00:00 000:000:000 4\n[1] row(s) selected.\n \nMach> SELECT _arrival_time, * from from_table duration FROM TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 20:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 13:00:00 000:000:000 4\n2016-06-12 14:00:00 000:000:000 5\n2016-06-12 15:00:00 000:000:000 6\n[3] row(s) selected.\n \nMach> SELECT _arrival_time, * from from_table duration FROM TO_DATE('2016-6-12 20:00:00', 'YYYY-MM-DD HH24:MI:SS') TO TO_DATE('2016-6-12 13:00:00', 'YYYY-MM-DD HH24:MI:SS');\n_arrival_time                   ID\n-----------------------------------------------\n2016-06-12 15:00:00 000:000:000 6\n2016-06-12 14:00:00 000:000:000 5\n2016-06-12 13:00:00 000:000:000 4\n[3] row(s) selected.\n```"
					}
					
				
		
				
					,
					
					"sdk-timezone-html": {
						"id": "sdk-timezone-html",
						"title": "Timezone",
						"version": "all",
						"categories": "",
						"url": " /sdk/timezone.html",
						"content": "# Index\n\n* [Timezone of Machbase](#timezone-of-machbase)\n* [Timezone Format in Machbase](#timezone-format-in-machbase)\n    * [machsql](#machsql)\n    * [machloader](#machloader)\n    * [SDK](#sdk)\n    * [Rest API](#rest-api)\n\n# Timezone of Machbase\n\nMachbase assumes that each client's Timezone is valid in each session.\n\nIn general, a time zone is specified as a string representing a specific time.\n\n```\n\"YYYY-MM-DD HH24:MI:SS ZZZ(Timezone String)\"\n\nExample) \n\"12:06:56.568+01:00\"  \n\"2006.07.10 at 15:08:56 -05:00\"\n\"09  AM, GMT+09:00\"\n```\n\nHowever, the above method not only has the inconvenience of having to designate a specific time based on the time zone every time, but also has a problem in that the amount of data transmission increases linearly when the time zone value is included for a large amount of data.\n\nTherefore, Machbase supports the method of specifying the time zone property for the session in which the client and server are connected.\n\nThe following is a step-by-step explanation of the time zone operation provided by Machbase.\n\n* The server operates based on the default time zone provided by the operating system in which the server is installed.\n    In other words, if no setting is made, Machbase reads and uses the time zone in which the OS operates.\n\n* If the client program connects to the server without setting the time zone, the client's time zone is set to the server's time zone.\n    That is, if the TIMEZONE set in the server is KST, it means that the client also operates in KST.\n\n* If the time zone is explicitly set in the client program, the corresponding session of the server operates in the time zone designated by the client.\n    That is, even if the TIMEZONE set in the server is KST, if the client sets the time zone as EDT when connecting, the session operates as EDT.\n\n# Timezone Format in Machbase\n\nMachbase provides only one format consisting of 5 characters to increase ease of use and remove complexity.\n\nThat is, the first character is a + or - sign indicating the sign of time, and the following two characters have a value between 00 and 23. And, it is assumed that the last two characters have a time from 00 to 59.\n\nThe following shows the format of TIMEZONE supported by Machbase.\n\n```\nex)\nTIMEZONE=+0900\nTIMEZONE=-0900\n```\n\n## machsql\n\nWhen machsql is started, you can set the time zone to operate through the following options.\n\n```\n-z, --timezone=+-HHMM\n```\n\nYou can check the currently set time zone through the SHOW TIMEZONE command.\n\n```\nSHOW TIMEZONE;\n\nMach> show timezone;\nTimezone : +0900\n```\n\n## machloader\n\nWhen running machloader, you can set the time zone to operate through the following options.\n\n```\n-z, --timezone=+-HHMM\n```\n\nIt connects to the designated time zone and time calculation operates based on the corresponding time zone.\n\n## SDK\n\nTIMEZONE has been added to the connection string, and the time zone for the session can be specified.\n\nIf TIMEZONE is not specified in the connection string, it operates based on the time zone of the server.\n\nThis is the same for CLI, ODBC, JDBC, and DOTNET.\n\nConnection String Example\n\n```\nSERVER=127.0.0.1;UID=SYS;PWD=MANAGER;CONNTYPE=1;NLS_USE=UTF8;PORT_NO=5656;TIMEZONE=+0300\n```\n\n## Rest API\n\nRest API operates based on the time zone specified in the HTTP protocol HEADER when requesting an operation.\n\nThe header is named The-Timezone-Machbase, and the usage is as follows.\n\n```\nAuthorization: Basic XXXXXXXXXXXXXXXXXXX\n...................\nThe-Timezone-Machbase: +0900\n...............\n```\n\nAs described above, you can specify the desired Timezone string.\n\nIf the Timezone is not specified, it operates as the Timezone of the server.\n\nRequest example: set to UTC\n\n```\ncurl -H \"The-Timezone-Machbase: +0000\" -G \"http://127.0.0.1:${ITF_HTTP_PORT}/machbase\" --data-urlencode 'q=select * from test_table order by c4 asc';\n{\n  \"error_code\": 0,\n  \"error_message\": \"\",\n  \"columns\": [\n    {\n      \"name\": \"C1\",\n      \"type\": 4,\n      \"length\": 6\n    },\n    {\n      \"name\": \"C2\",\n      \"type\": 8,\n      \"length\": 11\n    },\n    {\n      \"name\": \"C3\",\n      \"type\": 5,\n      \"length\": 20\n    },\n    {\n      \"name\": \"C4\",\n      \"type\": 6,\n      \"length\": 31\n    },\n    {\n      \"name\": \"C5\",\n      \"type\": 32,\n      \"length\": 15\n    }\n  ],\n  \"timezone\": \"+0000\",\n  \"data\": [\n    {\n      \"C1\": 1,\n      \"C2\": 2,\n      \"C3\": \"test1\",\n      \"C4\": \"1999-09-09 00:09:09 000:000:000\",\n      \"C5\": \"127.0.0.1\"\n    },\n  ]\n}\n```\n\nThe time zone value set in the \"timezone\" item is returned to the resulting JSON."
					}
					
				
		
				
					,
					
					"upgrade-upgrade-html": {
						"id": "upgrade-upgrade-html",
						"title": "Cluster Edition Upgrade",
						"version": "all",
						"categories": "",
						"url": " /upgrade/upgrade.html",
						"content": "# Index\n\n* [Coordinator Upgrade](#coordinator-upgrade)\n    * [Precautions](#precautions)\n    * [Coordinator Shutdown](#coordinator-shutdown)\n    * [Coordinator Backup (Optional)](#coordinator-backup-optional)\n    * [Coordinator Upgrade](#coordinator-upgrade)\n    * [Coordinator Startup](#coordinator-startup)\n* [Deployer Upgrade](#deployer-upgrade)\n    * [Precautions](#precautions)\n    * [Deployer Shutdown](#deployer-shutdown)\n    * [Deployer Backup (Optional)](#deployer-backup-optional)\n    * [Deployer Upgrade](#deployer-upgrade)\n    * [Deployer Startup](#deployer-startup)\n* [Package Registration](#package-registration)\n* [Broker/Warehouse Upgrade](#brokerwarehouse-upgrade)\n    * [Node Shutdown](#node-shutdown)\n    * [Node Upgrade](#node-upgrade)\n    * [Node Startup](#node-startup)\n* [Snapshot Failover](#snapshot-failover)\n    * [Snapshot basic concept](#snapshot-basic-concept)\n    * [How Snapshot Failover Works](#how-snapshot-failover-works)\n    * [Automatic Snapshot Execution](#automatic-snapshot-execution)\n    * [Take Snapshot manually](#take-snapshot-manually)\n    * [Recover scrapped node based on Snapshot](#recover-scrapped-node-based-on-snapshot)\n    * [Snapshot-based recovery process of scrapped nodes](#snapshot-based-recovery-process-of-scrapped-nodes)\n    * [Snapshot related properties](#snapshot-related-properties)\n\n\n# Coordinator Upgrade\n\nCoordinator / Deployer must be upgraded manually.\n\n## Precautions\n\n* You can not issue commands such as adding / starting / terminating / deleting nodes during the upgrade.\n* DDL or DELETE must not be in use. (INSERT, APPEND, SELECT do not matter.)\n\n## Coordinator Shutdown\n\n\nCoordinator / Deployer does not affect INSERT, APPEND, SELECT in Broker / Warehouse even if it is shut down. \n\nHowever, it does not detect that the Broker / Warehouse also shuts down while it is shutting down. (Normally detected after restart)\n\n```bash\nmachcoordinatoradmin --shutdown\n```\n\n## Coordinator Backup (Optional)\n\nBackup the dbs/ and conf/ directories located in $MACH_COORDINATOR_HOME.\n\n## Coordinator Upgrade\n\n* Proceed with full package instead of lightweight package.\n\nUnzip and overwrite the package to $MACH_COORDINATOR_HOME.\n\n```bash\ntar zxvf machbase-ent-new.official-LINUX-X86-64-release.tgz -C $MACHBASE_COORDINATOR_HOME\n```\n\n## Coordinator Startup\n\n```bash\nmachcoordinatoradmin --startup\n```\n\n\n# Deployer Upgrade\n\nThis has the same process as the Coordinator.\n\n## Precautions\n \n* You can not issue commands such as adding / starting / terminating / deleting nodes during the upgrade.\n\n## Deployer Shutdown\n\n```bash\nmachdeployeradmin --shutdown\n```\n\n## Deployer Backup (Optional)\n\nBack up the dbs/ and conf/ directories located in $MACH_DEPLOYER_HOME.\n\n## Deployer Upgrade\n\n* If you are running MWA or not running Collector on the Host the Deployer is installed, you can proceed with the lightweight package.\n\nUnzip and overwrite the package to $MACH_DEPLOYER_HOME.\n\n```bash\ntar zxvf machbase-ent-new.official-LINUX-X86-64-release.tgz -C $MACH_DEPLOYER_HOME\n```\n\n## Deployer Startup\n\n```bash\nmachdeployeradmin --startup\n```\n\n\n# Package Registration\n\nTo upgrade Broker / Warehouse, register the Package in Coordinator and proceed with the upgrade.\n\nFirst, move the package to the Host where $MACH_COORDINATOR_HOME is located.\n\nNext, add the package using the following command.\n\n```bash\nmachcoordinatoradmin --add-package=new_package --file-name=./machbase-ent-new.official-LINUX-X86-64-release-lightweight.tgz\n```\n\n|Option|Description|\n|--|--|\n|--add-package|Specifies the name of the package to add.|\n|--file-name|Specifies the path to the package file to add.**If a package with the same filename is added, you will receive an error, so check the file name.**|\n\n\n## Broker/Warehouse Upgrade\n\nIn the Coordinator, run the following command.\n\n# Node Shutdown\n\n```bash\nmachcoordinatoradmin --shutdown-node=localhost:5656\n```\n\n# Node Upgrade\n\n```bash\nmachcoordinatoradmin --upgrade-node=localhost:5656 --package-name=new_package\n```\n\n|Option|Description|\n|--|--|\n|--upgrade-node|Enters the name of the upgrade target Node.|\n|--package-name|Enters the name of the Package to be upgraded.|\n\n* If you upgrade the Node without shutting down the Node, it will automatically shut down the Node and perform the Node upgrade.\n  However, for stability, you should explicitly shut down the Node before upgrading.\n\n# Node Startup\n\n```bash\nmachcoordinatoradmin --startup-node=localhost:5656\n```\n\n\n# Snapshot Failover\n\nFrom Machbase 6.5 Cluster Edition, the Snapshot Failover function has been added.\n\nSnapshot failover is a function that provides quick recovery by recording snapshots when the DBMS is in a normal condition and performing failover only for the part where the problem occurs, excluding the normal snapshot when a specific warehouse fails.\n\n# Snapshot basic concept\n\nIt is a concept to record the location of normal data between warehouses in the group for each group of Cluster Edition.\n\nAll data before the snapshot created in the warehouse in the group are data in a normal state, and each snapshot is recorded for each group.\n\n# How Snapshot Failover Works\n\nWhen a problem occurs in a specific warehouse, the warehouse enters scrapped state and data recovery is required.\n\nWhen performing Snapshot Recovery, data after the snapshot is cleared based on the normal snapshot in the warehouse where the problem occurred, and the data after the baseline snapshot of the warehouse in the normal state in the same group is replicated to the warehouse where the problem occurred to complete the recovery.\n\n# Automatic Snapshot Execution\n\nBy default, automatic snapshot execution is enabled, and the snapshot execution interval is set to 60 seconds. If there are multiple warehouse groups in a cluster, only one group performs snapshots sequentially every snapshot interval.\n\nIf the execution interval is set to 0, automatic snapshot execution is disabled.\n\nSnapshot interval setting is reflected immediately when the command is executed.\n\n```bash\n# Snapshot Interval Setting\nmachcoordinatoradmin --snapshot-interval=[sec]\n  \n# Check the current snapshot interval\nmachcoordinatoradmin --configuration\n```\n\n# Take Snapshot manually\n\nSpecify **group_name** using the machcoordinatoradmin tool and manually perform Snapshot.\n\n**group_name** is preset like group1, group2.\n\nIf there are multiple groups in a cluster, snapshots must be performed for each group in order to take a full snapshot.\n\n```bash\n# Manually take a snapshot for group_name\nmachcoordinatoradmin --exec-snapshot --group='group_name'\n```\n\n# Recover scrapped node based on Snapshot\n\nIf a scrapped node occurs, it is restored as follows.\n\n```bash\n# Change the group state to readonly\n# Prevents group from being changed to normal state in later steps\nmachcoordinatoradmin --set-group-state=readonly --group=[groupname]\n  \n# Recover based on Snapshot\nmachcoordinatoradmin --snapshot-recover=[nodename]\n  \n# Replicate the latest data after snapshot through replication\n# When replication is finished, the state of the warehouse is automatically changed to normal.\nmachcoordinatoradmin --exec-sync=[nodename]\n  \n# Change the group state to readonly\nmachcoordinatoradmin --set-group-state=normal --group=[groupname]\n```\n\n# Snapshot-based recovery process of scrapped nodes\n\nWhen recovering a scrapped node with a snapshot, the following process is performed.\n\n```bash\n/* Initial cluster state */\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30420 | group1          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n/* warehouse 0 of group1 dies */\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly        | scrapped      | **unknown**   | ----------- |\n| warehouse   | localhost:30420 | group1          | readonly        | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# Change the group state to readonly\nmachcoordinatoradmin --set-group-state=readonly --group=[groupname]\n  \nkellen@kellen-ku:~$ machcoordinatoradmin --set-group-state=readonly --group=group1\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - 321a012d05.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nGroup Name: group1\nFlag      : 1\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly        | scrapped      | **unknown**   | ----------- |\n| warehouse   | localhost:30420 | group1          | readonly        | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# Restart the dead warehouse\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly        | scrapped      | scrapped      | ----------- |\n| warehouse   | localhost:30420 | group1          | readonly        | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# Recovery based on snapshot\nmachcoordinatoradmin --snapshot-recover=[nodename]\n  \nkellen@kellen-ku:~$ machcoordinatoradmin --snapshot-recover=localhost:30410\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - 321a012d05.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nNode-Name: localhost:30410\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly        | scrapped      | scrapped      | ----------- |\n| warehouse   | localhost:30420 | group1          | readonly        | normal        | normal        | ----------- |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# Replicate the latest data after snapshot through replication\nmachcoordinatoradmin --exec-sync=[nodename]\n  \nkellen@kellen-ku:~$ machcoordinatoradmin --exec-sync=localhost:30410\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - 321a012d05.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nNode-Name: localhost:30410\nSource:\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly        | scrapped      | scrapped      | stopped     |\n| warehouse   | localhost:30420 | group1          | readonly        | normal        | normal        | stopped     |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly        | sync-standby  | sync-standby  | running     |\n| warehouse   | localhost:30420 | group1          | readonly        | sync-active   | sync-active   | running     |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | readonly        | normal        | normal        | stopped     |\n| warehouse   | localhost:30420 | group1          | readonly        | normal        | normal        | stopped     |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n  \n# Change the group state to readonly\nmachcoordinatoradmin --set-group-state=normal --group=[groupname]\n  \nkellen@kellen-ku:~$ machcoordinatoradmin --set-group-state=normal --group=group1\n-------------------------------------------------------------------------\n     Machbase Coordinator Administration Tool\n     Release Version - 321a012d05.develop\n     Copyright 2014, MACHBASE Corp. or its subsidiaries\n     All Rights Reserved\n-------------------------------------------------------------------------\nGroup Name: group1\nFlag      : 0\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n|  Node Type  |    Node Name    |   Group Name    |   Group State   |    Desired & Actual State     |  RP State   |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n| coordinator | localhost:30110 | Coordinator     | normal          | primary       | primary       | ----------- |\n| coordinator | localhost:30120 | Coordinator     | normal          | normal        | normal        | ----------- |\n| deployer    | localhost:30210 | Deployer        | normal          | normal        | normal        | ----------- |\n| broker      | localhost:30310 | Broker          | normal          | leader        | leader        | ----------- |\n| broker      | localhost:30320 | Broker          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30410 | group1          | normal          | normal        | normal        | stopped     |\n| warehouse   | localhost:30420 | group1          | normal          | normal        | normal        | stopped     |\n| warehouse   | localhost:30510 | group2          | normal          | normal        | normal        | ----------- |\n| warehouse   | localhost:30520 | group2          | normal          | normal        | normal        | ----------- |\n+-------------+-----------------+-----------------+-----------------+-------------------------------+-------------+\n```\n\n# Snapshot related properties\n\n|Property|Description|Applies to|\n|--|--|--|\n|GROUP_SNAPSHOT_TIMEOUT_SEC|Determines the timeout time when executing SnapshotDefault : 60 (sec)Minimum : 0 (wait infinitely)Maximum : uint32_max (sec)|Write in each node's machbase.conf file|"
					}
					
				
		
				
					,
					
					"sql-ref-user-manage-html": {
						"id": "sql-ref-user-manage-html",
						"title": "User Management",
						"version": "all",
						"categories": "",
						"url": " /sql-ref/user-manage.html",
						"content": "# Index\n\n* [CREATE USER](#create-user)\n* [DROP USER](#drop-user)\n* [ALTER USER](#alter-user)\n* [CONNECT](#connect)\n* [GRANT/REVOKE](#grantrevoke)\n* [Managing User Example](#managing-user-example)\n\n\n# CREATE USER\n\n**create_user_stmt:**\n\n![create_user_stmt](/en/sql-ref/user_image/create_user_stmt.png)\n\n```sql\ncreate_user_stmt ::= 'CREATE USER' user_name 'IDENTIFIED BY' password\n```\n\nThe syntax for creating a user is:\n\n```sql\n-- Example\nCREATE USER new_user IDENTIFIED BY password\n```\n\n\n# DROP USER\n\n**drop_user_stmt:**\n\n![drop_user_stmt](/en/sql-ref/user_image/drop_user_stmt.png)\n\n```sql\ndrop_user_stmt ::= 'DROP USER' user_name\n```\n\nThe syntax for deleting a user is as follows. The SYS user can not be deleted, and if there is a table already created by the user to be deleted, an error is displayed.\n\n```sql\n-- Example\nDROP USER old_user\n```\n\n\n# ALTER USER\n\n**alter_user_pwd_stmt:**\n\n![alter_user_pwd_stmt](/en/sql-ref/user_image/alter_user_pwd_stmt.png)\n\n```sql\nalter_user_pwd_stmt ::= 'ALTER USER' user_name 'IDENTIFIED BY' password\n```\n\nThe user can change the password through the following syntax.\n\n```sql\n-- Example\nALTER USER user1 IDENTIFIED BY password\n```\n\n\n# CONNECT\n\n**user_connect_stmt:**\n\n![user_connect_stmt](/en/sql-ref/user_image/user_connect_stmt.png)\n\n```sql\nuser_connect_stmt: 'CONNECT' user_name '/' password\n```\n\nThe user can reconnect to another user via the following syntax without terminating the application.\n\n```sql\n-- Example\nCONNECT user1/password;\n```\n\n\n# GRANT/REVOKE\n\n![grant_stmt](/en/sql-ref/user_image/grant_stmt.png)\n\n![revoke_stmt](/en/sql-ref/user_image/revoke_stmt.png)\n\n![priv_value](/en/sql-ref/user_image/priv_value.png)\n\nGrants authority to the table to the user through the GRANT statement.\n\n```sql\n-- Grant user1 SELECT privileges on mytable\nGRANT SELECT ON mytable TO user1;\n \n-- Grant user1 all privileges on mytable\nGRANT ALL ON mytable TO user1;\n```\n\nRevokes the privilege granted to a user through the REVOKE statement.\n\n```sql\n-- Revoke UPDATE privilege on mytable granted to user1\nREVOKE UPDATE ON mytable FROM user1;\n \n-- Revoke all privileges on mytable granted to user1\nREVOKE ALL ON mytable FROM user1;\n```\n\n\n# Managing User Example\n\nHere is an example of the above query and its results.\n\n```sql\n############################################\n# Connect with SYS account\n############################################\nMach> create user demo identified by 'demo';\nCreated successfully.\n \nMach> drop user demo;\nDropped successfully.\n \nMach> create user demo1 identified by 'demo1';\nCreated successfully.\n \nMach> create user demo2 identified by 'demo2';\nCreated successfully.\n \nMach> alter user demo2 identified by 'demo22';\nAltered successfully.\n \nMach> create table demo1_table (id integer);\nCreated successfully.\n \nMach> create bitmap index demo1_table_index1 on demo1_table(id);\nCreated successfully.\n \nMach> insert into demo1_table values(99991);\n1 row(s) inserted.\n \nMach> insert into demo1_table values(99992);\n1 row(s) inserted.\n \nMach> insert into demo1_table values(99993);\n1 row(s) inserted.\n \nMach> select * from demo1_table;\nID\n--------------\n99993\n99992\n99991\n[3] row(s) selected.\n \n#Error: Can't drop the user connected.\nMach> drop user SYS;\n[ERR-02083 : Drop user error. You cannot drop yourself(SYS).]\n \n############################################\n# Connect DEMO1\n############################################\nMach> connect demo1/demo1;\nConnected successfully.\n \n#Error: can't alter other's account password\nMach> alter user demo2 identified by 'demo22';\n[ERR-02085 : ALTER user error. The user(DEMO2) does not have ALTER privileges.]\n \nMach> alter user demo1 identified by demo11;\nAltered successfully.\n \nMach> connect demo2/demo22;\nConnected successfully.\n \n#Error: wrong password\nMach> connect demo1/demo11234;\n [ERR-02081 : User authentication error. Invalid password (DEMO11234).]\n \n# Correct password\nMach> connect demo1/demo11;\nConnected successfully.\n \nMach> create table demo1_table (id integer);\nCreated successfully.\n \nMach> create bitmap index demo1_table_index1 on demo1_table(id);\nCreated successfully.\n \nMach> insert into demo1_table values(1);\n1 row(s) inserted.\n \nMach> insert into demo1_table values(2);\n1 row(s) inserted.\n \nMach> insert into demo1_table values(3);\n1 row(s) inserted.\n \nMach> select * from demo1_table;\nID\n--------------\n3\n2\n1\n[3] row(s) selected.\n \nMach> select * from demo1.demo1_table;\nID\n--------------\n3\n2\n1\n[3] row(s) selected.\n \n############################################\n# Connect SYS again\n############################################\nMach> connect SYS/MANAGER;\nConnected successfully.\n \nMach> select * from demo1_table;\nID\n--------------\n99993\n99992\n99991\n[3] row(s) selected.\n \nMach> select * from demo1.demo1_table;\nID\n--------------\n3\n2\n1\n[3] row(s) selected.\n \nMach> drop user demo1;\n[ERR-02084 : DROP user error. The user's tables still exist. Drop those tables first.]\n \nMach> connect demo1/demo11;\nConnected successfully.\n \nMach> drop table demo1_table;\nDropped successfully.\n \nMach> connect SYS/MANAGER;\nConnected successfully.\n \nMach> drop user demo1;\nDropped successfully.\n```"
					}
					
				
		
				
					,
					
					"tools-utils-html": {
						"id": "tools-utils-html",
						"title": "Utilities",
						"version": "all",
						"categories": "",
						"url": " /tools/utils.html",
						"content": "These are the utilities used for starting, shutting down, and entering/outputting data in the Machbase server.\n\n* [csvimport/csvexport](/en/tools/utils/csv.html)\n* [machadmin](/en/tools/utils/machadmin.html)\n* [machloader](/en/tools/utils/machloader.html)\n* [machsql](/en/tools/utils/machsql.html)"
					}
					
				
		
				
					,
					
					"config-monitor-virtualtable-html": {
						"id": "config-monitor-virtualtable-html",
						"title": "Virtual Table",
						"version": "all",
						"categories": "",
						"url": " /config-monitor/virtualtable.html",
						"content": "# Index\n\n- [Index](#index)\n- [Session/System](#sessionsystem)\n  - [V$PROPERTY](#vproperty)\n  - [V$SESSION](#vsession)\n  - [V$SESMEM](#vsesmem)\n  - [V$SESSTAT](#vsesstat)\n  - [V$SESTIME](#vsestime)\n- [V$SYSMEM](#vsysmem)\n  - [V$SYSSTAT](#vsysstat)\n  - [V$SYSTIME](#vsystime)\n  - [V$STMT](#vstmt)\n  - [V$VERSION](#vversion)\n- [Result Cache](#result-cache)\n  - [V$RS\\_CACHE\\_LIST](#vrs_cache_list)\n  - [V$RS\\_CACHE\\_STAT](#vrs_cache_stat)\n- [Storage](#storage)\n  - [V$STORAGE](#vstorage)\n  - [V$STORAGE\\_MOUNT\\_DATABASES](#vstorage_mount_databases)\n  - [V$CACHE](#vcache)\n  - [V$CACHE\\_OBJECTS](#vcache_objects)\n  - [V$STORAGE\\_DC\\_TABLESPACES](#vstorage_dc_tablespaces)\n  - [V$STORAGE\\_DC\\_TABLESPACE\\_DISKS](#vstorage_dc_tablespace_disks)\n  - [V$STORAGE\\_DC\\_DWFILES](#vstorage_dc_dwfiles)\n  - [V$STORAGE\\_DC\\_PAGECACHE](#vstorage_dc_pagecache)\n  - [V$STORAGE\\_DC\\_PAGECACHE\\_LRU\\_LST](#vstorage_dc_pagecache_lru_lst)\n  - [V$STORAGE\\_USAGE](#vstorage_usage)\n  - [V$STORAGE\\_TABLES](#vstorage_tables)\n- [Log Table](#log-table)\n  - [V$STORAGE\\_DC\\_TABLES](#vstorage_dc_tables)\n  - [V$STORAGE\\_DC\\_TABLES\\_STAT](#vstorage_dc_tables_stat)\n  - [V$STORAGE\\_DC\\_TABLE\\_COLUMNS](#vstorage_dc_table_columns)\n  - [V$STORAGE\\_DC\\_TABLE\\_COLUMN\\_PARTS](#vstorage_dc_table_column_parts)\n  - [V$STORAGE\\_DC\\_TABLE\\_INDEXES](#vstorage_dc_table_indexes)\n- [LSM(Log Structured Merge) Index](#lsmlog-structured-merge-index)\n  - [V$STORAGE\\_DC\\_LSMINDEX\\_LEVEL\\_PARTS](#vstorage_dc_lsmindex_level_parts)\n  - [V$STORAGE\\_DC\\_LSMINDEX\\_LEVEL\\_PARTS\\_CACHE](#vstorage_dc_lsmindex_level_parts_cache)\n  - [V$STORAGE\\_DC\\_LSMINDEX\\_LEVELS](#vstorage_dc_lsmindex_levels)\n  - [V$STORAGE\\_DC\\_LSMINDEX\\_FILES](#vstorage_dc_lsmindex_files)\n  - [V$STORAGE\\_DC\\_LSMINDEX\\_AGER\\_JOBS](#vstorage_dc_lsmindex_ager_jobs)\n- [Volatile Table](#volatile-table)\n  - [V$STORAGE\\_DC\\_VOLATILE\\_TABLE](#vstorage_dc_volatile_table)\n- [Tag Table](#tag-table)\n  - [V$STORAGE\\_TAG\\_TABLES](#vstorage_tag_tables)\n  - [V$STORAGE\\_TAG\\_CACHE](#vstorage_tag_cache)\n  - [V$STORAGE\\_TAG\\_CACHE\\_OBJECTS](#vstorage_tag_cache_objects)\n  - [V$STORAGE\\_TAG\\_TABLE\\_FILES](#vstorage_tag_table_files)\n  - [V$STORAGE\\_TAG\\_INDEX](#vstorage_tag_index)\n- [Tag Rollup](#tag-rollup)\n  - [V$ROLLUP](#vrollup)\n- [Stream](#stream)\n  - [V$STREAMS](#vstreams)\n- [License](#license)\n  - [V$LICENSE\\_INFO](#vlicense_info)\n  - [V$LICENSE\\_STATUS](#vlicense_status)\n- [Mutex](#mutex)\n  - [V$MUTEX](#vmutex)\n  - [V$MUTEX\\_WAIT\\_STAT](#vmutex_wait_stat)\n- [Cluster](#cluster)\n  - [V$NODE\\_STATUS](#vnode_status)\n  - [V$DDL\\_INFO](#vddl_info)\n  - [V$REPLICATION](#vreplication)\n  - [V$REPL\\_SENDER](#vrepl_sender)\n  - [V$REPL\\_SENDER\\_META](#vrepl_sender_meta)\n  - [V$REPL\\_RECEIVER](#vrepl_receiver)\n  - [V$REPL\\_RECEIVER\\_META](#vrepl_receiver_meta)\n  - [V$REPL\\_READER](#vrepl_reader)\n  - [V$REPL\\_READER\\_META](#vrepl_reader_meta)\n  - [V$REPL\\_WRITER](#vrepl_writer)\n  - [V$REPL\\_WRITER\\_META](#vrepl_writer_meta)\n- [Others](#others)\n  - [V$TABLES](#vtables)\n  - [V$COLUMNS](#vcolumns)\n  - [V$RETENTION\\_JOB](#vretention_job)\n\n\nThe Virtual Tables are virtual tables that represent various operational information of the Machbase server in the form of a table. The names of these tables begin with \"V$\".\n\nThis data is used to know what state the Machbase server is operating in.\nIn addition, various information can be obtained through JOIN operation with other tables in this virtual table.\n\nVirtual Tables are read-only and can not be added / deleted / updated by the user.\n\n\n# Session/System\n\n## V$PROPERTY\n\nDisplays the property information set in the server.\n\n|Column Name|Description|\n|--|--|\n|NAME|Property name|\n|VALUE|Property value|\n|TYPE|Data type|\n|DEFLT|Default value|\n|MIN|Minimum set value|\n|MAX|Maximum set value|\n\n## V$SESSION\n\nDisplays session information connected to the Machbase server.\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME (Cluster Only)|Name of the HOST which the session is connected.|\n|ID|Session identifier|\n|CLOSED|Whether connection is closed|\n|USER_ID|User identifier|\n|LOGIN_TIME|Connection time|\n|CLIENT_TYPE|Connected client type|\n|USER_NAME|User name|\n|USER_IP|User IP address|\n|SQL_LOGGING|Leave message in session trace log status1: Leaves errors occurring in the Parsing, Validation, Optimization steps2: Leaves result performance of DDL3: (Leaves both cases above)|\n|SHOW_HIDDEN_COLS|Whether hidden columns are shown upon SELECT|\n|FEEDBACK_APPEND_ERROR|Whether there is feedback to client on APPEND error|\n|DEFAULT_DATE_FORMAT|Default input format upon Datetime input |\n|HASH_BUCKET_SIZE|Number of Buckets in Temp Hashtable created when performing query|\n|AX_QPX_MEM|Maximum memory size available when performing query|\n|RS_CACHE_ENABLE|Whether Result Cache in in use|\n|RS_CACHE_TIME_BOUND_MSEC|Maximum elapsed time to store results when using Result Cache|\n|RS_CACHE_MAX_MEMORY_PER_QUERY|Maximum size of memory used per query when using Result Cache|\n|RS_CACHE_MAX_RECORD_PER_QUERY|Maximum number of results used per query when using Result Cache|\n|RS_CACHE_APPROXIMATE_RESULT_ENABLE|Whether to cache approximate query results when using Result Cache|\n|IDLE_TIMEOUT|Terminate the session if the client does nothing for that time after the session connected.|\n|QUERY_TIMEOUT|Response waiting time for query execution|\n\n\n## V$SESMEM\n\nDisplays session memory information.\n\n|Column Name|Description|\n|--|--|\n|SID|Session identifier|\n|ID|Memory manager identifier|\n|USAGE|Usage size|\n\n\n## V$SESSTAT\n\nDisplays statistical information of the session.\n\n|Column Name|Description|\n|--|--|\n|SID|Session identifier|\n|ID|Statistical information identifier|\n|VALUE|Statistical information value|\n\n\n## V$SESTIME\n\nDisplays the time information of the session.\n\n|Column Name|Description|\n|--|--|\n|SID|Session identifier|\n|ID|Performance unit identifier|\n|ACCUM_TICK|Cumulative time|\n|MAX_TICK|Maximum time (per each performance unit)|\n\n\n# V$SYSMEM\n\nDisplays memory information of the system.\n\n|Column Name|Description|\n|--|--|\n|ID|Memory manager identifier|\n|NAME|Memory manager name|\n|USAGE|Current usage|\n|MAX_USAGE|(Recorded) Maximum usage|\n\n\n## V$SYSSTAT\n\nDisplays statistical information of the system.\n\n|Column Name|Description|\n|ID|Statistical information identifier|\n|NAME|Statistical information name|\n|VALUE|Statistical information value|\n\n\n## V$SYSTIME\n\nDisplays the time information of the system.\n\n|Column Name|Description|\n|--|--|\n|ID|Performance unit identifier|\n|NAME|Performance unit name|\n|ACCUM_TICK|Cumulative time|\n|AVG_TICK|Average time (per each performance unit)|\n|MIN_TICK|Minimum Time (per each performance unit)|\n|MAX_TICK|Maximum Time (per each performance unit)|\n|COUNT|Performance frequency|\n\n\n## V$STMT\n\nDisplays information about the query statement that the user is currently executing.\n\n|Column Name|Description|\n|--|--|\n|ID|Query identifier|\n|SESS_ID|Performed query session identifier|\n|STATE|Query status|\n|RECORD_SIZE|Resulting record size of select statements|\n|QUERY|Query statement|\n \n\n## V$VERSION\n\nDisplays information about Machbase version.\n\n|Column Name|Description|\n|--|--|\n|BINARY_DB_MAJOR_VERSION|Database major version|\n|BINARY_DB_MINOR_VERSION|Database minor version|\n|BINARY_META_MAJOR_VERSION|META major version|\n|BINARY_META_MINOR_VERSION|META minor version|\n|BINARY_CM_MAJOR_VERSION|Client (Communication Level) major version|\n|BINARY_CM_MINOR_VERSION|Client (Communication Level) minor version|\n|BINARY_SIGNATURE|Version name of DB data files.|\n|FILE_DB_MAJOR_VERSION|File DB major version|\n|FILE_DB_MINOR_VERSION|File DB minor version|\n|FILE_META_MAJOR_VERSION|File META major version|\n|FILE_META_MINOR_VERSION|File META minor version|\n|FILE_CM_MAJOR_VERSION|File Client (Communication Level) major version|\n|FILE_CM_MINOR_VERSION|File Client (Communication Level) minor version|\n|FILE_CREATE_TIME|File creation time|\n|EDITION|Machbase type|\n\n\n# Result Cache\n\n## V$RS_CACHE_LIST\n\nDisplay the result cache list.\n\n|Column Name|Description|\n|--|--|\n|TOUCH_TIME|Time cache was used or created|\n|USER_ID|Cache user identifier|\n|QUERY|Cache query statement|\n|TIME_SPENT|Time spent producing result|\n|TABLE_COUNT|Number of tables associated with query statement|\n|RECORD_COUNT|Number of result records|\n|REFERENCE_COUNT|Number of sessions currently being referenced|\n|HIT_COUNT|Cache hit count|\n|AGGR_TOUCH_TIME|Time the cache was used or created for aggregate results|\n|AGGR_HIT_COUNT|Cache hit count for aggregate results|\n\n\n## V$RS_CACHE_STAT\n\nDisplay statistical information of result cache in one session.\n\n\n|Column Name|Description|\n|--|--|\n|CACHE_COUNT|Number of result caches|\n|CACHE_HIT|Total cache hit count|\n|AGGR_HIT|Total cache hit count for aggregate results|\n|CACHE_REPLACED|Cache replacement count|\n|CACHE_MEMORY_USAGE|Size of cache memory used|\n\n\n# Storage\n\n## V$STORAGE\n\nDisplays internal information of the storage system.\n\n|Column Name|Description|\n|--|--|\n|DC_TABLE_FILE_SIZE|Total capacity of disk column data|\n|DC_INDEX_FILE_SIZE|Total capacity of index file data|\n|DC_TABLESPACE_DWFILE_SIZE|Total capacity of DWFILE for all column data|\n|DC_KV_TABLE_FILE_SIZE|Total number of data files of TAGDATA table partition tables|\n\n\n## V$STORAGE_MOUNT_DATABASES\n\nDisplays the information of the mounted backup database using the mount function.\n\n|Column Name|Description|\n|--|--|\n|NAME|Mounted database name|\n|PATH|Backup file location|\n|BACKUP_TBSID|Backup database tablespace identifier|\n|BACKUP_SCN|Backup database identifier|\n|MOUNTDB|Backup time|\n|DB_BEGIN_TIME|Backup database first entry time|\n|DB_END_TIME|Backup database last entry time|\n|BACKUP_BEGIN_TIME|Backup begin time|\n|BACKUP_END_TIME|Backup end time|\n|FLAG|Property flag|\n\n\n## V$CACHE\n\nDisplays the comprehensive information on the cache objects containing the results read from the storage system.\n\n|Column Name|Description|\n|--|--|\n|OBJ_COUNT|Current number of result set cache objects|\n\n## V$CACHE_OBJECTS\n\nDisplays information about each cache object that contains the results read from the storage system.\n\n|Column Name|Description|\n|--|--|\n|OID|Object identifier|\n|REF_COUNT|Reference count|\n|FLAG|(Internal server use flag)|\n\n\n## V$STORAGE_DC_TABLESPACES\n\nDisplays the table space information of the storage system.\n\n|Column Name|Description|\n|--|--|\n|NAME|Tablespace name|\n|ID|Tablespace identifier|\n|FLAG|Flag indicating tablespace property|\n|REF_COUNT|Tablespace reference count|\n|DISK_COUNT|Tablespace disk count|\n\n\n## V$STORAGE_DC_TABLESPACE_DISKS\n\nDisplays the table space information of the storage system.\n\n|Column Name|Description|\n|--|--|\n|NAME|Disk name|\n|ID|Disk identifier|\n|TABLESPACE_ID|Disk tablespace identifier|\n|PATH|Disk path|\n|IO_THREAD_COUNT|I/O Thread count|\n|IO_JOB_COUNT|I/O Job count|\n|VIRTUAL_DISK_COUNT|Virtual disk count|\n\n\n## V$STORAGE_DC_DWFILES\n\nDisplays the information of the double-write file (DW File) operated by the storage system.\n\n\n|Column Name|Description|\n|--|--|\n|TBS_ID|Tablespace identifier|\n|DISK_ID|Disk identifier|\n|FILE|File path|\n|TABLE_ID|Table identifier|\n|COLUMN_ID|Column identifier|\n|PARTITION_ID|Partition identifier|\n|PAGE_ID|Page identifier|\n|DISK_OFFSET|Disk offset|\n|DISK_IMAGE_SIZE|Disk image size|\n|HEAD_CRC32CODE_IMAGE|Head CRC32 Code Image|\n|TAIL_CRC32CODE_IMAGE|Tail CRC32 Code Image|\n|CRC32CODE_PAGE|CRC32 Code Page|\n|HEAD_TIMESTAMP_PAGE|Head Timestamp Page|\n|TAIL_TIMESTAMP_PAGE|Tail Timestamp Page|\n\n\n## V$STORAGE_DC_PAGECACHE\n\nDisplays information about the Page Cache operating on the storage system\n\n|Column Name|Description|\n|--|--|\n|MAX_MEM_SIZE|Maximum memory size of Page Cache|\n|CUR_MEM_SIZE|Current memory size of Page Cache|\n|PAGE_CNT|Number of cached pages|\n|CHECK_TIME|Check time|\n\n\n## V$STORAGE_DC_PAGECACHE_LRU_LST\n\nDisplays information about the LRU List of Page Cache operated by the storage system.\n\n\n|Column Name|Description|\n|--|--|\n|OBJECT_ID|Object identifier|\n|LEVEL|Partition level|\n|PARTITION_ID|Partition identifier|\n|OFFSET|Page Cache Offset|\n|SIZE|Page size|\n|REF_CNT|Reference count|\n\n## V$STORAGE_USAGE\n\nDisplays the amount of storage used by the storage system.\n\n|Column Name|Description|\n|--|--|\n|TOTAL_SPACE|Total storage capacity where the $MACHBASE_HOME/dbs directory is located|\n|USED_SPACE|Total storage usage where the $MACHBASE_HOME/dbs directory is located|\n|USED_RATIO|Percentage of usage(%)|\n|RATIO_CAP|Storage usage limit. Data input/index construction stops when USED_RATIO reaches this limit.|\n\n\n## V$STORAGE_TABLES\n\nDisplay table details.\n\n|Column Name|Description|\n|--|--|\n|ID|Table ID|\n|TYPE|Table type - Persistent: LOG / TAG Table - Volatile: Volatile Table - Key-Value: Accompanying table of TAG table|\n|STATUS|Current Status - Creating...: Creating table by CREATE TABLE query - Normal: normal - Predrop: DROP TABLE query accepted - Dropping...: DROP TABLE query processing - Dropped: DROP TABLE query completed - Mounted: The backed up database loaded with the MOUNT query|\n|STORAGE_USAGE|Capacity occupied by the table in storage|\n\n\n# Log Table\n\n## V$STORAGE_DC_TABLES\n\nDisplays internal information about Log Table.\n\n|Column Name|Description|\n|--|--|\n|ID|Table identifier|\n|DATABASE_ID|Database identifier|\n|CREATE_SCN|System Change Number at time of creation|\n|UPDATE_SCN|System Change Number at time of most recent update|\n|DDL_REF_COUNT|Number of sessions referencing table in DDL syntax execution|\n|BEGIN_RID|Minimum table RID|\n|END_RID|Last row ID of table + 1|\n|BEGIN_META_RID|ID at start of recording meta information|\n|END_META_RID|ID at end of recording meta information|\n|END_SYNC_RID|Last row ID recorded on disk + 1|\n|FLAG|Flag indicating table property|\n|COLUMN_COUNT|Table column count|\n|INDEX_COUNT|Table index count|\n|INDEX_MIN_END_RID|Last RID recorded in index + 1|\n|LAST_ARRIVAL_TIME|Last recorded _arrival_time value|\n|LAST_CHECKPOINT_TIME|Last checkpoint time|\n|TYPE|Table type|\n|REMAINING_ROW_COUNT|Number of records not deleted when using auto delete|\n|KEPT_DURATION|How long to keep data when using auto delete function|\n\n## V$STORAGE_DC_TABLES_STAT\n\nDisplays internal information about Log Table.\n\n|Column Name|Description|\n|--|--|\n|TABLESPACE_ID|Tablespace identifier|\n|TABLE_ID|Table identifier|\n|COLUMN_ID|Column identifier|\n|COUNT|Record count|\n\n## V$STORAGE_DC_TABLE_COLUMNS\n\nDisplays information about the columns in the Log Table.\n\n|Column Name|Description|\n|TABLE_ID|Table identifier|\n|DATABASE_ID|Database identifier|\n|ID|Column identifier|\n|FLAG|Property flag|\n|SIZE|Column data size|\n|PARTITION_VALUE_COUNT|Maximum number of data stored in partition|\n|PAGE_VALUE_COUNT|Maximum number of data stored in page|\n|CACHE_VALUE_COUNT|Maximum number of cache values|\n|MINMAX_CACHE_SIZE|Maximum size of MIN / MAX cache for column partitions|\n|CUR_APPEND_PARTITION_ID|Current partition in progress of input identifier|\n|CUR_CACHE_PARTITION_COUNT|Number of partitions that have read data in current cache|\n|CUR_MINMAX_CACHE_SIZE|Current Min / MAX cache size|\n|END_RID_FOR_DEFAULT_VALUE|Location value of end rid maintaining default value|\n|DISK_FILE_SIZE|Total size of column partition data file for that column|\n|MEMORY_TOTAL_SIZE|Memory size used by table|\n|MEMORY_ALLOC_SIZE|Memory size allocated by table|\n\n\n## V$STORAGE_DC_TABLE_COLUMN_PARTS\n\nDisplays column partition information of log table.\n\n|Column Name|Description|\n|--|--|\n|TABLE_ID|Table identifier|\n|DATABASE_ID|Database identifier|\n|COLUMN_ID|Column identifier|\n|ID|Partition identifier|\n|FLAG|Flag indicating column property|\n|BEGIN_RID|First RID stored in partition|\n|END_RID|Last RID stored in partition|\n|END_SYNC_RID|Last RID SYNC ended.Data with a RID greater than the starting RID and less than the last SYNC RID is recorded in the partition file.|\n|MIN_TIME|First time data was entered into column partition|\n|MAX_TIME|Last time data was entered into column partition|\n|MAX_VALUE_COUNT_PER_PARTITION|Maximum partition data count|\n|MAX_VALUE_COUNT_PER_PAGE|Maximum page data count|\n|MAX_PAGE_COUNT|Maximum partition page count|\n|PAGE_SIZE|Page size stored in column partition|\n|PAGE_COUNT|Page count created in current column partition|\n|COMPRESS_RATIO|Column partition compression ratio. If it is 0, data compression has not been performed yet.|\n|DISK_FILENAME|Partition file name|\n|EXTERNAL_PART_SIZE|A large amount of data is written to the external partition file, indicating the size of the file|\n|MIN_VALUE|Minimum column partition value|\n|MAX_VALUE|Maximum column partition value|\n\n\n## V$STORAGE_DC_TABLE_INDEXES\n\nDisplays index information generated in Log Table.\n\n|Column Name|Description|\n|--|--|\n|TABLE_ID|Table identifier|\n|DATABASE_ID|Database identifier|\n|ID|Index identifier|\n|FLAG|Flag indicating index property|\n|TABLE_BEGIN_RID|First RID entered into table|\n|TABLE_END_RID|Last table RID|\n|BEGIN_RID|First index RID|\n|END_RID|Last index RID|\n|END_SYNC_RID|Last recorded RID in file + 1|\n|COLUMN_COUNT|Index column count|\n|BEGIN_PART_ID|Index first partition identifier|\n|END_PART_ID|Index last partition identifier|\n|FLUSH_REQUEST_COUNT|Number of index partitions requested to reflect on disk|\n|MAX_KEY_SIZE|Maximum key size|\n|INDEX_TYPE|Index type|\n|DISK_FILE_SIZE|Total size of index partition file for that index|\n|LAST_CHECKPOINT_TIME|Last checkpoint time|\n\n\n\n# LSM(Log Structured Merge) Index\n\n\n## V$STORAGE_DC_LSMINDEX_LEVEL_PARTS\n\nDisplays information about LSM Index partitions.\n\n|Column Name|Description|\n|--|--|\n|TABLE ID|Index table identifier|\n|TABLESPACE_ID|Tablespace identifier|\n|INDEX_ID|Index identifier|\n|LEVEL|Index partition LSM level|\n|PARTITION_ID|Partition identifier|\n|BEGIN_RID|First RID entered into partition|\n|END_RID|Last RID entered into partition + 1|\n|KEY_VALUE_COUNT|Key value count entered into partition|\n|KEY_VALUE_TABLE_SIZE|Size of page storing key value|\n|KEY_VALUE_TABLE_PAGE_COUNT|Number of pages storing key value|\n|MIN_KEY_VALUE|Minimum key value|\n|MAX_KEY_VALUE|Maximum key value|\n|BITMAP_TABLE_SIZE|Total size of page storing bitmap value|\n|BITMAP_TABLE_PAGE_COUNT|Number of pages storing bitmap value|\n|META_SIZE|Total size of page storing meta information|\n|META_PAGE_COUNT|Number of pages storing meta information|\n|TOTAL_BUILD_MSEC|Total time to complete partition|\n|KEYVAL_BUILD_MSEC|Total time to complete partition for KeyValue Mode|\n|BITMAP_BUILD_MSEC|Total time to complete partition for Bitmap Mode|\n\n\n## V$STORAGE_DC_LSMINDEX_LEVEL_PARTS_CACHE\n\nDisplays information about the LSM Index partition cache.\n\n\n|Column Name|Description|\n|--|--|\n|TABLESPACE_ID|Tablespace identifier|\n|TABLE_ID|Index Table identifier|\n|INDEX_ID|Index identifier|\n|LEVEL|Index partition LSM level|\n|PARTITION_ID|Partition identifier|\n|BEGIN_RID|First RID entered into partition|\n|END_RID|Last RID entered into partition + 1|\n|KEY_VALUE_COUNT|Number of key values entered into partition|\n|KEY_VALUE_TABLE_SIZE|Size of page storing key value|\n|KEY_VALUE_TABLE_PAGE_COUNT|Number of pages storing key value|\n|BITMAP_TABLE_SIZE|Total size of page storing bitmap value|\n|BITMAP_TABLE_PAGE_COUNT|Number of pages storing bitmap value|\n|META_SIZE|Total size of page storing meta information|\n|META_PAGE_COUNT|Number of pages storing meta information|\n|MEMORY_SIZE|Memory usage|\n|MEMORY_SIZE_RBTREE|Redblack Tree memory usage|\n\n\n## V$STORAGE_DC_LSMINDEX_LEVELS\n\nDisplays information about the level of the LSM index.\n\n|Column Name|Description|\n|--|--|\n|TABLE ID|Table identifier|\n|DATABASE_ID|Database identifier|\n|INDEX_ID|Index identifier|\n|LEVEL|Level|\n|BEGIN_RID|First partition RID|\n|END_RID|Last partition RID + 1|\n|META_BEGIN_RID|RID at start time of recording meta information|\n|META_END_RID|RID at end time of recording meta information|\n|DELETE_END_RID|Maximum deleted RID + 1|\n\n\n## V$STORAGE_DC_LSMINDEX_FILES\n\nDisplays information about the files that make up the LSM Index.\n\n|Column Name|Description|\n|--|--|\n|TABLE_ID|Table identifier|\n|DATABASE_ID|Database identifier|\n|INDEX_ID|Index identifier|\n|LEVEL|Index partition LSM level|\n|PARTITION_ID|Partition identifier|\n|BEGIN_RID|Partition first RID|\n|END_RID|Partition last RID + 1|\n|PATH|Index file location|\n\n\n## V$STORAGE_DC_LSMINDEX_AGER_JOBS\n\nDisplays working status of Ager responsible for LSM Index deletion.\n\n|Column Name|Description|\n|--|--|\n|TABLE_ID|Table identifier|\n|INDEX_ID|Index identifier|\n|LEVEL|Index partition LSM level|\n|BEGIN_RID|First partition RID|\n|END_RID|Last partition RID + 1|\n|STATE|Index Ager working status|\n\n\n# Volatile Table\n\n## V$STORAGE_DC_VOLATILE_TABLE\n\nDisplays information about Volatile Table.\n\n\n|Column Name|Description|\n|--|--|\n|MAX_MEM_SIZE|Maximum Volatile Tablespace size|\n|CUR_MEM_SIZE|Current Volatile Tablespace size|\n\n\n# Tag Table\n\n## V$STORAGE_TAG_TABLES\n\nDisplays information about the partition table in the Tagdata Table.\n\n\n|Column Name|Description|\n|--|--|\n|ID|Table identifier|\n|TABLE_BEGIN_RID|Table start RID|\n|TABLE_END_RID|Table end RID|\n|WRITE_END_RID|Last RID which is written to data file.|\n|EXT_ROW_COUNT|Number of entries to external partitions in VARCHAR records|\n|EXT_WRITE_COUNT|Number of entries to data files in VARCHAR records|\n|DISK_INDEX_END_RID|Index end RID stored in storage|\n|MEMORY_INDEX_END_RID|Table end RID in memory index|\n|DELETE_MIN_DATE|Minimum time of deleted data by execute  DELETE BETWEEN query|\n|DELETE_MAX_DATE|Maximum time of deleted data by execute  DELETE BETWEEN/BEFORE query|\n|INDEX_STATE|Current Index Build Stats - IDLE: Build Complete, waiting - PROGRESS: Build in progress - IOWAIT: Waiting for I/O operation in storage - PENDING: Waiting for table read lock - SHUTDOWN:  Stopped. DELETE operation or DROP operation in progress. - ABNORMAL: Abnormal end|\n|DELETE_STATE|Current DELETE operation state.There is no IDLE because it is performed only when a DELETE command is entered. - PROGRESS: Deletion in progress - IOWAIT: Waiting for I/O operation in storage - PENDING: Waiting for table read/write lock - SHUTDOWN: Stopped. DELETE operation or DROP operation in progress. - ABNORMAL: Abnormal end|\n|SAVE_STATE|Current Table Save operation state. - IDLE: Save Complete, waiting - PROGRESS: Save in progress - IOWAIT: Waiting for I/O operation in storage - PENDING: Waiting for table read lock - SHUTDOWN: Stopped. DELETE operation or DROP operation in progress. - ABNORMAL: Abnormal end|\n\n\n## V$STORAGE_TAG_CACHE\n\nDisplays the cache information used in the partition table of the Tagdata Table.\n\n\n|Column Name|Description|\n|--|--|\n|CATEGORY|Type of object in cache|\n|USED_MEMORY|Size of memory in use|\n|BLOCK_COUNT|Data cache count|\n|CACHE_HIT|Data cache hit count|\n|CACHE_MISS|Data cache miss count|\n|FLUSHOUT|Number of page flushouts due to data cache crash|\n|COLDREAD|Number of data pages read directly from storage|\n|MEMORY_WAIT|Number of times data memory waited for cache crash|\n|IO_WAIT|Data read operation wait count|\n\n## V$STORAGE_TAG_CACHE_OBJECTS\n\nDisplays detailed information about each cache block used in the partition table of the Tagdata Table.\n\n|Column Name|Description|\n|--|--|\n|CATEGORY|Object classification being cached|\n|LATEST_HIT|Last approach time|\n|STATUS|Cache status - None: Memory allocation done - Resides: Already stored in cache - Loading: Loading table data from storage - ERROR!: Error appears while loading data|\n|WAIT_COUNT|The number of waiting times because the cache could not be read in the Loading state|\n|REF_COUNT|Number of sessions currently referencing the cache block|\n|HIT_COUNT|Number of times a cache block was referenced|\n|TABLE_ID|Table Identifier|\n|FILE_ID|File Identifier|\n|PART_ID|Partition identifier inside the datafile|\n|SAVE_SCN|SCN of table save|\n|VSAVE_SCN|SCN of table save|\n|DELETE_SCN|SCN of delete operation|\n|OFFSET|Datafile offset|\n|DATA_SIZE|Data size before compression, or 0|\n\n\n## V$STORAGE_TAG_TABLE_FILES\n\nDisplays the file information of the partition table of the Tag Table.\n\n\n|Column Name|Description|\n|--|--|\n|TABLE_ID|Table identifier|\n|FILE_ID|File identifier|\n|STATE|Index status - COMPLETE: Data stored, index build complete - INDEXING: Index build in progress - FILLED: Data is full, waiting for Index build - PARTIAL: Data not yet full, waiting for Index build|\n|REF_COUNT|Number of sessions currently referencing the file|\n|ROW_COUNT|Number of records stored in the file, including those that were deleted|\n|DEL_COUNT|Number of records deleted from the file|\n|MIN_DATE|Minimum datatime value of this data file.|\n|MAX_DATE|Maximum datatime value of this data file.|\n\n\n## V$STORAGE_TAG_INDEX\n\nDisplays index information generated in Tag Table.\n\n|Column Name|Description|\n|--|--|\n|TABLE_ID|Table identifier|\n|INDEX_ID|Index identifier (if INDEX_ID is 4294967295 it is a default index that is created automatically when the tag table is created.)|\n|INDEX_STATE|Current index build state - IDLE: Build Complete, waiting - INDEXING: Build in progress - STORAGE FULL: Stopped because of disk full|\n|DISK_INDEX_END_RID|Index end RID stored in storage|\n|MEMORY_INDEX_END_RID|Table end RID in memory index|\n|TABLE_END_RID|Table end RID|\n\n\n# Tag Rollup\n\n## V$ROLLUP\n\nDisplays the Rollup information that stores information of the Tagdata table.\n\n|Column Name|Description|\n|--|--|\n|ID|Rollup job ID|\n|ROLLUP_TABLE_NAME|Table name to store Rollup information|\n|SOURCE_TABLE_NAME|Name of tag table that Rollup will query|\n|USER_ID|User ID of Rollup Table and Source Table|\n|ROOT_TABLE|Root Source Table Name|\n|INTERVAL|Rollup execution cycle (sec)|\n|END_RID|Source Table end RID|\n|ENABLED|Indicates Rollup progress status|\n|LAST_ELAPSED_MSEC|Number of recently entered records|\n\n\n\n# Stream\n\n## V$STREAMS\n\n|Column Name|Description|\n|--|--|\n|NAME|The name of stream query.|\n|LAST_EX_TIME|Last execution time of this query.|\n|TABLE_NAME|The name of table which searched from the query|\n|END_RID|The last RID read by stream query|\n|STATE|Current state of stream query|\n|QUERY_TXT|Query text|\n|ERROR_MSG|Error message of the last stream execution|\n|FREQUENCY|Minimum wait time for query execution. If it is 0, it is executed every record. If it is not 0, it is executed each time. The unit is nanoseconds.|\n\n\n# License\n\n## V$LICENSE_INFO\n\nDisplays license information.\n\n\n|Column Name|Description|\n|--|--|\n|INSTALL_DATE|Installation date|\n|TYPE|License type|\n|POLICY|License policy type|\n|CUSTOMER|Customer name|\n|ISSUE_DATE|Issue date|\n|ID|Host ID|\n|EXPIRY_DATE|Expiration date|\n|SIZE_LIMIT|Work input limit|\n|ADDENDUM|Additional data rate|\n|VIOLATION_ACTION|Indicates license violation|\n|VIOLATION_LIMIT|Number of violations to suspend service (monthly update)|\n|STOP_ACTION|Indicates database is terminated in the event of license violation|\n|RESET_FLAG|(Internal server use)|\n\n## V$LICENSE_STATUS\n\n\nDisplays the license status.\n\n|Column Name|Description|\n|--|--|\n|USER_DATA_PER_DAY|Amount of data that can be entered per day|\n|PREVIOUS_CHECK_DATE|Previous license check date|\n|VIOLATION_COUNT|License violation count|\n|STOP_ENABLED|Display whether license restrictions are enabled or not.|\n\n\n# Mutex\n\n## V$MUTEX\n\nDisplays current mutex status.\n\n|Column Name|Description|Note|\n|--|--|--|\n|OBJECT|Address of the mutex object| |\n|NAME|The name given when creating the mutex| |\n|TYPE|Mutex type| - Mutex: pmuMutex - RW Mutex: pmuRWMutex|\n|OWNER|ID of the thread that acquired the mutex| - Mutex: 0 if no thread acquired the mutex. - RW Mutex w/ Read-Lock: 0 - RW Mutex w/ Write-Lock: ID of the thread that acquired the write lock.|\n|LOCK_COUNT|Number of threads that acquired the mutex| - RW Mutex can be 2 or more.|\n|PEND_COUNT|Number of threads waiting to acquire a mutex| - Collect only when TRACE_MUTEX_WAIT_STATUS=1|\n|TRY_COUNT|Number of attempts to acquire the mutex| - Collect only when TRACE_MUTEX_WAIT_STATUS=1|\n|CONFLICT_COUNT|Number of failed to acquire mutex| - Collect only when TRACE_MUTEX_WAIT_STATUS=1|\n|WAIT_TICK|Sum of waiting time to acquire mutex| - Collect only when TRACE_MUTEX_WAIT_STATUS - Do not write to RW Mutex|\n|WAIT_TICK_AVG|Average time to success after an attempt to acquire a mutex| - Collect only when TRACE_MUTEX_WAIT_STATUS=1 - Do not write to RW Mutex|\n|HELD_TICK|Total time from acquiring the mutex to releasing it| - Collect only when TRACE_MUTEX_WAIT_STATUS=1 - Do not write to RW Mutex|\n|HELD_TICK_AVG|Average time from acquisition to release of mutex| - Collect only when TRACE_MUTEX_WAIT_STATUS=1 - Do not write to RW Mutex|\n\n\n## V$MUTEX_WAIT_STAT\n\nShows the call stack of the currently waiting mutex.\n\n|Column Name|Description|Note|\n|--|--|--|\n|THREAD_ID|ID of the thread waiting to acquire the mutex| |\n|OBJECT|Address of the mutex being acquired| - Same as OBJECT in V$MUTEX|\n|DEPTH|Call stack depth| - Collect only when TRACE_MUTEX_WAIT_STATUS=1|\n|SYMBOL|Symbol of function that called acquire mutex| - Collect only when TRACE_MUTEX_WAIT_STATUS=1|\n\n\n\n# Cluster\n\n## V$NODE_STATUS\n\nDisplays the Node status for each Cluster. Only one is displayed.\n\n\n|Column Name|Description|\n|--|--|\n|NODETYPE|Node type. There are two types that can be viewed by queries. - Broker - Warehouse|\n|STATE|Node status|\n\n\n## V$DDL_INFO\n\nDisplays DDL information performed by Cluster.\n\n|Column Name|Description|\n|--|--|\n|SEQUENCENUMBER|DDL sequence number|\n|TIME|DDL execution time|\n|VALUE|DDL query result value (Internal server use)|\n|CLIENT|Client name|\n|BROKER|Lead Broker Node name|\n|USER|User name|\n|SQL|DDL query value|\n\n## V$REPLICATION\n\nDisplays information about the replication operation.\n\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|MODE|(Internal server use)|\n|STATE|Node status|\n|ADDR|Replication Manager address|\n|PORT_NO|Replication Manager port number|\n|MAX_SENDER_COUNT|Maximum number of Senders that can be created|\n|RUN_SENDER_COUNT|Maximum number of active Senders|\n\n## V$REPL_SENDER\n\nDisplays Sender replication when running Replication.\n\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|ID|Sender identifier|\n|STATUS|Sender operational status|\n|PAYLOAD_RECV_COUNT|Number of payloads received from sender|\n|PAYLOAD_RECV_BYTES|Total payload size received from Sender|\n|QUEUE_REMAIN_COUNT|Number of buffers remaining in the Receive Queue|\n|NET_SEND_COUNT|Net send count|\n|NET_SEND_SIZE|Net send size|\n|NET_RECV_COUNT|Net receive count|\n|NET_RECV_SIZE|Net receive size|\n\n\n## V$REPL_SENDER_META\n\nDisplays Sender metadata when running Replication.\n\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|SENDER_ID|Sender identifier|\n|TABLE_ID|Target table identifier|\n|TABLE_TYPE|Target table type|\n|BEGIN_RID|Target record start RID|\n|END_RID|Target record end RID|\n\n## V$REPL_RECEIVER\n\nDisplays Receiver information when running Replication.\n\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|STATUS|Receiver operational status|\n|PAYLOAD_RECV_COUNT|Number of payloads received from sender|\n|PAYLOAD_RECV_BYTES|Total payload size received from Sender|\n|QUEUE_REMAIN_COUNT|Number of buffers remaining in the Receive Queue|\n|NET_SEND_COUNT|Net send count|\n|NET_SEND_SIZE|Net send size|\n|NET_RECV_COUNT|Net receive count|\n|NET_RECV_SIZE|Net receive size|\n\n## V$REPL_RECEIVER_META\n\nDisplays Receiver metadata when running Replication.\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|TABLE_ID|Target table identifier|\n|TABLE_TYPE|Target table type|\n|BEGIN_RID|Target record start RID|\n|END_RID|Target record end RID|\n\n\n## V$REPL_READER\n\nDisplays Reader information when running Replication.\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|SENDER_ID|Sender identifier|\n|ID|Reader identifier|\n|STATUS|Reader operation status|\n|FETCH_COUNT|FETCH count|\n\n## V$REPL_READER_META\n\nDisplays Reader metadata when running Replication.\n\n\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|SENDER_ID|Sender identifier|\n|ID|Reader identifier|\n|TABLE_ID|Target table identifier|\n|TABLE_TYPE|Target table type|\n|BEGIN_RID|Target record start RID|\n|END_RID|Target record end RID|\n\n\n## V$REPL_WRITER\n\nDisplays Writer information when running Replication.\n\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|ID|Writer identifier|\n|STATUS|Writer operational status|\n|APPEND_COUNT|APPEND count|\n\n## V$REPL_WRITER_META\n\nDisplays Writer metadata when running Replication.\n\n|Column Name|Description|\n|--|--|\n|HOSTNAME|Replication Node Hostname|\n|ID|Writer identifier|\n|TABLE_ID|Target table identifier|\n|TABLE_TYPE|Target table type|\n|BEGIN_RID|Target record start RID|\n|END_RID|Target record end RID|\n\n\n# Others\n\n## V$TABLES\n\nDisplays all Virtual Tables that start with \"V$\".\n\n|Column Name|Description|\n|--|--|\n|NAME|Table name|\n|TYPE|Table type|\n|DATABASE_ID|Database identifier|\n|ID|Table identifier|\n|USER ID|User who created table|\n|COLCOUNT|Column count|\n\n\n## V$COLUMNS\n\nDisplays column information of Virtual Tables.\n\n|Column Name|Description|\n|--|--|\n|NAME|Column name|\n|TYPE|Column data type|\n|DATABASE_ID|Database identifier|\n|ID|Column identifier|\n|LENGTH|Column size|\n|TABLE_ID|Table identifier|\n|FLAG|Private data|\n|PART_PAGE_COUNT|Unused|\n|PAGE_VALUE_COUNT|Unused|\n|MINMAX_CACHE_SIZE|Unused|\n|MAX_CACHE_PART_COUNT|Unused|\n\n## V$RETENTION_JOB\n\nDisplays table information to which RETENTION POLICY is applied.\n\n|Column Name|Description|\n|-------------------|------------------------------------------|\n| USER_NAME         | User name                              |\n| TABLE_NAME        | applied table name                      |\n| POLICY_NAME       | applied policy name                |\n| STATE             | RETENTION state (RUNNING/WAITING/STOPPED) |\n| LAST_DELETED_TIME | most recently deleted time                 |"
					}
					
				
		
				
					,
					
					"feature-tables-volatile-html": {
						"id": "feature-tables-volatile-html",
						"title": "Volatile Table",
						"version": "all",
						"categories": "",
						"url": " /feature-tables/volatile.html",
						"content": "# Concept\n\nThe volatile table is a temporary table in which all the data resides in the temporary memory space and enriches the data through joining with the log table.\n\nThe volatile table is a supplementary information table that stores various information of a specific device or equipment represented by a simple symbol or a number in a log table. It can be input and updated at high speed, and is used in a case where the present state of the data (which does not match the time series data) needs to be maintained in real time.\n\nThe characteristics of this table are as follows\n\n\n# Preserving Schema\nThe structure (schema) information of the volatile table is maintained even if the Machbase server is shut down and then restarted. To drop the table, you need to explicitly execute the DROP table.\n\n\n# Data Volatility\n\nThe data in the volatile table disappears as soon as the server is shut down. Therefore, when the server is started, the contents of the volatile table must be INSERTed again.\n\n\n# Index Providing and Join Function\n\nIt provides a RED-BLACK index, which is a real-time optimized index for fast data access of volatile tables. Therefore, it can be efficiently used in Join and searching process with log tables.\n\n* You can specify a primary key in the table column.\n* When inserting data having a duplicate primary key value, it is possible to UPDATE the value of existing data.\n* The condition clause ( WHERE clause) can be used to delete data that matches the primary key value condition.\n* The _ARRIVAL_TIME column does not exist.\n\n\n# Primary Key\n\nThe primary key can be created to form a unique constraint on a table column value and to specify a key column to distinguish table data.\n\nWhen inserting data into a volatile table with a primary key specified, the primary key column value of the insert data must be different from the other primary key column values ​​in the table. This constraint is called a unique constraint.\n\nThe creation constraints of the primary key are as follows.\n\nPrimary keys can only be created in volatile tables.\n\nYou can specify only one primary key column, and you can not specify more than one column as primary keys.\n\n\n# Update Function\n\nUnlike other table types, volatile tables provide a limited update function.\n\nIf the primary key value of the data to be inserted overlaps with one of the primary key values of the other data, the mode is changed to the 'update' mode instead of 'insertion', and another column value of the existing data having the duplicated key value is inserted, it is changed to the column value of the data to be processed. The update function can be used only in the volatile table with the primary key specified. If the primary key value is not specified during the insertion, the update function can not be used.\n\n\n# Delete Function\n\nVolatile tables provide the ability to delete specific data using primary key values.\n\nIf you add a condition clause (WHERE clause) in the DELETE clause to specify a primary key value, you can delete it only if there is data corresponding to that primary key value. The delete function can be used only on the volatile table with the primary key specified, and the condition (Primary Key Column) = (Value) that can be entered in the condition clause is limited.\n\n\n* [Creating and Managing Volatile Table](/en/feature-tables/volatile/create-manage.html)\n* [Volatile Data Extraction](/en/feature-tables/volatile/extract.html)\n* [Inserting and Updating Volatile Data](/en/feature-tables/volatile/insert-update.html)\n* [Deleting Volatile Data](/en/feature-tables/volatile/delete.html)\n* [Creating and Managing Volatile Index](/en/feature-tables/volatile/)\n* [Volatile Table Utilization Example](/en/feature-tables/volatile/ex.html)"
					}
					
				
		
				
					,
					
					"install-windows-windows-env-html": {
						"id": "install-windows-windows-env-html",
						"title": "Preparing Windows Environment for Installation",
						"version": "all",
						"categories": "",
						"url": " /install/windows/windows-env.html",
						"content": "# Open Firewall Port\n\nIf you install Machbase in Windows, you must open the port that Machbase uses in the Windows Firewall.\n\nIn general, Machbase uses  two ports: **5656 and 5001**\n\n1. To register the port on the  firewall , select Control Panel - Windows Firewall or Windows Defender Firewall. \n    On the Run screen, click the \"Advanced Settings\" menu.\n\n![winenv1](/en/install/windows/winenv1.png)\n\n2. Under Advanced Settings, select **Inbound Rules - New Rule** and click.\n\n![winenv2](/en/install/windows/winenv2.png)\n\n![winenv3](/en/install/windows/winenv3.png)\n\n3. When the New Rule Setup Wizard window is displayed, select the Port option as shown below and click Next.\n\n![winenv4](/en/install/windows/winenv4.png)\n\n4. Select the **TCP(T)** option , enter **5656,5001** in the **Specific Local Ports** field, and then click Next.\n\n![winenv5](/en/install/windows/winenv5.png)\n\n5. Select the **Allow The Connection** option and click **Next**.\n\n![winenv6](/en/install/windows/winenv6.png)\n\n6. Check **Domain** , **Private** , and **Public** and click **Next**.\n\n![winenv7](/en/install/windows/winenv7.png)\n\n7. Complete the **Name** and **Description** fields, and then click **Finish**.\n\n![winenv8](/en/install/windows/winenv8.png)"
					}
					
				
		
				
					,
					
					"install-windows-html": {
						"id": "install-windows-html",
						"title": "Windows Installation",
						"version": "all",
						"categories": "",
						"url": " /install/windows.html",
						"content": "* [Preparing Windows Environment for Installation](/en/install/windows/windows-env.html)\n* [MSI Installation](/en/install/windows/msi-install.html)"
					}
					
				
		
				
					,
					
					"assets-css-style-css": {
						"id": "assets-css-style-css",
						"title": "",
						"version": "all",
						"categories": "",
						"url": " /assets/css/style.css",
						"content": "@import \"jekyll-theme-primer\";"
					}
					
				
		
	};
</script>
<script src="/en/assets/js/lunr.min.js"></script>
<script src="/en/assets/js/search.js"></script>
<script
  src="/en/assets/js/jquery-3.3.1/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8= sha256-T+aPohYXbm0fRYDpJLr+zJ9RmYTswGsahAoIsNiMld4="
  crossorigin="anonymous"></script>

<script>
$(document).ready(function() {

    var toc = $('#TOC');

    // Select each header
    sections = $('.td-content h1');
        $.each(sections, function(idx, v) {
            section = $(v);
            var div_id = $(section).attr('id');
            var div_text = section.text().split('¶')[0];
            var parent = $("#" + div_id)
            var content = '<li id="link_' + div_id + '" class="md-nav__item"><a class="md-nav__link" href="#' + div_id + '" title="' + div_text +'">' + div_text +'</a></li>';
            $(toc).append(content);

            // Add section code to subnavigation
            var children = $('<nav class="md-nav"><ul class="md-nav__list"></nav></ul>')
            var contenders = $("#" + div_id).nextUntil("h1");
            $.each(contenders, function(idx, contender){
               if($(contender).is('h2') || $(contender).is('h3')) {
                   var contender_id = $(contender).attr('id');
                   var contender_text = $(contender).text().split('¶')[0];
                   var content = '<li class="md-nav__item"><a class="md-nav__link" href="#' + contender_id + '" title="' + contender_text +'">' + contender_text +'</a></li>';
                   children.append(content);
                }
             })
             $("#link_" + div_id).append(children);
        });
    });
</script>

<script>
var headers = ["h1", "h2", "h3", "h4"]
var colors = ["red", "orange", "green", "blue"]

$.each(headers, function(i, header){
    var color = colors[i];
    $(header).each(function () {
        var href=$(this).attr("id");
        $(this).append('<a class="headerlink" style="color:' + color + '" href="#' + href + '" title="Permanent link">¶</a>')
    });
})
</script>



	
              
              <!-- <style>
  .feedback--answer {
    display: inline-block;
  }
  .feedback--answer-no {
    margin-left: 1em;
  }
  .feedback--response {
    display: none;
    margin-top: 1em;
  }
  .feedback--response__visible {
    display: block;
  }
</style>
<h5 class="feedback--title">Feedback</h5>
<p class="feedback--question">Was this page helpful?</p>
<button class="feedback--answer feedback--answer-yes">Yes</button>
<button class="feedback--answer feedback--answer-no">No</button>
<p class="feedback--response feedback--response-yes">
  Glad to hear it! Please <a href="https://github.com/machbase/dbms-manual/issues/new">tell us how we can improve</a>.
</p>
<p class="feedback--response feedback--response-no">
  Sorry to hear that. Please <a href="https://github.com/machbase/dbms-manual/issues/new">tell us how we can improve</a>.
</p>
<script>
  const yesButton = document.querySelector('.feedback--answer-yes');
  const noButton = document.querySelector('.feedback--answer-no');
  const yesResponse = document.querySelector('.feedback--response-yes');
  const noResponse = document.querySelector('.feedback--response-no');
  const disableButtons = () => {
    yesButton.disabled = true;
    noButton.disabled = true;
  };
  const sendFeedback = (value) => {
    if (typeof ga !== 'function') return;
    const args = {
      command: 'send',
      hitType: 'event',
      category: 'Helpful',
      action: 'click',
      label: window.location.pathname,
      value: value
    };
    ga(args.command, args.hitType, args.category, args.action, args.label, args.value);
  };
  yesButton.addEventListener('click', () => {
    yesResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(1);
  });
  noButton.addEventListener('click', () => {
    noResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(0);
  });
</script><br/>

 -->
           </div>
          </main>
        </div>
      </div>
      <footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        <ul class="list-inline mb-0">  
          
            <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="Twitter" data-original-title="Twitter">
              <a class="text-white" target="_blank" href="https://twitter.com/machbase">
                <i class="fab fa-twitter"></i>
              </a>
            </li>
          
          
        </ul>
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        <ul class="list-inline mb-0">  
          <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="GitHub" data-original-title="GitHub">
            <a class="text-white" target="_blank" href="https://github.com/machbase/dbms-manual">
              <i class="fab fa-github"></i>
            </a>
          </li>
        </ul>
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">© 2023  All Rights Reserved</small>
        
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<script src="/en/assets/js/main.js"></script>

  </body>
</html>